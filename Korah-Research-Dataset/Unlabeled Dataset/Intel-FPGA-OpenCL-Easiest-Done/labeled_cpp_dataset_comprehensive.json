[
    {
        "label": "acl_thread.cpp",
        "data": "// Copyright (C) 2015-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// External library headers.\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_hal.h>\n#include <acl_thread.h>\n\nACL_TLS int acl_global_lock_count = 0;\nACL_TLS int acl_inside_sig_flag = 0;\nACL_TLS int acl_inside_sig_old_lock_count = 0;\nacl_mutex_wrapper_t acl_mutex_wrapper;\n\nstatic struct acl_condvar_s l_acl_global_condvar;\n\n// l_init_once() is defined in an OS-specific section below\nstatic void l_init_once();\n\nvoid acl_mutex_wrapper_t::lock() {\n  l_init_once();\n  if (acl_global_lock_count == 0) {\n    acl_acquire_condvar(&l_acl_global_condvar);\n  }\n  acl_global_lock_count++;\n}\n\nvoid acl_mutex_wrapper_t::unlock() {\n  acl_assert_locked();\n  acl_global_lock_count--;\n  if (acl_global_lock_count == 0) {\n    acl_release_condvar(&l_acl_global_condvar);\n  }\n}\n\nint acl_mutex_wrapper_t::suspend_lock() {\n  int old_lock_count = acl_global_lock_count;\n  acl_global_lock_count = 0;\n  if (old_lock_count > 0)\n    acl_release_condvar(&l_acl_global_condvar);\n  return old_lock_count;\n}\n\nvoid acl_mutex_wrapper_t::resume_lock(int lock_count) {\n  acl_assert_unlocked();\n  if (lock_count > 0)\n    acl_acquire_condvar(&l_acl_global_condvar);\n  acl_global_lock_count = lock_count;\n}\n\nvoid acl_wait_for_device_update(cl_context context) {\n  acl_assert_locked();\n  if (acl_get_hal()->get_debug_verbosity &&\n      acl_get_hal()->get_debug_verbosity() > 0) {\n    unsigned timeout = 5; // Seconds\n    // Keep waiting until signal is received\n    while (acl_timed_wait_condvar(&l_acl_global_condvar, timeout))\n      acl_context_print_hung_device_status(context);\n  } else {\n    acl_wait_condvar(&l_acl_global_condvar);\n  }\n}\n\nvoid acl_signal_device_update() { acl_signal_condvar(&l_acl_global_condvar); }\n\n#ifdef __linux__\n\n#include <linux/unistd.h>\n#include <signal.h>\n#include <sys/syscall.h>\n#include <sys/types.h>\n#include <time.h>\n#include <unistd.h>\n\nACL_TLS sigset_t acl_sigset; // Keeps the previous set of masked signals, when a\n                             // call to acl_sig_block_signals is made\n\n#if !defined(__GLIBC_PREREQ) || !__GLIBC_PREREQ(2, 30)\n// From gettid(2): The gettid() system call first appeared on Linux in\n// kernel 2.4.11. Library support was added in glibc 2.30. (Earlier\n// glibc versions did not provide a wrapper for this system call,\n// necessitating the use of syscall(2).)\nstatic inline pid_t gettid(void) { return (pid_t)syscall(__NR_gettid); }\n#endif\n\nint acl_get_thread_id() { return (int)gettid(); }\n\nint acl_get_pid() { return (int)getpid(); }\n\nstatic void l_init_once() {\n  // on windows this function does something\n}\n\n__attribute__((constructor)) static void l_global_lock_init() {\n  acl_init_condvar(&l_acl_global_condvar);\n  acl_mutex_wrapper = acl_mutex_wrapper_t();\n}\n\n__attribute__((destructor)) static void l_global_lock_uninit() {\n  acl_reset_condvar(&l_acl_global_condvar);\n}\n\n#else // !LINUX\n\n#include <windows.h>\n\nint acl_get_thread_id() { return (int)GetCurrentThreadId(); }\n\nint acl_get_pid() { return (int)GetCurrentProcessId(); }\n\nINIT_ONCE l_init_once_state = INIT_ONCE_STATIC_INIT;\n\nstatic BOOL CALLBACK l_init_once_callback(PINIT_ONCE InitOnce, PVOID Parameter,\n                                          PVOID *Context) {\n  // avoid compile errors caused by not using these arguments\n  (void)(InitOnce);\n  (void)(Parameter);\n  (void)(Context);\n\n  acl_init_condvar(&l_acl_global_condvar);\n  acl_mutex_wrapper = acl_mutex_wrapper_t();\n  return TRUE;\n}\n\nstatic void l_init_once() {\n  BOOL ret =\n      InitOnceExecuteOnce(&l_init_once_state, l_init_once_callback, NULL, NULL);\n  assert(ret);\n}\n\n#endif // !LINUX\n\n// Blocking/Unblocking signals (Actual implementation only for Linux)\nacl_signal_blocker::acl_signal_blocker() {\n#ifdef __linux__\n  sigset_t mask;\n  if (sigfillset(&mask)) {\n    assert(0 && \"Error in creating signal mask in status handler\");\n  }\n  if (pthread_sigmask(SIG_BLOCK, &mask, &acl_sigset)) {\n    assert(0 && \"Error in blocking signals in status handler\");\n  }\n#endif\n}\n\nacl_signal_blocker::~acl_signal_blocker() {\n#ifdef __linux__\n  if (pthread_sigmask(SIG_SETMASK, &acl_sigset, NULL)) {\n    assert(0 && \"Error in unblocking signals in status handler\");\n  }\n#endif\n}\n\n// Current thread releases mutex lock and sleeps briefly to allow other threads\n// a chance to execute. This function is useful for multithreaded hosts with\n// e.g. polling BSPs (using yield) to prevent one thread from hogging the mutex\n// while waiting for something like clFinish.\nvoid acl_yield_lock_and_thread() {\n  acl_suspend_lock_guard lock{acl_mutex_wrapper};\n#ifdef __arm__\n  // arm-linux-gnueabihf-g++ version used is 4.7.1.\n  // std::this_thread::yield can be enabled for it by defining\n  // _GLIBCXX_USE_SCHED_YIELD, but there are no known SoC BSPs that uses MMD\n  // yield. Leaving it to continue using nanosleep.\n  struct timespec ts = {0, 1}; // sleep for 1ns and 0s\n  nanosleep(&ts, NULL);\n#else\n  std::this_thread::yield();\n#endif\n}\n"
    },
    {
        "label": "acl_hal_mmd.cpp",
        "data": "// Copyright (C) 2013-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n//////////////////////////////////////////////////////////////////////////\n//                                                                      //\n// HAL for mmd - Generic memory mapped device                           //\n//                                                                      //\n//////////////////////////////////////////////////////////////////////////\n\n// System headers.\n#include <cassert>\n#include <cfloat>\n#include <cstdarg>\n#include <cstdio>\n#include <cstdlib>\n#include <cstring>\n#include <fstream>\n#include <iostream>\n#include <string>\n#include <vector>\n#ifdef __linux__\n#include <dirent.h>\n#include <dlfcn.h>\n#include <link.h>\n#endif\n\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4668)\n#include <windows.h>\n#pragma warning(pop)\n#else\n#define __USE_POSIX199309 1\n#include <time.h>\n#endif\n\n// External library headers.\n#include <acl_threadsupport/acl_threadsupport.h>\n#include <pkg_editor/pkg_editor.h>\n\n// Internal headers.\n#include <acl_auto.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_hal_mmd.h>\n#include <acl_kernel_if.h>\n#include <acl_platform.h>\n#include <acl_pll.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_types.h>\n#include <acl_util.h>\n#include <acl_version.h>\n\n// MMD headers.\n#include <MMD/aocl_mmd.h>\n#include <MMD/aocl_mmd_deprecated.h>\n\n// **************************************************************************\n// ******************* User-callable parts of the HAL ***********************\n// **************************************************************************\n\nvoid *acl_hal_mmd_shared_alloc(cl_device_id device, size_t size,\n                               size_t alignment, mem_properties_t *properties,\n                               int *error);\nvoid *acl_hal_mmd_host_alloc(const std::vector<cl_device_id> &devices,\n                             size_t size, size_t alignment,\n                             mem_properties_t *properties, int *error);\nint acl_hal_mmd_free(cl_context context, void *mem);\nint acl_hal_mmd_try_devices(cl_uint num_devices, const cl_device_id *devices,\n                            cl_platform_id platform);\nint acl_hal_mmd_close_devices(cl_uint num_devices, const cl_device_id *devices);\nvoid acl_hal_mmd_init_device(const acl_system_def_t *sysdef);\nvoid acl_hal_mmd_yield(cl_uint num_devices, const cl_device_id *devices);\ncl_ulong acl_hal_mmd_get_timestamp(void);\nvoid acl_hal_mmd_copy_hostmem_to_hostmem(cl_event event, const void *src,\n                                         void *dest, size_t size);\nvoid acl_hal_mmd_copy_hostmem_to_globalmem(cl_event event, const void *src,\n                                           void *dest, size_t size);\nvoid acl_hal_mmd_copy_globalmem_to_hostmem(cl_event event, const void *src,\n                                           void *dest, size_t size);\nvoid acl_hal_mmd_copy_globalmem_to_globalmem(cl_event event, const void *src,\n                                             void *dest, size_t size);\nvoid acl_hal_mmd_register_callbacks(\n    acl_event_update_callback event_update,\n    acl_kernel_update_callback kernel_update,\n    acl_profile_callback profile_update,\n    acl_device_update_callback device_update,\n    acl_process_printf_buffer_callback process_printf);\nvoid acl_hal_mmd_launch_kernel(unsigned int physical_device_id,\n                               acl_kernel_invocation_wrapper_t *wrapper);\nvoid acl_hal_mmd_unstall_kernel(unsigned int physical_device_id,\n                                int activation_id);\nint acl_hal_mmd_program_device(unsigned int physical_device_id,\n                               const acl_device_def_t *devdef,\n                               const struct acl_pkg_file *binary,\n                               int acl_program_mode);\ncl_bool acl_hal_mmd_query_temperature(unsigned int physical_device_id,\n                                      cl_int *temp);\nint acl_hal_mmd_get_device_official_name(unsigned int physical_device_id,\n                                         char *name, size_t size);\nint acl_hal_mmd_get_device_vendor_name(unsigned int physical_device_id,\n                                       char *name, size_t size);\nvoid acl_hal_mmd_kernel_interrupt(int handle, void *user_data);\nvoid acl_hal_mmd_device_interrupt(int handle, aocl_mmd_interrupt_info *data_in,\n                                  void *user_data);\nvoid acl_hal_mmd_status_handler(int handle, void *user_data, aocl_mmd_op_t op,\n                                int status);\nvoid *acl_hal_mmd_legacy_shared_alloc(cl_context context, size_t size,\n                                      unsigned long long *device_ptr_out);\nvoid acl_hal_mmd_legacy_shared_free(cl_context context, void *host_ptr,\n                                    size_t size);\nint acl_hal_mmd_pll_reconfigure(unsigned int physical_device_id,\n                                const char *pll_settings_str);\nvoid acl_hal_mmd_reset_kernels(cl_device_id device);\nvoid acl_hal_mmd_get_device_status(cl_uint num_devices,\n                                   const cl_device_id *devices);\nint acl_hal_mmd_get_debug_verbosity();\nint acl_hal_mmd_hostchannel_create(unsigned int physical_device_id,\n                                   char *channel_name, size_t num_packets,\n                                   size_t packet_size, int direction);\nint acl_hal_mmd_hostchannel_destroy(unsigned int physical_device_id,\n                                    int channel_handle);\nsize_t acl_hal_mmd_hostchannel_pull(unsigned int physical_device_id,\n                                    int channel_handle, void *host_buffer,\n                                    size_t read_size, int *status);\nsize_t acl_hal_mmd_hostchannel_push(unsigned int physical_device_id,\n                                    int channel_handle, const void *host_buffer,\n                                    size_t write_size, int *status);\nsize_t acl_hal_mmd_hostchannel_pull_no_ack(unsigned int physical_device_id,\n                                           int channel_handle,\n                                           void *host_buffer, size_t read_size,\n                                           int *status);\nsize_t acl_hal_mmd_hostchannel_push_no_ack(unsigned int physical_device_id,\n                                           int channel_handle,\n                                           const void *host_buffer,\n                                           size_t write_size, int *status);\nvoid *acl_hal_mmd_hostchannel_get_buffer(unsigned int physical_device_id,\n                                         int channel_handle,\n                                         size_t *buffer_size, int *status);\nsize_t acl_hal_mmd_hostchannel_ack_buffer(unsigned int physical_device_id,\n                                          int channel_handle, size_t ack_size,\n                                          int *status);\n\n// Following six calls provide access to the kernel profiling hardware\n// accel_id is the same as in the invocation image passed to\n// acl_hal_mmd_launch_kernel()\nint acl_hal_mmd_get_profile_data(unsigned int physical_device_id,\n                                 unsigned int accel_id, uint64_t *data,\n                                 unsigned int length);\nint acl_hal_mmd_reset_profile_counters(unsigned int physical_device_id,\n                                       unsigned int accel_id);\nint acl_hal_mmd_disable_profile_counters(unsigned int physical_device_id,\n                                         unsigned int accel_id);\nint acl_hal_mmd_enable_profile_counters(unsigned int physical_device_id,\n                                        unsigned int accel_id);\nint acl_hal_mmd_set_profile_shared_control(unsigned int physical_device_id,\n                                           unsigned int accel_id);\nint acl_hal_mmd_set_profile_start_count(unsigned int physical_device_id,\n                                        unsigned int accel_id, uint64_t value);\nint acl_hal_mmd_set_profile_stop_count(unsigned int physical_device_id,\n                                       unsigned int accel_id, uint64_t value);\n\nvoid acl_hal_mmd_simulation_streaming_kernel_start(\n    unsigned int physical_device_id, const std::string &kernel_name,\n    const int accel_id, const bool accel_has_agent_args);\nvoid acl_hal_mmd_simulation_streaming_kernel_done(\n    unsigned int physical_device_id, const std::string &kernel_name,\n    unsigned int &finish_counter);\n\nvoid acl_hal_mmd_simulation_set_kernel_cra_address_map(\n    unsigned int physical_device_id,\n    const std::vector<uintptr_t> &kernel_csr_address_map);\n\nsize_t acl_hal_mmd_read_csr(unsigned int physical_device_id, uintptr_t offset,\n                            void *ptr, size_t size);\n\nsize_t acl_hal_mmd_write_csr(unsigned int physical_device_id, uintptr_t offset,\n                             const void *ptr, size_t size);\n\nint acl_hal_mmd_simulation_device_global_interface_read(\n    unsigned int physical_device_id, const char *interface_name,\n    void *host_addr, size_t dev_addr, size_t size);\nint acl_hal_mmd_simulation_device_global_interface_write(\n    unsigned int physical_device_id, const char *interface_name,\n    const void *host_addr, size_t dev_addr, size_t size);\n\nsize_t acl_hal_mmd_hostchannel_sideband_pull_no_ack(\n    unsigned int physical_device_id, unsigned int port_name, int channel_handle,\n    void *host_buffer, size_t read_size, int *status);\n\nsize_t acl_hal_mmd_hostchannel_sideband_push_no_ack(\n    unsigned int physical_device_id, unsigned int port_name, int channel_handle,\n    const void *host_buffer, size_t write_size, int *status);\n\nstatic size_t acl_kernel_if_read(acl_bsp_io *io, dev_addr_t src, char *dest,\n                                 size_t size);\nstatic size_t acl_kernel_if_write(acl_bsp_io *io, dev_addr_t dest,\n                                  const char *src, size_t size);\nstatic size_t acl_pll_read(acl_bsp_io *io, dev_addr_t src, char *dest,\n                           size_t size);\nstatic size_t acl_pll_write(acl_bsp_io *io, dev_addr_t dest, const char *src,\n                            size_t size);\nstatic time_ns acl_bsp_get_timestamp(void);\n\nint acl_hal_mmd_has_svm_support(unsigned int physical_device_id, int *value);\nint acl_hal_mmd_has_physical_mem(unsigned int physical_device_id);\nint acl_hal_mmd_support_buffer_location(\n    const std::vector<cl_device_id> &devices);\n\nstatic void *\nacl_hal_get_board_extension_function_address(const char *func_name,\n                                             unsigned int physical_device_id);\n\nACL_HAL_EXPORT const acl_hal_t *\nacl_mmd_get_system_definition(acl_system_def_t *sys,\n                              acl_mmd_library_names_t *_libraries_to_load);\n\n// **************************************************************************\n// ************************ Constants and Globals ***************************\n// **************************************************************************\n\n#ifdef _WIN32\nstatic cl_ulong m_ticks_per_second = 0;\n#endif\n\ntypedef struct my_dl_wrapper {\n  void *handle;\n  my_dl_wrapper(void *handle) { this->handle = handle; }\n  ~my_dl_wrapper() {\n#ifdef _WIN32\n    FreeLibrary((HMODULE)this->handle);\n#else\n    dlclose(this->handle);\n#endif\n  }\n  // prohibit copying to avoid double-close of handle\n  my_dl_wrapper(const my_dl_wrapper &) = delete;\n  my_dl_wrapper &operator=(const my_dl_wrapper &) = delete;\n} my_dl_wrapper;\nstd::vector<std::unique_ptr<my_dl_wrapper>> mmd_libs;\nstd::vector<acl_mmd_dispatch_t> internal_mmd_dispatch;\n\nstatic size_t num_board_pkgs;\nstatic unsigned num_physical_devices = 0;\nstatic double min_MMD_version = DBL_MAX;\n\nstatic acl_bsp_io bsp_io_kern[ACL_MAX_DEVICE];\nstatic acl_bsp_io bsp_io_pll[ACL_MAX_DEVICE];\nstatic acl_kernel_if kern[ACL_MAX_DEVICE];\nstatic acl_pll pll[ACL_MAX_DEVICE];\nstatic acl_mmd_device_t device_info[ACL_MAX_DEVICE];\n\nstatic int uses_yield_ref = -1;\n\n// The PCIe HAL structure\nstatic acl_event_update_callback acl_event_update_fn = NULL;\nacl_kernel_update_callback acl_kernel_update_fn = NULL;\nacl_profile_callback acl_profile_fn = NULL;\nacl_device_update_callback acl_device_update_fn = NULL;\n\n// This will contain the device physical id to tell us which device across all\n// loaded BSPs (even with the same handle numbers) is calling the interrupt\n// handler.\nunsigned interrupt_user_data[ACL_MAX_DEVICE];\n\n// Device reprogram constants\nconst static size_t MIN_SOF_SIZE = 1;\nconst static size_t MIN_PLL_CONFIG_SIZE = 1;\n\nstatic acl_hal_t acl_hal_mmd = {\n    acl_hal_mmd_init_device,   // init_device\n    NULL,                      // yield: Populated based on MMD property\n    acl_hal_mmd_get_timestamp, // get_timestamp\n    acl_hal_mmd_copy_hostmem_to_hostmem,     // copy_hostmem_to_hostmem\n    acl_hal_mmd_copy_hostmem_to_globalmem,   // copy_hostmem_to_globalmem\n    acl_hal_mmd_copy_globalmem_to_hostmem,   // copy_globalmem_to_hostmem\n    acl_hal_mmd_copy_globalmem_to_globalmem, // copy_globalmem_to_globalmem\n    acl_hal_mmd_register_callbacks,          // register_callbacks\n    acl_hal_mmd_launch_kernel,               // launch_kernel\n    acl_hal_mmd_unstall_kernel,              // unstall_kernel\n    acl_hal_mmd_program_device,              // program_device\n    acl_hal_mmd_query_temperature,           // query_temperature\n    acl_hal_mmd_get_device_official_name,    // get_device_official_name\n    acl_hal_mmd_get_device_vendor_name,      // get_device_vendor_name\n    acl_hal_mmd_legacy_shared_alloc,         // legacy_shared_alloc\n    acl_hal_mmd_legacy_shared_free,          // legacy_shared_free\n    acl_hal_mmd_get_profile_data,            // get_profile_data\n    acl_hal_mmd_reset_profile_counters,      // reset_profile_counters\n    acl_hal_mmd_disable_profile_counters,    // disable_profile_counters\n    acl_hal_mmd_enable_profile_counters,     // enable_profile_counters\n    acl_hal_mmd_set_profile_shared_control,  // set_profile_shared_control\n    acl_hal_mmd_set_profile_start_count,     // set_profile_start_cycle\n    acl_hal_mmd_set_profile_stop_count,      // set_profile_stop_cycle\n    acl_hal_mmd_has_svm_support,             // has_svm_memory_support\n    acl_hal_mmd_has_physical_mem,            // has_physical_mem\n    acl_hal_mmd_support_buffer_location,     // support_buffer_location\n    acl_hal_get_board_extension_function_address, // get_board_extension_function_address\n    acl_hal_mmd_pll_reconfigure,                  // pll_reconfigure\n    acl_hal_mmd_reset_kernels,                    // reset_kernels\n    acl_hal_mmd_hostchannel_create,               // hostchannel_create\n    acl_hal_mmd_hostchannel_destroy,              // hostchannel_destroy\n    acl_hal_mmd_hostchannel_pull,                 // hostchannel_pull\n    acl_hal_mmd_hostchannel_push,                 // hostchannel_push\n    acl_hal_mmd_hostchannel_get_buffer,           // hostchannel_get_buffer\n    acl_hal_mmd_hostchannel_ack_buffer,           // hostchannel_ack_buffer\n    acl_hal_mmd_get_device_status,                // get_device_status\n    acl_hal_mmd_get_debug_verbosity,              // get_debug_verbosity\n    acl_hal_mmd_try_devices,                      // try_devices\n    acl_hal_mmd_close_devices,                    // close_devices\n    acl_hal_mmd_host_alloc,                       // host_alloc\n    acl_hal_mmd_free,                             // free\n    acl_hal_mmd_shared_alloc,                     // shared_alloc\n    acl_hal_mmd_simulation_streaming_kernel_start, // simulation_streaming_kernel_start\n    acl_hal_mmd_simulation_streaming_kernel_done, // simulation_streaming_kernel_done\n    acl_hal_mmd_simulation_set_kernel_cra_address_map, // simulation_set_kernel_cra_address_map\n    acl_hal_mmd_read_csr,                              // read_csr\n    acl_hal_mmd_write_csr,                             // write_csr\n    acl_hal_mmd_simulation_device_global_interface_read, // simulation_device_global_interface_read\n    acl_hal_mmd_simulation_device_global_interface_write, // simulation_device_global_interface_write\n    acl_hal_mmd_hostchannel_sideband_pull_no_ack, // hostchannel_sideband_pull_no_ack\n    acl_hal_mmd_hostchannel_sideband_push_no_ack, // hostchannel_sideband_push_no_ack\n    acl_hal_mmd_hostchannel_pull_no_ack,          // hostchannel_pull_no_ack\n    acl_hal_mmd_hostchannel_push_no_ack,          // hostchannel_push_no_ack\n};\n\n// **************************************************************************\n// ************************** Compiler Directives ***************************\n// **************************************************************************\n\n#ifndef MAX_NAME_SIZE\n#define MAX_NAME_SIZE 1204\n#endif\n#define MAX_BOARD_NAMES_LEN (ACL_MAX_DEVICE * 30 + 1)\n\n// Dynamically load board mmd & symbols\nextern \"C\" {\nvoid *null_fn = NULL;\n}\n#define IS_VALID_FUNCTION(X) (&X == NULL) ? 0 : 1\n#ifdef _MSC_VER\n#pragma comment(linker,                                                        \\\n                \"/alternatename:__imp_aocl_mmd_get_offline_info=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_get_info=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_open=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_close=null_fn\")\n#pragma comment(linker,                                                        \\\n                \"/alternatename:__imp_aocl_mmd_set_interrupt_handler=null_fn\")\n#pragma comment(                                                               \\\n    linker,                                                                    \\\n    \"/alternatename:__imp_aocl_mmd_set_device_interrupt_handler=null_fn\")\n#pragma comment(linker,                                                        \\\n                \"/alternatename:__imp_aocl_mmd_set_status_handler=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_yield=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_read=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_write=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_copy=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_reprogram=null_fn\")\n#pragma comment(linker,                                                        \\\n                \"/alternatename:__imp_aocl_mmd_shared_mem_alloc=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_shared_mem_free=null_fn\")\n#pragma comment(linker,                                                        \\\n                \"/alternatename:__imp_aocl_mmd_hostchannel_create=null_fn\")\n#pragma comment(linker,                                                        \\\n                \"/alternatename:__imp_aocl_mmd_hostchannel_destroy=null_fn\")\n#pragma comment(                                                               \\\n    linker, \"/alternatename:__imp_aocl_mmd_hostchannel_get_buffer=null_fn\")\n#pragma comment(                                                               \\\n    linker, \"/alternatename:__imp_aocl_mmd_hostchannel_ack_buffer=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_program=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_host_alloc=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_free=null_fn\")\n#pragma comment(linker, \"/alternatename:__imp_aocl_mmd_shared_alloc=null_fn\")\n#endif\n\n#define ADD_STATIC_FN_TO_HAL(STRUCT, X, REQUIRED)                              \\\n  if (!IS_VALID_FUNCTION(X)) {                                                 \\\n    if (REQUIRED) {                                                            \\\n      ACL_HAL_DEBUG_MSG_VERBOSE(                                               \\\n          1, \"Function X is not defined statically by board library\\n\");       \\\n      return NULL;                                                             \\\n    } else {                                                                   \\\n      STRUCT.X = NULL;                                                         \\\n    }                                                                          \\\n  }                                                                            \\\n  STRUCT.X = X;\n\n#define ADD_DYNAMIC_FN_TO_HAL(STRUCT, LIBRARY, X, REQUIRED, TYPE)              \\\n  STRUCT->X = (TYPE)my_dlsym(LIBRARY, #X, &error_msg);                         \\\n  if (!STRUCT->X && REQUIRED) {                                                \\\n    printf(\"Error: Symbol %s not found in board library\", #X);                 \\\n    if (error_msg && error_msg[0] != '\\0') {                                   \\\n      printf(\"(message: %s)\", error_msg);                                      \\\n    }                                                                          \\\n    printf(\"\\n\");                                                              \\\n    return CL_FALSE;                                                           \\\n  }\n\n#define info_assert(COND, ...)                                                 \\\n  do {                                                                         \\\n    if (!(COND)) {                                                             \\\n      printf(\"%s:%d:assert failure: \", __FILE__, __LINE__);                    \\\n      printf(__VA_ARGS__);                                                     \\\n      fflush(stdout);                                                          \\\n      assert(0);                                                               \\\n    }                                                                          \\\n  } while (0)\n\nstatic int debug_verbosity = 0;\n#define ACL_HAL_DEBUG_MSG_VERBOSE(verbosity, m, ...)                           \\\n  if (debug_verbosity >= verbosity)                                            \\\n    do {                                                                       \\\n      printf((m), ##__VA_ARGS__);                                              \\\n  } while (0)\n\n// **************************************************************************\n// ************************** Helper functions ******************************\n// **************************************************************************\n\n// Version string format should be MAJOR.MINOR(.PATCH)\n// To compare the MMD/HAL versions we will only compare the last two digits\n// of the MAJOR field together with the MINOR field, and ignore the PATCH field\n// if that exists. This function truncates the version string to exactly that\n// and convert it to double to be compared. Returns -1 if input is invalid.\ndouble l_parse_mmd_version_str(std::string version_str) {\n  size_t start_idx, length;\n\n  // Find the '.' between the MAJOR and MINOR field\n  std::string::size_type i = version_str.find('.');\n  if (i == std::string::npos || i < 2) {\n    return -1;\n  } else {\n    start_idx = i - 2;\n  }\n  // Check if there is a '.' for PATCH field\n  std::string::size_type j = version_str.find('.', i + 1);\n  length = (j == std::string::npos ? version_str.length() : j) - start_idx;\n\n  // Get the part of the MMD version string that will be used to compare\n  // for compatibility with the runtime HAL\n  std::string version_substr = version_str.substr(start_idx, length);\n  double mmd_version_num = 0;\n  try {\n    mmd_version_num = std::stod(version_substr);\n  } catch (const std::exception &) {\n    // Just return error and let the caller handle failure\n    return -1;\n  }\n\n  return mmd_version_num;\n}\n\n// MMD dynamic load helpers\n#ifdef _WIN32\nchar *acl_strtok(char *str, const char *delim, char **saveptr) {\n  return strtok_s(str, delim, saveptr);\n}\n#else // Linux\nchar *acl_strtok(char *str, const char *delim, char **saveptr) {\n  return strtok_r(str, delim, saveptr);\n}\n#endif\n\nstatic void *my_dlopen_flags(const char *library_name, int flag,\n                             char **error_msg) {\n  void *library;\n  acl_assert_locked();\n\n#ifdef _WIN32\n  // Removing Windows warning\n  flag = flag;\n  library = (void *)LoadLibraryA(library_name);\n\n  // Retrieve error string\n  DWORD err_id = GetLastError();\n  if (err_id == 0) {\n    *error_msg = \"\";\n  } else {\n    char *msg_buf = nullptr;\n    FormatMessageA(FORMAT_MESSAGE_ALLOCATE_BUFFER | FORMAT_MESSAGE_FROM_SYSTEM |\n                       FORMAT_MESSAGE_IGNORE_INSERTS,\n                   NULL, err_id, MAKELANGID(LANG_NEUTRAL, SUBLANG_DEFAULT),\n                   (LPSTR)&msg_buf, 0, NULL);\n    *error_msg = msg_buf;\n  }\n#else\n  library = dlopen(library_name, flag);\n  *error_msg = dlerror();\n#endif\n  return library;\n}\n\nstatic void *my_dlopen(const char *library_name, char **error_msg) {\n#ifdef _WIN32\n  return my_dlopen_flags(library_name, 0, error_msg);\n#else\n  return my_dlopen_flags(library_name, RTLD_NOW, error_msg);\n#endif\n}\n\nvoid *my_dlopen_global(const char *library_name, char **error_msg) {\n#ifdef _WIN32\n  return my_dlopen_flags(library_name, 0, error_msg);\n#else\n  return my_dlopen_flags(library_name, RTLD_NOW | RTLD_GLOBAL, error_msg);\n#endif\n}\n\nstatic void *my_dlsym(void *library, const char *function_name,\n                      char **error_msg) {\n  void *symbol;\n  acl_assert_locked();\n#ifdef _WIN32\n  if (!library || !function_name) {\n    *error_msg = \"library or function name is empty\";\n    return NULL;\n  }\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4152)\n#endif\n  symbol = GetProcAddress((HMODULE)library, function_name);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  *error_msg = \"\";\n#else\n  symbol = dlsym(library, function_name);\n  *error_msg = dlerror();\n#endif\n  return symbol;\n}\n\nstatic void my_dlclose(void *library) {\n  acl_assert_locked();\n#ifdef _WIN32\n  FreeLibrary((HMODULE)library);\n#else\n  dlclose(library);\n#endif\n}\n\ncl_bool l_load_board_functions(acl_mmd_dispatch_t *mmd_dispatch,\n                               const char *library_name, void *mmd_library,\n                               char *error_msg) {\n  acl_assert_locked();\n// my_dlsym returns a void pointer to be generic but the functions have a\n// specific prototype. Ignore the resulting warning.\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4152)\n#endif\n\n  mmd_dispatch->library_name = library_name;\n  mmd_dispatch->mmd_library = mmd_library;\n  ADD_DYNAMIC_FN_TO_HAL(\n      mmd_dispatch, mmd_library, aocl_mmd_get_offline_info, 1,\n      int (*)(aocl_mmd_offline_info_t, size_t, void *, size_t *));\n  ADD_DYNAMIC_FN_TO_HAL(\n      mmd_dispatch, mmd_library, aocl_mmd_get_info, 1,\n      int (*)(int, aocl_mmd_info_t, size_t, void *, size_t *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_open, 1,\n                        int (*)(const char *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_close, 1,\n                        int (*)(int));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library,\n                        aocl_mmd_set_interrupt_handler, 1,\n                        int (*)(int, aocl_mmd_interrupt_handler_fn, void *));\n  ADD_DYNAMIC_FN_TO_HAL(\n      mmd_dispatch, mmd_library, aocl_mmd_set_device_interrupt_handler, 0,\n      int (*)(int, aocl_mmd_device_interrupt_handler_fn, void *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_set_status_handler,\n                        1, int (*)(int, aocl_mmd_status_handler_fn, void *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_yield, 1,\n                        int (*)(int));\n  ADD_DYNAMIC_FN_TO_HAL(\n      mmd_dispatch, mmd_library, aocl_mmd_read, 1,\n      int (*)(int, aocl_mmd_op_t, size_t, void *, int, size_t));\n  ADD_DYNAMIC_FN_TO_HAL(\n      mmd_dispatch, mmd_library, aocl_mmd_write, 1,\n      int (*)(int, aocl_mmd_op_t, size_t, const void *, int, size_t));\n  ADD_DYNAMIC_FN_TO_HAL(\n      mmd_dispatch, mmd_library, aocl_mmd_copy, 1,\n      int (*)(int, aocl_mmd_op_t, size_t, int, size_t, size_t));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_reprogram, 0,\n                        int (*)(int, void *, size_t));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_shared_mem_alloc, 0,\n                        void *(*)(int, size_t, unsigned long long *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_shared_mem_free, 0,\n                        void (*)(int, void *, size_t));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_hostchannel_create,\n                        0, int (*)(int, char *, size_t, int));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_hostchannel_destroy,\n                        0, int (*)(int, int));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library,\n                        aocl_mmd_hostchannel_get_buffer, 0,\n                        void *(*)(int, int, size_t *, int *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library,\n                        aocl_mmd_hostchannel_ack_buffer, 0,\n                        size_t(*)(int, int, size_t, int *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_program, 0,\n                        int (*)(int, void *, size_t, aocl_mmd_program_mode_t));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_host_alloc, 0,\n                        void *(*)(int *, size_t, size_t, size_t,\n                                  aocl_mmd_mem_properties_t *, int *));\n  ADD_DYNAMIC_FN_TO_HAL(mmd_dispatch, mmd_library, aocl_mmd_free, 0,\n                        int (*)(void *));\n  ADD_DYNAMIC_FN_TO_HAL(\n      mmd_dispatch, mmd_library, aocl_mmd_shared_alloc, 0,\n      void *(*)(int, size_t, size_t, aocl_mmd_mem_properties_t *, int *));\n\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Successfully loaded board MMD %s\\n\",\n                            library_name);\n  return CL_TRUE;\n}\n\n#ifdef __linux__\nstatic bool lib_already_loaded = false;\n// Callback for dl_iterate_phdr; called for each library that is already loaded.\n// Check to see if the requested library matches any of the libraries that have\n// already been loaded.\n// Emits messages and sets the lib_already_loaded flag.\nstatic int lib_checker(struct dl_phdr_info *info, size_t size, void *data) {\n  const char *library_name = (char *)data;\n\n  // Library name starts after last occurrance of '/', if any\n  const char *loaded_lib_name = strrchr(info->dlpi_name, '/');\n  if (loaded_lib_name == NULL) {\n    loaded_lib_name = info->dlpi_name;\n  } else {\n    loaded_lib_name++;\n  }\n\n  if (strncmp(library_name, loaded_lib_name, MAX_NAME_SIZE) == 0) {\n    // This exact library was already loaded: emit error message.\n    std::cout << \"Warning: The BSP library '\" << library_name\n              << \"' was already loaded as '\" << info->dlpi_name\n              << \"'; not attempting to reload it.\\n\";\n    lib_already_loaded = true;\n  } else if (strncmp(library_name, loaded_lib_name, 6) == 0) {\n    // Somewhat arbitrary limit that allows for detection of versioned libraries\n    // with a name containing at least one unique character.\n    // e.g. libX.so and libX.so.1\n    std::cout << \"Warning: Attempting to load BSP library '\" << library_name\n              << \"' but library with similar name '\" << info->dlpi_name\n              << \"' has already been loaded.\\n\";\n  }\n\n  return 0;\n}\n#endif\n\ncl_bool l_load_single_board_library(const char *library_name,\n                                    size_t &num_boards_found,\n                                    cl_bool load_libraries) {\n  acl_assert_locked();\n\n  char *error_msg = nullptr;\n#ifdef __linux__\n  if (debug_verbosity > 0) {\n    // TODO: Add similar support for Windows?\n    // Check to see if this lib or a similarly named lib is already opened.\n    lib_already_loaded = false;\n    dl_iterate_phdr(lib_checker, (void *)library_name);\n    if (lib_already_loaded)\n      return CL_FALSE;\n  }\n#endif\n  auto *mmd_library = my_dlopen(library_name, &error_msg);\n  if (!mmd_library) {\n    std::cout << \"Error: Could not load board library \" << library_name;\n    if (error_msg && error_msg[0] != '\\0') {\n      std::cout << \" (error_msg: \" << error_msg << \")\";\n    }\n    std::cout << \"\\n\";\n    return CL_FALSE;\n  }\n\n  auto *test_symbol =\n      my_dlsym(mmd_library, \"aocl_mmd_get_offline_info\", &error_msg);\n  if (!test_symbol) {\n    // On Linux, for custom libraries close the library (which was opened\n    // locally) and then reopen globally. For Windows, there is no option (i.e.\n    // it is always global)\n#ifdef __linux__\n    my_dlclose(mmd_library);\n    ACL_HAL_DEBUG_MSG_VERBOSE(\n        1, \"This library is a custom library. Opening globally.\\n\");\n    mmd_library = my_dlopen_global(library_name, &error_msg);\n    if (!mmd_library) {\n      std::cout << \"Error: Could not load custom library \" << library_name;\n      if (error_msg && error_msg[0] != '\\0') {\n        std::cout << \" (error_msg: \" << error_msg << \")\";\n      }\n      std::cout << \"\\n\";\n      return CL_FALSE;\n    }\n#endif\n  } else {\n    if (load_libraries) {\n      auto result =\n          l_load_board_functions(&(internal_mmd_dispatch[num_boards_found]),\n                                 library_name, mmd_library, error_msg);\n      if (result == CL_FALSE) {\n        std::cout << \"Error: Could not load board library \" << library_name\n                  << \" due to failure to load symbols\\n\";\n        my_dlclose(mmd_library);\n        return result;\n      }\n    }\n    ++num_boards_found;\n  }\n\n  mmd_libs.push_back(std::make_unique<my_dl_wrapper>(mmd_library));\n  return CL_TRUE;\n}\n\n#ifdef _WIN32\ncl_bool l_load_board_libraries(cl_bool load_libraries) {\n  char library_name[1024] = {0};\n  size_t num_boards_found = 0;\n\n  // On Windows it will check the regkey. If ACL_BOARD_VENDOR_PATH is defined it\n  // will check HKCU, otherwise HKLM\n  auto *secondaryVendorPath = acl_getenv(\"ACL_BOARD_VENDOR_PATH\");\n  acl_assert_locked();\n\n  HKEY HKEY_TYPE =\n      !secondaryVendorPath ? HKEY_LOCAL_MACHINE : HKEY_CURRENT_USER;\n  std::string hkey_name = !secondaryVendorPath ? \"HKLM\" : \"HKCU\";\n\n  std::string boards_reg = \"SOFTWARE\\\\Intel\\\\OpenCL\\\\Boards\";\n  HKEY boards_key = nullptr;\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Opening key %s\\\\%s...\\n\", hkey_name.c_str(),\n                            boards_reg.c_str());\n  auto result =\n      RegOpenKeyExA(HKEY_TYPE, boards_reg.c_str(), 0, KEY_READ, &boards_key);\n  if (ERROR_SUCCESS == result) {\n    // for each value\n    for (DWORD dwIndex = 0;; ++dwIndex) {\n      DWORD dwLibraryNameSize = sizeof(library_name);\n      DWORD dwLibraryNameType = 0;\n      DWORD dwValue = 0;\n      DWORD dwValueSize = sizeof(dwValue);\n\n      // read the value name\n      ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Reading value %ld...\\n\", dwIndex);\n      result = RegEnumValueA(boards_key, dwIndex, library_name,\n                             &dwLibraryNameSize, NULL, &dwLibraryNameType,\n                             (LPBYTE)&dwValue, &dwValueSize);\n      // if RegEnumKeyEx fails, we are done with the enumeration\n      if (ERROR_SUCCESS != result) {\n        ACL_HAL_DEBUG_MSG_VERBOSE(\n            1, \"Failed to read value %ld, done reading key.\\n\", dwIndex);\n        break;\n      }\n      ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Value %s found...\\n\", library_name);\n\n      // Require that the value be a DWORD and equal zero\n      if (REG_DWORD != dwLibraryNameType) {\n        ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Value not a DWORD, skipping\\n\");\n        continue;\n      }\n      if (dwValue) {\n        ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Value not zero, skipping\\n\");\n        continue;\n      }\n\n      // add the library\n      cl_bool cl_result = l_load_single_board_library(\n          library_name, num_boards_found, load_libraries);\n      if (!cl_result) {\n        result = RegCloseKey(boards_key);\n        if (ERROR_SUCCESS != result) {\n          printf(\"Failed to close platforms key %s, ignoring\\n\",\n                 (char *)boards_key);\n        }\n        return cl_result;\n      }\n    }\n  } else {\n    if (hkey_name == \"HKCU\") {\n      ACL_HAL_DEBUG_MSG_VERBOSE(1,\n                                \"Warning: Failed to open the platforms key %s \"\n                                \"from HKCU. Will try to open it from HKLM\\n\",\n                                boards_reg.c_str());\n      ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Opening key %s\\\\%s...\\n\", \"HKLM\",\n                                boards_reg.c_str());\n      result = RegOpenKeyExA(HKEY_LOCAL_MACHINE, boards_reg.c_str(), 0,\n                             KEY_READ, &boards_key);\n      if (ERROR_SUCCESS == result) {\n        // for each value\n        for (DWORD dwIndex = 0;; ++dwIndex) {\n          DWORD dwLibraryNameSize = sizeof(library_name);\n          DWORD dwLibraryNameType = 0;\n          DWORD dwValue = 0;\n          DWORD dwValueSize = sizeof(dwValue);\n\n          // read the value name\n          ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Reading value %ld...\\n\", dwIndex);\n          result = RegEnumValueA(boards_key, dwIndex, library_name,\n                                 &dwLibraryNameSize, NULL, &dwLibraryNameType,\n                                 (LPBYTE)&dwValue, &dwValueSize);\n          // if RegEnumKeyEx fails, we are done with the enumeration\n          if (ERROR_SUCCESS != result) {\n            ACL_HAL_DEBUG_MSG_VERBOSE(\n                1, \"Failed to read value %ld, done reading key.\\n\", dwIndex);\n            break;\n          }\n          ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Value %s found...\\n\", library_name);\n\n          // Require that the value be a DWORD and equal zero\n          if (REG_DWORD != dwLibraryNameType) {\n            ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Value not a DWORD, skipping\\n\");\n            continue;\n          }\n          if (dwValue) {\n            ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Value not zero, skipping\\n\");\n            continue;\n          }\n\n          // add the library\n          auto cl_result = l_load_single_board_library(\n              library_name, num_boards_found, load_libraries);\n          if (!cl_result) {\n            result = RegCloseKey(boards_key);\n            if (ERROR_SUCCESS != result) {\n              printf(\"Failed to close platforms key %s, ignoring\\n\",\n                     (char *)boards_key);\n            }\n            return cl_result;\n          }\n        }\n      } else {\n        ACL_HAL_DEBUG_MSG_VERBOSE(\n            1,\n            \"Error: Failed to open platforms key %s to load board library at \"\n            \"runtime. Either link to the board library \",\n            boards_reg.c_str());\n        ACL_HAL_DEBUG_MSG_VERBOSE(\n            1, \"while compiling your host code or refer to your board vendor's \"\n               \"documentation on how to install the board library \");\n        ACL_HAL_DEBUG_MSG_VERBOSE(1, \"so that it can be loaded at runtime.\\n\");\n      }\n    } else {\n      ACL_HAL_DEBUG_MSG_VERBOSE(\n          1,\n          \"Error: Failed to open platforms key %s to load board library at \"\n          \"runtime. Either link to the board library \",\n          boards_reg.c_str());\n      ACL_HAL_DEBUG_MSG_VERBOSE(\n          1, \"while compiling your host code or refer to your board vendor's \"\n             \"documentation on how to install the board library \");\n      ACL_HAL_DEBUG_MSG_VERBOSE(1, \"so that it can be loaded at runtime.\\n\");\n    }\n  }\n\n  if (ERROR_SUCCESS == result) {\n    result = RegCloseKey(boards_key);\n    if (ERROR_SUCCESS != result) {\n      ACL_HAL_DEBUG_MSG_VERBOSE(1,\n                                \"Failed to close platforms key %s, ignoring\\n\",\n                                (char *)boards_key);\n    }\n  }\n\n  if (!load_libraries) {\n    num_board_pkgs = num_boards_found;\n    internal_mmd_dispatch.resize(num_board_pkgs);\n  }\n  return CL_TRUE;\n}\n#else // Linux\ncl_bool l_load_board_libraries(cl_bool load_libraries) {\n  // Keeping the old path for backward compatibility\n  std::string board_vendor_path_old = \"/opt/Intel/OpenCL_boards/\";\n  std::string board_vendor_path = \"/opt/Intel/OpenCL/Boards/\";\n  auto *customer_board_vendor_path = acl_getenv(\"ACL_BOARD_VENDOR_PATH\");\n  acl_assert_locked();\n\n  // If the customer_board_vendor_path is defined, runtime should only load the\n  // fcd file there Otherwise, runtime will load the fcd file from the default\n  // directory\n  if (customer_board_vendor_path) {\n    // append the '/' to the end of the customer_board_vendor_path\n    // and load it to board_vendor_path\n    board_vendor_path = customer_board_vendor_path + std::string(\"/\");\n  }\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Intel(R) FPGA Board Vendor Path: %s\\n\",\n                            board_vendor_path.c_str());\n\n  size_t num_boards_found = 0;\n  auto num_vendor_files_found = 0;\n  auto num_passes = 2;\n  DIR *dir = nullptr;\n  for (auto ipass = 0; ipass < num_passes; ++ipass) {\n    std::string vendor_path_to_use;\n    if (ipass == 0) {\n      dir = opendir(board_vendor_path_old.c_str());\n      if (!dir)\n        continue;\n\n      vendor_path_to_use = board_vendor_path_old;\n    } else {\n      dir = opendir(board_vendor_path.c_str());\n      if (!dir) {\n        ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Failed to open path %s\\n\",\n                                  board_vendor_path.c_str());\n        continue;\n      }\n      vendor_path_to_use = board_vendor_path;\n    }\n\n    // attempt to load all files in the directory\n    for (auto *dir_entry = readdir(dir); dir_entry; dir_entry = readdir(dir)) {\n      std::string extension = \".fcd\";\n\n      // make sure the file name ends in .fcd\n      std::string filename = dir_entry->d_name;\n      if (extension.length() > filename.length() ||\n          filename.substr(filename.length() - extension.length()) !=\n              extension) {\n        continue;\n      }\n\n      filename = vendor_path_to_use + filename;\n      num_vendor_files_found++;\n      ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Reading file %s\\n\", filename.c_str());\n\n      // open the file and read its contents\n      std::ifstream fin(filename);\n      if (!fin.is_open())\n        break;\n\n      std::string library_name;\n      while (std::getline(fin, library_name)) {\n        if (library_name == \"\") {\n          continue;\n        }\n\n        // add the library\n        ACL_HAL_DEBUG_MSG_VERBOSE(1,\n                                  \"Trying to dynamically load board MMD %s \"\n                                  \"(length is %zu, last char is '%c')\\n\",\n                                  library_name.c_str(), library_name.length(),\n                                  library_name[library_name.length() - 1]);\n        auto result_status = l_load_single_board_library(\n            library_name.c_str(), num_boards_found, load_libraries);\n        if (!result_status) {\n          printf(\"Failed to dynamically load board MMD %s\\n\",\n                 library_name.c_str());\n          // Ignoring this failure since other libraries may successfully load.\n        }\n      }\n      fin.close();\n    }\n    closedir(dir);\n  }\n\n  if (num_vendor_files_found == 0) {\n    ACL_HAL_DEBUG_MSG_VERBOSE(\n        1, \"Error: Did not find any board vendor files in %s\",\n        board_vendor_path.c_str());\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \" to load board library at runtime. Either \"\n                                 \"link to the board library while compiling \");\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"your host code or refer to your board \"\n                                 \"vendor's documentation on how to \");\n    ACL_HAL_DEBUG_MSG_VERBOSE(\n        1, \"install the board library so that it can be loaded at runtime.\\n\");\n  }\n\n  if (!load_libraries) {\n    num_board_pkgs = num_boards_found;\n    if (num_board_pkgs) {\n      internal_mmd_dispatch.resize(num_board_pkgs);\n    }\n  }\n\n  return num_boards_found == 0 ? CL_FALSE : CL_TRUE;\n}\n#endif\n\nvoid l_get_physical_devices(acl_mmd_dispatch_t *mmd_dispatch,\n                            unsigned &num_phys_devices) {\n  int num_boards;\n  char *ptr, *saveptr;\n  // This is a bit subtle, pointers to device names might get cached by\n  // various routines\n  static char buf[MAX_BOARD_NAMES_LEN];\n\n  mmd_dispatch->aocl_mmd_get_offline_info(AOCL_MMD_VERSION, sizeof(buf), buf,\n                                          NULL);\n  buf[sizeof(buf) - 1] = 0;\n  mmd_dispatch->mmd_version = l_parse_mmd_version_str(std::string(buf));\n  if (mmd_dispatch->mmd_version < 0) {\n    printf(\"  Invalid MMD version:     %s\\n\", buf);\n    printf(\"Contact the board package support vendor for resolution.\\n\");\n    fflush(stdout);\n    assert(0);\n  }\n  min_MMD_version =\n      (!MMDVERSION_LESSTHAN(min_MMD_version, mmd_dispatch->mmd_version))\n          ? mmd_dispatch->mmd_version\n          : min_MMD_version;\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : Getting info version: %s\\n\", buf);\n\n  static double hal_version = 0;\n  if (hal_version == 0) { // Just parse once at start-up\n    hal_version = l_parse_mmd_version_str(AOCL_MMD_VERSION_STRING);\n    if (hal_version < 0) { // This should theoretically never happen\n      printf(\"  Invalid runtime version: %s\\n\", AOCL_MMD_VERSION_STRING);\n      fflush(stdout);\n      assert(0);\n    }\n  }\n  if (MMDVERSION_LESSTHAN(hal_version,\n                          mmd_dispatch->mmd_version) || // MMD newer than HAL\n      MMDVERSION_LESSTHAN(mmd_dispatch->mmd_version,\n                          14.0)) // Before this wasn't forward compatible\n  {\n    printf(\"  Runtime version: %s\\n\", AOCL_MMD_VERSION_STRING);\n    printf(\"  MMD version:     %s\\n\", buf);\n    printf(\"MMD version is newer than the runtime version! Use the runtime \"\n           \"with version greater or equal to the MMD version, or contact the \"\n           \"board support package vendors if mismatch is unexpected.\\n\");\n    fflush(stdout);\n    assert(0);\n  }\n\n  // Disable yield as initialization\n  acl_hal_mmd.yield = NULL;\n\n  // Dump offline info\n  if (debug_verbosity > 0) {\n    mmd_dispatch->aocl_mmd_get_offline_info(AOCL_MMD_VENDOR_NAME, sizeof(buf),\n                                            buf, NULL);\n    buf[sizeof(buf) - 1] = 0;\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : Getting info vendor: %s\\n\", buf);\n    mmd_dispatch->aocl_mmd_get_offline_info(AOCL_MMD_NUM_BOARDS, sizeof(int),\n                                            &num_boards, NULL);\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : Getting info num_boards: %d\\n\",\n                              num_boards);\n    mmd_dispatch->aocl_mmd_get_offline_info(AOCL_MMD_BOARD_NAMES, sizeof(buf),\n                                            buf, NULL);\n    buf[sizeof(buf) - 1] = 0;\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : Getting info boards: %s\\n\", buf);\n  }\n\n  mmd_dispatch->aocl_mmd_get_offline_info(AOCL_MMD_BOARD_NAMES,\n                                          MAX_BOARD_NAMES_LEN, buf, NULL);\n  buf[MAX_BOARD_NAMES_LEN - 1] = 0;\n  // Probe the platform devices by going through all the possibilities in the\n  // semicolon delimited list\n  ptr = acl_strtok(buf, \";\", &saveptr);\n  while (ptr != NULL) {\n    num_phys_devices++;\n    ptr = acl_strtok(NULL, \";\", &saveptr);\n  }\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Found %d devices\\n\", num_phys_devices);\n}\n\n// Simulator MMD helpers\nstatic acl_mmd_dispatch_t *l_get_msim_mmd_layer() {\n#ifdef _WIN32\n  const char *acl_root_dir = acl_getenv(\"INTELFPGAOCLSDKROOT\");\n  info_assert(acl_root_dir,\n              \"INTELFPGAOCLSDKROOT environment variable is missing!\");\n  const std::string mmd_lib_name_str =\n      std::string(acl_root_dir) + \"\\\\host\\\\windows64\\\\bin\\\\aoc_cosim_mmd.dll\";\n\n  const char *mmd_lib_name = mmd_lib_name_str.c_str();\n  const char *sym_name = \"msim_mmd_layer\";\n#else\n  const char *mmd_lib_name = \"libaoc_cosim_mmd.so\";\n  const char *sym_name = \"msim_mmd_layer\";\n#endif\n\n  char *error_msg = nullptr;\n  auto *mmd_lib = my_dlopen(mmd_lib_name, &error_msg);\n  typedef acl_mmd_dispatch_t *(*fcn_type)();\n  if (!mmd_lib) {\n    std::cout << \"Error: Could not load simulation MMD library \"\n              << mmd_lib_name;\n    if (error_msg && error_msg[0] != '\\0') {\n      std::cout << \" (error_msg: \" << error_msg << \")\";\n    }\n    std::cout << \"\\n\";\n    return nullptr;\n  }\n  auto *sym = my_dlsym(mmd_lib, sym_name, &error_msg);\n  mmd_libs.push_back(std::make_unique<my_dl_wrapper>(mmd_lib));\n  if (!sym) {\n    std::cout << \"Error: Symbol \" << sym_name\n              << \" not found in simulation MMD library \";\n    if (error_msg && error_msg[0] != '\\0') {\n      std::cout << \"(message: \" << error_msg << \")\";\n    }\n    std::cout << \"\\n\";\n    return nullptr;\n  }\n\n  // Now call the function. Ignore the Windows cast to fcn pointer\n  // warning/error.\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4055)\n#endif\n  return ((fcn_type)sym)();\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n}\n\nstatic bool l_is_simulator_dispatch(acl_mmd_dispatch_t *mmd_dispatch) {\n  return mmd_dispatch->aocl_mmd_simulation_device_info != NULL;\n}\n\nstatic void l_update_simulator(int handle, unsigned int physical_device_id,\n                               const acl_device_def_autodiscovery_t &dev) {\n  std::vector<aocl_mmd_memory_info_t> mem_info(dev.num_global_mem_systems);\n  for (unsigned i = 0; i < mem_info.size(); ++i) {\n    mem_info[i].start =\n        reinterpret_cast<uintptr_t>(dev.global_mem_defs[i].range.begin);\n    mem_info[i].size =\n        reinterpret_cast<uintptr_t>(dev.global_mem_defs[i].range.next);\n  }\n\n  device_info[physical_device_id].mmd_dispatch->aocl_mmd_simulation_device_info(\n      handle, static_cast<int>(mem_info.size()), mem_info.data());\n}\n\n// Other helpers\n/**\n *  Converts MMD allocation capabilities to runtime allocation capabilities\n *  @param mmd_capabilities the capabilities as defined by the MMD\n *  @return the capabilities as defined by the runtime\n */\nunsigned l_convert_mmd_capabilities(unsigned mmd_capabilities) {\n  unsigned capability = 0;\n\n  if (mmd_capabilities & AOCL_MMD_MEM_CAPABILITY_SUPPORTED) {\n    capability |= ACL_MEM_CAPABILITY_SUPPORTED;\n  }\n  if (mmd_capabilities & AOCL_MMD_MEM_CAPABILITY_ATOMIC) {\n    capability |= ACL_MEM_CAPABILITY_ATOMIC;\n  }\n  if (mmd_capabilities & AOCL_MMD_MEM_CAPABILITY_CONCURRENT) {\n    capability |= ACL_MEM_CAPABILITY_CONCURRENT;\n  }\n  if (mmd_capabilities & AOCL_MMD_MEM_CAPABILITY_P2P) {\n    capability |= ACL_MEM_CAPABILITY_P2P;\n  }\n  return capability;\n}\n\nvoid l_override_pll(unsigned int physical_device_id) {\n  char *env_pllsettings_str = getenv(\"ACL_PLL_SETTINGS\");\n  acl_assert_locked();\n\n  if (env_pllsettings_str) {\n    int return_val =\n        acl_hal_mmd_pll_reconfigure(physical_device_id, env_pllsettings_str);\n    assert(return_val == 0);\n  }\n}\n\n// Attempt to add a single device\nstatic int l_try_device(unsigned int physical_device_id, const char *name,\n                        acl_system_def_t *sys,\n                        acl_mmd_dispatch_t *mmd_dispatch_for_board) {\n  acl_assert_locked();\n  assert(physical_device_id < ACL_MAX_DEVICE);\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : Calling mmd_open on %s\\n\", name);\n  acl_mmd_device_t *device = &(device_info[physical_device_id]);\n  // If device is available, we shouldn't overwrite this memory, so we keep it\n  // in the acl_mmd_device_t struct.\n  device->name = name;\n  auto tmp_handle = mmd_dispatch_for_board->aocl_mmd_open(device->name.c_str());\n  if (tmp_handle < 0) {\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : device not found!\\n\");\n    return 0;\n  }\n\n  device->mmd_dispatch = mmd_dispatch_for_board;\n  device->handle = tmp_handle;\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : mmd_open returned handle %d\\n\",\n                            device->handle);\n\n  if (debug_verbosity > 0) {\n    char buf[1024];\n    device->mmd_dispatch->aocl_mmd_get_info(device->handle, AOCL_MMD_BOARD_NAME,\n                                            sizeof(buf), buf, NULL);\n    buf[sizeof(buf) - 1] = 0;\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : Getting info board name: %s\\n\", buf);\n  }\n\n  // Get interfaces - for now assume one kernel, one pll and one memory\n  int kernel_interface = -1;\n  int pll_interface = -1;\n  int memory_interface = -1;\n  ACL_HAL_DEBUG_MSG_VERBOSE(1,\n                            \"HAL : Getting interfaces via aocl_mmd_get_info\\n\");\n  device->mmd_dispatch->aocl_mmd_get_info(device->handle,\n                                          AOCL_MMD_KERNEL_INTERFACES,\n                                          sizeof(int), &kernel_interface, NULL);\n  device->mmd_dispatch->aocl_mmd_get_info(device->handle,\n                                          AOCL_MMD_PLL_INTERFACES, sizeof(int),\n                                          &pll_interface, NULL);\n  device->mmd_dispatch->aocl_mmd_get_info(device->handle,\n                                          AOCL_MMD_MEMORY_INTERFACE,\n                                          sizeof(int), &memory_interface, NULL);\n  ACL_HAL_DEBUG_MSG_VERBOSE(\n      1, \"HAL : Found kernel, pll, and memory interfaces: %d %d %d\\n\",\n      kernel_interface, pll_interface, memory_interface);\n\n  // For now require pll dynamic reconfiguration\n  if (!(kernel_interface >= 0 && memory_interface >= 0) ||\n      (kernel_interface == memory_interface) ||\n      (kernel_interface == pll_interface) ||\n      (pll_interface == memory_interface)) {\n    printf(\"Error mmd_get_info: handles for kernel, pll, and memory must be \"\n           \"unique and greater than 0\\n\");\n  }\n  device->mmd_ifaces.kernel_interface = kernel_interface;\n  device->mmd_ifaces.pll_interface = pll_interface;\n  device->mmd_ifaces.memory_interface = memory_interface;\n\n  device->mmd_dispatch->aocl_mmd_set_status_handler(\n      device->handle, acl_hal_mmd_status_handler, NULL);\n\n  kern[physical_device_id].physical_device_id = physical_device_id;\n\n  // Initialize PLL\n  if (pll_interface >= 0) {\n    bsp_io_pll[physical_device_id].device_info = device;\n    bsp_io_pll[physical_device_id].read = acl_pll_read;\n    bsp_io_pll[physical_device_id].write = acl_pll_write;\n    bsp_io_pll[physical_device_id].get_time_ns = acl_bsp_get_timestamp;\n    bsp_io_pll[physical_device_id].printf = printf;\n    bsp_io_pll[physical_device_id].debug_verbosity = debug_verbosity;\n    info_assert(acl_pll_init(&pll[physical_device_id],\n                             bsp_io_pll[physical_device_id], \"\") == 0,\n                \"Failed to read PLL config\");\n\n    // If environment override set use it, and disable m_freq_per_kernel\n    l_override_pll(physical_device_id);\n\n    // Sanity check that PLL is locked\n    assert(acl_pll_is_locked(&pll[physical_device_id]));\n  }\n\n  // Initialize Kernel Interface\n  bsp_io_kern[physical_device_id].device_info = device;\n  bsp_io_kern[physical_device_id].read = acl_kernel_if_read;\n  bsp_io_kern[physical_device_id].write = acl_kernel_if_write;\n  bsp_io_kern[physical_device_id].get_time_ns = acl_bsp_get_timestamp;\n  bsp_io_kern[physical_device_id].printf = printf;\n  bsp_io_kern[physical_device_id].debug_verbosity = debug_verbosity;\n\n  bool is_simulator = l_is_simulator_dispatch(device->mmd_dispatch);\n  info_assert(acl_kernel_if_init(&kern[physical_device_id],\n                                 bsp_io_kern[physical_device_id], sys,\n                                 is_simulator) == 0,\n              \"Failed to initialize kernel interface\");\n\n  acl_kernel_if_reset(&kern[physical_device_id]);\n\n  // Register interrupt handlers\n  // Set kernel interrupt handler\n  interrupt_user_data[physical_device_id] = physical_device_id;\n  device->mmd_dispatch->aocl_mmd_set_interrupt_handler(\n      device->handle, acl_hal_mmd_kernel_interrupt,\n      &interrupt_user_data[physical_device_id]);\n\n  // ECC is handled by the device_interrupt\n  if (device->mmd_dispatch->aocl_mmd_set_device_interrupt_handler != NULL) {\n    device->mmd_dispatch->aocl_mmd_set_device_interrupt_handler(\n        device->handle, acl_hal_mmd_device_interrupt,\n        &interrupt_user_data[physical_device_id]);\n  }\n\n  {\n    // If we are using an old MMD that doesn't support these attributes, assume\n    // half duplex\n    unsigned int concurrent_reads = 1, concurrent_writes = 1,\n                 max_inflight_mem_ops = 1;\n\n    if (!MMDVERSION_LESSTHAN(mmd_dispatch_for_board->mmd_version, 18.1)) {\n      device->mmd_dispatch->aocl_mmd_get_info(\n          device->handle, AOCL_MMD_CONCURRENT_READS, sizeof(int),\n          &concurrent_reads, NULL);\n      device->mmd_dispatch->aocl_mmd_get_info(\n          device->handle, AOCL_MMD_CONCURRENT_WRITES, sizeof(int),\n          &concurrent_writes, NULL);\n      device->mmd_dispatch->aocl_mmd_get_info(\n          device->handle, AOCL_MMD_CONCURRENT_READS_OR_WRITES, sizeof(int),\n          &max_inflight_mem_ops, NULL);\n      // Sanity checking the output of the BSP\n      // Other combinations may have ambiguous ways of interpreting the data\n      assert(concurrent_reads == concurrent_writes);\n      assert(max_inflight_mem_ops == concurrent_reads + concurrent_writes ||\n             max_inflight_mem_ops == concurrent_reads);\n    }\n    sys->device[physical_device_id].physical_device_id = physical_device_id;\n    sys->device[physical_device_id].concurrent_reads = concurrent_reads;\n    sys->device[physical_device_id].concurrent_writes = concurrent_writes;\n    sys->device[physical_device_id].max_inflight_mem_ops = max_inflight_mem_ops;\n  }\n\n  {\n    // not supported on legacy devices\n    unsigned int host_capabilities = 0, shared_capabilities = 0,\n                 device_capabilities = 0;\n    if (!MMDVERSION_LESSTHAN(mmd_dispatch_for_board->mmd_version, 20.3)) {\n      device->mmd_dispatch->aocl_mmd_get_info(\n          device->handle, AOCL_MMD_HOST_MEM_CAPABILITIES, sizeof(unsigned),\n          &host_capabilities, NULL);\n      device->mmd_dispatch->aocl_mmd_get_info(\n          device->handle, AOCL_MMD_SHARED_MEM_CAPABILITIES, sizeof(unsigned),\n          &shared_capabilities, NULL);\n      device->mmd_dispatch->aocl_mmd_get_info(\n          device->handle, AOCL_MMD_DEVICE_MEM_CAPABILITIES, sizeof(unsigned),\n          &device_capabilities, NULL);\n    }\n\n    sys->device[physical_device_id].host_capabilities =\n        l_convert_mmd_capabilities(host_capabilities);\n    sys->device[physical_device_id].shared_capabilities =\n        l_convert_mmd_capabilities(shared_capabilities);\n    sys->device[physical_device_id].device_capabilities =\n        l_convert_mmd_capabilities(device_capabilities);\n  }\n\n  {\n    size_t min_host_mem_alignment = 2097152;\n    if (!MMDVERSION_LESSTHAN(mmd_dispatch_for_board->mmd_version, 20.3)) {\n      device->mmd_dispatch->aocl_mmd_get_info(\n          device->handle, AOCL_MMD_MIN_HOST_MEMORY_ALIGNMENT, sizeof(size_t),\n          &min_host_mem_alignment, NULL);\n    }\n    sys->device[physical_device_id].min_host_mem_alignment =\n        min_host_mem_alignment;\n  }\n\n  // Post-PLL config init function - at this point, it's safe to talk to the\n  // kernel CSR registers.\n  if (acl_kernel_if_post_pll_config_init(&kern[physical_device_id]))\n    return 0;\n\n  return 1;\n}\n\nvoid l_close_device(unsigned int physical_device_id,\n                    acl_mmd_dispatch_t *mmd_dispatch_for_board) {\n  acl_assert_locked();\n\n  mmd_dispatch_for_board->aocl_mmd_close(\n      device_info[physical_device_id].handle);\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL : Closing device %d\\n\", physical_device_id);\n\n  return;\n}\n\nstatic int src_dev_done;\nstatic int dst_dev_done;\nstatic void l_dev_to_dev_copy_handler(int handle, void *user_data,\n                                      aocl_mmd_op_t op, int status) {\n  // Removing Windows warning\n  user_data = user_data;\n  handle = handle;\n  status = status;\n  if (op == (aocl_mmd_op_t)&src_dev_done) {\n    src_dev_done = 1;\n  } else if (op == (aocl_mmd_op_t)&dst_dev_done) {\n    dst_dev_done = 1;\n  } else\n    assert(0 && \"dev_to_dev_copy got unexpected event\");\n}\n\n// **************************************************************************\n// **************************** HAL functions *******************************\n// **************************************************************************\n\nint acl_hal_mmd_pll_reconfigure(unsigned int physical_device_id,\n                                const char *pll_settings_str) {\n  pll_setting_t pll_setting;\n  acl_pll *current_pll = &pll[physical_device_id];\n\n  // parse manually. sscanf doesn't link.\n  int filled = 0;\n  char *space_loc = (char *)pll_settings_str;\n  unsigned int *dest = (unsigned int *)&pll_setting;\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL: Parsing ACL_PLL_SETTINGS string: %s\\n\",\n                            space_loc);\n  *dest = (unsigned)atoi(space_loc);\n  filled++;\n  dest++;\n  while ((space_loc = strchr(space_loc + 1, ' ')) != NULL) {\n    *dest = (unsigned)atoi(space_loc + 1);\n    filled++;\n    dest++;\n  }\n\n  if (filled == 9) {\n    return acl_pll_reconfigure(current_pll, pll_setting);\n  } else {\n    printf(\"HAL Warning: Failed to parse pll settings from ACL_PLL_SETTINGS \"\n           \"environment variable, ignoring pll override\\n\");\n    return -1;\n  }\n}\n\nint acl_hal_mmd_get_debug_verbosity() { return debug_verbosity; }\n\nACL_HAL_EXPORT const acl_hal_t *\nacl_mmd_get_system_definition(acl_system_def_t *sys,\n                              acl_mmd_library_names_t *_libraries_to_load) {\n  char *hal_debug_var;\n  int use_offline_only;\n\n#ifdef _WIN32\n  // We're really relying on this being called before anything else\n  LARGE_INTEGER li;\n  QueryPerformanceFrequency(&li);\n  m_ticks_per_second = (cl_ulong)li.QuadPart;\n  assert(m_ticks_per_second != 0);\n#endif\n  acl_assert_locked();\n\n  hal_debug_var = getenv(\"ACL_HAL_DEBUG\");\n  if (hal_debug_var) {\n    debug_verbosity = atoi(hal_debug_var);\n    ACL_HAL_DEBUG_MSG_VERBOSE(0, \"Setting debug level to %u\\n\",\n                              debug_verbosity);\n  }\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(1, \"%s\\n\", ACL_BANNER);\n\n#ifdef _WIN32\n#define MAX_PATH_LEN 512\n  char lib_path[MAX_PATH_LEN];\n  HMODULE hm = NULL;\n  if (!GetModuleHandleEx(GET_MODULE_HANDLE_EX_FLAG_FROM_ADDRESS |\n                             GET_MODULE_HANDLE_EX_FLAG_UNCHANGED_REFCOUNT,\n                         (LPCSTR)&acl_mmd_get_system_definition, &hm)) {\n    ACL_HAL_DEBUG_MSG_VERBOSE(\n        1, \"Error: Could not retrieve library path information \\n\");\n  } else {\n    if (!GetModuleFileName(hm, lib_path, sizeof(lib_path))) {\n      ACL_HAL_DEBUG_MSG_VERBOSE(\n          1, \"Error: Could not retrieve library path information \\n\");\n    } else {\n      ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Path to runtime library: %s \\n \", lib_path);\n    }\n  }\n#else\n  Dl_info lib_info;\n  if (!dladdr((const void *)&acl_mmd_get_system_definition, &lib_info)) {\n    ACL_HAL_DEBUG_MSG_VERBOSE(\n        1, \"Error: Could not retrieve library path information \\n\");\n  } else {\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Path to runtime library: %s \\n \",\n                              lib_info.dli_fname);\n  }\n#endif\n\n  // Dynamically load board mmd & symbols\n  (void)acl_get_offline_device_user_setting(&use_offline_only);\n  if (use_offline_only == ACL_CONTEXT_MPSIM) {\n\n    // Substitute the simulator MMD layer.\n    auto *result = l_get_msim_mmd_layer();\n    if (!result)\n      return nullptr;\n    else\n      internal_mmd_dispatch.push_back(*result);\n\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Use simulation MMD\\n\");\n    num_board_pkgs = 1;\n  } else if (IS_VALID_FUNCTION(aocl_mmd_get_offline_info)) {\n    num_board_pkgs = 1; // It is illegal to define more than one board package\n                        // while statically linking a board package to the host.\n    internal_mmd_dispatch.resize(num_board_pkgs);\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Board MMD is statically linked\\n\");\n\n    internal_mmd_dispatch[0].library_name = \"runtime_static\";\n    internal_mmd_dispatch[0].mmd_library = nullptr;\n\n    internal_mmd_dispatch[0].aocl_mmd_get_offline_info =\n        aocl_mmd_get_offline_info;\n\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_get_offline_info,\n                         1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_get_info, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_open, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_close, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0],\n                         aocl_mmd_set_interrupt_handler, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0],\n                         aocl_mmd_set_device_interrupt_handler, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_set_status_handler,\n                         1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_yield, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_read, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_write, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_copy, 1);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_reprogram, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_shared_mem_alloc,\n                         0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_shared_mem_free, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_hostchannel_create,\n                         0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_hostchannel_destroy,\n                         0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0],\n                         aocl_mmd_hostchannel_get_buffer, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0],\n                         aocl_mmd_hostchannel_ack_buffer, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_program, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_host_alloc, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_free, 0);\n    ADD_STATIC_FN_TO_HAL(internal_mmd_dispatch[0], aocl_mmd_shared_alloc, 0);\n  } else if (_libraries_to_load) {\n    for (auto ipass = 0; ipass < 2; ++ipass) {\n      cl_bool load_libraries;\n      size_t num_boards_found = 0;\n      acl_mmd_library_names_t *next_library;\n      if (ipass == 0) {\n        load_libraries = CL_FALSE;\n      } else {\n        load_libraries = CL_TRUE;\n      }\n      next_library = _libraries_to_load;\n      while (next_library) {\n        const auto &library_name = next_library->library_name;\n        ACL_HAL_DEBUG_MSG_VERBOSE(1,\n                                  \"Trying to dynamically load board MMD %s\\n\",\n                                  library_name.c_str());\n        auto result_status = l_load_single_board_library(\n            library_name.c_str(), num_boards_found, load_libraries);\n        if (!result_status) {\n          return NULL;\n        }\n        next_library = next_library->next;\n      }\n      if (!load_libraries) {\n        num_board_pkgs = num_boards_found;\n        internal_mmd_dispatch.resize(num_board_pkgs);\n      }\n    }\n  } else {\n    cl_bool result_status;\n    // Call twice. Once to count number of libraries and once to load them\n    result_status = l_load_board_libraries(CL_FALSE);\n    if (!result_status) {\n      ACL_HAL_DEBUG_MSG_VERBOSE(\n          1, \"Error: Could not load FPGA board libraries successfully.\\n\");\n      return NULL;\n    }\n    result_status = l_load_board_libraries(CL_TRUE);\n    if (!result_status) {\n      printf(\"Error: Could not load FPGA board libraries successfully.\\n\");\n      return NULL;\n    }\n  }\n\n  sys->num_devices = 0;\n  num_physical_devices = 0;\n  for (unsigned iboard = 0; iboard < num_board_pkgs; ++iboard) {\n    l_get_physical_devices(&(internal_mmd_dispatch[iboard]),\n                           num_physical_devices);\n    sys->num_devices = num_physical_devices;\n  }\n  return &acl_hal_mmd;\n}\n\nvoid acl_hal_mmd_get_device_status(cl_uint num_devices,\n                                   const cl_device_id *devices) {\n  unsigned physical_device_id;\n  for (unsigned idevice = 0; idevice < num_devices; idevice++) {\n    assert(devices[idevice]->opened_count > 0);\n\n    physical_device_id = devices[idevice]->def.physical_device_id;\n    acl_kernel_if_check_kernel_status(&kern[physical_device_id]);\n  }\n}\n\nint acl_hal_mmd_try_devices(cl_uint num_devices, const cl_device_id *devices,\n                            cl_platform_id platform) {\n\n  unsigned int physical_device_id = 0;\n  unsigned int idevice;\n  unsigned int failed_device_id;\n  char *ptr, *saveptr1, *saveptr2;\n  static char\n      buf[MAX_BOARD_NAMES_LEN]; // This is a bit subtle, pointers to device\n                                // names might get cached by various routines\n\n  for (unsigned iboard = 0; iboard < num_board_pkgs; ++iboard) {\n    internal_mmd_dispatch[iboard].aocl_mmd_get_offline_info(\n        AOCL_MMD_BOARD_NAMES, MAX_BOARD_NAMES_LEN, buf, NULL);\n    buf[MAX_BOARD_NAMES_LEN - 1] = 0; // ensure it's null terminated\n\n    // query for the polling/interrupt mode for each bsp\n    int uses_yield = 0;\n    if (!MMDVERSION_LESSTHAN(internal_mmd_dispatch[iboard].mmd_version, 14.1)) {\n      internal_mmd_dispatch[iboard].aocl_mmd_get_offline_info(\n          AOCL_MMD_USES_YIELD, sizeof(int), &uses_yield, NULL);\n    }\n    // Probe the platform devices by going through all the possibilities in the\n    // semicolon delimited list\n    ptr = acl_strtok(buf, \";\", &saveptr1);\n    while (ptr != NULL) {\n      // Now we must go through the device list to see if one of them matches\n      // this physical id:\n      for (idevice = 0; idevice < num_devices; idevice++) {\n        if (devices[idevice] == &(platform->device[physical_device_id])) {\n          if (devices[idevice]->opened_count == 0) {\n            if (l_try_device(physical_device_id, ptr,\n                             platform->initial_board_def,\n                             &(internal_mmd_dispatch[iboard]))) {\n              ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Device: %s device is available\\n\",\n                                        ptr);\n              if (uses_yield_ref != -1 && uses_yield != uses_yield_ref) {\n                fprintf(stderr,\n                        \" HAL : Multiple BSPs are installed with a mixing of \"\n                        \"using polling and using interrupts.\\n\");\n                fprintf(stderr,\n                        \"       This combination of BSPs is not supported by \"\n                        \"the Intel(R) FPGA SDK for OpenCL(TM) runtime.\\n\");\n                fprintf(stderr,\n                        \"       Uninstall the incompatible BSP(s) using \"\n                        \"\\\"aocl uninstall <board_package_path>\\\".\\n\");\n                failed_device_id = physical_device_id;\n                goto failed;\n              }\n              if (uses_yield) {\n                // Enable yield function.  In 14.0 this function was never\n                // called, in 14.1 it is called only if the offline attribute is\n                // set.\n                if (!MMDVERSION_LESSTHAN(\n                        internal_mmd_dispatch[iboard].mmd_version, 14.1)) {\n                  ((acl_hal_t *)acl_get_hal())->yield = acl_hal_mmd_yield;\n                }\n              }\n              uses_yield_ref = uses_yield;\n            } else {\n              ACL_HAL_DEBUG_MSG_VERBOSE(\n                  1, \"Device: %s device is NOT available\\n\", ptr);\n              failed_device_id = physical_device_id;\n              goto failed;\n            }\n          }\n          devices[idevice]->opened_count++;\n        }\n      }\n      ptr = acl_strtok(NULL, \";\", &saveptr1);\n      physical_device_id++;\n    }\n  }\n  return 0;\n\nfailed:\n\n  // Loop through exactly as above, closing devices until we hit the index that\n  // failed. Then return an error.\n  physical_device_id = 0;\n\n  for (unsigned iboard = 0; iboard < num_board_pkgs; ++iboard) {\n\n    internal_mmd_dispatch[iboard].aocl_mmd_get_offline_info(\n        AOCL_MMD_BOARD_NAMES, MAX_BOARD_NAMES_LEN, buf, NULL);\n    buf[MAX_BOARD_NAMES_LEN - 1] = 0; // ensure it's null terminated\n\n    // Probe the platform devices by going through all the possibilities in the\n    // semicolon delimited list\n    ptr = acl_strtok(buf, \";\", &saveptr2);\n    while (ptr != NULL) {\n      if (physical_device_id == failed_device_id) {\n        // We've gotten to the device that failed, which means we've closed all\n        // the devices that we opened. We can now return:\n        return CL_DEVICE_NOT_AVAILABLE;\n      }\n\n      // Now we must go through the device list to see if one of them matches\n      // this physical id:\n      for (idevice = 0; idevice < num_devices; idevice++) {\n        if (devices[idevice] == &(platform->device[physical_device_id])) {\n          if (devices[idevice]->opened_count == 1) { // we just opened it\n            l_close_device(physical_device_id,\n                           &(internal_mmd_dispatch[iboard]));\n          }\n          devices[idevice]->opened_count--;\n        }\n      }\n      ptr = acl_strtok(NULL, \";\", &saveptr2);\n      physical_device_id++;\n    }\n  }\n\n  assert(0 && \"Should never get here\");\n  return 1; // Prevent compiler warnings\n}\n\nint acl_hal_mmd_close_devices(cl_uint num_devices,\n                              const cl_device_id *devices) {\n  cl_uint idevice;\n\n  for (idevice = 0; idevice < num_devices; idevice++) {\n    assert(devices[idevice]->opened_count > 0);\n    devices[idevice]->opened_count--;\n\n    // We actually close it if there are no more contexts with it opened:\n    if (devices[idevice]->opened_count == 0) {\n      unsigned int physical_device_id =\n          devices[idevice]->def.physical_device_id;\n      l_close_device(physical_device_id,\n                     device_info[physical_device_id].mmd_dispatch);\n    }\n  }\n  return 0;\n}\n\nvoid acl_hal_mmd_init_device(const acl_system_def_t *sysdef) {\n  // Removing Windows warning\n  sysdef = sysdef;\n\n  // Perhaps tell kernel interface what cl_device looks like\n}\n\n// Program the FPGA device with the given binary.\n// the status returned:\n// 0 : program succeeds\n// ACL_PROGRAM_FAILED (-1) : program failed\n// ACL_PROGRAM_CANNOT_PRESERVE_GLOBAL_MEM (-2) : memory-preserved program failed\nint acl_hal_mmd_program_device(unsigned int physical_device_id,\n                               const acl_device_def_t *devdef,\n                               const struct acl_pkg_file *binary,\n                               int acl_program_mode) {\n  char *sof;\n  size_t sof_len;\n  bool is_simulator;\n  static cl_bool msg_printed = CL_FALSE;\n  int temp_handle = 0;\n  acl_assert_locked();\n\n  if (acl_pkg_section_exists(binary, ACL_PKG_SECTION_FPGA_BIN, &sof_len)) {\n    if (sof_len < MIN_SOF_SIZE) {\n      printf(\" mmd: program_device:  fpga.bin is too small, only %lu bytes.\\n\",\n             (long)sof_len);\n      fflush(stdout);\n      return ACL_PROGRAM_FAILED;\n    }\n  }\n  // Get reference to the fpga.bin, just in case.\n  ACL_HAL_DEBUG_MSG_VERBOSE(\n      1, \" mmd: program_device:  Get fpga.bin from binary...\\n\");\n  if (!acl_pkg_read_section_transient(binary, ACL_PKG_SECTION_FPGA_BIN, &sof)) {\n    printf(\"HAL program_device:  Could not get fpga.bin out of binary\\n\");\n    fflush(stdout);\n    return ACL_PROGRAM_FAILED;\n  }\n\n  size_t pll_config_len = 0;\n  std::string pll_config;\n  if (acl_pkg_section_exists(binary, ACL_PKG_SECTION_PLL_CONFIG,\n                             &pll_config_len)) {\n    if (pll_config_len < MIN_PLL_CONFIG_SIZE) {\n      printf(\n          \" mmd: program_device:  pll_config is too small, only %lu bytes.\\n\",\n          (long)pll_config_len);\n      fflush(stdout);\n      return ACL_PROGRAM_FAILED;\n    }\n    char *pll_config_read = nullptr;\n    if (!acl_pkg_read_section_transient(binary, ACL_PKG_SECTION_PLL_CONFIG,\n                                        &pll_config_read)) {\n      printf(\"HAL program_device:  Could not get pll_config out of binary\\n\");\n      fflush(stdout);\n      return ACL_PROGRAM_FAILED;\n    }\n    // The returned pll_config_read is not null terminated so have to\n    // assign the first pll_config_len characters to pll_config.\n    pll_config.assign(pll_config_read, pll_config_read + pll_config_len);\n  }\n\n  if (debug_verbosity) {\n    // Show the hash and version info, if they exist.\n    size_t version_len = 0;\n    size_t hash_len = 0;\n#define EXPECTED_HASH_LEN 40\n#define MAX_VERSION_LEN 40\n    char hash[EXPECTED_HASH_LEN + 1];\n    char version[MAX_VERSION_LEN + 1];\n    if (acl_pkg_section_exists(binary, ACL_PKG_SECTION_ACL_VERSION,\n                               &version_len) &&\n        version_len <= MAX_VERSION_LEN &&\n        acl_pkg_read_section(binary, ACL_PKG_SECTION_ACL_VERSION, version,\n                             version_len)) {\n      version[version_len] = 0; // terminate with NUL\n      printf(\"mmd: binary built with aocl version %s\\n\", version);\n    } else {\n      printf(\"mmd: no aocl version stored in binary\\n\");\n    }\n    if (acl_pkg_section_exists(binary, ACL_PKG_SECTION_HASH, &hash_len)) {\n      if (hash_len == EXPECTED_HASH_LEN &&\n          acl_pkg_read_section(binary, ACL_PKG_SECTION_HASH, hash, hash_len)) {\n        hash[EXPECTED_HASH_LEN] = 0; // terminate with NUL\n        printf(\"mmd: binary hash %s\\n\", hash);\n      } else {\n        if (hash_len != EXPECTED_HASH_LEN) {\n          printf(\"mmd: binary hash len should be %d but is %d\\n\",\n                 EXPECTED_HASH_LEN, (int)hash_len);\n        } else {\n          printf(\"mmd: Could not get hash from binary\\n\");\n        }\n      }\n    } else {\n      printf(\"mmd: no hash stored in binary\\n\");\n    }\n#undef EXPECTED_HASH_LEN\n    fflush(stdout);\n  }\n\n  // The message below may interfere with the simulator standard output in some\n  // cases (e.g. features/printf/test), where there are multiple kernels and\n  // resources are released between kernel runs.  For the simulator, only print\n  // this message once. This is a horrible kludge.\n  is_simulator =\n      l_is_simulator_dispatch(device_info[physical_device_id].mmd_dispatch);\n  msg_printed = (cl_bool)CL_FALSE;\n  if (!(is_simulator && msg_printed)) {\n    ACL_HAL_DEBUG_MSG_VERBOSE(1, \"Reprogramming device [%d] with handle %d\\n\",\n                              physical_device_id,\n                              device_info[physical_device_id].handle);\n    msg_printed = CL_TRUE;\n  }\n\n  // mmd check and use the correct reprogram flow accordingly\n  // only check the first board package\n  if (MMDVERSION_LESSTHAN(\n          device_info[physical_device_id].mmd_dispatch->mmd_version, 18.1)) {\n    // The mmd version is old. The runtime and hal would have to try reprogram\n    // the device with the old flow but first we need to check the\n    // aocl_mmd_reprogram is implemented.\n    if (!device_info[physical_device_id].mmd_dispatch->aocl_mmd_reprogram) {\n      fprintf(stderr, \"mmd program_device: The current board support package \"\n                      \"does not support device reprogramming. Exit.\\n\");\n      return ACL_PROGRAM_FAILED;\n    }\n    if (acl_program_mode == ACL_PROGRAM_PRESERVE_MEM) {\n      return ACL_PROGRAM_CANNOT_PRESERVE_GLOBAL_MEM;\n    } else {\n      ACL_HAL_DEBUG_MSG_VERBOSE(\n          1,\n          \"HAL program_device: MMD version [%f] does not match runtime, trying \"\n          \"memory-unpreserved reprogramming.\\n\",\n          device_info[physical_device_id].mmd_dispatch->mmd_version);\n      if ((device_info[physical_device_id].handle =\n               device_info[physical_device_id].mmd_dispatch->aocl_mmd_reprogram(\n                   device_info[physical_device_id].handle, sof, sof_len)) < 0) {\n        fprintf(stderr, \"mmd program_device: Board reprogram failed\\n\");\n        return ACL_PROGRAM_FAILED;\n      }\n    }\n  } else {\n    // The mmd version is up-to-date. Can safely try the new program flow\n    // first of all, check if the aocl_mmd_program is implemented\n    if (!device_info[physical_device_id].mmd_dispatch->aocl_mmd_program) {\n      fprintf(stderr, \"mmd program_device: The current board support package \"\n                      \"does not support device reprogramming. Exit.\\n\");\n      return ACL_PROGRAM_FAILED;\n    }\n    // check if the old reprogram API is implemented in the BSP. If so, exit as\n    // error\n    if (device_info[physical_device_id].mmd_dispatch->aocl_mmd_reprogram) {\n      fprintf(stderr, \"mmd program_device: aocl_mmd_reprogram is deprecated! \"\n                      \"Program with aocl_mmd_program instead. Exit.\\n\");\n      return ACL_PROGRAM_FAILED;\n    }\n    if (acl_program_mode == ACL_PROGRAM_PRESERVE_MEM) {\n      // first time try reprogram with memory preserving\n      ACL_HAL_DEBUG_MSG_VERBOSE(\n          1, \"HAL program_device: Trying memory-preserved programming\\n\");\n      temp_handle =\n          device_info[physical_device_id].mmd_dispatch->aocl_mmd_program(\n              device_info[physical_device_id].handle, sof, sof_len,\n              AOCL_MMD_PROGRAM_PRESERVE_GLOBAL_MEM);\n      if (temp_handle < 0) {\n        // memory-preserved program failed, needs to go back do the save/restore\n        // first and come back\n        ACL_HAL_DEBUG_MSG_VERBOSE(1, \"HAL program_device: memory-preserved \"\n                                     \"reprogramming unsuccessful\\n\");\n        return ACL_PROGRAM_CANNOT_PRESERVE_GLOBAL_MEM;\n      } else {\n        // memory-preserved program success, no need to reprogram again\n        device_info[physical_device_id].handle = temp_handle;\n      }\n    } else {\n      ACL_HAL_DEBUG_MSG_VERBOSE(\n          1, \"HAL program_device: Trying memory-unpreserved programming\\n\");\n      // already know memory-preserved program failed. have done save/restore.\n      // Safe to go through memory-unpreserved program\n      if ((device_info[physical_device_id].handle =\n               device_info[physical_device_id].mmd_dispatch->aocl_mmd_program(\n                   device_info[physical_device_id].handle, sof, sof_len, 0)) <\n          0) {\n        ACL_HAL_DEBUG_MSG_VERBOSE(\n            1, \"HAL program_device: memory-unpreserved programming failed\\n\");\n        fprintf(stderr, \"mmd program_device: Board reprogram failed\\n\");\n        return ACL_PROGRAM_FAILED;\n      }\n    }\n  }\n  // Need to remap the IDs after reprogramming\n  kern[physical_device_id].io.device_info = &(device_info[physical_device_id]);\n  bsp_io_kern[physical_device_id].device_info =\n      &(device_info[physical_device_id]);\n  bsp_io_pll[physical_device_id].device_info =\n      &(device_info[physical_device_id]);\n\n  // Tell the simulator (if present) about global memory sizes.\n  if (is_simulator) {\n    l_update_simulator(device_info[physical_device_id].handle,\n                       physical_device_id, devdef->autodiscovery_def);\n  }\n\n  device_info[physical_device_id].mmd_dispatch->aocl_mmd_set_status_handler(\n      device_info[physical_device_id].handle, acl_hal_mmd_status_handler, NULL);\n  acl_kernel_if_update(devdef->autodiscovery_def, &kern[physical_device_id]);\n  if (device_info[physical_device_id].mmd_ifaces.pll_interface >= 0) {\n    if (acl_pkg_section_exists(binary, ACL_PKG_SECTION_PLL_CONFIG,\n                               &pll_config_len)) {\n      info_assert(acl_pll_init(&pll[physical_device_id],\n                               bsp_io_pll[physical_device_id], pll_config) == 0,\n                  \"Failed to read PLL config\");\n\n    } else {\n      info_assert(acl_pll_init(&pll[physical_device_id],\n                               bsp_io_pll[physical_device_id], \"\") == 0,\n                  \"Failed to read PLL config\");\n      l_override_pll(physical_device_id);\n    }\n  }\n  acl_kernel_if_reset(&kern[physical_device_id]);\n\n  // Register interrupt handlers\n  // Set kernel interrupt handler\n  interrupt_user_data[physical_device_id] = physical_device_id;\n  device_info[physical_device_id].mmd_dispatch->aocl_mmd_set_interrupt_handler(\n      device_info[physical_device_id].handle, acl_hal_mmd_kernel_interrupt,\n      &interrupt_user_data[physical_device_id]);\n\n  // ECC is handled by the device_interrupt\n  if (device_info[physical_device_id]\n          .mmd_dispatch->aocl_mmd_set_device_interrupt_handler != NULL) {\n    device_info[physical_device_id]\n        .mmd_dispatch->aocl_mmd_set_device_interrupt_handler(\n            device_info[physical_device_id].handle,\n            acl_hal_mmd_device_interrupt,\n            &interrupt_user_data[physical_device_id]);\n  }\n  // Post-PLL config init function - at this point, it's safe to talk to the\n  // kernel CSR registers.\n  if (acl_kernel_if_post_pll_config_init(&kern[physical_device_id]))\n    return -1;\n\n  return 0;\n}\n\nvoid acl_hal_mmd_kernel_interrupt(int handle_in, void *user_data) {\n  unsigned physical_device_id;\n\n  // Callbacks received from non-dma transfers.\n  // (those calls are not initiated by a signal handler, so we need to block all\n  // signals here to avoid simultaneous calls to signal handler.)\n  // Must instantiate before acl_sig_started, destruct after acl_sig_finished.\n  acl_signal_blocker sig_blocker;\n\n  acl_sig_started();\n  // NOTE: all exit points of this function must first call acl_sig_finished()\n\n  // Removing Windows warning\n  user_data = user_data;\n  // Make sure the combination of handle_in and device id match\n  assert(user_data != NULL);\n  physical_device_id = *((unsigned *)user_data);\n\n  if (device_info[physical_device_id].handle == handle_in) {\n    assert(acl_kernel_if_is_valid(&kern[physical_device_id]));\n    acl_kernel_if_update_status(&kern[physical_device_id]);\n    acl_sig_finished();\n    return;\n  }\n\n  fprintf(stderr, \"physical_device_id= %d, handle_in = %d\\n\",\n          physical_device_id, handle_in);\n  info_assert(0, \"Failed to find handle \");\n}\n\nvoid acl_hal_mmd_device_interrupt(int handle_in,\n                                  aocl_mmd_interrupt_info *data_in,\n                                  void *user_data) {\n  unsigned physical_device_id;\n\n  // Callbacks received from non-dma transfers.\n  // (those calls are not initiated by a signal handler, so we need to block all\n  // signals here to avoid simultaneous calls to signal handler.)\n  // Must instantiate before acl_sig_started, destruct after acl_sig_finished.\n  acl_signal_blocker sig_blocker;\n\n  acl_sig_started();\n  // NOTE: all exit points of this function must first call acl_sig_finished()\n\n  // Make sure the combination of handle_in and device id match\n  assert(user_data != NULL);\n  assert(data_in != NULL);\n\n  physical_device_id = *((unsigned *)user_data);\n\n  if (device_info[physical_device_id].handle == handle_in) {\n    acl_device_update_fn(physical_device_id, data_in->exception_type,\n                         data_in->user_private_info, data_in->user_cb);\n    acl_sig_finished();\n    return;\n  }\n\n  fprintf(stderr, \"physical_device_id= %d, handle_in = %d\\n\",\n          physical_device_id, handle_in);\n  info_assert(0, \"Failed to find handle \");\n}\n\nvoid acl_hal_mmd_status_handler(int handle, void *user_data, aocl_mmd_op_t op,\n                                int status) {\n  // Callbacks received from non-dma transfers.\n  // (those calls are not initiated by a signal handler, so we need to block all\n  // signals here to avoid simultaneous calls to signal handler.)\n  // Must instantiate before acl_sig_started, destruct after acl_sig_finished.\n  acl_signal_blocker sig_blocker;\n\n  acl_sig_started();\n  // NOTE: all exit points of this function must first call acl_sig_finished()\n  // Removing Windows warning\n  handle = handle;\n  user_data = user_data;\n  assert(status == 0);\n  acl_event_update_fn((cl_event)op, CL_COMPLETE);\n\n  acl_sig_finished();\n}\n\nvoid acl_hal_mmd_register_callbacks(\n    acl_event_update_callback event_update,\n    acl_kernel_update_callback kernel_update,\n    acl_profile_callback profile_update,\n    acl_device_update_callback device_update,\n    acl_process_printf_buffer_callback process_printf) {\n  acl_assert_locked();\n  acl_event_update_fn = event_update;\n  acl_kernel_if_register_callbacks(kernel_update, profile_update,\n                                   process_printf);\n  acl_kernel_update_fn = kernel_update; // Keeping a copy to be able to use it\n                                        // for reseting the kernels.\n  acl_profile_fn = profile_update;\n  acl_device_update_fn = device_update;\n}\n\nvoid acl_hal_mmd_yield(cl_uint num_devices, const cl_device_id *devices) {\n  cl_uint idevice;\n  unsigned int physical_device_id;\n  int keep_going;\n  acl_assert_locked();\n\n  // Check each device and try to perform some slice of useful work\n  do {\n    keep_going = 0;\n    for (idevice = 0; idevice < num_devices; idevice++) {\n      assert(devices[idevice]->opened_count > 0);\n\n      physical_device_id = devices[idevice]->def.physical_device_id;\n      acl_kernel_if_check_kernel_status(&kern[physical_device_id]);\n      if (device_info[physical_device_id].mmd_dispatch->aocl_mmd_yield(\n              device_info[physical_device_id].handle))\n        keep_going = 1;\n    }\n  } while (keep_going);\n}\n\n// Host to host memory transfer (presently blocking)\nvoid acl_hal_mmd_copy_hostmem_to_hostmem(cl_event event, const void *src,\n                                         void *dest, size_t size) {\n  acl_assert_locked();\n\n  // Verify the callbacks are valid\n  assert(acl_event_update_fn != NULL);\n\n  // Host to host has nothing to do with the device\n  acl_event_update_fn(event, CL_RUNNING);\n  if (dest != src) {\n    safe_memcpy(dest, src, size, size, size);\n  }\n  acl_event_update_fn(event, CL_COMPLETE);\n}\n\n// Host to device-global-memory write\nvoid acl_hal_mmd_copy_hostmem_to_globalmem(cl_event event, const void *src,\n                                           void *dest, size_t size) {\n  int s;\n  unsigned int physical_device_id;\n  acl_assert_locked();\n  physical_device_id = ACL_GET_PHYSICAL_ID(dest);\n  assert(physical_device_id < num_physical_devices);\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(4, \"HAL Writing to memory: %zu bytes %zx -> %zx\\n\",\n                            size, (size_t)src, (size_t)dest);\n\n  // Verify the callbacks are valid\n  assert(acl_event_update_fn != NULL);\n  acl_event_update_fn(event, CL_RUNNING); // MMD device will send complete\n\n  s = device_info[physical_device_id].mmd_dispatch->aocl_mmd_write(\n      device_info[physical_device_id].handle, (aocl_mmd_op_t)event, size, src,\n      device_info[physical_device_id].mmd_ifaces.memory_interface,\n      (size_t)ACL_STRIP_PHYSICAL_ID(dest));\n  assert(s == 0 && \"mmd read/write failed\");\n}\n\n// Device-global to host memory read\nvoid acl_hal_mmd_copy_globalmem_to_hostmem(cl_event event, const void *src,\n                                           void *dest, size_t size) {\n  int s;\n  unsigned int physical_device_id;\n  acl_assert_locked();\n  physical_device_id = ACL_GET_PHYSICAL_ID(src);\n  assert(physical_device_id < num_physical_devices);\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(4,\n                            \"HAL Reading from memory: %zu bytes %zx -> %zx\\n\",\n                            size, (size_t)src, (size_t)dest);\n\n  // Verify the callbacks are valid\n  assert(acl_event_update_fn != NULL);\n  acl_event_update_fn(event, CL_RUNNING); // MMD device will send complete\n\n  s = device_info[physical_device_id].mmd_dispatch->aocl_mmd_read(\n      device_info[physical_device_id].handle, (aocl_mmd_op_t)event, size, dest,\n      device_info[physical_device_id].mmd_ifaces.memory_interface,\n      (size_t)ACL_STRIP_PHYSICAL_ID(src));\n  assert(s == 0 && \"mmd read/write failed\");\n}\n\n// Device-global to device-global\nvoid acl_hal_mmd_copy_globalmem_to_globalmem(cl_event event, const void *src,\n                                             void *dest, size_t size) {\n  int s = -1;\n  unsigned int physical_device_id_src;\n  unsigned int physical_device_id_dst;\n  acl_assert_locked();\n\n  physical_device_id_src = ACL_GET_PHYSICAL_ID(src);\n  assert(physical_device_id_src < num_physical_devices);\n  physical_device_id_dst = ACL_GET_PHYSICAL_ID(dest);\n  assert(physical_device_id_dst < num_physical_devices);\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(4, \"HAL Copying memory: %zu bytes %zx -> %zx\\n\",\n                            size, (size_t)src, (size_t)dest);\n\n  // Verify the callbacks are valid\n  assert(acl_event_update_fn != NULL);\n  acl_event_update_fn(event, CL_RUNNING); // MMD device will send complete\n\n  if (physical_device_id_src == physical_device_id_dst) {\n    // Let the MMD provider do the intra-device copy.\n    s = device_info[physical_device_id_src].mmd_dispatch->aocl_mmd_copy(\n        device_info[physical_device_id_src].handle, (aocl_mmd_op_t)event, size,\n        device_info[physical_device_id_src].mmd_ifaces.memory_interface,\n        (size_t)ACL_STRIP_PHYSICAL_ID(src),\n        (size_t)ACL_STRIP_PHYSICAL_ID(dest));\n  } else {\n    // Copy from device to device via host memory.\n\n    // Let's use a small static buffer as we move data from device to device\n#define BLOCK_SIZE (8 * 1024 * 1024)\n#ifdef _WIN32\n    __declspec(align(128)) static unsigned char data[2][BLOCK_SIZE];\n#else\n    static unsigned char data[2][BLOCK_SIZE] __attribute__((aligned(128)));\n#endif\n\n    size_t transfer_size;\n    size_t transfer_size_next;\n    int buffer;\n\n    // We're going to block here\n    transfer_size = (size > BLOCK_SIZE) ? BLOCK_SIZE : size;\n    transfer_size_next = 0;\n\n    // Read the initial block into data[0]\n    s = device_info[physical_device_id_src].mmd_dispatch->aocl_mmd_read(\n        device_info[physical_device_id_src].handle, NULL, transfer_size,\n        &data[0][0],\n        device_info[physical_device_id_src].mmd_ifaces.memory_interface,\n        (size_t)ACL_STRIP_PHYSICAL_ID(src));\n    src = (const char *)src + transfer_size;\n    size -= transfer_size;\n\n    buffer = 0;\n\n    device_info[physical_device_id_src]\n        .mmd_dispatch->aocl_mmd_set_status_handler(\n            device_info[physical_device_id_src].handle,\n            l_dev_to_dev_copy_handler, NULL);\n    device_info[physical_device_id_dst]\n        .mmd_dispatch->aocl_mmd_set_status_handler(\n            device_info[physical_device_id_dst].handle,\n            l_dev_to_dev_copy_handler, NULL);\n\n    do {\n\n      transfer_size_next = (size > BLOCK_SIZE) ? BLOCK_SIZE : size;\n\n      src_dev_done = 0;\n      dst_dev_done = 0;\n\n      device_info[physical_device_id_dst].mmd_dispatch->aocl_mmd_write(\n          device_info[physical_device_id_dst].handle, &dst_dev_done,\n          transfer_size, &data[buffer][0],\n          device_info[physical_device_id_dst].mmd_ifaces.memory_interface,\n          (size_t)ACL_STRIP_PHYSICAL_ID(dest));\n\n      if (transfer_size_next)\n        device_info[physical_device_id_src].mmd_dispatch->aocl_mmd_read(\n            device_info[physical_device_id_src].handle, &src_dev_done,\n            transfer_size_next, &data[1 - buffer][0],\n            device_info[physical_device_id_src].mmd_ifaces.memory_interface,\n            (size_t)ACL_STRIP_PHYSICAL_ID(src));\n      else\n        src_dev_done = 1;\n\n      while (!(src_dev_done && dst_dev_done)) {\n        device_info[physical_device_id_src].mmd_dispatch->aocl_mmd_yield(\n            device_info[physical_device_id_src].handle);\n        device_info[physical_device_id_dst].mmd_dispatch->aocl_mmd_yield(\n            device_info[physical_device_id_dst].handle);\n      }\n\n      src = (const char *)src + transfer_size_next;\n      dest = (char *)dest + transfer_size;\n\n      size -= transfer_size_next;\n      transfer_size = transfer_size_next;\n\n      // Flip the buffers\n      buffer = 1 - buffer;\n    } while (transfer_size_next > 0);\n    device_info[physical_device_id_src]\n        .mmd_dispatch->aocl_mmd_set_status_handler(\n            device_info[physical_device_id_src].handle,\n            acl_hal_mmd_status_handler, NULL);\n    device_info[physical_device_id_dst]\n        .mmd_dispatch->aocl_mmd_set_status_handler(\n            device_info[physical_device_id_dst].handle,\n            acl_hal_mmd_status_handler, NULL);\n    acl_event_update_fn(event, CL_COMPLETE);\n  }\n  assert(s == 0 && \"mmd read/write failed\");\n}\n\n// Launch a kernel\nvoid acl_hal_mmd_launch_kernel(unsigned int physical_device_id,\n                               acl_kernel_invocation_wrapper_t *wrapper) {\n  acl_assert_locked();\n\n  const auto &streaming_args = wrapper->streaming_args;\n  if (!streaming_args.empty()) {\n    device_info[physical_device_id]\n        .mmd_dispatch->aocl_mmd_simulation_streaming_kernel_args(\n            device_info[physical_device_id].handle, streaming_args);\n  }\n\n  acl_kernel_if_launch_kernel(&kern[physical_device_id], wrapper);\n}\n\nvoid acl_hal_mmd_unstall_kernel(unsigned int physical_device_id,\n                                int activation_id) {\n  acl_assert_locked();\n  acl_kernel_if_unstall_kernel(&kern[physical_device_id], activation_id);\n}\n\nvoid acl_hal_mmd_reset_kernels(cl_device_id device) {\n  unsigned int physical_device_id = device->def.physical_device_id;\n  acl_kernel_if_reset(&kern[physical_device_id]);\n  for (unsigned int k = 0; k < kern[physical_device_id].num_accel; ++k) {\n    for (unsigned int i = 0;\n         i < kern[physical_device_id].accel_invoc_queue_depth[k] + 1; ++i) {\n      int activation_id = kern[physical_device_id].accel_job_ids[k][i];\n      if (kern[physical_device_id].accel_job_ids[k][i] >= 0) {\n        kern[physical_device_id].accel_job_ids[k][i] = -1;\n        acl_kernel_update_fn(activation_id,\n                             -1); // Signal that it finished with error, since\n                                  // we forced it to finish\n      }\n    }\n  }\n}\n\n// ************************ Host pipe functions *************************\nint acl_hal_mmd_hostchannel_create(unsigned int physical_device_id,\n                                   char *channel_name, size_t num_packets,\n                                   size_t packet_size, int direction) {\n  int pcie_dev_handle;\n\n  pcie_dev_handle = device_info[physical_device_id].handle;\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_create);\n  return device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_hostchannel_create(\n          pcie_dev_handle, channel_name, num_packets * packet_size, direction);\n}\n\nint acl_hal_mmd_hostchannel_destroy(unsigned int physical_device_id,\n                                    int channel_handle) {\n  int pcie_dev_handle;\n\n  pcie_dev_handle = device_info[physical_device_id].handle;\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_destroy);\n  return device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_hostchannel_destroy(pcie_dev_handle,\n                                                  channel_handle);\n}\n\nsize_t acl_hal_mmd_hostchannel_pull(unsigned int physical_device_id,\n                                    int channel_handle, void *host_buffer,\n                                    size_t read_size, int *status) {\n  size_t buffer_size = 0;\n  size_t pulled;\n  int pcie_dev_handle;\n  void *pull_buffer;\n\n  *status = 0;\n  pcie_dev_handle = device_info[physical_device_id].handle;\n\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_get_buffer);\n  pull_buffer = device_info[physical_device_id]\n                    .mmd_dispatch->aocl_mmd_hostchannel_get_buffer(\n                        pcie_dev_handle, channel_handle, &buffer_size, status);\n\n  if ((NULL == pull_buffer) || (0 == buffer_size)) {\n    return 0;\n  }\n\n  // How much can be pulled to user buffer\n  buffer_size = (read_size > buffer_size) ? buffer_size : read_size;\n\n  // Copy the data into the user buffer\n  safe_memcpy(host_buffer, pull_buffer, buffer_size, buffer_size, buffer_size);\n\n  // acknowledge host channel MMD that copy of data from its buffer is done\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_ack_buffer);\n  pulled = device_info[physical_device_id]\n               .mmd_dispatch->aocl_mmd_hostchannel_ack_buffer(\n                   pcie_dev_handle, channel_handle, buffer_size, status);\n\n  // This shouldn't happen, but if the amount of data that was pulled, and the\n  // amount of data get buffer said was available are not equal, something went\n  // wrong\n  assert(pulled == buffer_size);\n\n  return pulled;\n}\n\nsize_t acl_hal_mmd_hostchannel_push(unsigned int physical_device_id,\n                                    int channel_handle, const void *host_buffer,\n                                    size_t write_size, int *status) {\n  size_t buffer_size = 0;\n  size_t pushed;\n  int pcie_dev_handle;\n  void *push_buffer;\n\n  *status = 0;\n  pcie_dev_handle = device_info[physical_device_id].handle;\n\n  // get the pointer to host channel push buffer\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_get_buffer);\n  push_buffer = device_info[physical_device_id]\n                    .mmd_dispatch->aocl_mmd_hostchannel_get_buffer(\n                        pcie_dev_handle, channel_handle, &buffer_size, status);\n\n  if ((NULL == push_buffer) || (0 == buffer_size)) {\n    return 0;\n  }\n\n  // How much can be pushed to push buffer\n  buffer_size = (write_size > buffer_size) ? buffer_size : write_size;\n\n  // Copy the data into the push buffer\n\n  safe_memcpy(push_buffer, host_buffer, buffer_size, buffer_size, buffer_size);\n\n  // Acknowledge host channel MMD that copy of data to its buffer is done\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_ack_buffer);\n  pushed = device_info[physical_device_id]\n               .mmd_dispatch->aocl_mmd_hostchannel_ack_buffer(\n                   pcie_dev_handle, channel_handle, buffer_size, status);\n\n  // This shouldn't happen, but if the amount of data that was pushed, and the\n  // amount of space get buffer said was available are not equal, something went\n  // wrong\n  assert(pushed == buffer_size);\n  return pushed;\n}\n\nsize_t acl_hal_mmd_hostchannel_pull_no_ack(unsigned int physical_device_id,\n                                           int channel_handle,\n                                           void *host_buffer, size_t read_size,\n                                           int *status) {\n  size_t buffer_size = 0;\n  size_t pulled;\n  int pcie_dev_handle;\n  void *pull_buffer;\n\n  *status = 0;\n  pcie_dev_handle = device_info[physical_device_id].handle;\n\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_get_buffer);\n  pull_buffer = device_info[physical_device_id]\n                    .mmd_dispatch->aocl_mmd_hostchannel_get_buffer(\n                        pcie_dev_handle, channel_handle, &buffer_size, status);\n\n  if ((NULL == pull_buffer) || (0 == buffer_size)) {\n    return 0;\n  }\n\n  // How much can be pulled to user buffer\n  buffer_size = (read_size > buffer_size) ? buffer_size : read_size;\n\n  // Copy the data into the user buffer\n  safe_memcpy(host_buffer, pull_buffer, buffer_size, buffer_size, buffer_size);\n\n  pulled = buffer_size;\n\n  return pulled;\n}\n\nsize_t acl_hal_mmd_hostchannel_push_no_ack(unsigned int physical_device_id,\n                                           int channel_handle,\n                                           const void *host_buffer,\n                                           size_t write_size, int *status) {\n  size_t buffer_size = 0;\n  size_t pushed;\n  int pcie_dev_handle;\n  void *push_buffer;\n\n  *status = 0;\n  pcie_dev_handle = device_info[physical_device_id].handle;\n\n  // get the pointer to host channel push buffer\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_get_buffer);\n\n  push_buffer = device_info[physical_device_id]\n                    .mmd_dispatch->aocl_mmd_hostchannel_get_buffer(\n                        pcie_dev_handle, channel_handle, &buffer_size, status);\n\n  if ((NULL == push_buffer) || (0 == buffer_size)) {\n    return 0;\n  }\n\n  // How much can be pushed to push buffer\n  buffer_size = (write_size > buffer_size) ? buffer_size : write_size;\n\n  // Copy the data into the push buffer\n\n  safe_memcpy(push_buffer, host_buffer, buffer_size, buffer_size, buffer_size);\n\n  pushed = buffer_size;\n  return pushed;\n}\n\nvoid *acl_hal_mmd_hostchannel_get_buffer(unsigned int physical_device_id,\n                                         int channel_handle,\n                                         size_t *buffer_size, int *status) {\n  int pcie_dev_handle;\n\n  pcie_dev_handle = device_info[physical_device_id].handle;\n  *status = 0;\n\n  // get the pointer to host channel mmd buffer\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_get_buffer);\n  return device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_hostchannel_get_buffer(\n          pcie_dev_handle, channel_handle, buffer_size, status);\n}\n\nsize_t acl_hal_mmd_hostchannel_ack_buffer(unsigned int physical_device_id,\n                                          int channel_handle, size_t ack_size,\n                                          int *status) {\n  int pcie_dev_handle;\n\n  pcie_dev_handle = device_info[physical_device_id].handle;\n  *status = 0;\n\n  // ack the host channel mmd buffer\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_ack_buffer);\n  return device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_hostchannel_ack_buffer(\n          pcie_dev_handle, channel_handle, ack_size, status);\n}\n\nsize_t acl_hal_mmd_hostchannel_sideband_pull_no_ack(\n    unsigned int physical_device_id, unsigned int port_name, int channel_handle,\n    void *host_buffer, size_t read_size, int *status) {\n\n  size_t buffer_size = 0;\n  size_t pulled;\n  int pcie_dev_handle;\n  void *pull_buffer;\n\n  *status = 0;\n  pcie_dev_handle = device_info[physical_device_id].handle;\n\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_get_sideband_buffer);\n  pull_buffer = device_info[physical_device_id]\n                    .mmd_dispatch->aocl_mmd_hostchannel_get_sideband_buffer(\n                        pcie_dev_handle, channel_handle,\n                        static_cast<aocl_mmd_hostchannel_port_id_t>(port_name),\n                        &buffer_size, status);\n\n  if ((NULL == pull_buffer) || (0 == buffer_size)) {\n    return 0;\n  }\n\n  // How much can be pulled to user buffer\n  buffer_size = (read_size > buffer_size) ? buffer_size : read_size;\n\n  // Copy the data into the user buffer\n  safe_memcpy(host_buffer, pull_buffer, buffer_size, buffer_size, buffer_size);\n\n  pulled = buffer_size;\n\n  return pulled;\n}\n\nsize_t acl_hal_mmd_hostchannel_sideband_push_no_ack(\n    unsigned int physical_device_id, unsigned int port_name, int channel_handle,\n    const void *host_buffer, size_t write_size, int *status) {\n\n  size_t buffer_size = 0;\n  size_t pushed;\n  int pcie_dev_handle;\n  void *push_buffer;\n\n  *status = 0;\n  pcie_dev_handle = device_info[physical_device_id].handle;\n\n  // get the pointer to host channel push buffer\n  assert(device_info[physical_device_id]\n             .mmd_dispatch->aocl_mmd_hostchannel_get_sideband_buffer);\n\n  push_buffer = device_info[physical_device_id]\n                    .mmd_dispatch->aocl_mmd_hostchannel_get_sideband_buffer(\n                        pcie_dev_handle, channel_handle,\n                        static_cast<aocl_mmd_hostchannel_port_id_t>(port_name),\n                        &buffer_size, status);\n\n  if ((NULL == push_buffer) || (0 == buffer_size)) {\n    return 0;\n  }\n\n  // How much can be pushed to push buffer\n  buffer_size = (write_size > buffer_size) ? buffer_size : write_size;\n\n  // Copy the data into the push buffer\n\n  safe_memcpy(push_buffer, host_buffer, buffer_size, buffer_size, buffer_size);\n\n  pushed = buffer_size;\n  return pushed;\n}\n// **********************************************************************\n\ncl_bool acl_hal_mmd_query_temperature(unsigned int physical_device_id,\n                                      cl_int *temp) {\n  float f;\n  acl_assert_locked();\n\n  device_info[physical_device_id].mmd_dispatch->aocl_mmd_get_info(\n      device_info[physical_device_id].handle, AOCL_MMD_TEMPERATURE,\n      sizeof(float), &f, NULL);\n  *temp = (cl_int)f;\n  return (cl_bool)1;\n}\n\nint acl_hal_mmd_get_device_official_name(unsigned int physical_device_id,\n                                         char *name, size_t size) {\n  int status;\n  acl_assert_locked();\n\n  status = device_info[physical_device_id].mmd_dispatch->aocl_mmd_get_info(\n      device_info[physical_device_id].handle, AOCL_MMD_BOARD_NAME, size, name,\n      NULL);\n  name[size - 1] = 0;\n  return status;\n}\n\nint acl_hal_mmd_get_device_vendor_name(unsigned int physical_device_id,\n                                       char *name, size_t size) {\n  int status;\n  acl_assert_locked();\n\n  status =\n      device_info[physical_device_id].mmd_dispatch->aocl_mmd_get_offline_info(\n          AOCL_MMD_VENDOR_NAME, size, name, NULL);\n  name[size - 1] = 0;\n  return status;\n}\n\n/**\n * Returns SVM capabilities as defined by OpenCL v2.0\n * May appear in 'value' bitmask: CL_DEVICE_SVM_COARSE_GRAIN_BUFFER,\n * CL_DEVICE_SVM_FINE_GRAIN_BUFFER, CL_DEVICE_SVM_FINE_GRAIN_SYSTEM\n * @param  physical_device_id which device to query\n * @param  value              bitmask of the type of SVM that is supported\n * @return                    1 if svm is supported, else 0\n */\nint acl_hal_mmd_has_svm_support(unsigned int physical_device_id, int *value) {\n  int status;\n  acl_assert_locked();\n  *value = 0;\n\n  // Before 14.1 all boards supported physical memory and did not support SVM.\n  // If this data is not defined and it is prior to 14.1, assume this board\n  // only support physical memory.\n  if (MMDVERSION_LESSTHAN(\n          device_info[physical_device_id].mmd_dispatch->mmd_version, 14.1)) {\n    return 0;\n  } else {\n    int mem_types = 0;\n    status =\n        device_info[physical_device_id].mmd_dispatch->aocl_mmd_get_offline_info(\n            AOCL_MMD_MEM_TYPES_SUPPORTED, sizeof(int), &mem_types, NULL);\n    if (status >= 0) {\n      if (mem_types & AOCL_MMD_SVM_COARSE_GRAIN_BUFFER) {\n        *value |= CL_DEVICE_SVM_COARSE_GRAIN_BUFFER;\n      }\n      if (mem_types & AOCL_MMD_SVM_FINE_GRAIN_BUFFER) {\n        *value |= CL_DEVICE_SVM_FINE_GRAIN_BUFFER;\n      }\n      if (mem_types & AOCL_MMD_SVM_FINE_GRAIN_SYSTEM) {\n        *value |= CL_DEVICE_SVM_FINE_GRAIN_SYSTEM;\n      }\n    }\n  }\n\n  return (*value > 0);\n}\n\n/**\n * Returns if device supports global physical memory\n * @param  physical_device_id which device to query\n * @return                    1 if supported, else 0\n */\nint acl_hal_mmd_has_physical_mem(unsigned int physical_device_id) {\n  acl_assert_locked();\n\n  // Before 14.1 all boards supported physical memory and did not support SVM.\n  // If this data is not defined and it is prior to 14.1, assume this board\n  // only support physical memory.\n  if (MMDVERSION_LESSTHAN(\n          device_info[physical_device_id].mmd_dispatch->mmd_version, 14.1)) {\n    return 1;\n  } else {\n    int mem_types = 0;\n    int ret = 0;\n    int status =\n        device_info[physical_device_id].mmd_dispatch->aocl_mmd_get_offline_info(\n            AOCL_MMD_MEM_TYPES_SUPPORTED, sizeof(int), &mem_types, NULL);\n    if (status >= 0) {\n      ret = (mem_types & AOCL_MMD_PHYSICAL_MEMORY);\n    }\n\n    return ret;\n  }\n}\n\n/**\n * Returns if a set of devices all support buffer location mem property\n * @param  a vector of devices on which to query\n * @return 1 if supported, else 0\n */\nint acl_hal_mmd_support_buffer_location(\n    const std::vector<cl_device_id> &devices) {\n  acl_assert_locked();\n\n  int bl_supported = 1;\n  for (const auto &device : devices) {\n    unsigned int physical_device_id = device->def.physical_device_id;\n    if (MMDVERSION_LESSTHAN(\n            device_info[physical_device_id].mmd_dispatch->mmd_version, 24.2)) {\n      bl_supported = 0;\n    }\n  }\n\n  return bl_supported;\n}\n\n#ifdef _WIN32\n// Query the system timer, return a timer value in ns\ncl_ulong acl_hal_mmd_get_timestamp() {\n  LARGE_INTEGER li;\n  double seconds;\n  INT64 ticks;\n\n  const INT64 NS_PER_S = 1000000000;\n  acl_assert_locked_or_sig();\n\n  QueryPerformanceCounter(&li);\n  ticks = li.QuadPart;\n  seconds = ticks / (double)m_ticks_per_second;\n  return (cl_ulong)((double)seconds * (double)NS_PER_S + 0.5);\n}\n#else\n// Query the system timer, return a timer value in ns\ncl_ulong acl_hal_mmd_get_timestamp() {\n  struct timespec a;\n  const cl_ulong NS_PER_S = 1000000000;\n  acl_assert_locked_or_sig();\n  // Must use the MONOTONIC clock because the REALTIME clock\n  // can go backwards due to adjustments by NTP for clock drift.\n  // The MONOTONIC clock provides a timestamp since some fixed point in\n  // the past, which might be system boot time or the start of the Unix\n  // epoch.  This matches the Windows QueryPerformanceCounter semantics.\n  // The MONOTONIC clock is to be used for measuring time intervals, and\n  // fits the semantics of timestamps from the *device* perspective as defined\n  // in OpenCL for clGetEventProfilingInfo.\n#ifdef CLOCK_MONOTONIC_RAW\n  clock_gettime(CLOCK_MONOTONIC_RAW, &a);\n#else\n  clock_gettime(CLOCK_MONOTONIC, &a);\n#endif\n\n  return (cl_ulong)(a.tv_nsec) + (cl_ulong)(a.tv_sec * NS_PER_S);\n}\n#endif\n\n// ********************* Profile hardware accessors *********************\nint acl_hal_mmd_get_profile_data(unsigned int physical_device_id,\n                                 unsigned int accel_id, uint64_t *data,\n                                 unsigned int length) {\n  acl_assert_locked_or_sig();\n  assert(physical_device_id < num_physical_devices);\n  return acl_kernel_if_get_profile_data(&kern[physical_device_id], accel_id,\n                                        data, length);\n}\n\nint acl_hal_mmd_reset_profile_counters(unsigned int physical_device_id,\n                                       unsigned int accel_id) {\n  acl_assert_locked_or_sig();\n  assert(physical_device_id < num_physical_devices);\n  return acl_kernel_if_reset_profile_counters(&kern[physical_device_id],\n                                              accel_id);\n}\n\nint acl_hal_mmd_disable_profile_counters(unsigned int physical_device_id,\n                                         unsigned int accel_id) {\n  acl_assert_locked();\n  assert(physical_device_id < num_physical_devices);\n  return acl_kernel_if_disable_profile_counters(&kern[physical_device_id],\n                                                accel_id);\n}\n\nint acl_hal_mmd_enable_profile_counters(unsigned int physical_device_id,\n                                        unsigned int accel_id) {\n  acl_assert_locked();\n  assert(physical_device_id < num_physical_devices);\n  return acl_kernel_if_enable_profile_counters(&kern[physical_device_id],\n                                               accel_id);\n}\n\nint acl_hal_mmd_set_profile_shared_control(unsigned int physical_device_id,\n                                           unsigned int accel_id) {\n  acl_assert_locked();\n  assert(physical_device_id < num_physical_devices);\n  return acl_kernel_if_set_profile_shared_control(&kern[physical_device_id],\n                                                  accel_id);\n}\n\nint acl_hal_mmd_set_profile_start_count(unsigned int physical_device_id,\n                                        unsigned int accel_id, uint64_t value) {\n  acl_assert_locked();\n  assert(physical_device_id < num_physical_devices);\n  return acl_kernel_if_set_profile_start_cycle(&kern[physical_device_id],\n                                               accel_id, value);\n}\n\nint acl_hal_mmd_set_profile_stop_count(unsigned int physical_device_id,\n                                       unsigned int accel_id, uint64_t value) {\n  acl_assert_locked();\n  assert(physical_device_id < num_physical_devices);\n  return acl_kernel_if_set_profile_stop_cycle(&kern[physical_device_id],\n                                              accel_id, value);\n}\n// **********************************************************************\n\n// Convert kernel and pll accessors to aocl_mmd\nstatic size_t acl_kernel_if_read(acl_bsp_io *io, dev_addr_t src, char *dest,\n                                 size_t size) {\n  acl_assert_locked_or_sig();\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(5,\n                            \"HAL Reading from Kernel: %zu bytes %zx -> %zx\\n\",\n                            size, (size_t)src, (size_t)dest);\n  return io->device_info->mmd_dispatch->aocl_mmd_read(\n             io->device_info->handle, NULL, size, (void *)dest,\n             io->device_info->mmd_ifaces.kernel_interface, (size_t)src) == 0\n             ? size\n             : 0;\n}\n\nstatic size_t acl_kernel_if_write(acl_bsp_io *io, dev_addr_t dest,\n                                  const char *src, size_t size) {\n  acl_assert_locked_or_sig();\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(5, \"HAL Writing to Kernel: %zu bytes %zx -> %zx\\n\",\n                            size, (size_t)src, (size_t)dest);\n  return io->device_info->mmd_dispatch->aocl_mmd_write(\n             io->device_info->handle, NULL, size, (const void *)src,\n             io->device_info->mmd_ifaces.kernel_interface, (size_t)dest) == 0\n             ? size\n             : 0;\n}\n\nstatic size_t acl_pll_read(acl_bsp_io *io, dev_addr_t src, char *dest,\n                           size_t size) {\n  acl_assert_locked_or_sig();\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(5, \"HAL Reading from PLL: %zu bytes %zx -> %zx\\n\",\n                            size, (size_t)src, (size_t)dest);\n  return io->device_info->mmd_dispatch->aocl_mmd_read(\n             io->device_info->handle, NULL, size, (void *)dest,\n             io->device_info->mmd_ifaces.pll_interface, (size_t)src) == 0\n             ? size\n             : 0;\n}\n\nstatic size_t acl_pll_write(acl_bsp_io *io, dev_addr_t dest, const char *src,\n                            size_t size) {\n  acl_assert_locked_or_sig();\n\n  ACL_HAL_DEBUG_MSG_VERBOSE(5, \"HAL Writing to PLL: %zu bytes %zx -> %zx\\n\",\n                            size, (size_t)src, (size_t)dest);\n  return io->device_info->mmd_dispatch->aocl_mmd_write(\n             io->device_info->handle, NULL, size, (const void *)src,\n             io->device_info->mmd_ifaces.pll_interface, (size_t)dest) == 0\n             ? size\n             : 0;\n}\n\nstatic time_ns acl_bsp_get_timestamp() {\n  return (time_ns)acl_hal_mmd_get_timestamp();\n}\n\nstatic void *\nacl_hal_get_board_extension_function_address(const char *func_name,\n                                             unsigned int physical_device_id) {\n  char *error_msg;\n  acl_assert_locked();\n\n  auto *fn_ptr =\n      my_dlsym(device_info[physical_device_id].mmd_dispatch->mmd_library,\n               func_name, &error_msg);\n\n  if (!fn_ptr) {\n    printf(\"Error: Unable to find function name %s in board library %s (%p)\\n\",\n           func_name,\n           device_info[physical_device_id].mmd_dispatch->library_name.c_str(),\n           device_info[physical_device_id].mmd_dispatch->mmd_library);\n    return nullptr;\n  }\n\n  return fn_ptr;\n}\n\n// *************************** USM functions ****************************\n// Shared memory allocator\nvoid *acl_hal_mmd_legacy_shared_alloc(cl_context context, size_t size,\n                                      unsigned long long *device_ptr_out) {\n  unsigned int idevice;\n  acl_mmd_dispatch_t *mmd_dispatch_found = NULL;\n  acl_assert_locked();\n\n  if (num_board_pkgs == 0) {\n    printf(\"mmd legacy_shared_alloc: No board libaries found so cannot \"\n           \"allocate shared memory\\n\");\n    return NULL;\n  }\n  // Since this function is called for the context and not for a specific\n  // device, it is only supported when only one board package is used within\n  // that context.\n  if (num_board_pkgs > 1) {\n    mmd_dispatch_found = device_info[0].mmd_dispatch;\n    for (idevice = 1; idevice < context->num_devices; ++idevice) {\n      unsigned device_id = context->device[idevice]->def.physical_device_id;\n      if (mmd_dispatch_found != device_info[device_id].mmd_dispatch) {\n        printf(\"mmd legacy_shared_alloc: Can only allocate shared memory from \"\n               \"a single board library but context contains multiple board \"\n               \"libraries\\n\");\n        return NULL;\n      }\n    }\n  }\n\n  if (MMDVERSION_LESSTHAN(min_MMD_version, 20.3)) {\n    // Deprecated\n    return device_info[0].mmd_dispatch->aocl_mmd_shared_mem_alloc(\n        device_info[0].handle, size, device_ptr_out);\n  } else {\n    int error = 0;\n    std::vector<cl_device_id> devices = std::vector<cl_device_id>(\n        context->device, context->device + context->num_devices);\n    void *mem = acl_hal_mmd_host_alloc(devices, size, 0, nullptr, &error);\n    device_ptr_out = static_cast<unsigned long long *>(mem);\n    if (error) {\n      switch (error) {\n      case CL_OUT_OF_HOST_MEMORY:\n        if (debug_verbosity >= 1)\n          printf(\"mmd legacy_shared_alloc: Unable to allocate %zu bytes\\n\",\n                 size);\n        break;\n      case CL_INVALID_VALUE:\n        if (debug_verbosity >= 1)\n          printf(\"mmd legacy_shared_alloc: Unsupported alignment of 0\\n\");\n        break;\n      case CL_INVALID_PROPERTY:\n        if (debug_verbosity >= 1)\n          printf(\"mmd legacy_shared_alloc: Unsuported properties\\n\");\n        break;\n      default:\n        if (debug_verbosity >= 1)\n          printf(\"mmd legacy_shared_alloc: Unable to allocate memory\\n\");\n        break;\n      }\n    }\n    return mem;\n  }\n}\n\nvoid acl_hal_mmd_legacy_shared_free(cl_context context, void *host_ptr,\n                                    size_t size) {\n  unsigned int idevice;\n  acl_mmd_dispatch_t *mmd_dispatch_found = NULL;\n  acl_assert_locked();\n\n  if (num_board_pkgs == 0) {\n    printf(\"mmd legacy_shared_free: No board libaries found so cannot free \"\n           \"shared memory\\n\");\n    return;\n  }\n  // Since this function is called for the context and not for a specific\n  // device, it is only supported when only one board package is used within\n  // that context.\n  if (num_board_pkgs > 1) {\n    mmd_dispatch_found = device_info[0].mmd_dispatch;\n    for (idevice = 1; idevice < context->num_devices; ++idevice) {\n      unsigned device_id = context->device[idevice]->def.physical_device_id;\n      if (mmd_dispatch_found != device_info[device_id].mmd_dispatch) {\n        printf(\n            \"mmd legacy_shared_free: Can only free shared memory from a single \"\n            \"board library but context contains multiple board libraries\\n\");\n        return;\n      }\n    }\n  }\n\n  if (MMDVERSION_LESSTHAN(min_MMD_version, 20.3)) {\n    // Deprecated\n    device_info[0].mmd_dispatch->aocl_mmd_shared_mem_free(device_info[0].handle,\n                                                          host_ptr, size);\n  } else {\n    int error = acl_hal_mmd_free(context, host_ptr);\n    if (error) {\n      switch (error) {\n      case CL_INVALID_VALUE:\n        if (debug_verbosity >= 1)\n          printf(\"mmd legacy_shared_free: Invalid pointer provided\\n\");\n        break;\n      default:\n        if (debug_verbosity >= 1)\n          printf(\"mmd legacy_shared_free: Unknown error during deallocation\\n\");\n        break;\n      }\n    }\n  }\n}\n\nvoid *acl_hal_mmd_shared_alloc(cl_device_id device, size_t size,\n                               size_t alignment, mem_properties_t *properties,\n                               int *error) {\n  // Note we do not support devices in the same context with different MMDs\n  // Safe to get the mmd handle from first device\n  void *result = NULL;\n  unsigned int physical_device_id = device->def.physical_device_id;\n  acl_mmd_dispatch_t *dispatch = device_info[physical_device_id].mmd_dispatch;\n  if (!dispatch->aocl_mmd_shared_alloc) {\n    // Not implemented by the board.\n    return result;\n  }\n\n  int handle = device_info[physical_device_id].handle;\n  result = dispatch->aocl_mmd_shared_alloc(\n      handle, size, alignment, (aocl_mmd_mem_properties_t *)properties, error);\n\n  if (error) {\n    switch (*error) {\n    case AOCL_MMD_ERROR_INVALID_HANDLE:\n      assert(*error != AOCL_MMD_ERROR_INVALID_HANDLE &&\n             \"Error: Invalid device provided\");\n      break;\n    case AOCL_MMD_ERROR_OUT_OF_MEMORY:\n      *error = CL_OUT_OF_HOST_MEMORY;\n      break;\n    case AOCL_MMD_ERROR_UNSUPPORTED_ALIGNMENT:\n      *error = CL_INVALID_VALUE;\n      break;\n    case AOCL_MMD_ERROR_UNSUPPORTED_PROPERTY:\n      *error = CL_INVALID_PROPERTY;\n      break;\n    }\n  }\n  return result;\n}\n\nvoid *acl_hal_mmd_host_alloc(const std::vector<cl_device_id> &devices,\n                             size_t size, size_t alignment,\n                             mem_properties_t *properties, int *error) {\n  // Note we do not support devices in the same context with different MMDs\n  // Safe to get the mmd handle from first device\n  void *result = NULL;\n  assert(!devices.empty());\n  unsigned int physical_device_id = devices[0]->def.physical_device_id;\n  acl_mmd_dispatch_t *dispatch = device_info[physical_device_id].mmd_dispatch;\n  if (!dispatch->aocl_mmd_host_alloc) {\n    // Not implemented by the board. It is safe to assume buffer is forgotten\n    return result;\n  }\n  int *handles = acl_new_arr<int>(devices.size());\n  if (handles == NULL) {\n    return result;\n  }\n\n  for (size_t i = 0; i < devices.size(); i++) {\n    physical_device_id = devices[i]->def.physical_device_id;\n    handles[i] = device_info[physical_device_id].handle;\n  }\n\n  result = dispatch->aocl_mmd_host_alloc(\n      handles, devices.size(), size, alignment,\n      (aocl_mmd_mem_properties_t *)properties, error);\n  acl_delete_arr(handles);\n\n  if (error) {\n    switch (*error) {\n    case AOCL_MMD_ERROR_INVALID_HANDLE:\n      assert(*error != AOCL_MMD_ERROR_INVALID_HANDLE &&\n             \"Error: Invalid device provided\");\n      break;\n    case AOCL_MMD_ERROR_OUT_OF_MEMORY:\n      *error = CL_OUT_OF_HOST_MEMORY;\n      break;\n    case AOCL_MMD_ERROR_UNSUPPORTED_ALIGNMENT:\n      *error = CL_INVALID_VALUE;\n      break;\n    case AOCL_MMD_ERROR_UNSUPPORTED_PROPERTY:\n      *error = CL_INVALID_PROPERTY;\n      break;\n    }\n  }\n  return result;\n}\n\nint acl_hal_mmd_free(cl_context context, void *mem) {\n  // This call is device agnostic get the mmd handle from first device\n  // Note we do not support devices in the same context with different MMDs\n  cl_device_id device = context->device[0];\n  unsigned int physical_device_id = device->def.physical_device_id;\n  acl_mmd_dispatch_t *dispatch = device_info[physical_device_id].mmd_dispatch;\n  int result = 0;\n\n  if (!dispatch->aocl_mmd_free) {\n    // Not implemented by the board. It is safe to assume buffer is forgotten\n    return result;\n  }\n  result = dispatch->aocl_mmd_free(mem);\n  switch (result) {\n  case AOCL_MMD_ERROR_INVALID_POINTER:\n    return CL_INVALID_VALUE;\n  default:\n    return result;\n  }\n}\n// **********************************************************************\n\nsize_t acl_hal_mmd_read_csr(unsigned int physical_device_id, uintptr_t offset,\n                            void *ptr, size_t size) {\n  return device_info[physical_device_id].mmd_dispatch->aocl_mmd_read(\n      device_info[physical_device_id].handle, NULL, size, (void *)ptr,\n      device_info[physical_device_id].mmd_ifaces.kernel_interface,\n      (size_t)offset);\n}\n\nsize_t acl_hal_mmd_write_csr(unsigned int physical_device_id, uintptr_t offset,\n                             const void *ptr, size_t size) {\n  return device_info[physical_device_id].mmd_dispatch->aocl_mmd_write(\n      device_info[physical_device_id].handle, NULL, size, (const void *)ptr,\n      device_info[physical_device_id].mmd_ifaces.kernel_interface,\n      (size_t)offset);\n}\n\nvoid acl_hal_mmd_simulation_streaming_kernel_start(\n    unsigned int physical_device_id, const std::string &kernel_name,\n    const int accel_id, const bool accel_has_agent_args) {\n  device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_simulation_streaming_kernel_start(\n          device_info[physical_device_id].handle, kernel_name, accel_id,\n          accel_has_agent_args);\n}\n\nvoid acl_hal_mmd_simulation_streaming_kernel_done(\n    unsigned int physical_device_id, const std::string &kernel_name,\n    unsigned int &finish_counter) {\n  device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_simulation_streaming_kernel_done(\n          device_info[physical_device_id].handle, kernel_name, finish_counter);\n}\n\nvoid acl_hal_mmd_simulation_set_kernel_cra_address_map(\n    unsigned int physical_device_id,\n    const std::vector<uintptr_t> &kernel_csr_address_map) {\n  if (l_is_simulator_dispatch(device_info[physical_device_id].mmd_dispatch)) {\n    device_info[physical_device_id]\n        .mmd_dispatch->aocl_mmd_simulation_set_kernel_cra_address_map(\n            device_info[physical_device_id].handle, kernel_csr_address_map);\n  }\n}\n\nint acl_hal_mmd_simulation_device_global_interface_read(\n    unsigned int physical_device_id, const char *interface_name,\n    void *host_addr, size_t dev_addr, size_t size) {\n  return device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_simulation_device_global_interface_read(\n          device_info[physical_device_id].handle, interface_name, host_addr,\n          dev_addr, size);\n}\n\nint acl_hal_mmd_simulation_device_global_interface_write(\n    unsigned int physical_device_id, const char *interface_name,\n    const void *host_addr, size_t dev_addr, size_t size) {\n  return device_info[physical_device_id]\n      .mmd_dispatch->aocl_mmd_simulation_device_global_interface_write(\n          device_info[physical_device_id].handle, interface_name, host_addr,\n          dev_addr, size);\n}\n"
    },
    {
        "label": "acl_device_program_info.cpp",
        "data": "// Copyright (C) 2019-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <sstream>\n\n// External library headers.\n#include <acl_hash/acl_hash.h>\n\n// Internal headers.\n#include <acl_support.h>\n#include <acl_types.h>\n\nstatic std::string l_get_hashed_kernel_name(const std::string &kernel_name) {\n  acl_hash_context_t ctx;\n  acl_hash_init_sha1(&ctx);\n  std::string kernel_name_str(kernel_name);\n  acl_hash_add(&ctx, kernel_name_str.c_str(), kernel_name_str.length());\n  char sha1_kernel_name[ACL_HASH_SHA1_DIGEST_BUFSIZE];\n  acl_hash_hexdigest(&ctx, sha1_kernel_name, sizeof(sha1_kernel_name));\n\n  return std::string(\"kernel_\") + std::string(sha1_kernel_name);\n}\n\nacl_device_program_info_t::acl_device_program_info_t() {\n  device_binary.set_dev_prog(this);\n}\n\nacl_device_program_info_t::~acl_device_program_info_t() {\n  if (device) {\n    if (device->last_bin == &device_binary) {\n      device->last_bin = nullptr;\n    }\n    if (device->loaded_bin == &device_binary) {\n      device->loaded_bin = nullptr;\n    }\n\n    for (const auto &sdb : m_split_device_binaries) {\n      if (device->last_bin == &(sdb.second)) {\n        device->last_bin = nullptr;\n      }\n      if (device->loaded_bin == &(sdb.second)) {\n        device->loaded_bin = nullptr;\n      }\n    }\n  }\n}\n\nacl_device_binary_t &\nacl_device_program_info_t::add_split_binary(const std::string &hashed_name) {\n  assert(program->context->split_kernel);\n\n  auto &dev_bin = m_split_device_binaries[hashed_name];\n  dev_bin.set_dev_prog(this);\n  return dev_bin;\n}\n\nconst acl_device_binary_t *\nacl_device_program_info_t::get_device_binary(const std::string &name) const {\n  if (program->context->split_kernel) {\n    const auto sdb =\n        m_split_device_binaries.find(l_get_hashed_kernel_name(name));\n    if (sdb != m_split_device_binaries.end()) {\n      return &(sdb->second);\n    }\n  } else {\n    return &device_binary;\n  }\n\n  return nullptr;\n}\n\nconst acl_device_binary_t *\nacl_device_program_info_t::get_or_create_device_binary(\n    const std::string &name) {\n  if (program->context->split_kernel) {\n    const auto hashed_name = l_get_hashed_kernel_name(name);\n    const auto sdb = m_split_device_binaries.find(hashed_name);\n\n    // When the user has disabled preloading of kernels we need to\n    // load it now.\n    const char *preload = acl_getenv(\"CL_PRELOAD_SPLIT_BINARIES_INTELFPGA\");\n    if (sdb == m_split_device_binaries.end() && preload &&\n        std::string(preload) == \"0\") {\n      std::stringstream ss;\n      ss << program->context->program_library_root << \"/\" << hashed_name\n         << \".aocx\";\n      auto filename = ss.str();\n\n      auto &dev_bin = add_split_binary(hashed_name);\n      dev_bin.load_content(filename);\n\n      // Too late to handle errors now. User must guarantee all kernel files are\n      // present when they disable preloading device binaries. This is not a\n      // user-facing flow so it is okay to impose these restrictions.\n      auto status = dev_bin.load_binary_pkg(0, 1);\n      assert(status == CL_SUCCESS);\n      // Need to unload the binary and only load it on an as needed\n      // basis due to high memory usage when there are many split binaries.\n      dev_bin.unload_content();\n\n      return &dev_bin;\n    } else {\n      return get_device_binary(name);\n    }\n  }\n\n  return get_device_binary(name);\n}\n\nconst acl_accel_def_t *\nacl_device_program_info_t::get_kernel_accel_def(const std::string &name) const {\n  if (!program->context->uses_dynamic_sysdef) {\n    // When using pre-loaded binary mode the accel defs are stored in the\n    // device's device def instead of the binary's device def.\n    for (const auto &a : device->def.autodiscovery_def.accel) {\n      if (a.iface.name == name)\n        return &a;\n    }\n    return nullptr;\n  }\n\n  const auto *db = get_device_binary(name);\n  if (db) {\n    return db->get_accel_def(name);\n  } else {\n    return nullptr;\n  }\n}\n\nsize_t acl_device_program_info_t::get_num_kernels() const {\n  if (program->context->split_kernel) {\n    // Each split device binary represents one kernel.\n    return m_split_device_binaries.size();\n  } else {\n    return device_binary.get_devdef().autodiscovery_def.accel.size();\n  }\n}\n\nstd::set<std::string> acl_device_program_info_t::get_all_kernel_names() const {\n  std::set<std::string> result;\n\n  if (program->context->split_kernel) {\n    for (const auto &db : m_split_device_binaries) {\n      for (const auto &a : db.second.get_devdef().autodiscovery_def.accel) {\n        result.insert(a.iface.name);\n      }\n    }\n  } else {\n    for (const auto &a : device_binary.get_devdef().autodiscovery_def.accel) {\n      result.insert(a.iface.name);\n    }\n  }\n\n  assert(result.size() == get_num_kernels());\n\n  return result;\n}\n"
    },
    {
        "label": "acl_bsp_io.cpp",
        "data": "// Copyright (C) 2013-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// Internal headers.\n#include <acl_bsp_io.h>\n#include <acl_thread.h>\n\nint acl_bsp_io_is_valid(acl_bsp_io *bsp_io) {\n  acl_assert_locked_or_sig();\n  return ((bsp_io) && (bsp_io->read) && (bsp_io->write) &&\n          (bsp_io->get_time_ns))\n             ? 1\n             : 0;\n}\n"
    },
    {
        "label": "acl_offline_hal.cpp",
        "data": "// Copyright (C) 2012-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// HALs for use when we only have offline devices or emulates them.\n//\n// Device memory is emulated via host memory.\n// Kernels launch and complete right away, but do no computation.\n//\n\n// System headers.\n#include <assert.h>\n#include <stdarg.h>\n#include <stdio.h>\n#include <string.h>\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl_hal.h>\n#include <acl_hostch.h>\n#include <acl_offline_hal.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_types.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n//////////////////////////////\n// Global variables\n\nstatic int acl_offline_hal_close_devices(cl_uint num_devices,\n                                         const cl_device_id *devices);\nstatic int acl_offline_hal_try_devices(cl_uint num_devices,\n                                       const cl_device_id *devices,\n                                       cl_platform_id platform);\nstatic void acl_offline_hal_init_device(const acl_system_def_t *sysdef) {\n  sysdef = sysdef;\n}\nstatic cl_ulong acl_offline_hal_get_timestamp(void);\nstatic void acl_offline_hal_copy_hostmem_to_hostmem(cl_event event,\n                                                    const void *src, void *dest,\n                                                    size_t size);\nstatic void acl_offline_hal_copy_hostmem_to_globalmem(cl_event event,\n                                                      const void *src,\n                                                      void *dest, size_t size);\nstatic void acl_offline_hal_copy_globalmem_to_hostmem(cl_event event,\n                                                      const void *src,\n                                                      void *dest, size_t size);\nstatic void acl_offline_hal_copy_globalmem_to_globalmem(cl_event event,\n                                                        const void *src,\n                                                        void *dest,\n                                                        size_t size);\nstatic void acl_offline_hal_register_callbacks(\n    acl_event_update_callback event_update,\n    acl_kernel_update_callback kernel_update,\n    acl_profile_callback profile_update,\n    acl_device_update_callback device_update,\n    acl_process_printf_buffer_callback process_printf);\nstatic void\nacl_offline_hal_launch_kernel(unsigned int physical_id,\n                              acl_kernel_invocation_wrapper_t *wrapper);\nstatic void acl_offline_hal_unstall_kernel(unsigned int physical_id,\n                                           int activation_id);\nstatic int acl_offline_hal_program_device(unsigned int physical_id,\n                                          const acl_device_def_t *devdef,\n                                          const struct acl_pkg_file *binary,\n                                          int acl_program_mode);\nstatic cl_bool acl_offline_hal_query_temperature(unsigned int physical_id,\n                                                 cl_int *temp);\nstatic int\nacl_offline_hal_get_device_official_name(unsigned int physical_device_id,\n                                         char *name, size_t size);\nstatic int\nacl_offline_hal_get_device_vendor_name(unsigned int physical_device_id,\n                                       char *name, size_t size);\nstatic int acl_offline_hal_get_profile_data(unsigned int physical_device_id,\n                                            unsigned int accel_id,\n                                            uint64_t *data,\n                                            unsigned int length);\nstatic int\nacl_offline_hal_reset_profile_counters(unsigned int physical_device_id,\n                                       unsigned int accel_id);\nstatic int\nacl_offline_hal_disable_profile_counters(unsigned int physical_device_id,\n                                         unsigned int accel_id);\nstatic int\nacl_offline_hal_enable_profile_counters(unsigned int physical_device_id,\n                                        unsigned int accel_id);\nstatic int\nacl_offline_hal_set_profile_shared_control(unsigned int physical_device_id,\n                                           unsigned int accel_id);\nstatic int\nacl_offline_hal_set_profile_start_cycle(unsigned int physical_device_id,\n                                        unsigned int accel_id, uint64_t value);\nstatic int\nacl_offline_hal_set_profile_stop_cycle(unsigned int physical_device_id,\n                                       unsigned int accel_id, uint64_t value);\nstatic int\nacl_offline_hal_has_svm_memory_support(unsigned int physical_device_id,\n                                       int *value);\nstatic int acl_offline_hal_has_physical_mem(unsigned int physical_device_id);\nstatic void *acl_offline_hal_get_board_extension_function_address(\n    const char *func_name, unsigned int physical_device_id);\nstatic int acl_offline_hal_pll_reconfigure(unsigned int physical_device_id,\n                                           const char *pll_settings_str);\nstatic void acl_offline_hal_reset_kernels(cl_device_id device);\n\nstatic acl_event_update_callback acl_offline_hal_event_callback = NULL;\nstatic acl_kernel_update_callback acl_offline_hal_kernel_callback = NULL;\nstatic acl_profile_callback acl_offline_hal_profile_callback = NULL;\nstatic acl_device_update_callback acl_offline_hal_device_callback = NULL;\nstatic acl_process_printf_buffer_callback\n    acl_offline_process_printf_buffer_callback = NULL;\n\nstatic const acl_hal_t acl_offline_hal = {\n    acl_offline_hal_init_device,\n    0,\n    acl_offline_hal_get_timestamp,\n    acl_offline_hal_copy_hostmem_to_hostmem,\n    acl_offline_hal_copy_hostmem_to_globalmem,\n    acl_offline_hal_copy_globalmem_to_hostmem,\n    acl_offline_hal_copy_globalmem_to_globalmem,\n    acl_offline_hal_register_callbacks,\n    acl_offline_hal_launch_kernel,\n    acl_offline_hal_unstall_kernel,\n    acl_offline_hal_program_device,\n    acl_offline_hal_query_temperature,\n    acl_offline_hal_get_device_official_name,\n    acl_offline_hal_get_device_vendor_name,\n    NULL,\n    NULL,\n    acl_offline_hal_get_profile_data,\n    acl_offline_hal_reset_profile_counters,\n    acl_offline_hal_disable_profile_counters,\n    acl_offline_hal_enable_profile_counters,\n    acl_offline_hal_set_profile_shared_control,\n    acl_offline_hal_set_profile_start_cycle,\n    acl_offline_hal_set_profile_stop_cycle,\n    acl_offline_hal_has_svm_memory_support,\n    acl_offline_hal_has_physical_mem,\n    NULL,\n    acl_offline_hal_get_board_extension_function_address,\n    acl_offline_hal_pll_reconfigure,\n    acl_offline_hal_reset_kernels,\n    NULL,\n    NULL,\n    NULL,\n    NULL,\n    NULL,\n    NULL,\n    NULL,\n    NULL,\n    acl_offline_hal_try_devices,\n    acl_offline_hal_close_devices,\n    NULL,\n    NULL};\n\n//////////////////////////////\n// A very simple HAL\n\nconst acl_hal_t *acl_get_offline_hal(void) {\n  acl_assert_locked();\n  return &acl_offline_hal;\n}\n\nstatic cl_ulong acl_offline_hal_get_timestamp(void) {\n  acl_assert_locked();\n  return acl_get_time_ns();\n}\n\nstatic void acl_offline_hal_copy_hostmem_to_hostmem(cl_event event,\n                                                    const void *src, void *dest,\n                                                    size_t size) {\n  acl_assert_locked();\n\n  acl_offline_hal_event_callback(\n      event, CL_RUNNING); // in \"real life\" this in response to a hw message\n  acl_print_debug_msg(\" Copying %zu bytes from %p to %p event %p\\n\", size, src,\n                      dest, event);\n  memmove(dest, src, size);\n  acl_offline_hal_event_callback(\n      event, CL_COMPLETE); // in \"real life\" this in response to a hw message\n}\n\n// The other variants get their own function\nstatic void acl_offline_hal_copy_hostmem_to_globalmem(cl_event event,\n                                                      const void *src,\n                                                      void *dest, size_t size) {\n  acl_assert_locked();\n\n  // For offline purposes, the same.\n  acl_offline_hal_copy_hostmem_to_hostmem(event, src, dest, size);\n}\nstatic void acl_offline_hal_copy_globalmem_to_hostmem(cl_event event,\n                                                      const void *src,\n                                                      void *dest, size_t size) {\n  acl_assert_locked();\n\n  // For offline purposes, the same.\n  acl_offline_hal_copy_hostmem_to_hostmem(event, src, dest, size);\n}\nstatic void acl_offline_hal_copy_globalmem_to_globalmem(cl_event event,\n                                                        const void *src,\n                                                        void *dest,\n                                                        size_t size) {\n  acl_assert_locked();\n\n  // For offline purposes, the same.\n  acl_offline_hal_copy_hostmem_to_hostmem(event, src, dest, size);\n}\n\nstatic cl_bool acl_offline_hal_query_temperature(unsigned int physical_id,\n                                                 cl_int *temp) {\n  acl_assert_locked();\n\n  *temp = 0; // Avoid Windows warning\n  physical_id = physical_id;\n  return 1; // Fake success\n}\n\nstatic int\nacl_offline_hal_get_device_official_name(unsigned int physical_device_id,\n                                         char *name, size_t size) {\n  static const char *the_name = \"Offline Device\";\n  acl_assert_locked();\n  physical_device_id = physical_device_id; // Avoid Windows warning\n  if (size > strnlen(the_name, MAX_NAME_SIZE) + 1)\n    size = strnlen(the_name, MAX_NAME_SIZE) + 1;\n  strncpy(name, the_name, size);\n  return (int)size;\n}\n\nstatic int\nacl_offline_hal_get_device_vendor_name(unsigned int physical_device_id,\n                                       char *name, size_t size) {\n  static const char *the_name = \"Intel(R) Corporation\";\n  acl_assert_locked();\n  physical_device_id = physical_device_id; // Avoid Windows warning\n  if (size > strnlen(the_name, MAX_NAME_SIZE) + 1)\n    size = strnlen(the_name, MAX_NAME_SIZE) + 1;\n  strncpy(name, the_name, size);\n  return (int)size;\n}\n\nstatic void acl_offline_hal_register_callbacks(\n    acl_event_update_callback event_update,\n    acl_kernel_update_callback kernel_update,\n    acl_profile_callback profile_update,\n    acl_device_update_callback device_update,\n    acl_process_printf_buffer_callback process_printf) {\n  acl_assert_locked();\n  acl_offline_hal_event_callback = event_update;\n  acl_offline_hal_kernel_callback = kernel_update;\n  acl_offline_hal_profile_callback = profile_update;\n  acl_offline_hal_device_callback = device_update;\n  acl_offline_process_printf_buffer_callback = process_printf;\n}\n\n// Send a message to the accelerator controller to start the kernel.\n// When emulating an offline device, just set it to running and then\n// complete.\nstatic void acl_offline_hal_launch_kernel(\n    unsigned int physical_id,\n    acl_kernel_invocation_wrapper_t *invocation_wrapper) {\n  cl_int activation_id = invocation_wrapper->image->activation_id;\n  acl_assert_locked();\n\n  physical_id = physical_id;\n  // For emulating an offline device, just say we start and finish right away.\n  acl_offline_hal_kernel_callback(activation_id, CL_RUNNING);\n  acl_offline_hal_kernel_callback(activation_id, CL_COMPLETE);\n}\n\nstatic void acl_offline_hal_unstall_kernel(unsigned int physical_id,\n                                           int activation_id) {\n  activation_id =\n      activation_id; // avoid warning and hence build break on Windows.\n  physical_id = physical_id;\n  // nothing, kernel printf is not supported\n}\n\nstatic int acl_offline_hal_program_device(unsigned int physical_id,\n                                          const acl_device_def_t *devdef,\n                                          const struct acl_pkg_file *binary,\n                                          int acl_program_mode) {\n  devdef = devdef;\n  binary = binary;\n  physical_id = physical_id;\n  acl_program_mode = acl_program_mode;\n  return 0; // Signal success\n}\n\nstatic int acl_offline_hal_get_profile_data(unsigned int physical_device_id,\n                                            unsigned int accel_id,\n                                            uint64_t *data,\n                                            unsigned int length) {\n  unsigned i;\n  physical_device_id =\n      physical_device_id; // avoid warning and hence build break on Windows.\n  accel_id = accel_id;\n  for (i = 0; i < length; ++i) {\n    data[i] = 0;\n  }\n  length = length;\n  return 0; // Signal success\n}\n\nstatic int\nacl_offline_hal_reset_profile_counters(unsigned int physical_device_id,\n                                       unsigned int accel_id) {\n  physical_device_id =\n      physical_device_id; // avoid warning and hence build break on Windows.\n  accel_id = accel_id;\n  return 0; // Signal success\n}\n\nstatic int\nacl_offline_hal_disable_profile_counters(unsigned int physical_device_id,\n                                         unsigned int accel_id) {\n  physical_device_id =\n      physical_device_id; // avoid warning and hence build break on Windows.\n  accel_id = accel_id;\n  return 0; // Signal success\n}\n\nstatic int\nacl_offline_hal_enable_profile_counters(unsigned int physical_device_id,\n                                        unsigned int accel_id) {\n  physical_device_id =\n      physical_device_id; // avoid warning and hence build break on Windows.\n  accel_id = accel_id;\n  return 0; // Signal success\n}\n\nstatic int\nacl_offline_hal_set_profile_shared_control(unsigned int physical_device_id,\n                                           unsigned int accel_id) {\n  physical_device_id =\n      physical_device_id; // avoid warning and hence build break on Windows.\n  accel_id = accel_id;\n  return 0; // Signal success\n}\n\nstatic int\nacl_offline_hal_set_profile_start_cycle(unsigned int physical_device_id,\n                                        unsigned int accel_id, uint64_t value) {\n  physical_device_id =\n      physical_device_id; // avoid warning and hence build break on Windows.\n  accel_id = accel_id;\n  value = value;\n  return 0; // Signal success\n}\n\nstatic int\nacl_offline_hal_set_profile_stop_cycle(unsigned int physical_device_id,\n                                       unsigned int accel_id, uint64_t value) {\n  physical_device_id =\n      physical_device_id; // avoid warning and hence build break on Windows.\n  accel_id = accel_id;\n  value = value;\n  return 0; // Signal success\n}\n\nstatic int\nacl_offline_hal_has_svm_memory_support(unsigned int physical_device_id,\n                                       int *value) {\n  acl_assert_locked();\n  physical_device_id = physical_device_id; // Avoid Windows warning\n  *value = (CL_DEVICE_SVM_COARSE_GRAIN_BUFFER |\n            CL_DEVICE_SVM_FINE_GRAIN_BUFFER | CL_DEVICE_SVM_FINE_GRAIN_SYSTEM);\n  return 1;\n}\n\nstatic int acl_offline_hal_has_physical_mem(unsigned int physical_device_id) {\n  acl_assert_locked();\n  physical_device_id = physical_device_id; // Avoid Windows warning\n  return 1;\n}\n\nstatic int acl_offline_hal_pll_reconfigure(unsigned int physical_device_id,\n                                           const char *pll_settings_str) {\n  physical_device_id = physical_device_id;\n  pll_settings_str = pll_settings_str;\n  return 0; // Signal success\n}\n\nstatic void acl_offline_hal_reset_kernels(cl_device_id device) {\n  device = device;\n}\n\nstatic int acl_offline_hal_try_devices(cl_uint num_devices,\n                                       const cl_device_id *devices,\n                                       cl_platform_id platform) {\n  cl_uint i;\n  platform = platform; // Avoid windows warning on unused parameters\n\n  // offline devices are always half duplex\n  for (i = 0; i < num_devices; i++) {\n    devices[i]->def.concurrent_reads = 1;\n    devices[i]->def.concurrent_writes = 1;\n    devices[i]->def.max_inflight_mem_ops = 1;\n    devices[i]->def.host_capabilities = 0;\n    devices[i]->def.shared_capabilities = 0;\n    devices[i]->def.device_capabilities = 0;\n    devices[i]->def.min_host_mem_alignment = 0;\n    devices[i]->opened_count++;\n  }\n  return 0;\n}\n\nstatic int acl_offline_hal_close_devices(cl_uint num_devices,\n                                         const cl_device_id *devices) {\n  unsigned int idevice;\n  // Avoid the windows warnings\n  num_devices = num_devices;\n  devices = devices;\n\n  for (idevice = 0; idevice < num_devices; idevice++) {\n    assert(devices[idevice]->opened_count > 0);\n    devices[idevice]->opened_count--;\n  }\n  return 0;\n}\n\nstatic void *acl_offline_hal_get_board_extension_function_address(\n    const char *func_name, unsigned int physical_device_id) {\n  // Avoiding Windows warnings\n  func_name = func_name;\n  physical_device_id = physical_device_id;\n\n  return NULL;\n}\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_kernel.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <algorithm>\n#include <cassert>\n#include <iostream>\n#include <sstream>\n#include <stdio.h>\n#include <unordered_map>\n#include <vector>\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_hash/acl_hash.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n#include <pkg_editor/pkg_editor.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hostch.h>\n#include <acl_icd_dispatch.h>\n#include <acl_kernel.h>\n#include <acl_mem.h>\n#include <acl_profiler.h>\n#include <acl_program.h>\n#include <acl_sampler.h>\n#include <acl_support.h>\n#include <acl_svm.h>\n#include <acl_types.h>\n#include <acl_usm.h>\n#include <acl_util.h>\n#include <acl_visibility.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n// Kernels\n// ========\n//\n// Lifecycle of cl_kernel:\n//    States are:\n//       \"new\"             - initial state. It's attached to a built program.\n//       \"ready_to_run\"    - All its arguments ar specified\n//\n// Data model:\n//\n//    cl_kernel has:\n//       - name\n//       - reference to program\n//       - reference to interface, discovered via the\n//       program->context->device[]->name\n//       - arg values\n//\n//    acl_kernel_interface_t has:\n//       - num_args\n//       - arg_info[]\n//          .addr_space: one of LOCAL|GLOBAL|CONSTANT\n//          .category: PLAIN, MEM_OBJ, SAMPLER\n//          .size\n\n//////////////////////////////\n// Variables and macros\n\n// Prints more logs for debugging purposes.\nstatic int debug_verbosity = 0;\n#define ACL_KERNEL_DEBUG_MSG_VERBOSE(verbosity, m, ...)                        \\\n  do {                                                                         \\\n    if (debug_verbosity >= verbosity) {                                        \\\n      printf((m), ##__VA_ARGS__);                                              \\\n      fflush(stdout);                                                          \\\n    }                                                                          \\\n  } while (0)\n\n// Local functions\n\nstatic size_t l_round_up_for_alignment(size_t x);\n\nstatic int l_init_kernel(cl_kernel kernel, cl_program program,\n                         const acl_accel_def_t *accel_def,\n                         const acl_device_binary_t *dev_bin,\n                         cl_int *errcode_ret);\n\nstatic cl_int l_load_consistently_built_kernels_in_program(\n    cl_program program,\n    std::vector<std::pair<const acl_device_binary_t *, const acl_accel_def_t *>>\n        &accel_ret);\n\nstatic int l_kernel_interfaces_match(const acl_accel_def_t &a,\n                                     const acl_accel_def_t &b);\n\nstatic size_t l_local_mem_size(cl_kernel kernel);\n\nstatic cl_int l_enqueue_kernel_with_type(\n    cl_command_queue command_queue, cl_kernel kernel, cl_uint work_dim,\n    const size_t *global_work_offset, const size_t *global_work_size,\n    const size_t *local_work_size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event, cl_command_type type);\nstatic void l_get_arg_offset_and_size(cl_kernel kernel, cl_uint arg_index,\n                                      size_t *start_idx_ret, size_t *size_ret);\nstatic cl_int l_copy_and_adjust_arguments_for_device(\n    cl_kernel kernel, cl_device_id device, char *buf, cl_uint *num_bytes,\n    acl_mem_migrate_t *memory_migration,\n    std::vector<aocl_mmd_streaming_kernel_arg_info_t> &streaming_args);\n\nstatic void l_abort_use_of_wrapper(acl_kernel_invocation_wrapper_t *wrapper);\n\nstatic void l_complete_kernel_execution(cl_event event);\n\nstatic cl_bool l_check_mem_type_support_on_kernel_arg(\n    cl_kernel kernel, cl_uint arg_index,\n    acl_system_global_mem_type_t expected_type);\n\nunsigned int l_get_kernel_arg_mem_id(const cl_kernel kernel, cl_uint arg_index);\n\nACL_DEFINE_CL_OBJECT_ALLOC_FUNCTIONS(cl_kernel);\n\n//////////////////////////////\n// OpenCL API\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainKernelIntelFPGA(cl_kernel kernel) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n  acl_retain(kernel);\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainKernel(cl_kernel kernel) {\n  return clRetainKernelIntelFPGA(kernel);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseKernelIntelFPGA(cl_kernel kernel) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n\n  acl_print_debug_msg(\"Release kernel %p\\n\", kernel);\n  if (1 == acl_ref_count(kernel)) {\n\n    // kernel had profile data, free the buffer\n    if (kernel->profile_data) {\n      acl_free(kernel->profile_data);\n      kernel->profile_data = 0;\n    }\n\n    if (kernel->printf_device_buffer) {\n      clReleaseMemObject(kernel->printf_device_buffer);\n      kernel->printf_device_buffer = 0;\n    }\n\n    if (kernel->printf_device_ptr) {\n      clSVMFree(kernel->program->context, kernel->printf_device_ptr);\n      kernel->printf_device_ptr = 0;\n    }\n\n    if (kernel->arg_value) {\n      acl_delete_arr(kernel->arg_value);\n      kernel->arg_value = nullptr;\n    }\n\n    acl_untrack_object(kernel);\n\n    acl_release(kernel);\n    acl_program_forget_kernel(kernel->program, kernel);\n    clReleaseProgram(kernel->program);\n\n  } else {\n    acl_release(kernel);\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseKernel(cl_kernel kernel) {\n  return clReleaseKernelIntelFPGA(kernel);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_kernel CL_API_CALL clCreateKernelIntelFPGA(\n    cl_program program, const char *kernel_name, cl_int *errcode_ret) {\n  cl_int status;\n  cl_kernel kernel = 0;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Can't call the callback, because we have no valid context.\n  if (!acl_program_is_valid(program))\n    BAIL(CL_INVALID_PROGRAM);\n\n  if (!kernel_name)\n    BAIL_INFO(CL_INVALID_VALUE, program->context, \"kernel_name is NULL\");\n\n  // What device program is associated with this kernel?\n  // Right now we only support one device per kernel.\n  const acl_device_binary_t *dev_bin = nullptr;\n  const auto *accel_def = acl_find_accel_def(program, kernel_name, dev_bin,\n                                             &status, program->context, 0);\n\n  if (status != CL_SUCCESS)\n    BAIL(status); // already signaled callback\n\n  kernel = acl_program_alloc_kernel(program);\n  if (kernel == 0) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, program->context,\n              \"Could not allocate a program object\");\n  }\n\n  l_init_kernel(kernel, program, accel_def, dev_bin, errcode_ret);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  return kernel;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_kernel CL_API_CALL clCreateKernel(cl_program program,\n                                                  const char *kernel_name,\n                                                  cl_int *errcode_ret) {\n  return clCreateKernelIntelFPGA(program, kernel_name, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clCreateKernelsInProgramIntelFPGA(\n    cl_program program, cl_uint num_kernels, cl_kernel *kernels,\n    cl_uint *num_kernels_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_program_is_valid(program)) {\n    return CL_INVALID_PROGRAM;\n  }\n\n  auto context = program->context;\n\n  std::vector<std::pair<const acl_device_binary_t *, const acl_accel_def_t *>>\n      accel_ret;\n  auto status =\n      l_load_consistently_built_kernels_in_program(program, accel_ret);\n\n  if (status != CL_SUCCESS) {\n    return status; // already signaled\n  }\n  if (accel_ret.size() == 0) {\n    ERR_RET(CL_INVALID_PROGRAM_EXECUTABLE, context,\n            \"No kernels were built across all devices with the same interface\");\n  }\n\n  // Check return buffer spec\n  if (num_kernels == 0 && kernels) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"num_kernels is zero but kernels array is specified\");\n  }\n  if (num_kernels > 0 && kernels == 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"num_kernels is non-zero but kernels array is not specified\");\n  }\n\n  if (kernels) {\n    // User wants to send the kernels back.\n\n    // Result buffer isn't big enough.\n    if (num_kernels < accel_ret.size()) {\n      return CL_INVALID_VALUE;\n    }\n\n    // The definitions are in accel_ret. Create the kernels.\n    status = CL_SUCCESS;\n    for (cl_uint i = 0; i < accel_ret.size() && status == CL_SUCCESS; ++i) {\n      cl_kernel kernel = acl_program_alloc_kernel(program);\n      if (kernel) {\n        l_init_kernel(kernel, program, accel_ret[i].second, accel_ret[i].first,\n                      &status);\n        kernels[i] = kernel;\n      } else {\n        status = CL_OUT_OF_HOST_MEMORY;\n        acl_context_callback(context, \"Could not allocate a kernel object\");\n        // Unwind the ones we've created\n        for (cl_uint j = 0; j < i; j++) {\n          clReleaseKernel(kernels[j]);\n        }\n      }\n    }\n  }\n\n  if (num_kernels_ret)\n    *num_kernels_ret = static_cast<cl_uint>(accel_ret.size());\n\n  return status;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclCreateKernelsInProgram(cl_program program, cl_uint num_kernels,\n                         cl_kernel *kernels, cl_uint *num_kernels_ret) {\n  return clCreateKernelsInProgramIntelFPGA(program, num_kernels, kernels,\n                                           num_kernels_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetKernelArgIntelFPGA(cl_kernel kernel,\n                                                        cl_uint arg_index,\n                                                        size_t arg_size,\n                                                        const void *arg_value) {\n  const acl_kernel_arg_info_t *arg_info = 0;\n  cl_context context;\n  cl_bool is_pipe = CL_FALSE;\n  cl_bool is_sampler = CL_FALSE;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n\n  context = kernel->program->context;\n\n  if (arg_index >= kernel->accel_def->iface.args.size()) {\n    ERR_RET(CL_INVALID_ARG_INDEX, context, \"Argument index is too large\");\n  }\n\n  arg_info = &(kernel->accel_def->iface.args[arg_index]);\n\n  // Check for valid mem object or sampler\n  if (arg_info->category == ACL_ARG_MEM_OBJ) {\n    // In OpenCL 1.2 a pointer to a NULL value is also allowed for arg_values\n    // representing buffers.\n    if (arg_value && (*(cl_mem *)arg_value) &&\n        !acl_mem_is_valid(*(cl_mem *)arg_value))\n      ERR_RET(CL_INVALID_MEM_OBJECT, context,\n              \"Non-memory object passed in as memory object argument\");\n\n  } else if (arg_info->category == ACL_ARG_SAMPLER) {\n    if (arg_value && (arg_size != sizeof(cl_sampler) ||\n                      !acl_sampler_is_valid(*(cl_sampler *)arg_value))) {\n      ERR_RET(CL_INVALID_SAMPLER, context,\n              \"Non-sampler object passed in as sampler object argument\");\n    }\n    is_sampler = CL_TRUE;\n  } else if (arg_size != arg_info->size && arg_value &&\n             arg_size == sizeof(cl_sampler) &&\n             acl_sampler_is_valid(*(cl_sampler *)arg_value)) {\n    is_sampler = CL_TRUE;\n  }\n\n  // Check argument size, and value pointer.\n  switch (arg_info->addr_space) {\n  case ACL_ARG_ADDR_LOCAL: /* Size is number of local bytes to allocate */\n    if (arg_size == 0) {\n      ERR_RET(CL_INVALID_ARG_SIZE, context,\n              \"Pointer-to-local argument specified zero size\");\n    }\n    if (arg_value != 0) {\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"Pointer-to-local argument specified with a non-null value\");\n    }\n\n    /* We instantiated a specific mem capacity to handle this pointer.\n     * Make sure that user didn't ask for more at runtime than they\n     * specified (and we instantiated) at kernel compile time */\n    {\n      unsigned lmem_size_instantiated = arg_info->lmem_size_bytes;\n      if (arg_size > lmem_size_instantiated) {\n        ERR_RET(CL_INVALID_ARG_SIZE, context,\n                \"Pointer-to-local argument requested size is larger than \"\n                \"maximum specified at compile time\");\n      }\n    }\n    break;\n\n  case ACL_ARG_ADDR_GLOBAL:\n  case ACL_ARG_ADDR_CONSTANT:\n    if (arg_size != sizeof(cl_mem)) {\n      ERR_RET(CL_INVALID_ARG_SIZE, context,\n              \"Pointer-to-global or Pointer-to-constant argument size is \"\n              \"not the size of cl_mem\");\n    }\n    // Can pass NULL or pointer to NULL in arg_value, or it must be a valid\n    // memory object.\n    if (arg_value && (*(cl_mem *)arg_value) &&\n        !acl_mem_is_valid(*(cl_mem *)arg_value)) {\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"Pointer-to-global or Pointer-to-constant argument value is \"\n              \"not a valid memory object\");\n    }\n\n    if (arg_value && (*(cl_mem *)arg_value) &&\n        arg_info->type_qualifier == ACL_ARG_TYPE_PIPE &&\n        (*(cl_mem *)arg_value)->mem_object_type == CL_MEM_OBJECT_PIPE) {\n      is_pipe = CL_TRUE;\n    }\n    // If this buffer is an SVM buffer, assume that the user wants the memory to\n    // be in sync. Treat this the same as an SVM kernel arg and return.\n    if (arg_value && (*(cl_mem *)arg_value) && (*(cl_mem *)arg_value)->is_svm) {\n      return clSetKernelArgSVMPointerIntelFPGA(\n          kernel, arg_index, (*(cl_mem *)arg_value)->host_mem.aligned_ptr);\n    }\n    break;\n\n  case ACL_ARG_ADDR_NONE:\n    if (arg_value == NULL) {\n      ERR_RET(CL_INVALID_ARG_VALUE, context, \"Argument value is NULL\");\n    }\n    if (is_sampler && arg_value != 0 &&\n        acl_sampler_is_valid_ptr(*((cl_sampler *)arg_value))) {\n      if (arg_size != sizeof(cl_sampler)) {\n        ERR_RET(CL_INVALID_ARG_SIZE, context,\n                \"Sampler argument size is not the size of cl_sampler\");\n      }\n      if (arg_info->size != sizeof(int)) {\n        ERR_RET(CL_INVALID_ARG_SIZE, context,\n                \"Argument size is the wrong size\");\n      }\n    } else if (arg_size == sizeof(cl_mem) &&\n               acl_pipe_is_valid_pointer(*((cl_mem *)arg_value), kernel)) {\n      is_pipe = CL_TRUE;\n    } else if (arg_size != arg_info->size) {\n      ERR_RET(CL_INVALID_ARG_SIZE, context, \"Argument size is the wrong size\");\n    }\n    break;\n  }\n\n  // May be a pipe - but currently the pipe object is empty (no allocated\n  // memory) and we don't actually read pipe arguments in the kernel so we\n  // shouldn't spend time setting up the argument properly.\n  if (is_pipe) {\n    cl_mem pipe_ptr = *((cl_mem *)arg_value);\n    assert(pipe_ptr != NULL);\n\n    kernel->arg_is_svm[arg_index] = CL_FALSE;\n    kernel->arg_is_ptr[arg_index] = CL_FALSE;\n    kernel->arg_defined[arg_index] = 1;\n\n    /* If this is a host pipe, create a host channel and bind them together */\n    if (arg_info->host_accessible && pipe_ptr->host_pipe_info != NULL) {\n      if (pipe_ptr->host_pipe_info->m_binded_kernel != NULL) {\n        ERR_RET(CL_INVALID_ARG_VALUE, context,\n                \"This pipe has already been bound to a kernel. Cannot \"\n                \"rebind to a new kernel\");\n      }\n\n      // Check to see if the kernel argument's width matches up with our cl_pipe\n      if (!context->uses_dynamic_sysdef) {\n        // In mode 3\n        // All the device need to have the same host pipe def\n        for (unsigned int i = 0; i < kernel->program->num_devices; ++i) {\n          bool found = false;\n          for (const auto &hostpipe_info :\n               kernel->program->device[i]\n                   ->def.autodiscovery_def.acl_hostpipe_info) {\n            if (arg_info->pipe_channel_id == hostpipe_info.name) {\n              // Check direction\n              if (pipe_ptr->flags & CL_MEM_HOST_READ_ONLY &&\n                  hostpipe_info.is_dev_to_host) {\n                // Direction match\n              } else if (pipe_ptr->flags & CL_MEM_HOST_WRITE_ONLY &&\n                         hostpipe_info.is_host_to_dev) {\n                // Direction match\n              } else {\n                ERR_RET(CL_INVALID_ARG_VALUE, context,\n                        \"Host accessible pipe direction is not the same \"\n                        \"of cl_pipe\");\n              }\n              // Check width\n              if (pipe_ptr->fields.pipe_objs.pipe_packet_size !=\n                  hostpipe_info.data_width) {\n                ERR_RET(CL_INVALID_ARG_SIZE, context,\n                        \"Host accessible pipe size is not the same size \"\n                        \"of cl_pipe\");\n              }\n              // Check max buffer size\n              if (pipe_ptr->fields.pipe_objs.pipe_max_packets >\n                  hostpipe_info.max_buffer_depth) {\n                ERR_RET(CL_INVALID_ARG_VALUE, context,\n                        \"Host accessible pipe max packets size is \"\n                        \"smaller than cl_pipe requested size\");\n              }\n              found = true;\n            }\n          }\n          assert(found);\n        }\n      } else {\n        // Not in mode 3\n        bool found = false;\n        for (const auto &hostpipe_info :\n             kernel->dev_bin->get_devdef()\n                 .autodiscovery_def.acl_hostpipe_info) {\n          if (arg_info->pipe_channel_id == hostpipe_info.name) {\n            // Check direction\n            if (pipe_ptr->flags & CL_MEM_HOST_READ_ONLY &&\n                hostpipe_info.is_dev_to_host) {\n              // Direction match\n            } else if (pipe_ptr->flags & CL_MEM_HOST_WRITE_ONLY &&\n                       hostpipe_info.is_host_to_dev) {\n              // Direction match\n            } else {\n              ERR_RET(\n                  CL_INVALID_ARG_VALUE, context,\n                  \"Host accessible pipe direction is not the same of cl_pipe\");\n            }\n            // Check width\n            if (pipe_ptr->fields.pipe_objs.pipe_packet_size !=\n                hostpipe_info.data_width) {\n              ERR_RET(\n                  CL_INVALID_ARG_SIZE, context,\n                  \"Host accessible pipe size is not the same size of cl_pipe\");\n            }\n            // Check max buffer size\n            if (pipe_ptr->fields.pipe_objs.pipe_max_packets >\n                hostpipe_info.max_buffer_depth) {\n              ERR_RET(CL_INVALID_ARG_VALUE, context,\n                      \"Host accessible pipe max packets size is smaller \"\n                      \"than cl_pipe requested size\");\n            }\n            found = true;\n          }\n        }\n        assert(found);\n      }\n\n      // Here we bind the kernel, but we delay hostpipe binding until kernel\n      // enqueue\n      pipe_ptr->host_pipe_info->m_binded_kernel = kernel;\n      pipe_ptr->host_pipe_info->host_pipe_channel_id =\n          arg_info->pipe_channel_id;\n\n      // Always do the binding until kernel enqueue time, because we can only\n      // figure out which device at enqueue time\n      pipe_ptr->host_pipe_info->binded = false;\n    }\n    return CL_SUCCESS;\n  }\n\n  // Now try saving the value.\n  // Intel x86 is little-endian, i.e. least significant byte is stored in\n  // the lowest numbered address.\n  {\n    // Determine where to write the value.\n    size_t start_idx = 0;\n    size_t iface_arg_size = 0;\n    l_get_arg_offset_and_size(kernel, arg_index, &start_idx, &iface_arg_size);\n\n    // We would write beyond the end of the array!\n    // Kinda late to inform the user... Maybe it should happen at kernel\n    // creation time, or at system initialization...\n#ifndef REMOVE_VALID_CHECKS\n    if ((start_idx + iface_arg_size) > kernel->arg_value_size) {\n      ERR_RET(CL_INVALID_KERNEL, context,\n              \"Argument overflows the space allocated for kernel arguments\");\n    }\n#endif\n\n    // If the board has both SVM and DGM, make sure kernel argument is DGM\n    if (arg_info->addr_space == ACL_ARG_ADDR_GLOBAL ||\n        arg_info->addr_space == ACL_ARG_ADDR_CONSTANT) {\n      cl_bool context_has_device_with_physical_mem = CL_FALSE;\n      cl_bool context_has_device_with_svm = CL_FALSE;\n      for (unsigned idevice = 0; idevice < context->num_devices; ++idevice) {\n        if (acl_svm_device_supports_physical_memory(\n                context->device[idevice]->def.physical_device_id)) {\n          context_has_device_with_physical_mem = CL_TRUE;\n          break;\n        }\n      }\n      for (unsigned idevice = 0; idevice < context->num_devices; ++idevice) {\n        if (acl_svm_device_supports_any_svm(\n                context->device[idevice]->def.physical_device_id)) {\n          context_has_device_with_svm = CL_TRUE;\n          break;\n        }\n      }\n      if (context_has_device_with_svm && context_has_device_with_physical_mem &&\n          kernel->dev_bin->get_devdef()\n                  .autodiscovery_def.num_global_mem_systems > 1 &&\n          !l_check_mem_type_support_on_kernel_arg(\n              kernel, arg_index, ACL_GLOBAL_MEM_DEVICE_PRIVATE)) {\n        ERR_RET(CL_INVALID_ARG_VALUE, context,\n                \"cl_mem object was set on kernel argument that doesn't \"\n                \"have attribute to access device private memory\");\n      }\n    }\n\n    // If using heterogeneous memory, try to set a default mem_id for the cl_mem\n    // we are using This is optimization\n    if (arg_info->addr_space == ACL_ARG_ADDR_GLOBAL) {\n      // We need to check if this arg is assigned to the correct memory\n      if (arg_value &&\n          (*(cl_mem *)arg_value)) { // Can pass in a NULL pointer or pointer to\n                                    // NULL, to provide NULL arg\n        cl_mem mem = *(cl_mem *)arg_value;\n\n        if (((mem->flags & CL_MEM_HETEROGENEOUS_INTELFPGA) ||\n             !arg_info->buffer_location.empty()) &&\n            mem->allocation_deferred) {\n\n          if (!arg_info->buffer_location.empty()) {\n            for (unsigned gmem_idx = 0;\n                 gmem_idx < kernel->dev_bin->get_devdef()\n                                .autodiscovery_def.num_global_mem_systems;\n                 gmem_idx++) {\n              // Look for a buffer, if we found this one, use it\n              assert(!kernel->dev_bin->get_devdef()\n                          .autodiscovery_def.global_mem_defs[gmem_idx]\n                          .name.empty());\n              if (arg_info->buffer_location ==\n                  kernel->dev_bin->get_devdef()\n                      .autodiscovery_def.global_mem_defs[gmem_idx]\n                      .name) {\n                // This is actually just a *HINT* since the allocation hasn't\n                // happened yet !\n                mem->mem_id = gmem_idx;\n                break;\n              }\n            }\n          }\n        }\n      }\n    }\n\n    if (arg_info->addr_space != ACL_ARG_ADDR_LOCAL) {\n      if (arg_value == 0) {\n        // Example: NULL arg for __global or __constant.\n        // Store a zero value for the pointer.\n        cl_ulong null_ptr = 0;\n        safe_memcpy(&(kernel->arg_value[start_idx]), &null_ptr, iface_arg_size,\n                    kernel->arg_value_size - start_idx, iface_arg_size);\n        kernel->arg_is_svm[arg_index] = CL_FALSE;\n        kernel->arg_is_ptr[arg_index] = CL_FALSE;\n      } else if (is_sampler) {\n        cl_sampler sampler = *(cl_sampler *)arg_value;\n        assert(sampler != NULL);\n\n        int sampler_bitfield = 0;\n\n        sampler_bitfield = 0;\n\n        switch (sampler->normalized_coords) {\n        case CL_TRUE:\n          sampler_bitfield |= CLK_NORMALIZED_COORDS_TRUE;\n          break;\n        case CL_FALSE:\n          sampler_bitfield |= CLK_NORMALIZED_COORDS_FALSE;\n          break;\n        // Default is CL_TRUE\n        default:\n          sampler_bitfield |= CLK_NORMALIZED_COORDS_TRUE;\n          break;\n        }\n\n        switch (sampler->addressing_mode) {\n        case CL_ADDRESS_NONE:\n          sampler_bitfield |= CLK_ADDRESS_NONE;\n          break;\n        case CL_ADDRESS_MIRRORED_REPEAT:\n          sampler_bitfield |= CLK_ADDRESS_MIRRORED_REPEAT;\n          break;\n        case CL_ADDRESS_REPEAT:\n          sampler_bitfield |= CLK_ADDRESS_REPEAT;\n          break;\n        case CL_ADDRESS_CLAMP_TO_EDGE:\n          sampler_bitfield |= CLK_ADDRESS_CLAMP_TO_EDGE;\n          break;\n        case CL_ADDRESS_CLAMP:\n          sampler_bitfield |= CLK_ADDRESS_CLAMP;\n          break;\n        // Default is CL_ADDRESS_CLAMP\n        default:\n          sampler_bitfield |= CLK_ADDRESS_CLAMP;\n          break;\n        }\n\n        switch (sampler->filter_mode) {\n        case CL_FILTER_NEAREST:\n          sampler_bitfield |= CLK_FILTER_NEAREST;\n          break;\n        case CL_FILTER_LINEAR:\n          sampler_bitfield |= CLK_FILTER_LINEAR;\n          break;\n        // Default is CL_FILTER_NEAREST\n        default:\n          sampler_bitfield |= CLK_FILTER_NEAREST;\n          break;\n        }\n\n        safe_memcpy(&(kernel->arg_value[start_idx]), &sampler_bitfield,\n                    iface_arg_size, kernel->arg_value_size - start_idx,\n                    iface_arg_size);\n        kernel->arg_is_svm[arg_index] = CL_FALSE;\n        kernel->arg_is_ptr[arg_index] = CL_FALSE;\n      } else {\n        safe_memcpy(&(kernel->arg_value[start_idx]), arg_value, iface_arg_size,\n                    kernel->arg_value_size - start_idx, iface_arg_size);\n        kernel->arg_is_svm[arg_index] = CL_FALSE;\n        kernel->arg_is_ptr[arg_index] = CL_FALSE;\n      }\n    } else {\n      // A LOCAL param is an integer saying how many bytes the local\n      // storage should be.\n      // The number of bytes is specified by the ***arg_size*** argument.\n      safe_memcpy(&(kernel->arg_value[start_idx]), &arg_size, iface_arg_size,\n                  kernel->arg_value_size - start_idx, iface_arg_size);\n      kernel->arg_is_svm[arg_index] = CL_FALSE;\n      kernel->arg_is_ptr[arg_index] = CL_FALSE;\n    }\n\n    kernel->arg_defined[arg_index] = 1;\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetKernelArg(cl_kernel kernel,\n                                               cl_uint arg_index,\n                                               size_t arg_size,\n                                               const void *arg_value) {\n  return clSetKernelArgIntelFPGA(kernel, arg_index, arg_size, arg_value);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetKernelArgSVMPointerIntelFPGA(\n    cl_kernel kernel, cl_uint arg_index, const void *arg_value) {\n  cl_context context;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n#ifndef REMOVE_VALID_CHECKS\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n\n  context = kernel->program->context;\n\n  if (arg_index >= kernel->accel_def->iface.args.size()) {\n    ERR_RET(CL_INVALID_ARG_INDEX, context, \"Argument index is too large\");\n  }\n\n  if (arg_value == NULL) {\n    ERR_RET(CL_INVALID_ARG_VALUE, context, \"SVM argument is NULL\");\n  }\n\n  unsigned expected_alignment =\n      kernel->accel_def->iface.args[arg_index].alignment;\n  expected_alignment =\n      expected_alignment ? expected_alignment : ACL_MEM_ALIGN; // For tests\n  if ((uintptr_t)arg_value % expected_alignment != 0) {\n    if (expected_alignment == ACL_MEM_ALIGN) {\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"SVM argument is not aligned correctly for type.  Ensure the \"\n              \"kernel argument is targeting the correct buffer location.\");\n    } else {\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"SVM argument is not aligned correctly for type.\");\n    }\n  }\n#endif\n\n  // Now try saving the value.\n  {\n    // Determine where to write the value.\n    size_t start_idx = 0;\n    size_t iface_arg_size = 0;\n    l_get_arg_offset_and_size(kernel, arg_index, &start_idx, &iface_arg_size);\n\n    // We would write beyond the end of the array!\n    // Kinda late to inform the user... Maybe it should happen at kernel\n    // creation time, or at system initialization...\n#ifndef REMOVE_VALID_CHECKS\n    if ((start_idx + iface_arg_size) > kernel->arg_value_size) {\n      ERR_RET(CL_INVALID_KERNEL, context,\n              \"Argument overflows the space allocated for kernel arguments\");\n    }\n    // If the board has both SVM and DGM, make sure kernel argument is SVM\n    cl_bool context_has_device_with_physical_mem = CL_FALSE;\n    cl_bool context_has_device_with_svm = CL_FALSE;\n    for (unsigned idevice = 0; idevice < context->num_devices; ++idevice) {\n      if (acl_svm_device_supports_physical_memory(\n              context->device[idevice]->def.physical_device_id)) {\n        context_has_device_with_physical_mem = CL_TRUE;\n        break;\n      }\n    }\n    for (unsigned idevice = 0; idevice < context->num_devices; ++idevice) {\n      if (acl_svm_device_supports_any_svm(\n              context->device[idevice]->def.physical_device_id)) {\n        context_has_device_with_svm = CL_TRUE;\n        break;\n      }\n    }\n    if (context_has_device_with_svm && context_has_device_with_physical_mem &&\n        kernel->dev_bin->get_devdef().autodiscovery_def.num_global_mem_systems >\n            1 &&\n        !l_check_mem_type_support_on_kernel_arg(\n            kernel, arg_index, ACL_GLOBAL_MEM_SHARED_VIRTUAL)) {\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"SVM pointer was set on kernel argument that doesn't have \"\n              \"attribute to access SVM\");\n    }\n#endif\n\n    // Track usage of buffers.\n    // Note: The OpenCL 1.2 spec forbids the kernel object from updating\n    // reference counts for the arguments.  See clSetKernelArg spec.\n    // We just have to trust the user to not release the memory object too\n    // early.\n    safe_memcpy(&(kernel->arg_value[start_idx]), &arg_value, iface_arg_size,\n                kernel->arg_value_size - start_idx, iface_arg_size);\n    kernel->arg_is_svm[arg_index] = CL_TRUE;\n    kernel->arg_is_ptr[arg_index] = CL_TRUE;\n\n    kernel->arg_defined[arg_index] = 1;\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetKernelArgSVMPointer(\n    cl_kernel kernel, cl_uint arg_index, const void *arg_value) {\n  return clSetKernelArgSVMPointerIntelFPGA(kernel, arg_index, arg_value);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetKernelArgMemPointerINTEL(\n    cl_kernel kernel, cl_uint arg_index, const void *arg_value) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n\n  cl_context context = kernel->program->context;\n\n  if (arg_index >= kernel->accel_def->iface.args.size()) {\n    ERR_RET(CL_INVALID_ARG_INDEX, context, \"Argument index is too large\");\n  }\n\n  // Determine where to write the value.\n  size_t start_idx = 0;\n  size_t iface_arg_size = 0;\n  l_get_arg_offset_and_size(kernel, arg_index, &start_idx, &iface_arg_size);\n\n  // We would write beyond the end of the array!\n  // Kinda late to inform the user... Maybe it should happen at kernel\n  // creation time, or at system initialization...\n#ifndef REMOVE_VALID_CHECKS\n  if ((start_idx + iface_arg_size) > kernel->arg_value_size) {\n    ERR_RET(CL_INVALID_KERNEL, context,\n            \"Argument overflows the space allocated for kernel arguments\");\n  }\n\n  unsigned expected_alignment =\n      kernel->accel_def->iface.args[arg_index].alignment;\n  expected_alignment =\n      expected_alignment ? expected_alignment : ACL_MEM_ALIGN; // For tests\n  if ((uintptr_t)arg_value % expected_alignment != 0) {\n    if (expected_alignment == ACL_MEM_ALIGN) {\n      ERR_RET(\n          CL_INVALID_ARG_VALUE, context,\n          \"Pointer argument is not aligned correctly for type.  If you are \"\n          \"using unified shared memory compile the kernel with the -usm flag.\");\n    } else {\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"Pointer argument is not aligned correctly for type.\");\n    }\n  }\n\n  if (!acl_usm_ptr_belongs_to_context(context, arg_value)) {\n    ERR_RET(CL_INVALID_ARG_VALUE, context,\n            \"Pointer argument is not allocated using USM or not \"\n            \"allocated in correct context.\");\n  }\n\n  // Ensure the USM allocation (arg_value) is compatible with what the kernel\n  // argument is expecting.\n  //\n  // All information we have about the global memory system comes from the\n  // autodiscovery string now.\n  //\n  // We know which interface the kernel argument is connected to.\n  //\n  // However, all we know about the USM allocation is which type of allocation\n  // it is (host, shared, or device).  Assuming the board_spec.xml has the\n  // allocation_type attribute set on the appropriate interfaces we can\n  // determine which interface this will correspond to.\n\n  unsigned kernel_arg_mem_id = l_get_kernel_arg_mem_id(kernel, arg_index);\n  const auto *kernel_arg_mem =\n      &kernel->dev_bin->get_devdef()\n           .autodiscovery_def.global_mem_defs[kernel_arg_mem_id];\n\n  acl_usm_allocation_t *usm_alloc =\n      acl_get_usm_alloc_from_ptr(context, arg_value);\n  if (usm_alloc == nullptr) {\n    ERR_RET(CL_INVALID_ARG_VALUE, context,\n            \"Pointer argument is not allocated using USM or not \"\n            \"allocated in correct context.\");\n  }\n\n  // Try to find the memory interface that corresponds to this allocation.\n  const acl_system_global_mem_def_t *allocation_mem = nullptr;\n  for (unsigned gmem_idx = 0;\n       gmem_idx <\n       kernel->dev_bin->get_devdef().autodiscovery_def.num_global_mem_systems;\n       gmem_idx++) {\n    auto allocation_type = kernel->dev_bin->get_devdef()\n                               .autodiscovery_def.global_mem_defs[gmem_idx]\n                               .allocation_type;\n    if ((allocation_type & ACL_GLOBAL_MEM_HOST_ALLOCATION &&\n         usm_alloc->type == CL_MEM_TYPE_HOST_INTEL) ||\n        (allocation_type & ACL_GLOBAL_MEM_SHARED_ALLOCATION &&\n         usm_alloc->type == CL_MEM_TYPE_SHARED_INTEL) ||\n        (allocation_type & ACL_GLOBAL_MEM_DEVICE_ALLOCATION &&\n         usm_alloc->type == CL_MEM_TYPE_DEVICE_INTEL)) {\n      allocation_mem = &kernel->dev_bin->get_devdef()\n                            .autodiscovery_def.global_mem_defs[gmem_idx];\n      break;\n    }\n  }\n\n  // We will only be able to perform these checks if the \"allocation_type\" field\n  // was set for the appropriate interfaces in the board_spec.xml and it is an\n  // OpenCL compile. Compiler assumes unlabeled pointers are device global mem.\n  // This assumption is only true for OpenCL compiles.\n  if (allocation_mem && kernel->accel_def->is_sycl_compile == 0) {\n    if (kernel_arg_mem->allocation_type ==\n        ACL_GLOBAL_MEM_UNDEFINED_ALLOCATION) {\n      // The allocation_type field was not indicated in the board_spec.xml for\n      // the interface associated with this argument, or it does not contain an\n      // expected value.\n      acl_context_callback(\n          context,\n          \"Warning: Unable to determine expected USM \"\n          \"allocation type for this kernel argument.  Functional or performance\"\n          \" issues may be encountered.\");\n    }\n\n    if (usm_alloc->type == CL_MEM_TYPE_DEVICE_INTEL) {\n      if (!(kernel_arg_mem->allocation_type &\n            ACL_GLOBAL_MEM_DEVICE_ALLOCATION)) {\n        if (kernel_arg_mem->allocation_type & ACL_GLOBAL_MEM_HOST_ALLOCATION) {\n          // Host not compatible with device memory.\n          ERR_RET(CL_INVALID_ARG_VALUE, context,\n                  \"Argument expects host allocation but pointer is to \"\n                  \"USM device memory\");\n        } else if (kernel_arg_mem->allocation_type &\n                   ACL_GLOBAL_MEM_SHARED_ALLOCATION) {\n          // Shared not compatible with device memory.\n          ERR_RET(CL_INVALID_ARG_VALUE, context,\n                  \"Argument expects shared allocation but pointer is to \"\n                  \"USM device memory\");\n        }\n      } else if (allocation_mem != kernel_arg_mem) {\n        ERR_RET(CL_INVALID_ARG_VALUE, context,\n                \"Possibly incompatible interface used for device memory \"\n                \"allocation.\");\n      }\n    } else if (usm_alloc->type == CL_MEM_TYPE_SHARED_INTEL) {\n      if (!(kernel_arg_mem->allocation_type &\n            ACL_GLOBAL_MEM_SHARED_ALLOCATION)) {\n        if (kernel_arg_mem->allocation_type &\n            ACL_GLOBAL_MEM_DEVICE_ALLOCATION) {\n          ERR_RET(CL_INVALID_ARG_VALUE, context,\n                  \"Argument expects device allocation but pointer is to \"\n                  \"USM shared memory\");\n        } else if (kernel_arg_mem->allocation_type &\n                   ACL_GLOBAL_MEM_HOST_ALLOCATION) {\n          bool compatible = false;\n          for (const auto &can_access : kernel_arg_mem->can_access_list) {\n            if (can_access == allocation_mem->name) {\n              compatible = true;\n              acl_context_callback(context,\n                                   \"Warning: \"\n                                   \"Argument expects host allocation but \"\n                                   \"pointer is to USM shared memory. \"\n                                   \"Performance issues may be encountered.\");\n              break;\n            }\n          }\n          if (!compatible) {\n            ERR_RET(CL_INVALID_ARG_VALUE, context,\n                    \"Argument expects host allocation but pointer is to \"\n                    \"USM shared memory\");\n          }\n        }\n      } else if (allocation_mem != kernel_arg_mem) {\n        ERR_RET(CL_INVALID_ARG_VALUE, context,\n                \"Possibly incompatible interface used for shared memory \"\n                \"allocation.\");\n      }\n    } else if (usm_alloc->type == CL_MEM_TYPE_HOST_INTEL) {\n      if (!(kernel_arg_mem->allocation_type & ACL_GLOBAL_MEM_HOST_ALLOCATION)) {\n        if (kernel_arg_mem->allocation_type &\n            ACL_GLOBAL_MEM_DEVICE_ALLOCATION) {\n          ERR_RET(CL_INVALID_ARG_VALUE, context,\n                  \"Argument expects device allocation but pointer is to \"\n                  \"USM host memory\");\n        } else if (kernel_arg_mem->allocation_type &\n                   ACL_GLOBAL_MEM_SHARED_ALLOCATION) {\n          bool compatible = false;\n          for (const auto &can_access : kernel_arg_mem->can_access_list) {\n            if (can_access == allocation_mem->name) {\n              compatible = true;\n              acl_context_callback(context,\n                                   \"Warning: \"\n                                   \"Argument expects shared allocation but \"\n                                   \"pointer is to USM host memory. \"\n                                   \"Performance issues may be encountered.\");\n              break;\n            }\n          }\n          if (!compatible) {\n            ERR_RET(CL_INVALID_ARG_VALUE, context,\n                    \"Argument expects shared allocation but pointer is \"\n                    \"to USM host memory\");\n          }\n        }\n      } else if (allocation_mem != kernel_arg_mem) {\n        ERR_RET(\n            CL_INVALID_ARG_VALUE, context,\n            \"Possibly incompatible interface used for host memory allocation.\");\n      }\n    }\n  } else {\n    // The allocation_type field was not indicated in the board_spec.xml for\n    // this allocation type.\n    // Only issue this warning if the BSP supports host and/or shared\n    // allocations.\n    if ((acl_get_hal()->shared_alloc || acl_get_hal()->host_alloc) &&\n        kernel->accel_def->is_sycl_compile == 0) {\n      acl_context_callback(\n          context,\n          \"Warning: Unable to determine memory interface\"\n          \" associated with this allocation.  Functional or performance issues \"\n          \"may be encountered.\");\n    }\n  }\n\n#endif\n\n  safe_memcpy(&(kernel->arg_value[start_idx]), &arg_value, iface_arg_size,\n              kernel->arg_value_size - start_idx, iface_arg_size);\n  kernel->arg_is_svm[arg_index] = CL_FALSE;\n  kernel->arg_is_ptr[arg_index] = CL_TRUE;\n\n  kernel->arg_defined[arg_index] = 1;\n\n  // double vector size if size < arg_index\n  while (kernel->ptr_arg_vector.size() <= arg_index) {\n    kernel->ptr_arg_vector.resize(kernel->ptr_arg_vector.size() * 2);\n  }\n  kernel->ptr_arg_vector[arg_index] = usm_alloc->range.begin;\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclSetKernelExecInfoIntelFPGA(cl_kernel kernel, cl_kernel_exec_info param_name,\n                             size_t param_value_size, const void *param_value) {\n  cl_context context;\n  cl_int status = CL_SUCCESS;\n  size_t iparam;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n  context = kernel->program->context;\n\n  if (param_value == NULL)\n    ERR_RET(CL_INVALID_VALUE, context, \"param_value cannot be NULL\");\n\n  switch (param_name) {\n  case CL_KERNEL_EXEC_INFO_SVM_PTRS: {\n    iparam = 0;\n    // param_value_size must be a coefficient of sizeof(void*)\n    if (param_value_size % sizeof(void *) != 0)\n      ERR_RET(CL_INVALID_VALUE, context, \"param_value_size is not valid\");\n\n    // The pointers must be valid svm pointers or svm pointers + offset into the\n    // SVM region.\n    for (iparam = 0; iparam < param_value_size / (sizeof(void *)); iparam++) {\n      if (!acl_ptr_is_contained_in_context_svm(\n              context, ((void **)param_value)[iparam])) {\n        ERR_RET(CL_INVALID_VALUE, context,\n                \"param_value contains a pointer that is not contained \"\n                \"in the SVM region\");\n      }\n    }\n    break;\n  }\n  case CL_KERNEL_EXEC_INFO_SVM_FINE_GRAIN_SYSTEM:\n    if (param_value_size != sizeof(cl_bool))\n      ERR_RET(CL_INVALID_VALUE, context, \"param_value_size is not valid\");\n    // We currently don't support any fine-grain system SVM:\n    if (*(cl_bool *)param_value == CL_TRUE)\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"No devices in context associated with \"\n              \"kernel support fine-grain system SVM allocations\");\n    break;\n  case CL_KERNEL_EXEC_INFO_INDIRECT_HOST_ACCESS_INTEL:\n    if (param_value_size != sizeof(cl_bool))\n      ERR_RET(CL_INVALID_VALUE, context, \"param_value_size is not valid\");\n\n    if (*((cl_bool *)param_value) != CL_TRUE &&\n        *((cl_bool *)param_value) != CL_FALSE) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"param_value is not valid cl_bool value\");\n    }\n    break;\n  case CL_KERNEL_EXEC_INFO_INDIRECT_DEVICE_ACCESS_INTEL:\n    if (param_value_size != sizeof(cl_bool))\n      ERR_RET(CL_INVALID_VALUE, context, \"param_value_size is not valid\");\n\n    if (*((cl_bool *)param_value) != CL_TRUE &&\n        *((cl_bool *)param_value) != CL_FALSE) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"param_value is not valid cl_bool value\");\n    }\n    break;\n  case CL_KERNEL_EXEC_INFO_INDIRECT_SHARED_ACCESS_INTEL:\n    if (param_value_size != sizeof(cl_bool))\n      ERR_RET(CL_INVALID_VALUE, context, \"param_value_size is not valid\");\n\n    if (*((cl_bool *)param_value) != CL_TRUE &&\n        *((cl_bool *)param_value) != CL_FALSE) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"param_value is not valid cl_bool value\");\n    }\n    break;\n  case CL_KERNEL_EXEC_INFO_USM_PTRS_INTEL:\n    iparam = 0;\n    kernel->ptr_hashtable.clear();\n    // param_value_size must be a coefficient of sizeof(void*)\n    if (param_value_size % sizeof(void *) != 0)\n      ERR_RET(CL_INVALID_VALUE, context, \"param_value_size is not valid\");\n\n    // The pointers must be valid device pointer\n    for (iparam = 0; iparam < param_value_size / (sizeof(void *)); iparam++) {\n      acl_usm_allocation_t *usm_alloc =\n          acl_get_usm_alloc_from_ptr(context, ((void **)param_value)[iparam]);\n      if (!usm_alloc) {\n        ERR_RET(CL_INVALID_VALUE, context,\n                \"param_value contains a pointer that is not part of context\");\n      }\n      kernel->ptr_hashtable.insert(usm_alloc->range.begin);\n    }\n    break;\n  default:\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid param_name\");\n  }\n\n  return status;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclSetKernelExecInfo(cl_kernel kernel, cl_kernel_exec_info param_name,\n                    size_t param_value_size, const void *param_value) {\n  return clSetKernelExecInfoIntelFPGA(kernel, param_name, param_value_size,\n                                      param_value);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetKernelArgInfoIntelFPGA(\n    cl_kernel kernel, cl_uint arg_indx, cl_kernel_arg_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  // According to specs, if the kernel is not created using source, we can\n  // return CL_KERNEL_ARG_INFO_NOT_AVAILABLE. But we have the info anyways, so\n  // we return it.\n  acl_result_t result;\n  cl_context context;\n  cl_program program;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n\n  program = kernel->program;\n  context = program->context;\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          context);\n\n  if (arg_indx >= kernel->accel_def->iface.args.size())\n    ERR_RET(CL_INVALID_ARG_INDEX, context, \"Invalid kernel arg index.\");\n  // addr_space and type_qualifier is always available via autodiscovery, the\n  // other three parameters are optionally loaded in the autodiscovery string,\n  // therefore any one of the three parameter being empty infers information not\n  // available\n  if ((kernel->accel_def->iface.args[arg_indx].name.empty()) &&\n      !(param_name == CL_KERNEL_ARG_ADDRESS_QUALIFIER ||\n        param_name == CL_KERNEL_ARG_TYPE_QUALIFIER))\n    ERR_RET(CL_KERNEL_ARG_INFO_NOT_AVAILABLE, context,\n            \"Kernel arg info not available.\");\n\n  // filtering the arguments that are added by the compiler to handle printfs.\n  // In such cases, the arguments won't have any type, hence the type_name is\n  // empty.\n  if (!kernel->accel_def->iface.args[arg_indx].name.empty() &&\n      kernel->accel_def->iface.args[arg_indx].type_name.empty())\n    ERR_RET(CL_INVALID_ARG_INDEX, context, \"Invalid kernel arg index.\");\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_KERNEL_ARG_ADDRESS_QUALIFIER: {\n    cl_uint add_qualifier_map[] = {\n        CL_KERNEL_ARG_ADDRESS_PRIVATE, CL_KERNEL_ARG_ADDRESS_LOCAL,\n        CL_KERNEL_ARG_ADDRESS_GLOBAL, CL_KERNEL_ARG_ADDRESS_CONSTANT};\n    // we use different constants for these, so have to map them to OpenCL\n    // standard constants. our constants: ACL_ARG_ADDR_NONE, ACL_ARG_ADDR_LOCAL,\n    // ACL_ARG_ADDR_GLOBAL, ACL_ARG_ADDR_CONSTANT. NONE (which is the default)\n    // is the same as PRIVATE.\n    RESULT_ENUM(\n        add_qualifier_map[kernel->accel_def->iface.args[arg_indx].addr_space]);\n    break;\n  }\n  case CL_KERNEL_ARG_ACCESS_QUALIFIER: {\n    // mapping from our access qualifier enum (0,1,2,3) to OpenCL standard\n    // constants.\n    cl_uint access_qualifier_map[] = {\n        CL_KERNEL_ARG_ACCESS_NONE, CL_KERNEL_ARG_ACCESS_READ_ONLY,\n        CL_KERNEL_ARG_ACCESS_WRITE_ONLY, CL_KERNEL_ARG_ACCESS_READ_WRITE};\n    RESULT_ENUM(access_qualifier_map[kernel->accel_def->iface.args[arg_indx]\n                                         .access_qualifier]);\n    break;\n  }\n  case CL_KERNEL_ARG_TYPE_NAME:\n    RESULT_STR(kernel->accel_def->iface.args[arg_indx].type_name.c_str());\n    break;\n\n  case CL_KERNEL_ARG_TYPE_QUALIFIER:\n    RESULT_ENUM(kernel->accel_def->iface.args[arg_indx].type_qualifier);\n    break;\n\n  case CL_KERNEL_ARG_NAME:\n    RESULT_STR(kernel->accel_def->iface.args[arg_indx].name.c_str());\n    break;\n\n  default:\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid kernel arg info query\");\n  }\n\n  if (result.size == 0) {\n    return CL_INVALID_VALUE;\n  } // should have already signaled\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetKernelArgInfo(\n    cl_kernel kernel, cl_uint arg_indx, cl_kernel_arg_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  return clGetKernelArgInfoIntelFPGA(kernel, arg_indx, param_name,\n                                     param_value_size, param_value,\n                                     param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetKernelInfoIntelFPGA(\n    cl_kernel kernel, cl_kernel_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  cl_context context;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n\n  context = kernel->program->context;\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          context);\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_KERNEL_FUNCTION_NAME:\n    RESULT_STR(kernel->accel_def->iface.name.c_str());\n    break;\n  case CL_KERNEL_NUM_ARGS:\n    RESULT_UINT(static_cast<unsigned>(kernel->accel_def->iface.args.size()));\n    break;\n  case CL_KERNEL_REFERENCE_COUNT:\n    RESULT_UINT(acl_ref_count(kernel));\n    break;\n  case CL_KERNEL_CONTEXT:\n    RESULT_PTR(kernel->program->context);\n    break;\n  case CL_KERNEL_PROGRAM:\n    RESULT_PTR(kernel->program);\n    break;\n  case CL_KERNEL_ATTRIBUTES:\n    RESULT_STR(\"\");\n    break;\n\n  default:\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid kernel info query\");\n  }\n\n  if (result.size == 0) {\n    return CL_INVALID_VALUE;\n  } // should have already signaled\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetKernelInfo(cl_kernel kernel,\n                                                cl_kernel_info param_name,\n                                                size_t param_value_size,\n                                                void *param_value,\n                                                size_t *param_value_size_ret) {\n  return clGetKernelInfoIntelFPGA(kernel, param_name, param_value_size,\n                                  param_value, param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetKernelWorkGroupInfoIntelFPGA(\n    cl_kernel kernel, cl_device_id device, cl_kernel_work_group_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  cl_context context;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_kernel_is_valid(kernel)) {\n    return CL_INVALID_KERNEL;\n  }\n\n  context = kernel->program->context;\n\n  // Check or set device arg.\n  if (device != 0) {\n    // Must be on the list of devices for the program.\n    cl_program program = kernel->program;\n    cl_uint idev;\n    int matched = 0;\n    for (idev = 0; idev < program->num_devices && !matched; idev++) {\n      if (device == program->device[idev]) {\n        matched = 1;\n      }\n    }\n    if (!matched) {\n      ERR_RET(CL_INVALID_DEVICE, context,\n              \"Kernel program is not built for the specified device\");\n    }\n  } else {\n    // Must only be one device for this kernel.\n    if (kernel->program->num_devices != 1) {\n      ERR_RET(CL_INVALID_DEVICE, context,\n              \"Device is not specified, but kernel is not built for a unique \"\n              \"device\");\n    }\n    device = kernel->program->device[0];\n  }\n\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          context);\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_KERNEL_GLOBAL_WORK_SIZE:\n    // check if builtin kernel or a custom device (latter not applicable to\n    // intel fpgas)\n    if (kernel->program->uses_builtin_kernels) {\n      RESULT_SIZE_T3(acl_platform.max_work_item_sizes,\n                     acl_platform.max_work_item_sizes,\n                     acl_platform.max_work_item_sizes);\n      break;\n    } else {\n      return CL_INVALID_VALUE;\n    }\n  case CL_KERNEL_WORK_GROUP_SIZE:\n    RESULT_SIZE_T(kernel->accel_def->max_work_group_size);\n    break;\n  case CL_KERNEL_COMPILE_WORK_GROUP_SIZE: {\n    const size_t *p = &(kernel->accel_def->compile_work_group_size[0]);\n    RESULT_SIZE_T3(p[0], p[1], p[2]);\n    break;\n  }\n  case CL_KERNEL_LOCAL_MEM_SIZE:\n    RESULT_SIZE_T(l_local_mem_size(kernel));\n    break;\n  case CL_KERNEL_PREFERRED_WORK_GROUP_SIZE_MULTIPLE:\n    RESULT_SIZE_T(1);\n    break; // No preference\n  case CL_KERNEL_PRIVATE_MEM_SIZE:\n    RESULT_ULONG(0);\n    break;\n  default:\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid kernel info query\");\n  }\n\n  if (result.size == 0) {\n    return CL_INVALID_VALUE;\n  } // already signalled.\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetKernelWorkGroupInfo(\n    cl_kernel kernel, cl_device_id device, cl_kernel_work_group_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  return clGetKernelWorkGroupInfoIntelFPGA(kernel, device, param_name,\n                                           param_value_size, param_value,\n                                           param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueNativeKernelIntelFPGA(\n    cl_command_queue command_queue, void (*user_func)(void *), void *args,\n    size_t cb_args, cl_uint num_mem_objects, const cl_mem *mem_list,\n    const void **args_mem_loc, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  // Avoid warnings\n  command_queue = command_queue;\n  user_func = user_func;\n  args = args;\n  cb_args = cb_args;\n  num_mem_objects = num_mem_objects;\n  mem_list = mem_list;\n  args_mem_loc = args_mem_loc;\n  num_events_in_wait_list = num_events_in_wait_list;\n  event_wait_list = event_wait_list;\n  event = event;\n\n  // We don't support native kernels.\n  ERR_RET(CL_INVALID_OPERATION, command_queue->context,\n          \"Native kernels are not supported.\");\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueNativeKernel(\n    cl_command_queue command_queue, void (*user_func)(void *), void *args,\n    size_t cb_args, cl_uint num_mem_objects, const cl_mem *mem_list,\n    const void **args_mem_loc, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueNativeKernelIntelFPGA(\n      command_queue, user_func, args, cb_args, num_mem_objects, mem_list,\n      args_mem_loc, num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclEnqueueTaskIntelFPGA(cl_command_queue command_queue, cl_kernel kernel,\n                       cl_uint num_events_in_wait_list,\n                       const cl_event *event_wait_list, cl_event *event) {\n  size_t task_global_work_size = 1;\n  size_t task_local_work_size = 1;\n  cl_int ret;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  ret = l_enqueue_kernel_with_type(\n      command_queue, kernel,\n      1, // task work dim\n      0, // global work offset\n      &task_global_work_size, &task_local_work_size, num_events_in_wait_list,\n      event_wait_list, event, CL_COMMAND_TASK);\n  return ret;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueTask(cl_command_queue command_queue,\n                                              cl_kernel kernel,\n                                              cl_uint num_events_in_wait_list,\n                                              const cl_event *event_wait_list,\n                                              cl_event *event) {\n  return clEnqueueTaskIntelFPGA(command_queue, kernel, num_events_in_wait_list,\n                                event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueNDRangeKernelIntelFPGA(\n    cl_command_queue command_queue, cl_kernel kernel, cl_uint work_dim,\n    const size_t *global_work_offset, const size_t *global_work_size,\n    const size_t *local_work_size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  cl_int ret;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  ret = l_enqueue_kernel_with_type(\n      command_queue, kernel, work_dim, global_work_offset, global_work_size,\n      local_work_size, num_events_in_wait_list, event_wait_list, event,\n      CL_COMMAND_NDRANGE_KERNEL);\n\n  return ret;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueNDRangeKernel(\n    cl_command_queue command_queue, cl_kernel kernel, cl_uint work_dim,\n    const size_t *global_work_offset, const size_t *global_work_size,\n    const size_t *local_work_size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueNDRangeKernelIntelFPGA(\n      command_queue, kernel, work_dim, global_work_offset, global_work_size,\n      local_work_size, num_events_in_wait_list, event_wait_list, event);\n}\n\n// This will force reset all of the running kernels, including the autorun\n// kernels for the devices associated with the program\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clResetKernelsIntelFPGA(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context))\n    return CL_INVALID_CONTEXT;\n  if (num_devices == 0 && device_list != NULL)\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"num_devices is 0 while device list is not NULL\");\n  if (device_list) {\n    // The supplied devices must be associated with the context.\n    cl_uint idev, ictxdev;\n    for (idev = 0; idev < num_devices; idev++) {\n      // Optimize the common case: the caller is just using the same\n      // device list as passed in to the context creation\n      int saw_it = idev < context->num_devices &&\n                   context->device[idev] == device_list[idev];\n      for (ictxdev = 0; ictxdev < context->num_devices && !saw_it; ictxdev++) {\n        saw_it = (context->device[ictxdev] == device_list[idev]);\n      }\n      if (!saw_it) {\n        ERR_RET(CL_INVALID_DEVICE, context,\n                \"A specified device is not associated with the context\");\n      }\n    }\n    // Ok, each device is associated with the context.\n  } else {\n    // Build for all devices in the context.\n    // Heinous to overload the variables...\n    num_devices = context->num_devices;\n    device_list = context->device;\n  }\n\n  {\n    cl_uint idev;\n    for (idev = 0; idev < num_devices; idev++) {\n      acl_print_debug_msg(\"reseting all the kernels of device %u\\n\",\n                          device_list[idev]->def.physical_device_id);\n      acl_get_hal()->reset_kernels(device_list[idev]);\n    }\n  }\n  acl_idle_update(context); // nudge the scheduler to take care of the rest.\n\n  return CL_SUCCESS;\n}\n\n//////////////////////////////\n// Internal\n\nstatic void l_get_arg_offset_and_size(cl_kernel kernel, cl_uint arg_index,\n                                      size_t *start_idx_ret, size_t *size_ret) {\n  size_t start_idx = 0;\n  cl_uint i;\n  acl_assert_locked();\n\n  for (i = 0, start_idx = 0; i < arg_index; i++) {\n    start_idx += kernel->accel_def->iface.args[i].size;\n  }\n  *start_idx_ret = start_idx;\n  *size_ret = kernel->accel_def->iface.args[arg_index].size;\n}\n\nstatic void l_abort_use_of_wrapper(acl_kernel_invocation_wrapper_t *wrapper) {\n  acl_assert_locked();\n\n  // Make this wrapper available to later kernel launches.\n  acl_reset_ref_count(wrapper);\n}\n\nstatic cl_int l_enqueue_kernel_with_type(\n    cl_command_queue command_queue, cl_kernel kernel, cl_uint work_dim,\n    const size_t *global_work_offset, const size_t *global_work_size,\n    const size_t *local_work_size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event, cl_command_type type) {\n  // Errors:\n  // Coded, Unit tested:\n  //    CL_INVALID_PROGRAM_EXECUTABLE\n  //    CL_INVALID_COMMAND_QUEUE\n  //    CL_INVALID_KERNEL\n  //    CL_INVALID_CONTEXT\n  //    CL_INVALID_KERNEL_ARGS\n  //    CL_INVALID_WORK_DIMENSION\n  //    CL_INVALID_GLOBAL_OFFSET\n  //    CL_OUT_OF_RESOURCES : global work sizes out of range for device size_t\n  // Coded, for conformance testing:\n  //    CL_INVALID_CONTEXT\n  //    CL_INVALID_WORK_GROUP_SIZE\n  //    CL_INVALID_WORK_GROUP_SIZE\n  //    CL_INVALID_WORK_GROUP_SIZE\n  //    CL_INVALID_WORK_ITEM_SIZE\n  //    CL_MEM_OBJECT_ALLOCATION_FAILURE\n  //    CL_INVALID_EVENT_WAIT_LIST\n  //    CL_OUT_OF_HOST_MEMORY\n  // Not coded at all:\n  //    CL_OUT_OF_RESOURCES : others\n\n  cl_device_id device = 0;\n  size_t effective_work_group_size[3] = {\n      0, 0, 0}; // 0,0,0 means we should determine it automatically.\n  size_t effective_global_work_size[3] = {0, 0, 0};\n  cl_event launch_event = 0;\n  acl_kernel_invocation_wrapper_t *wrapper = 0, *serialization_wrapper = 0;\n  acl_dev_kernel_invocation_image_t *invocation = 0;\n  cl_context context;\n  acl_mem_migrate_t memory_migration;\n  memory_migration.num_mem_objects = 0;\n  cl_int status = CL_SUCCESS;\n  int serialization_needed = 0;\n  acl_assert_locked();\n\n#ifndef REMOVE_VALID_CHECKS\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  context = command_queue->context;\n  if (!acl_kernel_is_valid(kernel)) {\n    ERR_RET(CL_INVALID_KERNEL, context, \"Invalid kernel\");\n  }\n\n  if (kernel->program->context != command_queue->context) {\n    acl_context_callback(context,\n                         \"Kernel context does not match command queue context\");\n    acl_context_callback(kernel->program->context,\n                         \"Kernel context does not match command queue context\");\n    return CL_INVALID_CONTEXT;\n  }\n#endif\n\n  device = command_queue->device;\n\n  /* Refine the accel def to be specific to the device for this queue */\n  const acl_accel_def_t *new_accel_def = acl_find_accel_def(\n      kernel->program, kernel->accel_def->iface.name, kernel->dev_bin, &status,\n      kernel->program->context, device);\n  if (!new_accel_def || status != CL_SUCCESS) {\n    ERR_RET(CL_INVALID_PROGRAM_EXECUTABLE, context,\n            \"Kernel program is not built for the device associated with the \"\n            \"command queue\");\n  }\n\n  // Update to the found def:\n  kernel->accel_def = new_accel_def;\n\n#ifndef REMOVE_VALID_CHECKS\n  // All kernel args must have been set.\n  // If any args are SVM pointers the device must support SVM. Similarily, if\n  // any args are non-SVM pointers the device must support physical memory.\n  cl_bool device_supports_svm, device_support_physical_memory;\n  cl_uint num_args_not_set = 0;\n  device_supports_svm = acl_svm_device_supports_any_svm(\n      command_queue->device->def.physical_device_id);\n  device_support_physical_memory = acl_svm_device_supports_physical_memory(\n      command_queue->device->def.physical_device_id);\n  for (cl_uint iarg = 0; iarg < kernel->accel_def->iface.args.size(); ++iarg) {\n    if (!kernel->arg_defined[iarg])\n      num_args_not_set++;\n\n    if (kernel->arg_is_svm[iarg] && !device_supports_svm) {\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"Kernel argument is SVM pointer but device does not support SVM\");\n    }\n    if (kernel->accel_def->iface.args[iarg].category == ACL_ARG_MEM_OBJ &&\n        !kernel->arg_is_svm[iarg] && !device_support_physical_memory &&\n        kernel->accel_def->iface.args[iarg].addr_space != ACL_ARG_ADDR_LOCAL) {\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"Kernel argument is non-SVM pointer but device does not \"\n              \"support physical memories\");\n    }\n  }\n\n  if (num_args_not_set > 0) {\n    ERR_RET(CL_INVALID_KERNEL_ARGS, context,\n            \"Some kernel arguments are not set\");\n  }\n\n  // The program must have been built for this device.\n  cl_program program = kernel->program;\n  cl_uint dev_idx =\n      program->num_devices; // index into kernel->program->device[] that matches\n                            // command_queue->device\n  for (cl_uint idev = 0; idev < program->num_devices; idev++) {\n    if (program->device[idev] == device) {\n      dev_idx = idev;\n      break;\n    }\n  }\n\n  // Kernel's program is not associated with this command queue's device\n  if (dev_idx == program->num_devices) {\n    ERR_RET(CL_INVALID_PROGRAM_EXECUTABLE, context,\n            \"Kernel program is not built for the device associated with the \"\n            \"command queue\");\n  }\n  // Kernel's program has not been compiled for this device\n  if (!kernel->dev_bin ||\n      kernel->dev_bin->get_dev_prog()->build_status != CL_BUILD_SUCCESS) {\n    ERR_RET(CL_INVALID_PROGRAM_EXECUTABLE, context,\n            \"Kernel program is not built for the device associated with the \"\n            \"command queue\");\n  }\n\n  if (work_dim < 1 || 3 < work_dim) {\n    ERR_RET(CL_INVALID_WORK_DIMENSION, context, \"Invalid work dimension\");\n  }\n\n  if (!global_work_size) {\n    ERR_RET(CL_INVALID_GLOBAL_WORK_SIZE, context, \"global_work_size is NULL\");\n  }\n\n  if (global_work_offset) {\n    for (unsigned i = 0; i < work_dim; ++i) {\n      // global_work_offset is only used in OpenCL 1.1 and later.\n      // Also the user must not have explicitly disabled support for it\n      // using the uses_global_work_offset kernel attribute.\n      if (kernel->accel_def->uses_global_work_offset == 0 &&\n          global_work_offset[i] != 0) {\n        ERR_RET(CL_INVALID_GLOBAL_OFFSET, context,\n                \"Non-zero global_work_offset is not allowed. Kernel program \"\n                \"was built with a version \"\n                \"of the Intel(R) FPGA SDK for OpenCL(TM) that only supports \"\n                \"OpenCL 1.0 or the \"\n                \"uses_global_work_offset kernel attribute was set to 0.\");\n      }\n\n      // The kernel id iterator hardware is optimized out when\n      // max_global_work_dim is set to 0 so we cannot support a non-zero\n      // global_work_offset.\n      if (kernel->accel_def->max_global_work_dim == 0 &&\n          global_work_offset[i] != 0) {\n        ERR_RET(CL_INVALID_GLOBAL_OFFSET, context,\n                \"Non-zero global_work_offset is not allowed for kernel with \"\n                \"max_global_work_dim attribute set to 0.\");\n      }\n\n      // According to the OpenCL specifications the sum of global_work_offset\n      // and global_work_size cannot overflow a variable of type size_t on the\n      // device. On SoC systems the host size_t is 32-bits but the FPGA is still\n      // a 64-bit device so we need to explicitly cast these to cl_ulong before\n      // checking overflow.\n      if ((cl_ulong)global_work_size[i] + (cl_ulong)global_work_offset[i] <\n          (cl_ulong)global_work_size[i]) {\n        std::stringstream ss;\n        ss << \"Invalid global work offset at dimension \" << i\n           << \". The sum of global work size and global work offset: \"\n           << global_work_size[i] << \" + \" << global_work_offset[i]\n           << \" exceeds the maximum value storeable in a variable of type \"\n              \"size_t \"\n              \"on the device without overflowing.\";\n        ERR_RET(CL_INVALID_GLOBAL_OFFSET, context, ss.str().c_str());\n      }\n    }\n  }\n#endif\n\n  // Determine effective global work size.\n  for (cl_uint idim = 0; idim < work_dim; idim++) {\n    acl_print_debug_msg(\" global work size[%d] = %zu\\n\", idim,\n                        global_work_size[idim]);\n\n#ifndef REMOVE_VALID_CHECKS\n    // Check that the passed-in global work sizes are compatible with the\n    // max_global_work_dim declared for the kernel (if any)\n    unsigned int kernel_max_global_work_dim =\n        kernel->accel_def->max_global_work_dim;\n    if (kernel_max_global_work_dim != 0 && idim >= kernel_max_global_work_dim &&\n        global_work_size[idim] != 1) {\n      std::stringstream ss;\n      ss << \"Invalid global work size for kernel with max_global_work_dim \"\n            \"attribute set to \"\n         << kernel_max_global_work_dim << \". global_work_size[\" << idim\n         << \"] must be 1.\";\n      ERR_RET(CL_INVALID_GLOBAL_WORK_SIZE, context, ss.str().c_str());\n    } else if (kernel_max_global_work_dim == 0 && global_work_size[idim] != 1) {\n      ERR_RET(\n          CL_INVALID_GLOBAL_WORK_SIZE, context,\n          \"Invalid global work size for kernel with max_global_work_dim \"\n          \"attribute set to 0. All elements of global_work_size must be 1.\");\n    }\n#endif\n    effective_global_work_size[idim] = global_work_size[idim];\n  }\n\n  // Determine effective work group size.\n  if (local_work_size) {\n    cl_uint idim;\n    for (idim = 0; idim < work_dim; idim++) {\n      unsigned int kernel_max_global_work_dim;\n      size_t kernel_compile_work_group_size_idim;\n      acl_print_debug_msg(\" local work size[%d] = %zu\\n\", idim,\n                          local_work_size[idim]);\n      // If the work group size was specified in the kernel source, then\n      // it must match the passed-in size. If the kernel is workgroup invariant,\n      // then this check can be safely skipped.\n#ifndef REMOVE_VALID_CHECKS\n      kernel_compile_work_group_size_idim =\n          kernel->accel_def->compile_work_group_size[idim];\n      if (!kernel->accel_def->is_workgroup_invariant &&\n          kernel_compile_work_group_size_idim &&\n          kernel_compile_work_group_size_idim != local_work_size[idim]) {\n        std::stringstream ss;\n        ss << \"Specified work group size \" << local_work_size[idim]\n           << \" does not match reqd_work_group_size kernel attribute \"\n           << kernel_compile_work_group_size_idim << \".\";\n        ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context, ss.str().c_str());\n      }\n\n      // Check that the passed-in local work sizes are compatible with the\n      // max_global_work_dim declared for the kernel (if any)\n      kernel_max_global_work_dim = kernel->accel_def->max_global_work_dim;\n      if (kernel_max_global_work_dim != 0 &&\n          idim >= kernel_max_global_work_dim && local_work_size[idim] != 1) {\n        std::stringstream ss;\n        ss << \"Invalid local work size for kernel with max_global_work_dim \"\n              \"attribute set to \"\n           << kernel_max_global_work_dim << \". local_work_size[\" << idim\n           << \"] must be 1.\";\n        ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context, ss.str().c_str());\n      } else if (kernel_max_global_work_dim == 0 &&\n                 local_work_size[idim] != 1) {\n        ERR_RET(\n            CL_INVALID_WORK_GROUP_SIZE, context,\n            \"Invalid local work size for kernel with max_global_work_dim \"\n            \"attribute set to 0. All elements of local_work_size must be 1.\");\n      }\n#endif\n\n      // Save this value.\n      effective_work_group_size[idim] = local_work_size[idim];\n    }\n  } else if (kernel->accel_def->compile_work_group_size[0]) {\n    // Take the size as defined in the kernel source.\n    cl_uint idim;\n    for (idim = 0; idim < work_dim; idim++) {\n      effective_work_group_size[idim] =\n          kernel->accel_def->compile_work_group_size[idim];\n      acl_print_debug_msg(\" reqd work size[%d] = %zu\\n\", idim,\n                          effective_work_group_size[idim]);\n    }\n  } else {\n    // If the neither the user nor the kernel cares, then for now just\n    // use work group size of 1 in each interesting dimension.\n    // This is an easy default which will always pass the remaining\n    // tests, and use minimal resources on the hardware.\n    // But it might be slow. In which case the developer should be more\n    // explicit.\n    cl_uint idim;\n    for (idim = 0; idim < work_dim; idim++) {\n      effective_work_group_size[idim] = 1;\n      acl_print_debug_msg(\" defaulting work grp size[%d] = %zu\\n\", idim,\n                          effective_work_group_size[idim]);\n    }\n  }\n\n#ifndef REMOVE_VALID_CHECKS\n  // Check that all dimensions of the work group have non-zero size\n  // Check that each dim does not exceed device limit\n  // Check that total local work item count does not exceed device limit\n  size_t max_items;\n  size_t max_dim_items[3];\n  size_t num_items = 1;\n  clGetDeviceInfo(device, CL_DEVICE_MAX_WORK_ITEM_SIZES, sizeof(max_dim_items),\n                  &max_dim_items, 0);\n  clGetDeviceInfo(device, CL_DEVICE_MAX_WORK_GROUP_SIZE, sizeof(max_items),\n                  &max_items, 0);\n\n  for (cl_uint idim = 0; idim < work_dim; idim++) {\n    acl_print_debug_msg(\" eff work grp size[%d] = %zu\\n\", idim,\n                        effective_work_group_size[idim]);\n    if (effective_work_group_size[idim] == 0) {\n      ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context,\n              \"Work group size is zero in one or more dimensions\");\n    }\n    if (effective_work_group_size[idim] > max_dim_items[idim]) {\n      ERR_RET(CL_INVALID_WORK_ITEM_SIZE, context,\n              \"Work group size in one or more dimensions exceeds device limit \"\n              \"specified in CL_DEVICE_MAX_WORK_ITEM_SIZES\");\n    }\n\n    num_items *= effective_work_group_size[idim];\n\n    // If the user hasn't specified implicitly that they are using more\n    // than one workgroup (ie. local or global thread IDs query, or work-group\n    // ID query) then it is possible that the max_work_group_size_arr has the\n    // format {max_work_group_size, 0, 0}, in which case we just skip the\n    // individual dimension check and just check that the (product of dimensions\n    // < max_work_group_size)\n    unsigned int kernel_max_work_group_size_arr =\n        kernel->accel_def->max_work_group_size_arr[idim];\n    if (kernel_max_work_group_size_arr > 0 &&\n        effective_work_group_size[idim] > kernel_max_work_group_size_arr) {\n      std::stringstream ss;\n      ss << \"Work group size \" << effective_work_group_size[idim]\n         << \" exceeds kernel-specific limit \" << kernel_max_work_group_size_arr\n         << \".\";\n      ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context, ss.str().c_str());\n    }\n  }\n\n  if (num_items > max_items) {\n    std::stringstream ss;\n    ss << \"Total work group size \" << num_items\n       << \" exceeds device limit specified in CL_DEVICE_MAX_WORK_GROUP_SIZE \"\n       << max_items << \".\";\n    ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context, ss.str().c_str());\n  }\n\n  // Check that number threads no larger than the attribute\n  // max_work_group_size. If user didn't specify the kernel attribute,\n  // then a default from AlteraCLParam.h was set.  Letting the kernel\n  // size go over this limit will overrun barriers and cause a hang.\n  unsigned int kernel_max_work_group_size =\n      kernel->accel_def->max_work_group_size;\n  if (num_items > kernel_max_work_group_size) {\n    std::stringstream ss;\n    ss << \"Total work group size \" << num_items\n       << \" exceeds kernel-specific limit \" << kernel_max_work_group_size\n       << \".\";\n    ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context, ss.str().c_str());\n  }\n\n  // Check global_work_size\n  for (cl_uint idim = 0; idim < work_dim; idim++) {\n    if (effective_global_work_size[idim] > 4294967295U) {\n      // This check is technically not OpenCL spec compliant since we should\n      // support any value that can be represented in a variable of type size_t.\n      // However, our CRA limits global_work_size values to 32-bit unsigned ints\n      // so the max value is 2^32-1. After the CRA is changed to take a 64-bit\n      // global_work_size input this check isn't needed because any size_t value\n      // except 0 is valid.\n      ERR_RET(CL_OUT_OF_RESOURCES, context,\n              \"Global work size exceeds limit of 4294967295 in one or more \"\n              \"dimensions\");\n    }\n\n    // Can evenly divide the global space into work groups.\n    if (effective_global_work_size[idim] == 0) {\n      ERR_RET(CL_INVALID_GLOBAL_WORK_SIZE, context,\n              \"Global work size is zero in one or more dimensions\");\n    }\n    if (effective_work_group_size[idim] &&\n        effective_global_work_size[idim] % effective_work_group_size[idim]) {\n      std::stringstream ss;\n      ss << \"Work group size \" << effective_work_group_size[idim]\n         << \" does not divide evenly into global work size \"\n         << effective_global_work_size[idim] << \".\";\n      ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context, ss.str().c_str());\n    }\n  }\n\n  // If the kernel code can't tell that the workgroup is smaller than\n  // the entire global space, then just use one workgroup.\n  // Do this after all checks, so we have already given the right error\n  // messages.\n  if (kernel->accel_def->is_workgroup_invariant) {\n    for (cl_uint idim = 0; idim < work_dim; idim++) {\n      effective_work_group_size[idim] = effective_global_work_size[idim];\n    }\n  }\n#endif // REMOVE_VALID_CHECKS\n\n  // handle vectorization information, if auto-discovery is used.\n  if (kernel->accel_def->num_vector_lanes != 0) {\n    // ACL compiler makes sure that vectorization is only applied when vector\n    // lanes evenly divides work_group_size[0]\n    if ((effective_work_group_size[0] % kernel->accel_def->num_vector_lanes) !=\n        0) {\n      ERR_RET(CL_INVALID_WORK_GROUP_SIZE, context,\n              \"Kernel vector lane parameter does not evenly divide into work \"\n              \"group size in dimension 0\");\n    }\n    // Scale down local_size0 and global_size0\n    effective_work_group_size[0] =\n        effective_work_group_size[0] / kernel->accel_def->num_vector_lanes;\n    effective_global_work_size[0] =\n        effective_global_work_size[0] / kernel->accel_def->num_vector_lanes;\n  }\n\n  if (debug_mode > 0) {\n    printf(\" vectorized x%d\\n\", kernel->accel_def->num_vector_lanes);\n    printf(\" eff work grp size[0] = %zu\\n\", effective_work_group_size[0]);\n    printf(\" eff global work size[0] = %zu\\n\", effective_global_work_size[0]);\n  }\n\n  ACL_KERNEL_DEBUG_MSG_VERBOSE(1, \"KERNEL: In enqueue kernel; device id: %u \\n\",\n                               device->def.physical_device_id);\n\n  // Kernel host pipe binding\n  for (const auto &pipe : context->pipe_vec) {\n    // If this is a host pipe\n    if (pipe->host_pipe_info != nullptr) {\n      if (pipe->host_pipe_info->binded) {\n        // The host pipe was binded to host already, maybe because a previous\n        // enqueue We just need to check if it was binded to the correct device\n        // Relaunching\n        if (kernel == pipe->host_pipe_info->m_binded_kernel) {\n          ;\n        }\n        // Else case: we don't do anything, because this host pipe was binded to\n        // another kernel\n      } else {\n        // Here the host pipe is not bineded to a host cl_pipe\n        if (pipe->host_pipe_info->m_binded_kernel) {\n          // clSetKernelArg was called to store the info needed for the binding\n          if (kernel == pipe->host_pipe_info->m_binded_kernel) {\n            // Only bind and process pipe transactions if the program is loaded\n            // onto the device. Otherwise, we do them after device programming\n            if (device->loaded_bin &&\n                device->loaded_bin->get_dev_prog()->program ==\n                    kernel->program) {\n              if (!context->uses_dynamic_sysdef) {\n                // In mode 3\n                status = acl_bind_pipe_to_channel(\n                    pipe, device, device->def.autodiscovery_def);\n              } else {\n                // Not in mode 3\n                status = acl_bind_pipe_to_channel(\n                    pipe, device,\n                    kernel->dev_bin->get_devdef().autodiscovery_def);\n              }\n              if (status != CL_SUCCESS) {\n                ERR_RET(status, context, \"Host pipe binding error\");\n              }\n              acl_process_pipe_transactions(pipe);\n            }\n          }\n          // Else case: we don't do anything, because this host pipe will need\n          // to be binded to another kernel\n        } else {\n          // clSetKernelArg not called\n          // Two cases here:\n          // 1. User forgot to call clSetKernelArg, this should be reported as\n          // error\n          //    Error reporting is done above(so we should't hit this case here)\n          // 2. User enqueue a different kernel than the one this host pipe is\n          // bounded to\n          //    Not an error, do nothing\n        }\n      }\n    }\n  }\n\n  cl_uint kernel_arg_bytes = 0;\n  // If the global work size is more than one but the compiler didn't infere it,\n  // serializing the execution of workitems also warn the user to set up proper\n  // attribute for efficient throughput\n  if (kernel->accel_def->is_workitem_invariant) {\n    cl_uint idim2;\n    for (idim2 = 0; idim2 < work_dim; idim2++) {\n      if (effective_global_work_size[idim2] > 1) {\n        serialization_needed = 1;\n        acl_context_callback(\n            context, \"Warning: Launching a kernel with global_work_size > 1, \"\n                     \"while the kernel was compiled to be work-item invariant. \"\n                     \"This will lead to low performance, due to serialization \"\n                     \"of work-items. \"\n                     \"To avoid this issue, explicitly specify a \"\n                     \"req_work_group_size larger than (1,1,1).\");\n        break;\n      }\n    }\n  }\n  if (serialization_needed) {\n    // Set up a wrapper that launches only one workitem, but keep track of the\n    // whole ndrange through a backup wrapper.\n    serialization_wrapper = acl_get_unused_kernel_invocation_wrapper(context);\n    if (serialization_wrapper == 0) {\n      // If some kernels have terminated recently, then maybe we can\n      // reclaim their invocation images, and retry the allocation.\n      // Do this conditionally to avoid queue processing overhead.\n      acl_idle_update(context);\n      serialization_wrapper = acl_get_unused_kernel_invocation_wrapper(context);\n      if (serialization_wrapper == 0) {\n        ERR_RET(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not allocate a kernel invocation object\");\n      }\n    }\n    acl_retain(serialization_wrapper);\n    serialization_wrapper->event = 0;\n\n    invocation = serialization_wrapper->image;\n    invocation->padding = 0;\n    invocation->work_group_size = 1;\n\n    invocation->activation_id =\n        -1; // Will be updated when submitted to the device op queue.\n    invocation->accel_id = kernel->accel_def->id;\n    invocation->work_dim = 1;\n\n    invocation->global_work_size[0] = invocation->global_work_size[1] =\n        invocation->global_work_size[2] = 1;\n    invocation->num_groups[0] = invocation->num_groups[1] =\n        invocation->num_groups[2] = 1;\n    invocation->local_work_size[0] = invocation->local_work_size[1] =\n        invocation->local_work_size[2] = 1;\n    invocation->global_work_offset[0] = invocation->global_work_offset[1] =\n        invocation->global_work_offset[2] = 0;\n\n    // Allocate same amount of space as the kernel args. May be different from\n    // the adjusted size below.\n    invocation->arg_value = acl_new_arr<char>(kernel->arg_value_size);\n    if (kernel->arg_value == nullptr) {\n      ERR_RET(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate memory for kernel arguments\");\n    }\n\n    kernel_arg_bytes = (cl_uint)l_copy_and_adjust_arguments_for_device(\n        kernel, device, &(invocation->arg_value[0]), &kernel_arg_bytes,\n        &memory_migration, serialization_wrapper->streaming_args);\n\n    assert(kernel_arg_bytes <= kernel->arg_value_size);\n\n    invocation->arg_value_size = kernel_arg_bytes;\n  }\n\n  // Set up the launch parameters\n  wrapper = acl_get_unused_kernel_invocation_wrapper(context);\n  if (wrapper == 0) {\n    // If some kernels have terminated recently, then maybe we can\n    // reclaim their invocation images, and retry the allocation.\n    // Do this conditionally to avoid queue processing overhead.\n    acl_idle_update(context);\n    wrapper = acl_get_unused_kernel_invocation_wrapper(context);\n    if (wrapper == 0) {\n      ERR_RET(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a kernel invocation object\");\n    }\n  }\n\n  acl_retain(wrapper);\n  wrapper->event = 0;\n\n  invocation = wrapper->image;\n  invocation->padding = 0;\n  invocation->work_group_size = 1; // Will be updated later.\n\n  invocation->activation_id =\n      -1; // Will be updated when submitted to the device op queue.\n  invocation->accel_id = kernel->accel_def->id;\n  invocation->work_dim = work_dim;\n\n  for (cl_uint idim = 0; idim < work_dim; idim++) {\n    // Earlier size checks ensure these casts are safe.\n    invocation->global_work_size[idim] =\n        (cl_uint)effective_global_work_size[idim];\n    invocation->num_groups[idim] = (cl_uint)(effective_global_work_size[idim] /\n                                             effective_work_group_size[idim]);\n    invocation->local_work_size[idim] =\n        (cl_uint)effective_work_group_size[idim];\n    invocation->work_group_size *= (cl_uint)effective_work_group_size[idim];\n    invocation->global_work_offset[idim] =\n        global_work_offset ? (cl_ulong)global_work_offset[idim] : 0L;\n  }\n  for (cl_uint idim = work_dim; idim < 3; idim++) {\n    invocation->global_work_size[idim] = 1;\n    invocation->num_groups[idim] = 1;\n    invocation->local_work_size[idim] = 1;\n    invocation->global_work_offset[idim] = 0L;\n  }\n\n  memory_migration.num_mem_objects = 0;\n\n  // Allocate same amount of space as the kernel args. May be different from the\n  // adjusted size below.\n  invocation->arg_value = acl_new_arr<char>(kernel->arg_value_size);\n  if (kernel->arg_value == nullptr) {\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, context,\n            \"Could not allocate memory for kernel arguments\");\n  }\n\n  status = l_copy_and_adjust_arguments_for_device(\n      kernel, device, &(invocation->arg_value[0]), &kernel_arg_bytes,\n      &memory_migration, wrapper->streaming_args);\n\n  if (status != CL_SUCCESS) {\n    ERR_RET(status, context, \"Argument error\");\n  }\n\n  assert(kernel_arg_bytes <= kernel->arg_value_size);\n\n  invocation->arg_value_size = kernel_arg_bytes;\n\n  // The HAL kernel launch method will write the invocation\n  // parameters into the accelerator's control/status registers.\n\n  // Schedule the kernel invocation.\n  status = acl_create_event(command_queue, num_events_in_wait_list,\n                            event_wait_list, type, &launch_event);\n\n  if (status != CL_SUCCESS) {\n    // Bail out. We'll never do the operation.\n    // Already signaled error via callback\n    l_abort_use_of_wrapper(wrapper);\n    return status;\n  }\n  acl_retain(kernel);\n  launch_event->cmd.info.ndrange_kernel.kernel = kernel;\n  launch_event->cmd.info.ndrange_kernel.device = device;\n  launch_event->cmd.info.ndrange_kernel.accel_id = kernel->accel_def->id;\n  launch_event->cmd.info.ndrange_kernel.dev_bin = kernel->dev_bin;\n  if (serialization_needed) {\n    launch_event->cmd.info.ndrange_kernel.serialization_wrapper = wrapper;\n    launch_event->cmd.info.ndrange_kernel.invocation_wrapper =\n        serialization_wrapper;\n  } else {\n    launch_event->cmd.info.ndrange_kernel.serialization_wrapper = NULL;\n    launch_event->cmd.info.ndrange_kernel.invocation_wrapper = wrapper;\n  }\n  launch_event->cmd.info.ndrange_kernel.memory_migration = memory_migration;\n  launch_event->completion_callback = l_complete_kernel_execution;\n  if (acl_kernel_is_valid(kernel)) {\n    launch_event->ptr_hashtable = kernel->ptr_hashtable;\n  }\n  for (auto arg_ptr : kernel->ptr_arg_vector) {\n    if (arg_ptr != NULL) {\n      launch_event->ptr_hashtable.insert(arg_ptr);\n    }\n  }\n\n  wrapper->event =\n      launch_event; // ... because Option3 CSR doesn't have a RUNNING status bit\n  if (serialization_wrapper)\n    serialization_wrapper->event = launch_event;\n\n  // Queue management overhead becomes significant once there are many command\n  // queues.  When we enqueue a task/transfer/etc, should we check all command\n  // queues or just the queue this event belongs to?  I've added\n  // acl_idle_update_queue to do that but do not use it (yet).\n  acl_idle_update(context); // Launch right away if we can.\n  // acl_idle_update_queue( command_queue );\n\n  if (event) {\n    *event = launch_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(launch_event);\n    acl_idle_update(context); // Clean up early.\n  }\n\n  return CL_SUCCESS;\n}\n\n// Reset contents of the kernel object.\n//\n// It doesn't belong to any program, etc.\nvoid acl_reset_kernel(cl_kernel kernel) {\n  acl_assert_locked();\n\n  if (acl_kernel_is_valid_ptr(kernel)) {\n    acl_reset_ref_count(kernel);\n    kernel->program = nullptr;\n    kernel->accel_def = nullptr;\n    kernel->dev_bin = nullptr;\n    std::fill(kernel->arg_defined.begin(), kernel->arg_defined.end(), 0);\n  }\n}\n\n// Just a regular old reference counted object.\nint acl_kernel_is_valid(cl_kernel kernel) {\n  acl_assert_locked();\n\n#ifdef REMOVE_VALID_CHECKS\n  return 1;\n#else\n  if (!acl_kernel_is_valid_ptr(kernel))\n    return 0;\n  if (!acl_ref_count(kernel))\n    return 0;\n  if (!acl_program_is_valid(kernel->program))\n    return 0;\n  return 1;\n#endif\n}\n\nstatic int l_init_kernel(cl_kernel kernel, cl_program program,\n                         const acl_accel_def_t *accel_def,\n                         const acl_device_binary_t *dev_bin,\n                         cl_int *errcode_ret) {\n  cl_mem tmp_printf_buffer_start = (cl_mem)0;\n  void *tmp_printf_ptr_start = 0;\n  cl_int err = CL_SUCCESS;\n  cl_uint printf_buffer_size;\n  unsigned int num_profile_counters;\n  acl_assert_locked();\n\n#ifndef REMOVE_VALID_CHECKS\n  if (!acl_program_is_valid(program))\n    BAIL(CL_INVALID_PROGRAM);\n  if (!acl_context_is_valid(program->context))\n    BAIL(CL_INVALID_CONTEXT);\n#endif\n  {\n    const char *kernel_debug_var = getenv(\"ACL_KERNEL_DEBUG\");\n    if (kernel_debug_var) {\n      debug_verbosity = atoi(kernel_debug_var);\n      ACL_KERNEL_DEBUG_MSG_VERBOSE(0, \"Setting kernel debug level to %u\\n\",\n                                   debug_verbosity);\n    }\n  }\n\n  acl_reset_kernel(kernel);\n  acl_retain(kernel);\n  kernel->dispatch = &acl_icd_dispatch;\n  kernel->program = program;\n  kernel->accel_def = accel_def;\n  // Dynamically allocate memory equal to the total size of kernel args and\n  // store how many bytes this is.\n  size_t total_arguments_size = 0;\n  for (acl_kernel_arg_info_t info : accel_def->iface.args) {\n    total_arguments_size += info.size;\n  }\n  kernel->arg_defined.resize(accel_def->iface.args.size(), 0);\n  kernel->arg_value_size = total_arguments_size;\n  kernel->arg_value = acl_new_arr<char>(total_arguments_size);\n  if (kernel->arg_value == nullptr) {\n    acl_release(kernel);\n    BAIL(CL_OUT_OF_HOST_MEMORY);\n  }\n  kernel->arg_is_svm.resize(accel_def->iface.args.size(), CL_FALSE);\n  kernel->arg_is_ptr.resize(accel_def->iface.args.size(), CL_FALSE);\n  kernel->dev_bin = dev_bin;\n  auto kernel_arg_num = static_cast<cl_uint>(\n      accel_def->iface.args.size() -\n      (!kernel->accel_def->printf_format_info.empty() ? 2 : 0));\n\n  // Check if there is hardware profile data in the kernel,\n  // if so, allocate the buffer to store the data when kernel completes\n  num_profile_counters = accel_def->profiling_words_to_readback;\n  if (num_profile_counters != 0) {\n    unsigned i;\n\n    kernel->profile_data =\n        (uint64_t *)acl_malloc(num_profile_counters * sizeof(uint64_t));\n    if (kernel->profile_data == 0) {\n      acl_release(kernel);\n      BAIL(CL_OUT_OF_HOST_MEMORY);\n    }\n    for (i = 0; i < num_profile_counters; ++i) {\n      kernel->profile_data[i] = 0;\n    }\n  } else {\n    kernel->profile_data = 0;\n  }\n\n  // Check if there are printfs in the kernel\n  kernel->printf_device_buffer = 0; // Default is none.\n  kernel->printf_device_ptr = 0;    // Default is none.\n  // Keep track of already processed buffer size\n  // It will be reset when the buffer is full and dumped.\n  kernel->processed_printf_buffer_size = 0;\n  if (!accel_def->printf_format_info.empty()) {\n    auto gmem_idx = static_cast<size_t>(\n        acl_get_default_memory(kernel->dev_bin->get_devdef()));\n    int default_memory_is_svm = kernel->dev_bin->get_devdef()\n                                    .autodiscovery_def.global_mem_defs[gmem_idx]\n                                    .type == ACL_GLOBAL_MEM_SHARED_VIRTUAL;\n    if (default_memory_is_svm) {\n      tmp_printf_ptr_start =\n          clSVMAlloc(program->context, CL_MEM_READ_WRITE,\n                     acl_get_platform()->printf_buffer_size, ACL_MEM_ALIGN);\n\n      tmp_printf_buffer_start = clCreateBuffer(\n          program->context, CL_MEM_USE_HOST_PTR,\n          acl_get_platform()->printf_buffer_size, tmp_printf_ptr_start, &err);\n      if (err != CL_SUCCESS) {\n        clSVMFree(program->context, tmp_printf_ptr_start);\n        acl_release(kernel);\n        BAIL(err);\n      }\n    } else {\n      tmp_printf_ptr_start = NULL;\n      tmp_printf_buffer_start =\n          clCreateBuffer(program->context, CL_MEM_READ_WRITE,\n                         acl_get_platform()->printf_buffer_size, 0, &err);\n      if (err != CL_SUCCESS) {\n        acl_release(kernel);\n        BAIL(err);\n      }\n    }\n\n    acl_print_debug_msg(\n        \"setting kernel arg #%d to printf buffer start address %p\\n\",\n        kernel_arg_num, tmp_printf_buffer_start->block_allocation->range.begin);\n\n    // printf buffer start address as kernel argument\n    err |= clSetKernelArg(kernel, kernel_arg_num++, sizeof(cl_mem),\n                          &tmp_printf_buffer_start);\n\n    acl_print_debug_msg(\"setting kernel arg #%d to printf buffer size %d\\n\",\n                        kernel_arg_num, acl_get_platform()->printf_buffer_size);\n\n    // set printf buffer size argument\n    printf_buffer_size = acl_get_platform()->printf_buffer_size;\n    err |= clSetKernelArg(kernel, kernel_arg_num++, sizeof(cl_uint),\n                          &printf_buffer_size);\n\n    if (err != CL_SUCCESS) {\n      acl_release(kernel);\n      clReleaseMemObject(tmp_printf_buffer_start);\n      if (tmp_printf_ptr_start)\n        clSVMFree(program->context, tmp_printf_ptr_start);\n      BAIL(err);\n    }\n\n    acl_print_debug_msg(\"setting device buffer[%d]=%p\\n\", accel_def->id,\n                        tmp_printf_buffer_start->block_allocation->range.begin);\n\n    kernel->printf_device_ptr = tmp_printf_ptr_start;\n    kernel->printf_device_buffer = tmp_printf_buffer_start;\n  }\n\n  // initialize ptr_arg_vector with size current kernel argument number\n  kernel->ptr_arg_vector.resize(accel_def->iface.args.size());\n\n#ifdef REMOVE_VALID_CHECKS\n  if (errcode_ret)\n    *errcode_ret = CL_SUCCESS;\n#else\n  if (errcode_ret)\n    *errcode_ret = err;\n#endif\n\n  clRetainProgram(program);\n\n  acl_print_debug_msg(\"Created kernel %p\\n\", kernel);\n  acl_track_object(ACL_OBJ_KERNEL, kernel);\n\n  return CL_SUCCESS;\n}\n\n// Find the accelerator implementing the given kernel.\n// Also return the acl_device_binary_t via dev_bin_ret. Just support one\n// device associated with a kernel, for now.\n//\n// This API will likely change once we support proper program compilation.\n//\n// Error codes must be consistent with those from clCreateKernel\nconst acl_accel_def_t *\nacl_find_accel_def(cl_program program, const std::string &kernel_name,\n                   const acl_device_binary_t *&dev_bin_ret, cl_int *errcode_ret,\n                   cl_context context, cl_device_id which_device) {\n  const acl_accel_def_t *result = nullptr;\n  int num_devices_having_built_program = 0;\n  acl_assert_locked();\n\n  if (!acl_program_is_valid(program))\n    BAIL_INFO(CL_INVALID_PROGRAM, context, \"Invalid program\");\n\n  // Search through devices in this program for which the build status is\n  // successful.\n  for (cl_uint idev = 0; idev < program->num_devices && result == 0; idev++) {\n    auto *dev_prog = program->dev_prog[idev];\n    if (dev_prog && dev_prog->build_status == CL_BUILD_SUCCESS) {\n      if (which_device != 0 && program->device[idev] != which_device) {\n        /* This isn't the device I'm looking for */\n        continue;\n      }\n\n      num_devices_having_built_program++;\n      dev_bin_ret = dev_prog->get_or_create_device_binary(kernel_name);\n      result = dev_prog->get_kernel_accel_def(kernel_name);\n    }\n  }\n\n  if (num_devices_having_built_program == 0)\n    BAIL_INFO(CL_INVALID_PROGRAM_EXECUTABLE, context, \"No programs are built\");\n  if (result == nullptr)\n    BAIL_INFO(CL_INVALID_KERNEL_NAME, context,\n              \"Specified kernel was not built for any devices\");\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  return result;\n}\n\n// Determine which accelerator definitions are consistent on all\n// devices for the given program.\n// Return value is consistent with clCreateKernelsInProgram\n//\n// Return the acl_device_binary_t and acl_accel_def_t for the consistent\n// accelerater definitions on the first device which has a successfully\n// built program.\nstatic cl_int l_load_consistently_built_kernels_in_program(\n    cl_program program,\n    std::vector<std::pair<const acl_device_binary_t *, const acl_accel_def_t *>>\n        &accel_ret) {\n  cl_int first_built_dev =\n      -1; // index of the first device having the program built for it\n  cl_context context = program->context;\n  acl_assert_locked();\n\n  // The strategy is to copy all accelerator definitions from the first\n  // device for which the program has been built.\n  // For later devices, we check consistency with the kernels of the first\n  // device.  We may have to eliminate some kernels during this stage.\n\n  for (cl_uint idev = 0; idev < program->num_devices; idev++) {\n    // If the program isn't built for this device, then skip this device.\n    if (!program->dev_prog[idev] ||\n        !program->dev_prog[idev]->build_status == CL_BUILD_SUCCESS)\n      continue;\n\n    if (first_built_dev == -1) {\n      if (context->uses_dynamic_sysdef) {\n        const auto names = program->dev_prog[idev]->get_all_kernel_names();\n        for (const auto &n : names) {\n          accel_ret.push_back(\n              {program->dev_prog[idev]->get_or_create_device_binary(n),\n               program->dev_prog[idev]->get_kernel_accel_def(n)});\n        }\n      } else {\n        for (const auto &a :\n             program->device[idev]->def.autodiscovery_def.accel) {\n          accel_ret.push_back(\n              {program->dev_prog[idev]->get_or_create_device_binary(\n                   a.iface.name),\n               &a});\n        }\n      }\n\n      first_built_dev = static_cast<cl_int>(idev);\n      continue;\n    }\n\n    // Every kernel in this device must match the same-named kernel\n    // that we've seen before. Also each kernel must be built on all\n    // devices that have a built dev_prog.\n\n    if (context->uses_dynamic_sysdef) {\n      auto end = std::remove_if(\n          accel_ret.begin(), accel_ret.end(),\n          [&program, &idev](std::pair<const acl_device_binary_t *,\n                                      const acl_accel_def_t *> &a) {\n            const auto *orig = a.second;\n            const auto *other = program->dev_prog[idev]->get_kernel_accel_def(\n                a.second->iface.name);\n\n            return !(orig && other && l_kernel_interfaces_match(*orig, *other));\n          });\n\n      accel_ret.erase(end, accel_ret.end());\n    } else {\n      auto end = std::remove_if(\n          accel_ret.begin(), accel_ret.end(),\n          [&program, &idev](std::pair<const acl_device_binary_t *,\n                                      const acl_accel_def_t *> &a) {\n            const auto &orig = *(a.second);\n            for (const auto &oa :\n                 program->device[idev]->def.autodiscovery_def.accel) {\n              if (orig.iface.name == oa.iface.name) {\n                return !l_kernel_interfaces_match(orig, oa);\n              }\n            }\n\n            // Did not find matching kernel in the dev_prog\n            // for this device so this kernel is not consistently\n            // built on all devices.\n            return true;\n          });\n\n      accel_ret.erase(end, accel_ret.end());\n    }\n  }\n\n  return CL_SUCCESS;\n}\n\nstatic int l_kernel_interfaces_match(const acl_accel_def_t &a,\n                                     const acl_accel_def_t &b) {\n  acl_assert_locked();\n  if (a.iface.name != b.iface.name)\n    return 0; // Name is part of the interface, formally.\n  if (a.iface.args.size() != b.iface.args.size())\n    return 0;\n\n  for (cl_uint iarg = 0; iarg < a.iface.args.size(); ++iarg) {\n    const auto &aarg = a.iface.args[iarg];\n    const auto &barg = b.iface.args[iarg];\n    if (aarg.addr_space != barg.addr_space)\n      return 0;\n    if (aarg.category != barg.category)\n      return 0;\n    if (aarg.size != barg.size)\n      return 0;\n  }\n  return 1;\n}\n\nstatic size_t l_round_up_for_alignment(size_t x) {\n  // This assumes ACL_MEM_ALIGN is a power of 2.\n  // This code requires no branches.  So maybe it's faster because there\n  // should not be any pipeline stalls.\n  unsigned fuzz = (ACL_MEM_ALIGN - 1) & x; // Lower address bits\n  unsigned offset = (ACL_MEM_ALIGN - fuzz) & (ACL_MEM_ALIGN - 1);\n  return x + offset;\n}\n\n// Return number of bytes of local memory used by this kernel\n// See clGetKernelWorkGroupInfo\nstatic size_t l_local_mem_size(cl_kernel kernel) {\n  acl_assert_locked();\n\n  // There are two parts:  The statically determined amount, and the\n  // amounts from clSetKernelArg.  These are defined for each local\n  // address space used by the kernel, so need to sum them all up.\n  size_t total = 0;\n  for (const auto &aspace : kernel->accel_def->local_aspaces) {\n    // Add in the static demand\n    total += l_round_up_for_alignment(aspace.static_demand);\n  }\n\n  // The sizes of __local arguments that have been set.\n  for (cl_uint iarg = 0, arg_value_idx = 0;\n       iarg < kernel->accel_def->iface.args.size(); ++iarg) {\n    const acl_kernel_arg_info_t &arg_info = kernel->accel_def->iface.args[iarg];\n    if (kernel->arg_defined[iarg]) {\n      if (arg_info.addr_space == ACL_ARG_ADDR_LOCAL) {\n        // Must use memcpy here just in case the argument pointer is not\n        // aligned for a (size_t*).\n        // In other words, we can't just cast to (size_t*) and\n        // dereference.  We don't want a bus error on an exotic\n        // architecture.\n        size_t arg_local_mem_size = 0;\n        safe_memcpy(&arg_local_mem_size, &(kernel->arg_value[arg_value_idx]),\n                    arg_info.size, sizeof(size_t), arg_info.size);\n        total += l_round_up_for_alignment(arg_local_mem_size);\n      }\n    }\n    arg_value_idx += arg_info.size;\n  }\n\n  return total;\n}\n\nint acl_num_non_null_mem_args(cl_kernel kernel) {\n  int result = 0;\n  acl_assert_locked();\n\n  for (cl_uint iarg = 0, arg_value_idx = 0;\n       iarg < kernel->accel_def->iface.args.size(); ++iarg) {\n    const acl_kernel_arg_info_t &arg_info = kernel->accel_def->iface.args[iarg];\n    if (kernel->arg_defined[iarg] && arg_info.category == ACL_ARG_MEM_OBJ) {\n      // Must use memcpy here just in case the argument pointer is not aligned.\n      cl_mem mem_obj;\n      safe_memcpy(&mem_obj, &(kernel->arg_value[arg_value_idx]), arg_info.size,\n                  sizeof(cl_mem), arg_info.size);\n      if (mem_obj != 0)\n        result++;\n    }\n    arg_value_idx += arg_info.size;\n  }\n\n  return result;\n}\n\n// Copies argument value of `len` bytes from source buffer `src` to\n// destination buffer `dst` and returns the number of bytes copied;\n// unless `streaming_arg_info_available` is true to indicate a\n// streaming kernel argument, in which case this function appends\n// the argument interface name provided in `streaming_arg_info` and\n// the argument value to `streaming_args` and returns zero.\nstatic size_t l_copy_argument_to_buffer_or_streaming(\n    void *dst, const void *src, size_t len,\n    std::vector<aocl_mmd_streaming_kernel_arg_info_t> &streaming_args,\n    bool streaming_arg_info_available,\n    const acl_streaming_kernel_arg_info &streaming_arg_info) {\n  if (streaming_arg_info_available) {\n#ifdef MEM_DEBUG_MSG\n    printf(\" streaming\");\n#endif\n    streaming_args.emplace_back(aocl_mmd_streaming_kernel_arg_info_t{\n        streaming_arg_info.interface_name,\n        std::vector<char>(static_cast<const char *>(src),\n                          static_cast<const char *>(src) + len)});\n    return 0;\n  }\n\n  safe_memcpy(dst, src, len, len, len);\n  return len;\n}\n\n// Copy kernel arguments to another buffer.\n//\n// Adjust for:\n//    - Device sizes as necessary.\n//       We assume device sizes are never larger than host sizes; that's why we\n//       don't bother sizing the buffer.\n//\n// We assume all arguments have been defined.\n//\n// Returns number of bytes written to the device-side buffer in num_bytes.\n// Returns failure if memory could not be reserved on the device.\nstatic cl_int l_copy_and_adjust_arguments_for_device(\n    cl_kernel kernel, cl_device_id device, char *buf, cl_uint *num_bytes,\n    acl_mem_migrate_t *memory_migration,\n    std::vector<aocl_mmd_streaming_kernel_arg_info_t> &streaming_args) {\n  // indices into the host and device arg value buffer arrays.\n  size_t host_idx = 0;\n  size_t device_idx = 0;\n\n  // Where **in kernel __local memory space** does the next __local\n  // object go?  We assume __local addresses start at 0.\n  // Yes, we're using a host pointer to emulate the values going into\n  // a device pointer (so make it pointer-to-const...).\n  //\n  // We have multiple local aspaces - each needs its own bump allocator\n  // with a base address of 0\n\n  // Need to determine sizeof device's pointers.\n  const cl_uint dev_local_ptr_size = 4; // Always.\n  const cl_uint dev_global_ptr_size = device->address_bits >> 3;\n\n  // Bump allocator pointer for each local aspace\n  // Maps the aspace ID to the next available local memory address.\n  std::unordered_map<unsigned, size_t> next_local;\n\n  acl_assert_locked();\n\n  // Pre-allocate the static portions described by the kernel, once for\n  // each aspace\n  for (const auto &aspace : kernel->accel_def->local_aspaces) {\n    next_local[aspace.aspace_id] +=\n        l_round_up_for_alignment(aspace.static_demand);\n  }\n\n  streaming_args.clear();\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"kernel args\\n\");\n#endif\n\n  for (cl_uint iarg = 0; iarg < kernel->accel_def->iface.args.size(); ++iarg) {\n#ifdef MEM_DEBUG_MSG\n    printf(\"arg %d \", iarg);\n#endif\n\n    const acl_kernel_arg_info_t *arg_info =\n        &(kernel->accel_def->iface.args[iarg]);\n\n    if (arg_info->addr_space == ACL_ARG_ADDR_LOCAL) {\n#ifdef MEM_DEBUG_MSG\n      printf(\"local\");\n#endif\n\n      const unsigned int this_aspace =\n          kernel->accel_def->iface.args[iarg].aspace_number;\n\n      // This arg is a pointer to __local.\n      cl_ulong local_size = 0;\n      device_idx += l_copy_argument_to_buffer_or_streaming(\n          buf + device_idx, &(next_local[this_aspace]), dev_local_ptr_size,\n          streaming_args, arg_info->streaming_arg_info_available,\n          arg_info->streaming_arg_info);\n      // Now reserve space for this object.\n      // Yes, this is a bump allocator. :-)\n      safe_memcpy(&local_size, &(kernel->arg_value[host_idx]), arg_info->size,\n                  sizeof(cl_ulong), arg_info->size);\n      // (Need cast to size_t on 32-bit platforms)\n      next_local[this_aspace] += l_round_up_for_alignment((size_t)local_size);\n    } else if (arg_info->category == ACL_ARG_MEM_OBJ &&\n               !kernel->arg_is_svm[iarg] && !kernel->arg_is_ptr[iarg]) {\n      // Must use memcpy here just in case the argument pointer is not aligned.\n      cl_mem mem_obj =\n          NULL; // It is important to initialize this, since in emulation,\n                // arg_info->size = 0 for pipe arguments.\n\n      // On the emulator, the argument size of a pipe (which is a mem object) is\n      // 0. Since we copy in 0 bytes, we should read out 0 bytes.\n      const size_t copy_sz =\n          (arg_info->size == 0) ? arg_info->size : sizeof(cl_mem);\n      safe_memcpy(&mem_obj, &(kernel->arg_value[host_idx]), copy_sz,\n                  sizeof(cl_mem), copy_sz);\n\n#ifdef MEM_DEBUG_MSG\n      printf(\"mem_obj %zx \", (size_t)mem_obj);\n#endif\n\n      if (mem_obj == 0) {\n#ifdef MEM_DEBUG_MSG\n        printf(\"null \");\n#endif\n\n        /*\n        2.0 Spec:\n        \"If the argument is a buffer object, the arg_value pointer can be NULL\n        or point to a NULL value in which case a NULL value will be used as the\n        value for the argument declared as a pointer to global or constant\n        memory in the kernel.\"\n        */\n        const cl_ulong null_ptr = 0;\n        device_idx += l_copy_argument_to_buffer_or_streaming(\n            buf + device_idx, &null_ptr, dev_global_ptr_size, streaming_args,\n            arg_info->streaming_arg_info_available,\n            arg_info->streaming_arg_info);\n        // Shared physical memory:\n      } else if (mem_obj->host_mem.device_addr != 0L) {\n#ifdef MEM_DEBUG_MSG\n        printf(\"shared physical mem\");\n#endif\n        // Write the address into the invocation image:\n        device_idx += l_copy_argument_to_buffer_or_streaming(\n            buf + device_idx, &(mem_obj->host_mem.device_addr),\n            dev_global_ptr_size, streaming_args,\n            arg_info->streaming_arg_info_available,\n            arg_info->streaming_arg_info);\n        // Regular buffer:\n      } else {\n#ifdef MEM_DEBUG_MSG\n        printf(\"regular buffer \");\n#endif\n        const unsigned int needed_mem_id =\n            l_get_kernel_arg_mem_id(kernel, iarg);\n        const unsigned int needed_physical_id = device->def.physical_device_id;\n\n// Always enqueue a migration, even if the memory is where it should be there\n// could be something in the queue ahead of us which will move the memory.\n#ifdef MEM_DEBUG_MSG\n        printf(\"needed_physical_id %d needed_mem_id %d \", needed_physical_id,\n               needed_mem_id);\n#endif\n\n        // first, is there a reserved region?\n        if (mem_obj->reserved_allocations_count[needed_physical_id].size() ==\n            0) {\n          acl_resize_reserved_allocations_for_device(mem_obj, device->def);\n        }\n        if (mem_obj->reserved_allocations_count[needed_physical_id]\n                                               [needed_mem_id] == 0) {\n          if (mem_obj->reserved_allocations[needed_physical_id]\n                                           [needed_mem_id] == NULL) {\n#ifdef MEM_DEBUG_MSG\n            printf(\"needs reservation \");\n#endif\n\n            // Need to reserve\n            if (!acl_reserve_buffer_block(mem_obj,\n                                          &(acl_get_platform()->global_mem),\n                                          needed_physical_id, needed_mem_id)) {\n              // What happens if this fails and there are other mem objects for\n              // this kernel that have reserved allocations? Could result in a\n              // memory leak....\n              return CL_MEM_OBJECT_ALLOCATION_FAILURE;\n            }\n          } else {\n#ifdef MEM_DEBUG_MSG\n            printf(\"already reserved (reserved allocation %zx block allocation \"\n                   \"%zx) \",\n                   (size_t)(mem_obj->reserved_allocations[needed_physical_id]\n                                                         [needed_mem_id]),\n                   (size_t)(mem_obj->block_allocation));\n#endif\n          }\n        }\n        mem_obj\n            ->reserved_allocations_count[needed_physical_id][needed_mem_id]++;\n#ifdef MEM_DEBUG_MSG\n        printf(\"count %d \",\n               mem_obj->reserved_allocations_count[needed_physical_id]\n                                                  [needed_mem_id]);\n#endif\n\n        // copy the address of the reserved allocation into the invocation\n        // image:\n        const void *mem_addr =\n            mem_obj->reserved_allocations[needed_physical_id][needed_mem_id]\n                ->range.begin;\n        device_idx += l_copy_argument_to_buffer_or_streaming(\n            buf + device_idx, &mem_addr, dev_global_ptr_size, streaming_args,\n            arg_info->streaming_arg_info_available,\n            arg_info->streaming_arg_info);\n\n        if (memory_migration->num_mem_objects == 0) {\n          // First time allocation, 128 was chosen because previously, number of\n          // kernel arguments were set to an hardcoded limit of 128\n          const unsigned int initial_alloc = 128;\n\n          memory_migration->src_mem_list =\n              (acl_mem_migrate_wrapper_t *)acl_malloc(\n                  initial_alloc * sizeof(acl_mem_migrate_wrapper_t));\n          if (!memory_migration->src_mem_list) {\n            return CL_OUT_OF_RESOURCES;\n          }\n\n          memory_migration->num_alloc = initial_alloc;\n        } else if (memory_migration->num_mem_objects >=\n                   memory_migration->num_alloc) {\n          const unsigned int next_alloc = memory_migration->num_alloc * 2;\n          // check for overflow, num_alloc is a 32-bit unsigned integer and\n          // unsigned integer overflow is defined behaviour\n          if (next_alloc < memory_migration->num_alloc)\n            return CL_OUT_OF_RESOURCES;\n\n          acl_mem_migrate_wrapper_t *new_src_mem_list =\n              (acl_mem_migrate_wrapper_t *)acl_realloc(\n                  memory_migration->src_mem_list,\n                  next_alloc * sizeof(acl_mem_migrate_wrapper_t));\n\n          if (!new_src_mem_list) {\n            return CL_OUT_OF_RESOURCES;\n          }\n\n          memory_migration->src_mem_list = new_src_mem_list;\n          memory_migration->num_alloc = next_alloc;\n        }\n\n        const unsigned int index = memory_migration->num_mem_objects;\n        memory_migration->src_mem_list[index].src_mem = mem_obj;\n        memory_migration->src_mem_list[index].destination_physical_device_id =\n            needed_physical_id;\n        memory_migration->src_mem_list[index].destination_mem_id =\n            needed_mem_id;\n        ++memory_migration->num_mem_objects;\n      }\n    } else {\n#ifdef MEM_DEBUG_MSG\n      printf(\"const\");\n#endif\n\n      // Host and device sizes are the same.\n      // E.g. for cl_uint, SVM ptr etc.\n      device_idx += l_copy_argument_to_buffer_or_streaming(\n          buf + device_idx, kernel->arg_value + host_idx, arg_info->size,\n          streaming_args, arg_info->streaming_arg_info_available,\n          arg_info->streaming_arg_info);\n    }\n    host_idx += arg_info->size;\n#ifdef MEM_DEBUG_MSG\n    printf(\"\\n\");\n#endif\n  }\n\n  *num_bytes = (cl_uint)device_idx;\n  return CL_SUCCESS;\n}\n\nint acl_kernel_has_unmapped_subbuffers(acl_mem_migrate_t *mem_migration) {\n  acl_assert_locked();\n\n  for (unsigned int ibuf = 0; ibuf < mem_migration->num_mem_objects; ibuf++) {\n    if (mem_migration->src_mem_list[ibuf].src_mem->auto_mapped) {\n      return 1;\n    }\n  }\n  return 0;\n}\n\nbool acl_device_has_reprogram_device_globals(cl_device_id device) {\n  const auto &device_global_mem_defs =\n      device->def.autodiscovery_def.device_global_mem_defs;\n  return device_global_mem_defs.end() !=\n         std::find_if(device_global_mem_defs.begin(),\n                      device_global_mem_defs.end(),\n                      [](const auto &name_and_def) {\n                        return !name_and_def.second.can_skip_programming;\n                      });\n}\n\nint acl_submit_kernel_device_op(cl_event event) {\n  // No user-level scheduling blocks this kernel enqueue from running.\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  // This may also imply reprogramming the device and transferring buffers\n  // to and from the device.\n  // Return a positive number if we committed device ops, zero otherwise.\n  int result = 0;\n  acl_assert_locked();\n\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  if (!acl_command_queue_is_valid(event->command_queue)) {\n    return result;\n  }\n  if (event->last_device_op) {\n    return result;\n  }\n\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n  acl_device_op_t *last_op = 0;\n  int ok = 1;\n\n  cl_device_id device = event->cmd.info.ndrange_kernel.device;\n  auto *dev_bin = event->cmd.info.ndrange_kernel.dev_bin;\n\n  // Precautionary, but it also nudges the device scheduler to try\n  // to free up old operation slots.\n  acl_forget_proposed_device_ops(doq);\n\n  bool need_reprogram = true;\n  if (device->last_bin) {\n    // compare hash of last program that went through device op queue and the\n    // program required by kernel\n    need_reprogram =\n        device->last_bin->get_devdef().autodiscovery_def.binary_rand_hash !=\n        dev_bin->get_devdef().autodiscovery_def.binary_rand_hash;\n  } else if (!acl_device_has_reprogram_device_globals(device)) {\n    // last_bin is null suggests there is no reprograms scheduled at this\n    // point so if the target device contains device global with reprogram\n    // init mode we force a reprogram, otherwise check random hash\n    // compare hash of program that is on the device and the program\n    // required by kernel\n    need_reprogram = device->def.autodiscovery_def.binary_rand_hash !=\n                     dev_bin->get_devdef().autodiscovery_def.binary_rand_hash;\n  }\n\n  // Always reprogram in split kernel mode. This is a temporary workaround.\n  if (event->context->split_kernel) {\n    need_reprogram = true;\n  }\n\n  if (need_reprogram) {\n    // Need to reprogram the device before this kernel runs.\n    last_op = acl_propose_device_op(doq, ACL_DEVICE_OP_REPROGRAM, event);\n    ok = (last_op != NULL);\n  }\n\n  // Arrange for mem migration\n  for (unsigned ibuf = 0;\n       ok &&\n       ibuf < event->cmd.info.ndrange_kernel.memory_migration.num_mem_objects;\n       ibuf++) {\n    last_op = acl_propose_indexed_device_op(doq, ACL_DEVICE_OP_MEM_MIGRATION,\n                                            event, ibuf);\n    ok = (last_op != NULL);\n    cl_mem mem_to_migrate =\n        event->cmd.info.ndrange_kernel.memory_migration.src_mem_list[ibuf]\n            .src_mem;\n    if (ok && mem_to_migrate->flags & CL_MEM_COPY_HOST_PTR &&\n        !acl_is_sub_or_parent_buffer(mem_to_migrate)) {\n      // Mem migration to copy the contents of the host pointer to the device\n      // buffer has been proposed but not executed at this point. However,\n      // writable_copy_on_host needs to be set to false so we don't try to\n      // migrate again while the current migration is in the queue. Don't enter\n      // if this a sub buffer or a buffer with sub buffers. In that case, the\n      // data is kept on the host and copied when it is actually used to\n      // prevents us from copying over data that may be modified by an\n      // overlapping sub-buffer before this location is used.\n      mem_to_migrate->writable_copy_on_host = 0;\n    }\n  }\n\n  // Schedule the actual kernel launch.\n  last_op = acl_propose_device_op(doq, ACL_DEVICE_OP_KERNEL, event);\n  ok = ok && (last_op != NULL);\n\n  if (last_op) {\n    // The activation_id is the index into the device op queue.\n    event->cmd.info.ndrange_kernel.invocation_wrapper->image->activation_id =\n        last_op->id;\n  }\n\n  if (ok) {\n    // We managed to enqueue everything.\n    cl_kernel kernel = event->cmd.info.ndrange_kernel.kernel;\n\n    // If current last bin is loaded on the board and we are going to update\n    // last bin pointer without any reprogram, we should update the loaded bin\n    // to reflect that as well.\n    if (!need_reprogram && device->last_bin == device->loaded_bin &&\n        device->last_bin != event->cmd.info.ndrange_kernel.dev_bin) {\n      device->loaded_bin = event->cmd.info.ndrange_kernel.dev_bin;\n    }\n    device->last_bin = event->cmd.info.ndrange_kernel.dev_bin;\n    event->last_device_op = last_op;\n\n    // Mark this accelerator as being occupied.\n    dev_bin->get_dev_prog()->current_event[kernel->accel_def] = event;\n\n    // Commit the operations.\n    acl_commit_proposed_device_ops(doq);\n    result = 1;\n  } else {\n    // Did not succeed in enqueueing everything.\n    // Back off, and wait until later when we have more space in the\n    // device op queue.\n    acl_forget_proposed_device_ops(doq);\n  }\n  return result;\n}\n\n// This implements the device op to launch a kernel.\n// Everything else has already been set up: the device has the right\n// programming file, and all the buffers are in place.\nvoid acl_launch_kernel(void *user_data, acl_device_op_t *op) {\n#ifdef MEM_DEBUG_MSG\n  printf(\"acl_launch_kernel\\n\");\n#endif\n  cl_event event = op->info.event;\n  acl_assert_locked();\n  user_data = user_data; // Only used by the test mock.\n  if (acl_event_is_valid(event) &&\n      acl_command_queue_is_valid(event->command_queue)) {\n    const acl_hal_t *hal = acl_get_hal();\n    struct acl_kernel_invocation_wrapper_t *invocation_wrapper =\n        event->cmd.info.ndrange_kernel.invocation_wrapper;\n\n    // OpenCL 1.2 section 5.4.3 says it's the programmer's responsibility to\n    // unmap buffers before any kernel might use them. (Well, if it's mapped for\n    // read it can be used read-only by the kernels or copy/read/write buffer\n    // commands.) So don't try hard to maintain consistency.\n\n    // Some kernel arguments might be buffers accessible to\n    // the host only.\n    // Those buffer copies were enqueued on the device op queue\n    // as part of the same operation group, and before\n    // this kernel operation, so they've already completed.\n\n    // All memory migration events should be complete by now.\n\n    hal->launch_kernel(\n        event->cmd.info.ndrange_kernel.device->def.physical_device_id,\n        invocation_wrapper);\n  } else {\n    acl_set_device_op_execution_status(op, -1);\n  }\n}\n\n// Called when we get a kernel interrupt indicating that profiling data is ready\nvoid acl_profile_update(int activation_id) {\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n\n  if (activation_id >= 0 && activation_id < doq->max_ops) {\n    // This address is stable, given a fixed activation_id.\n    // So we don't run into race conditions.\n    acl_device_op_t *op = doq->op + activation_id;\n\n    (void)acl_process_profiler_scan_chain(op);\n  }\n}\n\n// Handle a status update from within a HAL interrupt.\n// We can't do much: only update a flag in the right spot.\nvoid acl_receive_kernel_update(int activation_id, cl_int status) {\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n\n  // This function can potentially be called by a HAL that does not use the\n  // ACL global lock, so we need to use acl_lock() instead of\n  // acl_assert_locked(). However, the MMD HAL calls this function from a unix\n  // signal handler, which can't lock mutexes, so we don't lock in that case.\n  // All functions called from this one therefore have to use\n  // acl_assert_locked_or_sig() instead of just acl_assert_locked().\n  std::unique_lock lock{acl_mutex_wrapper, std::defer_lock};\n  if (!acl_is_inside_sig()) {\n    lock.lock();\n  }\n\n  if (activation_id >= 0 && activation_id < doq->max_ops) {\n    // This address is stable, given a fixed activation_id.\n    // So we don't run into race conditions.\n    acl_device_op_t *op = doq->op + activation_id;\n\n    // We should only have been called with status CL_RUNNING (1),\n    // CL_COMPLETE (0), or negative for error.\n    if (status > CL_RUNNING)\n      status = ACL_ERR_INVALID_KERNEL_UPDATE;\n    if (op->execution_status >\n        status) { // Don't bother if status is not really updated.\n      acl_set_device_op_execution_status(op, status);\n    } else {\n      (void)acl_process_profiler_scan_chain(op);\n    }\n\n    // Signal all waiters.\n    acl_signal_device_update();\n  }\n}\n\n// The kernel invocation has completed.\n// Clean up.\n// This is idempotent.\nstatic void l_complete_kernel_execution(cl_event event) {\n  acl_assert_locked();\n\n  // Release the kernel object.  It was retained when we enqueued the\n  // Task/NDRange.\n  clReleaseKernel(event->cmd.info.ndrange_kernel.kernel);\n}\n\n// Check if the memory the kernel argument was compiled to supports\n// expected_type. Returns CL_TRUE if memory type is supported. Returns CL_FALSE\n// if it is not.\nstatic cl_bool l_check_mem_type_support_on_kernel_arg(\n    cl_kernel kernel, cl_uint arg_index,\n    acl_system_global_mem_type_t expected_type) {\n  cl_bool supported = CL_FALSE;\n  const auto &arg_info = kernel->accel_def->iface.args[arg_index];\n\n  // If buffer location attribute has been set on kernel arg\n  if (!arg_info.buffer_location.empty()) {\n    for (unsigned gmem_idx = 0;\n         gmem_idx <\n         kernel->dev_bin->get_devdef().autodiscovery_def.num_global_mem_systems;\n         gmem_idx++) {\n      // look for a buffer, if it matches, make sure it's expected type\n      assert(!kernel->dev_bin->get_devdef()\n                  .autodiscovery_def.global_mem_defs[gmem_idx]\n                  .name.empty());\n      if (arg_info.buffer_location ==\n          kernel->dev_bin->get_devdef()\n              .autodiscovery_def.global_mem_defs[gmem_idx]\n              .name) {\n        if (kernel->dev_bin->get_devdef()\n                .autodiscovery_def.global_mem_defs[gmem_idx]\n                .type == expected_type) {\n          // Yes, kernel argument was compiled with access to expected memory\n          supported = CL_TRUE;\n        }\n        break;\n      }\n    }\n  } else {\n    // buffer_location attribute was not set on kernel argument.\n    // The compiler currently connects this to first (default) memeory,\n    // so check if default is expected type.\n    auto default_gmem_idx = static_cast<size_t>(\n        acl_get_default_memory(kernel->dev_bin->get_devdef()));\n    if (kernel->dev_bin->get_devdef()\n            .autodiscovery_def.global_mem_defs[default_gmem_idx]\n            .type == expected_type) {\n      supported = CL_TRUE;\n    }\n  }\n\n  return supported;\n}\n\nunsigned int l_get_kernel_arg_mem_id(const cl_kernel kernel,\n                                     cl_uint arg_index) {\n  /* Get expected buffer location of kernel argument */\n  const acl_kernel_arg_info_t *arg_info =\n      &(kernel->accel_def->iface.args[arg_index]);\n  unsigned needed_mem_id;\n  // If the arg_info defines a buffer location\n  if (!arg_info->buffer_location.empty()) {\n    for (unsigned gmem_idx = 0;\n         gmem_idx <\n         kernel->dev_bin->get_devdef().autodiscovery_def.num_global_mem_systems;\n         gmem_idx++) {\n      // look for a buffer, if we found this one, use it\n      assert(!kernel->dev_bin->get_devdef()\n                  .autodiscovery_def.global_mem_defs[gmem_idx]\n                  .name.empty());\n      if (arg_info->buffer_location ==\n          kernel->dev_bin->get_devdef()\n              .autodiscovery_def.global_mem_defs[gmem_idx]\n              .name) {\n        needed_mem_id = gmem_idx;\n        return needed_mem_id;\n      }\n    }\n    // If no buffer location specified, use the default.\n    // We use default memory instead of default device global memory\n    // because default memory is what the compiler already compiled\n    // the argument to access.\n    // In clSetKernelArg, we already verified that default memory\n    // is of type ACL_GLOBAL_MEM_DEVICE_PRIVATE.\n  } else {\n    needed_mem_id =\n        (unsigned)acl_get_default_memory(kernel->dev_bin->get_devdef());\n    return needed_mem_id;\n  }\n  assert(0 && \"Failed to get kernel argument mem id\\n\");\n  return 0;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_hal.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <string.h>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_hal.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n//////////////////////////////\n// Global variables\nstatic acl_hal_t acl_hal_value;\nstatic int acl_hal_is_valid = 0;\nint debug_mode = 0;\nstatic int l_is_valid_hal(const acl_hal_t *hal);\nacl_mmd_library_names_t *libraries_to_load = NULL;\n\nconst acl_hal_t *acl_get_hal(void) {\n  // NOTE: This function is called from multiple threads (i.e. in the\n  // signal handler) without a lock. It's probably fine\n  // though since the HAL is normally just set once on startup and then not\n  // changed.\n  if (acl_hal_is_valid) {\n    return &acl_hal_value;\n  } else {\n    return 0;\n  }\n}\n\nint acl_set_hal(const acl_hal_t *new_hal) {\n  acl_assert_locked();\n  const char *debug_env = getenv(\"ACL_DEBUG\");\n  if (debug_env) {\n    debug_mode = atoi(debug_env);\n  }\n  if (l_is_valid_hal(new_hal)) {\n    acl_hal_value = *new_hal;\n    acl_hal_is_valid = 1;\n    return 1;\n  } else {\n    debug_mode = 0;\n    return 0;\n  }\n}\n\nvoid acl_reset_hal(void) {\n  acl_assert_locked();\n\n  for (unsigned int i = 0; i < sizeof(acl_hal_t); i++) {\n    ((char *)&acl_hal_value)[i] = 0;\n  }\n  acl_hal_is_valid = 0;\n  debug_mode = 0;\n}\n\nstatic int l_is_valid_hal(const acl_hal_t *hal) {\n  acl_assert_locked();\n\n  if (hal == 0)\n    return 0;\n  if (hal->init_device == 0)\n    return 0;\n  if (hal->get_timestamp == 0)\n    return 0;\n  if (hal->copy_hostmem_to_hostmem == 0)\n    return 0;\n  if (hal->copy_hostmem_to_globalmem == 0)\n    return 0;\n  if (hal->copy_globalmem_to_hostmem == 0)\n    return 0;\n  if (hal->copy_globalmem_to_globalmem == 0)\n    return 0;\n  if (hal->launch_kernel == 0)\n    return 0;\n  if (hal->unstall_kernel == 0)\n    return 0;\n  if (hal->program_device == 0)\n    return 0;\n  if (hal->query_temperature == 0)\n    return 0;\n  if (hal->get_device_official_name == 0)\n    return 0;\n  if (hal->get_device_vendor_name == 0)\n    return 0;\n  if (hal->get_profile_data == 0)\n    return 0;\n  if (hal->reset_profile_counters == 0)\n    return 0;\n  if (hal->disable_profile_counters == 0)\n    return 0;\n  if (hal->enable_profile_counters == 0)\n    return 0;\n  if (hal->set_profile_shared_control == 0)\n    return 0;\n  if (hal->set_profile_start_cycle == 0)\n    return 0;\n  if (hal->set_profile_stop_cycle == 0)\n    return 0;\n  if (hal->has_svm_memory_support == 0)\n    return 0;\n  if (hal->has_physical_mem == 0)\n    return 0;\n  if (hal->pll_reconfigure == 0)\n    return 0;\n  if (hal->reset_kernels == 0)\n    return 0;\n  return 1;\n}\n\nint acl_print_debug_msg(const char *msg, ...) {\n  int num_chars_printed = 0;\n  if (debug_mode > 0) {\n    va_list ap;\n    va_start(ap, msg);\n    num_chars_printed = vprintf(msg, ap);\n    va_end(ap);\n  }\n  return num_chars_printed;\n}\n\nextern CL_API_ENTRY void CL_API_CALL\nclSetBoardLibraryIntelFPGA(char *library_name) {\n  acl_mmd_library_names_t *next_library = NULL;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  acl_print_debug_msg(\"Adding library '%s' to list of libraries to open\\n\",\n                      library_name);\n\n  next_library = acl_new<acl_mmd_library_names_t>();\n  assert(next_library);\n  next_library->library_name = library_name;\n\n  if (libraries_to_load == NULL) {\n    libraries_to_load = next_library;\n  } else {\n    acl_mmd_library_names_t *insertion_point;\n    insertion_point = libraries_to_load;\n    while (insertion_point->next != NULL) {\n      insertion_point = insertion_point->next;\n    }\n    insertion_point->next = next_library;\n  }\n\n  return;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_profiler.cpp",
        "data": "// Copyright (C) 2014-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// Profiler\n// ========\n//\n// Handle the parsing and printing out of profiler output\n//\n\n// System headers.\n#include <assert.h>\n#include <atomic>\n#include <fcntl.h>\n#include <mutex>\n#include <sstream>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n#include <sys/stat.h>\n#include <sys/types.h>\n#ifdef _WIN32\n#include <io.h>\n#include <share.h>\n#else // Linux\n#include <unistd.h>\n#endif\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_kernel.h>\n#include <acl_profiler.h>\n#include <acl_program.h>\n#include <acl_support.h>\n#include <acl_types.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n// From stackoverflow: \"C Preprocessor: concatenate int to string\"\n#define STR_HELPER(x) #x\n#define STR(x) STR_HELPER(x)\n#define ACL_PROFILER_OUTPUT_FILENAME \"profile.mon\"\n#define ACL_PROFILER_TIMER_ENVNAME \"ACL_PROFILE_TIMER\"\n#define ACL_PROFILER_ENABLE_ENVNAME \"ACL_RUNTIME_PROFILING_ACTIVE\"\n\n// The number of cycles that need to delay for to allow for proper readback of\n// tessellated counters At the moment, have 4 segments in the counter, so should\n// delay for 4 extra cycles (set to 5 in case the number of counter segments is\n// increased to 5 - unlikely to go higher than this)\n#define TESS_DELAY 5\n\nstatic unsigned long profile_timer_on = 0;\nstatic unsigned long profile_enable = 0;\nstatic int pfile_fd = -1;\nstatic int opened_count = 0;\nstatic int failed_to_open_count = 0;\nstatic bool pfile_was_opened = false;\nstatic unsigned long long autorun_start_time = 0;\n\n// Shared Counters Control Value\n// -2 => not set\n// -1 => shared is off\n// 0 - 3 => one of the controls shared can be set to\nstatic int profiler_shared_control_val = -2;\n\nconstexpr unsigned int profile_data_buffer_size = 1024 * 1024;    // 1M\nconstexpr unsigned int profile_data_temp_buffer_size = 10 * 1024; // 10K\n\nint dump_profile_buffer_to_file();\nint write_profile_info_to_file(unsigned num_profile_counters,\n                               unsigned device_id, unsigned command_queue_id,\n                               const std::string &name, uint64_t *profile_data,\n                               cl_command_type op_type,\n                               unsigned long long start_time,\n                               unsigned long long end_time, int shared_counter);\n\nclass profile_data_buffer_t {\nprivate:\n  char cbuf[profile_data_buffer_size];\n  std::atomic<unsigned int> buffer_index;\n  std::atomic<unsigned int> file_index;\n\npublic:\n  profile_data_buffer_t() {\n    buffer_index.store(0, std::memory_order_release);\n    file_index.store(0, std::memory_order_release);\n  }\n  const char *get_buffer() { return cbuf; }\n  void write(const char *readout, unsigned int str_len) {\n    unsigned int cur_buffer_index =\n        buffer_index.load(std::memory_order_acquire);\n    for (unsigned int i = 0; i < str_len; i++) {\n      cbuf[cur_buffer_index] = readout[i];\n      cur_buffer_index = (cur_buffer_index + 1) % profile_data_buffer_size;\n    }\n    buffer_index.store(cur_buffer_index, std::memory_order_release);\n  }\n  unsigned int size() {\n    unsigned int cur_file_index = file_index.load(std::memory_order_acquire);\n    unsigned int cur_buffer_index =\n        buffer_index.load(std::memory_order_acquire);\n    if (cur_file_index <= cur_buffer_index) {\n      return cur_buffer_index - cur_file_index;\n    } else {\n      return cur_buffer_index + profile_data_buffer_size - cur_file_index;\n    }\n  }\n  unsigned int get_buffer_index() {\n    return buffer_index.load(std::memory_order_acquire);\n  }\n  unsigned int get_file_index() {\n    return file_index.load(std::memory_order_acquire);\n  }\n  void update_file_index(unsigned int new_file_index) {\n    file_index.store(new_file_index, std::memory_order_release);\n  }\n};\n\nclass profile_temporary_buffer_t {\nprivate:\n  char temp_buf[profile_data_temp_buffer_size];\n  char *buf_ptr;\n  unsigned int total_chars_written;\n\npublic:\n  profile_temporary_buffer_t() { clear(); }\n  void clear() {\n    buf_ptr = &temp_buf[0];\n    *buf_ptr = '\\0';\n    total_chars_written = 0;\n  }\n  unsigned int size() { return total_chars_written; }\n  const char *get_buffer() { return temp_buf; }\n  void write(const char *fmt, ...) {\n    int chars_written = 0;\n    va_list args;\n    va_start(args, fmt);\n    chars_written =\n        vsnprintf(buf_ptr, profile_data_temp_buffer_size, fmt, args);\n    buf_ptr += chars_written;\n    total_chars_written += chars_written;\n    va_end(args);\n  }\n};\n\n// The runtime and signal threads will write to their own buffer to avoid any\n// concurrency problem since they will receive and store the data in parallel.\n// Store the data from the runtime thread\nstatic profile_data_buffer_t profile_runtime_data_buffer;\n\n// Store the data from the signal handler thread\nstatic profile_data_buffer_t profile_signal_interrupt_data_buffer;\nstd::mutex lock;\n\nint dump_profile_data_to_buffer(unsigned num_profile_counters,\n                                unsigned device_id, unsigned command_queue_id,\n                                const char *name, uint64_t *profile_data,\n                                cl_command_type op_type,\n                                unsigned long long start_time,\n                                unsigned long long end_time,\n                                int curr_shared_counters_type) {\n\n  if (op_type == ACL_DEVICE_OP_KERNEL) {\n    assert(num_profile_counters == 0 || profile_data != 0);\n  }\n\n  // temp_buf will construct the output string from the kernel's data\n  // temp_buf will ensure a complete output line will be passed to the runtime\n  // or signal handler data buffer.\n  profile_temporary_buffer_t temp_buf;\n  temp_buf.write(\"%u,%u,%s,%llu,%llu\", device_id, command_queue_id, name,\n                 start_time, end_time);\n  if (op_type == ACL_DEVICE_OP_KERNEL) {\n    temp_buf.write(\",%d\", curr_shared_counters_type);\n    temp_buf.write(\",%u\", num_profile_counters);\n    for (unsigned i = 0; i < num_profile_counters; i++) {\n      temp_buf.write(\",%llu\", (long long unsigned)profile_data[i]);\n    }\n  }\n  temp_buf.write(\"\\n\");\n\n  if (acl_is_inside_sig()) {\n    profile_signal_interrupt_data_buffer.write(temp_buf.get_buffer(),\n                                               temp_buf.size());\n  } else {\n    profile_runtime_data_buffer.write(temp_buf.get_buffer(), temp_buf.size());\n  }\n  dump_profile_buffer_to_file();\n\n  return 1;\n}\n\nvoid set_env_shared_counter_val() {\n  // Get the shared counter value (or set to -1 if its not set/out of expected\n  // bounds)\n  char *curr_shared_counters_str = getenv(ACL_PROFILER_SHARED_COUNTER);\n  // Valid inputs are 0-3, so checks will ensure atoi doesn't allow for inputs\n  // such as \"\"\n  if (curr_shared_counters_str != NULL &&\n      strnlen(curr_shared_counters_str, 2) == 1 &&\n      isdigit(curr_shared_counters_str[0])) {\n    profiler_shared_control_val = atoi(curr_shared_counters_str);\n    return;\n    // Check if the input was \"-1\" in which case we don't want to print a\n    // warning.\n  } else if (curr_shared_counters_str != NULL &&\n             (strnlen(curr_shared_counters_str, 3) != 2 ||\n              strncmp(curr_shared_counters_str, \"-1\", 2) != 0)) {\n    acl_print_debug_msg(\n        \"WARNING: Unable to set shared to %s. Setting to default (off).\\n\",\n        curr_shared_counters_str);\n  }\n  profiler_shared_control_val = -1;\n}\n\nint get_env_profile_shared_counter_val() {\n  // Hasn't been set yet\n  if (profiler_shared_control_val <= -2 || profiler_shared_control_val > 3) {\n    set_env_shared_counter_val();\n  }\n  return profiler_shared_control_val;\n}\n\nlong int write_wrapper(int fd, const void *buf, unsigned int count) {\n#ifdef _WIN32\n  return _write(fd, buf, count);\n#else // Linux\n  return write(fd, buf, count);\n#endif\n}\n\nint write_buffer_data_to_file(profile_data_buffer_t *profile_data_buffer) {\n  unsigned int bytes_to_write = profile_data_buffer->size();\n  if (bytes_to_write == 0) {\n    return 1;\n  }\n\n  const char *buffer = profile_data_buffer->get_buffer();\n  unsigned int buffer_index = profile_data_buffer->get_buffer_index();\n  unsigned int file_index = profile_data_buffer->get_file_index();\n  long int bytes_written;\n  if (file_index <= buffer_index) {\n    bytes_written =\n        write_wrapper(pfile_fd, &buffer[file_index], bytes_to_write);\n  } else {\n    unsigned int bytes_to_arr_end = profile_data_buffer_size - file_index;\n    bytes_written =\n        write_wrapper(pfile_fd, &buffer[file_index], bytes_to_arr_end);\n    bytes_written += write_wrapper(pfile_fd, &buffer[0], buffer_index);\n  }\n  profile_data_buffer->update_file_index(buffer_index);\n\n  if (bytes_written < 0 || (unsigned long int)bytes_written != bytes_to_write) {\n    acl_print_debug_msg(\"Could not write profile data to file!\\n\");\n    return 0; // failure\n  }\n\n  return 1; // success\n}\n\nint dump_profile_buffer_to_file() {\n  if (opened_count < 1 || !lock.try_lock()) {\n    return 1;\n  }\n\n  int is_success = 1;\n  is_success &=\n      write_buffer_data_to_file(&profile_signal_interrupt_data_buffer);\n  is_success &= write_buffer_data_to_file(&profile_runtime_data_buffer);\n\n  lock.unlock();\n\n  return is_success;\n}\n\nint write_profile_info_to_file(unsigned num_profile_counters,\n                               unsigned device_id, unsigned command_queue_id,\n                               const std::string &name, uint64_t *profile_data,\n                               cl_command_type op_type,\n                               unsigned long long start_time,\n                               unsigned long long end_time,\n                               int shared_counter) {\n\n  if (!profile_enable)\n    return 0;\n\n  std::unique_lock lock{acl_mutex_wrapper, std::defer_lock};\n  if (!acl_is_inside_sig()) {\n    lock.lock();\n  }\n\n  acl_open_profiler_file();\n\n  // file open was unsuccessful, fail with a message\n  if (opened_count < 1) {\n    acl_print_debug_msg(\"Profiler output file is not opened: \" STR(\n        ACL_PROFILER_OUTPUT_FILENAME) \"\\n\");\n    return 0;\n  }\n\n  // temp_buf will construct the output string from the kernel's data\n  // temp_buf will ensure a complete output line will be passed to the runtime\n  // or signal handler data buffer.\n  profile_temporary_buffer_t temp_buf;\n\n  temp_buf.write(\"%d,\", device_id);\n  temp_buf.write(\"%d,\", command_queue_id);\n  temp_buf.write(\"%s,\", name.c_str());\n  temp_buf.write(\"%llu,\", start_time);\n  temp_buf.write(\"%llu,\", end_time);\n  temp_buf.write(\"%d\", shared_counter);\n\n  if (op_type == ACL_DEVICE_OP_KERNEL) {\n    temp_buf.write(\",%u\", num_profile_counters);\n    assert(num_profile_counters == 0 || profile_data != 0);\n    for (unsigned i = 0; i < num_profile_counters; i++) {\n      temp_buf.write(\",\");\n      temp_buf.write(\"%llu\", (long long unsigned)profile_data[i]);\n    }\n  }\n  temp_buf.write(\"\\n\");\n\n  long int bytes_written =\n      write_wrapper(pfile_fd, temp_buf.get_buffer(), temp_buf.size());\n\n  acl_close_profiler_file();\n\n  if (bytes_written < 0 ||\n      (unsigned long int)bytes_written != temp_buf.size()) {\n    acl_print_debug_msg(\"Could not write profile data to file!\\n\");\n    return 0;\n  }\n  return 1;\n}\n\nint write_profile_info_to_file_from_device(\n    cl_device_id device_id, cl_context context, const char *name,\n    uint64_t *profile_data, cl_command_type op_type,\n    unsigned long long start_time, unsigned long long end_time,\n    unsigned num_profile_counters, int shared_counter) {\n  acl_assert_locked();\n  if (!profile_enable)\n    return 0;\n\n  acl_open_profiler_file();\n\n  // file open was unsuccessful, fail with a message\n  if (opened_count < 1) {\n    acl_context_callback(context, \"Profiler output file is not opened: \" STR(\n                                      ACL_PROFILER_OUTPUT_FILENAME) \"\\n\");\n    return 0;\n  }\n\n  // temp_buf will construct the output string from the kernel's data\n  // temp_buf will ensure a complete output line will be passed to the runtime\n  // or signal handler data buffer.\n  profile_temporary_buffer_t temp_buf;\n\n  temp_buf.write(\"%d,\", device_id->id);\n  temp_buf.write(\"%d,\", 0); // Set command_queue id to just be 0\n  temp_buf.write(\"%s,\", name);\n  temp_buf.write(\"%llu,\", start_time);\n  temp_buf.write(\"%llu,\", end_time);\n  temp_buf.write(\"%d\", shared_counter);\n\n  if (op_type == ACL_DEVICE_OP_KERNEL) {\n    unsigned i;\n    temp_buf.write(\",%u\", num_profile_counters);\n    assert(num_profile_counters == 0 || profile_data != 0);\n    for (i = 0; i < num_profile_counters; i++) {\n      temp_buf.write(\",\");\n      temp_buf.write(\"%llu\", (long long unsigned)profile_data[i]);\n    }\n  }\n  temp_buf.write(\"\\n\");\n\n  long int bytes_written =\n      write_wrapper(pfile_fd, temp_buf.get_buffer(), temp_buf.size());\n\n  acl_close_profiler_file();\n\n  if (bytes_written < 0 ||\n      (unsigned long int)bytes_written != temp_buf.size()) {\n    acl_print_debug_msg(\"Could not write profile data to file!\\n\");\n\n    return 0;\n  }\n\n  return 1;\n}\n\nvoid acl_enable_profiler_scan_chain(acl_device_op_t *op) {\n\n  cl_event event;\n  unsigned int physical_device_id;\n  unsigned int accel_id;\n  acl_assert_locked();\n\n  event = op->info.event;\n\n  acl_print_debug_msg(\"acl_enable_profiler_scan_chain\\n\");\n\n  if (!event) {\n    acl_print_debug_msg(\n        \"acl_enable_profiler_scan_chain is called for NULL event\\n\");\n    return;\n  }\n\n  if (event->cmd.info.ndrange_kernel.device == 0) {\n    return;\n  }\n\n  physical_device_id =\n      event->cmd.info.ndrange_kernel.device->def.physical_device_id;\n  accel_id = event->cmd.info.ndrange_kernel.accel_id;\n  acl_get_hal()->set_profile_shared_control(physical_device_id, accel_id);\n  acl_get_hal()->enable_profile_counters(physical_device_id, accel_id);\n}\n\nvoid acl_init_profiler() {\n  char *timer_on;\n  char *profiler_enable_env_var = getenv(ACL_PROFILER_ENABLE_ENVNAME);\n  if (profiler_enable_env_var != NULL &&\n      strnlen(profiler_enable_env_var, 2) == 1 &&\n      strncmp(profiler_enable_env_var, \"1\", 1) == 0) {\n    profile_enable = 1;\n  } else {\n    profile_enable = 0;\n  }\n\n  // Set the shared control value\n  set_env_shared_counter_val();\n\n  timer_on = getenv(ACL_PROFILER_TIMER_ENVNAME);\n  acl_assert_locked();\n  if (timer_on != NULL && strnlen(timer_on, 2) == 1 &&\n      strncmp(timer_on, \"1\", 1) == 0) {\n    profile_timer_on = 1;\n  } else {\n    profile_timer_on = 0;\n  }\n  acl_print_debug_msg(\"profile_timer_on=%lu\\n\", profile_timer_on);\n}\n\nunsigned long is_profile_enabled() { return profile_enable; }\n\nunsigned long is_profile_timer_on() { return profile_timer_on; }\n\nvoid acl_set_autorun_start_time() {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  autorun_start_time = acl_get_hal()->get_timestamp();\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clGetProfileInfoIntelFPGA(cl_event event) {\n  cl_context context;\n  cl_kernel kernel;\n  unsigned num_profile_counters;\n  uint64_t *profile_data;\n  unsigned int physical_device_id;\n  unsigned int accel_id;\n  int i;\n  _cl_command_queue *command_queue;\n  cl_device_id device_id;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_event_is_valid(event)) {\n    acl_print_debug_msg(\"clGetProfileInfoIntelFPGA is called for NULL event\\n\");\n    return CL_INVALID_EVENT;\n  }\n  if (event->execution_status != CL_RUNNING) {\n    acl_print_debug_msg(\n        \"clGetProfileInfoIntelFPGA is called for non-running event\\n\");\n    return CL_INVALID_EVENT;\n  }\n\n  context = event->context;\n\n  if (!acl_context_is_valid(context)) {\n    acl_print_debug_msg(\n        \"clGetProfileInfoIntelFPGA is called for NULL context\\n\");\n    return CL_INVALID_CONTEXT;\n  }\n\n  command_queue = event->command_queue;\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    ERR_RET(CL_INVALID_COMMAND_QUEUE, context,\n            \"clGetProfileInfoIntelFPGA is called for NULL command_queue\");\n  }\n\n  device_id = command_queue->device;\n\n  if (!acl_device_is_valid(device_id)) {\n    ERR_RET(CL_INVALID_DEVICE, context,\n            \"clGetProfileInfoIntelFPGA is called for NULL device_id\");\n  }\n\n  profile_data = 0;\n  kernel = event->cmd.info.ndrange_kernel.kernel;\n  if (!acl_kernel_is_valid(kernel)) {\n    ERR_RET(CL_INVALID_KERNEL, context, \"Invalid kernel attached to event\");\n  }\n\n  // use autodiscovery info to find out how many words will be read from the\n  // profiler\n  num_profile_counters = kernel->accel_def->profiling_words_to_readback;\n\n  if (num_profile_counters == 0) {\n    // there is not profiler data and we are not printing timers\n    // nothing to print\n    ERR_RET(CL_PROFILING_INFO_NOT_AVAILABLE, context, \"No profile information\");\n  }\n\n  // this kernel has profiling data, get it\n  if (num_profile_counters) {\n    profile_data = kernel->profile_data;\n\n    // expected to be already allocated at kernel creation\n    assert(profile_data);\n\n    physical_device_id =\n        event->cmd.info.ndrange_kernel.device->def.physical_device_id;\n    accel_id = event->cmd.info.ndrange_kernel.accel_id;\n\n    acl_get_hal()->disable_profile_counters(physical_device_id, accel_id);\n    // This for loop is needed to create a delay between counter disabling and\n    // the start of shifting to allow tessellated counters to finish their\n    // addition It is only needed for S10 and other familes that enable\n    // tessellation - can be removed for other families\n    for (i = 0; i < TESS_DELAY; i++) {\n      acl_get_hal()->disable_profile_counters(physical_device_id, accel_id);\n    }\n    acl_get_hal()->get_profile_data(physical_device_id, accel_id, profile_data,\n                                    num_profile_counters);\n    acl_get_hal()->set_profile_shared_control(physical_device_id, accel_id);\n    acl_get_hal()->enable_profile_counters(physical_device_id, accel_id);\n  }\n\n  // Get the curr shared counter\n  int curr_shared_counters = get_env_profile_shared_counter_val();\n\n  if (!write_profile_info_to_file(\n          num_profile_counters, (unsigned)device_id->id,\n          (unsigned)command_queue->id, kernel->accel_def->iface.name.c_str(),\n          profile_data, ACL_DEVICE_OP_KERNEL,\n          (unsigned long long)event->timestamp[CL_RUNNING],\n          (unsigned long long)0, curr_shared_counters)) {\n    ERR_RET(CL_OUT_OF_RESOURCES, context, \"Unabled to dump profile data\");\n  }\n\n  return CL_SUCCESS;\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clGetProfileDataDeviceIntelFPGA(\n    cl_device_id device_id, cl_program program, cl_bool read_enqueue_kernels,\n    cl_bool read_auto_enqueued, cl_bool clear_counters_after_readback,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret,\n    cl_int *errcode_ret) {\n\n  read_enqueue_kernels = read_enqueue_kernels;\n  cl_context context;\n  unsigned num_profile_counters;\n  unsigned int physical_device_id;\n  unsigned int accel_id;\n  int i;\n  cl_int status;\n  unsigned long long profiled_time;\n\n  clear_counters_after_readback = clear_counters_after_readback;\n  param_value_size = param_value_size;\n  param_value = param_value;\n  param_value_size_ret = param_value_size_ret;\n  errcode_ret = errcode_ret;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Check if valid device_id\n  if (!acl_device_is_valid(device_id)) {\n    return CL_INVALID_DEVICE;\n  }\n\n  // Check if valid program\n  if (!acl_program_is_valid(program)) {\n    return CL_INVALID_PROGRAM;\n  }\n\n  // If program is valid, then context is valid because acl_program_is_valid\n  // checks acl_context_is_valid.\n  context = program->context;\n\n  // Find the special readback kernel\n  if (read_auto_enqueued) {\n    const acl_device_binary_t *dev_bin = nullptr;\n\n    const acl_accel_def_t *accel_def =\n        acl_find_accel_def(program, ACL_PROFILE_AUTORUN_KERNEL_NAME, dev_bin,\n                           &status, program->context, 0);\n    // Return error if that readback kernel is not present\n    if (status != CL_SUCCESS) {\n      // Cannot find the readback kernel for autorun even though the host is\n      // asking for it\n      const char *message = \"Cannot find readback kernel \" STR(\n          ACL_PROFILE_AUTORUN_KERNEL_NAME) \" for autorun profiling. Make sure \"\n                                           \"the .aocx was compiled with \"\n                                           \"autorun kernel profiling enabled\";\n      ERR_RET(status, program->context, message);\n    }\n    assert(accel_def != NULL);\n\n    // use autodiscovery info to find out how many words will be read from the\n    // profiler\n    num_profile_counters = accel_def->profiling_words_to_readback;\n\n    if (num_profile_counters == 0) {\n      // Message will say \"No profile information for kernel\n      // \"Intel_Internal_Collect_Autorun_Profiling\" for reading back autorun\n      // profile data\"\n      const char *message = \"No profile information for kernel \" STR(\n          ACL_PROFILE_AUTORUN_KERNEL_NAME) \" for reading back autorun profile \"\n                                           \"data\";\n      ERR_RET(CL_PROFILING_INFO_NOT_AVAILABLE, program->context, message);\n    } else {\n      uint64_t *readback_profile_data;\n      readback_profile_data = (uint64_t *)acl_malloc(\n          num_profile_counters * sizeof(uint64_t)); // see acl_kernel.c\n\n      // Expected to be already allocated at kernel creation\n      assert(readback_profile_data);\n\n      physical_device_id = device_id->def.physical_device_id;\n      accel_id = accel_def->id;\n\n      acl_get_hal()->disable_profile_counters(physical_device_id, accel_id);\n      // This for loop is needed to create a delay between counter disabling and\n      // the start of shifting to allow tessellated counters to finish their\n      // addition It is only needed for S10 and other familes that enable\n      // tessellation - can be removed for other families\n      for (i = 0; i < TESS_DELAY; i++) {\n        acl_get_hal()->disable_profile_counters(physical_device_id, accel_id);\n      }\n      profiled_time = acl_get_hal()->get_timestamp();\n      acl_get_hal()->get_profile_data(physical_device_id, accel_id,\n                                      readback_profile_data,\n                                      num_profile_counters);\n      acl_get_hal()->set_profile_shared_control(physical_device_id, accel_id);\n      acl_get_hal()->enable_profile_counters(physical_device_id, accel_id);\n\n      // Get the curr shared counter\n      int curr_shared_counters = get_env_profile_shared_counter_val();\n\n      if (!write_profile_info_to_file_from_device(\n              device_id, context, accel_def->iface.name.c_str(),\n              readback_profile_data, ACL_DEVICE_OP_KERNEL, autorun_start_time,\n              profiled_time, num_profile_counters, curr_shared_counters)) {\n        return CL_OUT_OF_RESOURCES;\n      }\n    }\n  }\n\n  return CL_SUCCESS;\n}\n\nint acl_process_autorun_profiler_scan_chain(unsigned int physical_device_id,\n                                            unsigned int accel_id) {\n  if (!profile_enable)\n    return 1;\n\n  unsigned num_profile_counters = 0;\n  uint64_t *profile_data = nullptr;\n  std::string name = \"\";\n\n  std::unique_lock lock{acl_mutex_wrapper, std::defer_lock};\n  if (!acl_is_inside_sig()) {\n    lock.lock();\n  }\n\n  const acl_device_binary_t *binary =\n      acl_get_platform()->device[physical_device_id].loaded_bin;\n  if (binary == nullptr) {\n    return 0;\n  }\n  const acl_accel_def_t *accel_def =\n      binary->get_dev_prog()->get_kernel_accel_def(\n          ACL_PROFILE_AUTORUN_KERNEL_NAME);\n  if (accel_def == nullptr) {\n    return 0;\n  }\n\n  num_profile_counters = accel_def->profiling_words_to_readback;\n  name = accel_def->iface.name;\n\n  // This kernel has profiling data, get it\n  if (num_profile_counters) {\n    profile_data =\n        (uint64_t *)acl_malloc(num_profile_counters * sizeof(uint64_t));\n\n    assert(profile_data);\n\n    acl_get_hal()->get_profile_data(physical_device_id, accel_id, profile_data,\n                                    num_profile_counters);\n  } else {\n    // There is no profiler data - nothing to print\n    return 0;\n  }\n\n  // Get the curr shared counter\n  int curr_shared_counters = get_env_profile_shared_counter_val();\n\n  // Dump data to buffer\n  dump_profile_data_to_buffer(\n      (unsigned)num_profile_counters, (unsigned)physical_device_id, (unsigned)0,\n      name.c_str(), profile_data, ACL_DEVICE_OP_KERNEL,\n      (unsigned long long)autorun_start_time,\n      (unsigned long long)acl_get_hal()->get_timestamp(), curr_shared_counters);\n\n  if (profile_data) {\n    acl_free(profile_data);\n  }\n\n  return 1;\n}\n\nint acl_process_profiler_scan_chain(acl_device_op_t *op) {\n\n  if (!profile_enable)\n    return 1;\n\n  cl_event event;\n  cl_kernel kernel;\n  unsigned num_profile_counters;\n  uint64_t *profile_data;\n  unsigned int physical_device_id;\n  unsigned int accel_id;\n  cl_command_type op_type;\n  _cl_command_queue *command_queue;\n  cl_device_id device_id;\n  std::unique_lock lock{acl_mutex_wrapper, std::defer_lock};\n  if (!acl_is_inside_sig()) {\n    lock.lock();\n  }\n\n  char name[MAX_NAME_SIZE];\n  num_profile_counters = 0;\n  op_type = op->info.type;\n  event = op->info.event;\n\n  acl_print_debug_msg(\"acl_process_profiler_scan_chain\\n\");\n\n  if (!acl_is_inside_sig()) {\n    // Check event for validity.\n    // This also transitively checks the context.\n    if (!acl_event_is_valid(event)) {\n      acl_print_debug_msg(\n          \"acl_process_profiler_scan_chain is called for an invalid event\\n\");\n      return 0;\n    }\n    if (!acl_command_queue_is_valid(event->command_queue)) {\n      acl_print_debug_msg(\"acl_process_profiler_scan_chain is called for an \"\n                          \"event with an invalid command_queue\\n\");\n      return 0;\n    }\n    if (acl_event_is_done(event)) {\n      acl_print_debug_msg(\n          \"acl_process_profiler_scan_chain is called for a completed event\\n\");\n      return 0;\n    }\n  }\n\n  command_queue = event->command_queue;\n  device_id = command_queue->device;\n\n  if (!device_id) {\n    acl_print_debug_msg(\n        \"acl_process_profiler_scan_chain is called for NULL device_id\\n\");\n    return 0;\n  }\n\n  // this is not a kernel event and we are not printing timers\n  // so nothing to print\n  if (op_type != ACL_DEVICE_OP_KERNEL && profile_timer_on != 1) {\n    return 0;\n  }\n\n  profile_data = 0;\n  if (op_type == ACL_DEVICE_OP_KERNEL) {\n    kernel = event->cmd.info.ndrange_kernel.kernel;\n\n    // use autodiscovery info to find out how many words will be read from the\n    // profiler\n    num_profile_counters = kernel->accel_def->profiling_words_to_readback;\n\n    if (num_profile_counters == 0 && profile_timer_on != 1) {\n      // there is not profiler data and we are not printing timers\n      // nothing to print\n      return 0;\n    }\n\n    snprintf(name, MAX_NAME_SIZE, \"%s\", kernel->accel_def->iface.name.c_str());\n\n    // this kernel has profiling data, get it\n    if (num_profile_counters) {\n      profile_data = kernel->profile_data;\n\n      // expected to be already allocated at kernel creation\n      assert(profile_data);\n\n      physical_device_id =\n          event->cmd.info.ndrange_kernel.device->def.physical_device_id;\n      accel_id = event->cmd.info.ndrange_kernel.accel_id;\n\n      acl_get_hal()->get_profile_data(physical_device_id, accel_id,\n                                      profile_data, num_profile_counters);\n    }\n  } else if (profile_timer_on != 1) {\n    // if ACL_PROFILE_TIMER is not set, do not print info about the rest of\n    // the events\n    return 0;\n  } else if (op_type == ACL_DEVICE_OP_MEM_TRANSFER_COPY) {\n    snprintf(name, MAX_NAME_SIZE, \".mem_transfer_copy\");\n  } else if (op_type == ACL_DEVICE_OP_MEM_TRANSFER_READ) {\n    snprintf(name, MAX_NAME_SIZE, \".mem_transfer_read\");\n  } else if (op_type == ACL_DEVICE_OP_MEM_TRANSFER_WRITE) {\n    snprintf(name, MAX_NAME_SIZE, \".mem_transfer_write\");\n  } else if (op_type == ACL_DEVICE_OP_REPROGRAM) {\n    snprintf(name, MAX_NAME_SIZE, \".reprogram\");\n  } else if (op_type == ACL_DEVICE_OP_MEM_MIGRATION) {\n    snprintf(name, MAX_NAME_SIZE, \".mem_migration\");\n  } else if (op_type == ACL_DEVICE_OP_USM_MEMCPY) {\n    snprintf(name, MAX_NAME_SIZE, \".usm_memcpy\");\n  } else if (op_type == ACL_DEVICE_OP_HOSTPIPE_READ) {\n    snprintf(name, MAX_NAME_SIZE, \".hostpipe_read\");\n  } else if (op_type == ACL_DEVICE_OP_HOSTPIPE_WRITE) {\n    snprintf(name, MAX_NAME_SIZE, \".hostpipe_write\");\n  } else if (op_type == ACL_DEVICE_OP_DEVICE_GLOBAL_READ) {\n    snprintf(name, MAX_NAME_SIZE, \".device_global_read\");\n  } else if (op_type == ACL_DEVICE_OP_DEVICE_GLOBAL_WRITE) {\n    snprintf(name, MAX_NAME_SIZE, \".device_global_write\");\n  } else {\n    // Ignore unknown op_type (don't attempt to extract any profiling from it or\n    // get timestamps)\n    acl_print_debug_msg(\"Unknown device op type: '%d'\\n\", int(op_type));\n    return 0;\n  }\n\n  // For temporal profiling, we want to record the time at which the profiler\n  // counters were read back, or the kernel finish time (if available).  For\n  // non-temporal profiling we will only have the finish time.\n  unsigned long long end_time = op->timestamp[CL_COMPLETE];\n  if (end_time == 0)\n    end_time = acl_get_hal()->get_timestamp();\n\n  // Get the shared counter value (or set to -1 if its not set/out of expected\n  // bounds)\n  int curr_shared_counter = get_env_profile_shared_counter_val();\n\n  // Dump data to buffer\n  dump_profile_data_to_buffer(\n      (unsigned)num_profile_counters, (unsigned)device_id->id,\n      (unsigned)command_queue->id, name, profile_data, op_type,\n      (unsigned long long)op->timestamp[CL_RUNNING], end_time,\n      curr_shared_counter);\n\n  if (op->timestamp[CL_COMPLETE] != 0) {\n    // Dump buffer to file since this is the last one.\n    dump_profile_buffer_to_file();\n  }\n\n  return 1;\n}\n\nint acl_open_profiler_file() {\n  acl_assert_locked();\n  if (!profile_enable)\n    return 0;\n  if (opened_count > 0) {\n    opened_count += 1;\n    return 1;\n  }\n  if (!pfile_was_opened) {\n// First time opening the file\n#ifdef _WIN32\n    pfile_fd = _sopen(ACL_PROFILER_OUTPUT_FILENAME,\n                      _O_WRONLY | _O_CREAT | _O_TRUNC, _SH_DENYWR, _S_IWRITE);\n#else // Linux\n    pfile_fd = open(ACL_PROFILER_OUTPUT_FILENAME, O_WRONLY | O_CREAT | O_TRUNC,\n                    S_IRUSR | S_IWUSR | S_IRGRP);\n#endif\n  } else {\n#ifdef _WIN32\n    pfile_fd =\n        _sopen(ACL_PROFILER_OUTPUT_FILENAME, _O_WRONLY | O_APPEND, _SH_DENYWR);\n#else // Linux\n    pfile_fd = open(ACL_PROFILER_OUTPUT_FILENAME, O_WRONLY | O_APPEND);\n#endif\n  }\n  if (pfile_fd < 0) {\n    acl_print_debug_msg(\"Could not open profiler output file: '%s'!\\n\",\n                        ACL_PROFILER_OUTPUT_FILENAME);\n    failed_to_open_count += 1;\n    return 0;\n  }\n  opened_count += 1;\n  pfile_was_opened = true;\n  return 1;\n}\n\nint acl_close_profiler_file() {\n  acl_assert_locked();\n  if (!profile_enable)\n    return 1;\n  assert(opened_count + failed_to_open_count >= 1);\n  if (failed_to_open_count > 0) {\n    failed_to_open_count -= 1;\n    return 1;\n  } else if (opened_count > 1) {\n    opened_count -= 1;\n    return 1;\n  } else {\n\n#ifdef _WIN32\n    _close(pfile_fd);\n#else // Linux\n    close(pfile_fd);\n#endif\n\n    opened_count -= 1;\n    return 1;\n  }\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_support.cpp",
        "data": "// Copyright (C) 2012-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n#ifndef ACL_HAS_STDLIB_STDIO\n#error \"Sorry, we only support platforms with certain basic capabilities!\"\n#endif\n\n// System headers.\n#include <cassert>\n#include <errno.h>\n#include <sstream>\n#include <stdarg.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string>\n\n#ifdef _WIN32\n#include <direct.h>\n#include <process.h>\n#include <windows.h>\n#else\n#include <dlfcn.h>\n#include <glob.h>\n#include <sys/stat.h>\n#include <sys/time.h>\n#include <time.h>\n#include <unistd.h>\n#endif\n\n// External library headers.\n#include <acl_check_sys_cmd/acl_check_sys_cmd.h>\n\n// Internal headers.\n#include <acl_hal.h>\n#include <acl_support.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\nint l_malloc_enable = 1;\n\nstruct acl_file_handle_t;\n\n// Normalize backslashes in path to forward-slashes.\nstd::string l_normalize_path(const std::string &path) {\n  auto result = path;\n  for (auto &ch : result) {\n    if (ch == '\\\\') {\n      ch = '/';\n    }\n  }\n  return result;\n}\n\nconst char *acl_getenv(const char *name) { return getenv(name); }\n\nstruct acl_file_handle_t *acl_get_handle_for_stdout(void) {\n  return (struct acl_file_handle_t *)stdout;\n}\nstruct acl_file_handle_t *acl_get_handle_for_stderr(void) {\n  return (struct acl_file_handle_t *)stderr;\n}\n\nstruct acl_file_handle_t *acl_fopen(const char *name, const char *mode) {\n  FILE *fp = fopen(name, mode);\n  if (fp)\n    errno = 0; // Signal ok. Confusing otherwise.\n  return (struct acl_file_handle_t *)fp;\n}\n\nint acl_fclose(struct acl_file_handle_t *handle) {\n  FILE *fp = (FILE *)handle;\n  return fclose(fp);\n}\n#ifdef _WIN32\nint acl_unlink(const char *filename) { return DeleteFile(filename); }\n#else\nint acl_unlink(const char *filename) { return unlink(filename); }\n#endif\n\nint acl_printf(const char *format, ...) {\n  int result;\n  va_list args;\n  va_start(args, format);\n  result = vprintf(format, args);\n  va_end(args);\n  return result;\n}\n\nint acl_fprintf(struct acl_file_handle_t *handle, const char *format, ...) {\n  int result;\n  FILE *fp = (FILE *)handle;\n  va_list args;\n  va_start(args, format);\n  result = vfprintf(fp, format, args);\n  va_end(args);\n  return result;\n}\n\nsize_t acl_fwrite(const void *ptr, size_t size, size_t nelem,\n                  struct acl_file_handle_t *handle) {\n  FILE *fp = (FILE *)handle;\n  return fwrite(ptr, size, nelem, fp);\n}\n\nvoid acl_notify_print(const char *errinfo, const void *private_info, size_t cb,\n                      void *user_data) {\n  private_info = private_info;\n  cb = cb;\n  user_data = user_data;\n  printf(\"Error: %s\\n\", errinfo);\n}\n\n// This is used in unit tests to test CL_OUT_OF_HOST_MEMORY error codes\nvoid acl_set_malloc_enable(int malloc_enable) {\n  l_malloc_enable = malloc_enable;\n}\n\nvoid *acl_malloc(size_t size) { return l_malloc_enable ? malloc(size) : NULL; }\n\nvoid *acl_realloc(void *buffer, size_t size) {\n  return l_malloc_enable ? realloc(buffer, size) : NULL;\n}\n\nvoid acl_free(void *buffer) { free(buffer); }\n\nstd::string acl_realpath_existing(const std::string &path) {\n  acl_assert_locked();\n  std::vector<char> p(ACL_PATH_MAX);\n\n#ifdef _WIN32\n  if (!_fullpath(p.data(), path.c_str(), ACL_PATH_MAX)) {\n    return \"\";\n  }\n\n  // Normalize to forward slashes.\n  for (auto &ch : p) {\n    if (ch == '\\\\') {\n      ch = '/';\n    }\n  }\n#else\n  // Use POSIX relpath.\n  // Relies on ACL_PATH_MAX being the same as PATH_MAX.\n  if (!realpath(path.c_str(), p.data())) {\n    return \"\";\n  }\n\n  if (p[0] != '/') {\n    // Not an absolute path. Prepend current working directory.\n    auto *cwd = getcwd(0, 0);\n    if (!cwd)\n      return \"\";\n\n    std::stringstream ss;\n    ss << cwd << \"/\" << p.data();\n    free(cwd);\n\n    if (!realpath(ss.str().c_str(), p.data())) {\n      return \"\";\n    }\n  }\n#endif\n\n  return std::string(p.data());\n}\n\n// Change directory.\n// Return 0 on failure, non-zero on success.\n// Note: this inverts the usual sense.\nint acl_chdir(const char *dir) {\n  acl_assert_locked();\n  // These will set errno\n#ifdef _WIN32\n  return !_chdir(dir);\n#else\n  return !chdir(dir);\n#endif\n}\n\nstd::vector<std::string> acl_glob(const std::string &pattern) {\n  acl_assert_locked();\n  std::vector<std::string> result;\n\n#ifdef _WIN32\n  WIN32_FIND_DATA ffd;\n  HANDLE hFind = FindFirstFile(pattern.c_str(), &ffd);\n  if (INVALID_HANDLE_VALUE != hFind) {\n    result.push_back(ffd.cFileName);\n    while (FindNextFile(hFind, &ffd)) {\n      result.push_back(ffd.cFileName);\n    }\n  }\n  FindClose(hFind);\n#else // Linux\n  glob_t globbuf;\n  auto err = glob(pattern.c_str(), 0, nullptr, &globbuf);\n\n  if (!err) {\n    for (size_t i = 0; i < globbuf.gl_pathc; ++i) {\n      result.push_back(globbuf.gl_pathv[i]);\n    }\n  }\n  globfree(&globbuf);\n#endif\n\n  return result;\n}\n\nint acl_has_error(void) { return errno != 0; }\n\nconst char *acl_support_last_error(void) { return strerror(errno); }\n\n#ifdef _WIN32\n#define l_mkdir(d) _mkdir(d)\n#define l_chdir(d) _chdir(d)\n#else\n#define l_mkdir(d) mkdir(d, 0777)\n#define l_chdir(d) chdir(d)\n#endif\n\nstatic int l_make_path_to_dir(const std::string &path);\nint acl_make_path_to_dir(const std::string &path) {\n  acl_assert_locked();\n  if (path == \"\")\n    return 0;\n  else {\n    // Remember where we started!\n    auto original_dir = acl_realpath_existing(\".\");\n    if (original_dir == \"\")\n      return 0;\n\n    acl_print_debug_msg(\"acl_make_path_to_dir: cwd is %s\\n\",\n                        original_dir.c_str());\n\n    auto result = l_make_path_to_dir(path);\n\n    // Go back\n    auto final_chdir = l_chdir(original_dir.c_str());\n    if (final_chdir != 0) {\n      acl_print_debug_msg(\"acl_make_path_to_dir: can't chdir back to %s: %d\\n\",\n                          original_dir.c_str(), final_chdir);\n      result = 0;\n    }\n    acl_print_debug_msg(\"acl_make_path_to_dir: result is %d\\n\", result);\n    return result;\n  }\n}\n\nstatic int l_make_path_to_dir(const std::string &path) {\n  acl_assert_locked();\n\n  acl_print_debug_msg(\"\\nacl_make_path_to_dir: path arg is %s\\n\", path.c_str());\n\n  auto normalized_path = l_normalize_path(path);\n  // Root directory always exists.\n  if (normalized_path == \"/\")\n    return 1;\n  // Trim trailing forward slash if it exists.\n  if (normalized_path[normalized_path.length() - 1] == '/') {\n    normalized_path = normalized_path.substr(0, normalized_path.length() - 1);\n  }\n\n  // Walk the directory path and create any missing directories along the way.\n  std::string::size_type curr_pos = 0;\n  while (curr_pos < normalized_path.length() && curr_pos != std::string::npos) {\n    auto end_pos = normalized_path.find_first_of('/', curr_pos);\n    std::string component = normalized_path.substr(\n        curr_pos, end_pos == std::string::npos ? std::string::npos\n                                               : end_pos - curr_pos + 1);\n\n    // Current component doesn't exist so create it.\n    auto status = l_chdir(component.c_str());\n    if (status != 0) {\n      auto mkdir_status = l_mkdir(component.c_str());\n      if (mkdir_status != 0) {\n        acl_print_debug_msg(\"Can't make path component '%s': %d\\n\",\n                            component.c_str(), mkdir_status);\n        return 0;\n      }\n      auto chdir_status = l_chdir(component.c_str());\n      if (chdir_status != 0) {\n        acl_print_debug_msg(\"Can't chdir path component '%s': %d\\n\",\n                            component.c_str(), chdir_status);\n        return 0;\n      }\n    }\n\n    if (end_pos == std::string::npos) {\n      curr_pos = std::string::npos;\n    } else {\n      curr_pos = end_pos + 1;\n      // Consecutive forward slashes should be treated like a single forward\n      // slash.\n      while (curr_pos < normalized_path.length() &&\n             normalized_path[curr_pos] == '/') {\n        curr_pos++;\n      }\n    }\n  }\n\n  return 1;\n}\n\n// Extract content of section sect_name into a new unique out_file.\n// If no such section exists, do nothing.\nint acl_create_unique_filename(std::string &name) {\n  static int enumerator = 0;\n\n  std::stringstream fnss;\n#ifdef _WIN32\n  const char *tmpdir = acl_getenv(\"TEMP\");\n  if (!tmpdir) {\n    tmpdir = acl_getenv(\"TMP\");\n    if (!tmpdir) {\n      tmpdir = \".\";\n    }\n  }\n  fnss << tmpdir << \"\\\\aocl-\" << acl_getpid() << \"-kernel\" << enumerator++\n       << \".dll\";\n#else\n  const char *tmpdir = acl_getenv(\"TMPDIR\");\n  if (!tmpdir) {\n    tmpdir = \"/tmp\";\n  }\n  fnss << tmpdir << \"/aocl-\" << acl_getpid() << \"-kernel\" << enumerator++\n       << \".so\";\n#endif\n\n  name = std::move(fnss.str());\n  acl_print_debug_msg(\"Using temporary kernel file %s\\n\", name.c_str());\n\n  return 1;\n}\n\nint acl_system(const char *command) {\n  acl_assert_locked();\n  assert(system_cmd_is_valid(command));\n  return system(command);\n}\n\n// Dynamic library loading support\n#ifdef _WIN32\nvoid *acl_dlopen(const char *file) { return LoadLibrary(file); }\n\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4054)\n#endif\nvoid *acl_dlsym(void *handle, const char *name) {\n  return (void *)GetProcAddress((HMODULE)handle, name);\n}\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\nchar *acl_dlerror(void) {\n  DWORD error_code = 0;\n  char *buf = 0;\n  error_code = GetLastError();\n  if (error_code) {\n    if (!FormatMessage(FORMAT_MESSAGE_ALLOCATE_BUFFER |\n                           FORMAT_MESSAGE_FROM_SYSTEM |\n                           FORMAT_MESSAGE_IGNORE_INSERTS,\n                       NULL, error_code, 0, buf, 0, NULL)) {\n      printf(\"Failed to format Windows error message for %ld\\n\", error_code);\n      exit(4);\n    }\n    return buf;\n  } else {\n    return 0;\n  }\n}\n\nint acl_dlpresent(void) { return &LoadLibrary != nullptr; }\n\nint acl_dlclose(void *handle) { return (int)FreeLibrary((HMODULE)handle); }\n\n// Supportfunction needed to create \"unique\" filenames\nint acl_getpid() { return _getpid(); }\n#else\n#define RTLD_NOW 0x00002 /* Immediate function call binding.  */\nACL_EXPORT\nextern void *dlopen(__const char *__file, int __mode) __attribute__((weak));\nACL_EXPORT\nextern void *dlsym(void *__restrict __handle, __const char *__restrict __name)\n    __attribute__((weak));\nACL_EXPORT\nextern char *dlerror(void) __attribute__((weak));\nACL_EXPORT\nextern int dlclose(void *) __attribute__((weak));\n\nvoid *acl_dlopen(const char *file) { return dlopen(file, RTLD_NOW); }\n\nvoid *acl_dlsym(void *dllhandle, const char *symbolname) {\n  return dlsym(dllhandle, symbolname);\n}\n\nchar *acl_dlerror(void) { return dlerror(); }\n\nint acl_dlpresent(void) { return &dlopen != nullptr; }\n\nint acl_dlclose(void *dllhandle) { return !dlclose(dllhandle); }\n\n// Supportfunction needed to create \"unique\" filenames\nint acl_getpid() { return getpid(); }\n#endif\n\n// Get wall clock time in ns.\n// Taken from the PCIe HAL.\n#ifdef _WIN32\n// Query the system timer, return a timer value in ns\nstatic INT64 ticks_per_second;\nstatic int ticks_per_second_valid = 0;\ncl_ulong acl_get_time_ns() {\n  LARGE_INTEGER li;\n  double seconds;\n  INT64 ticks;\n  const INT64 NS_PER_S = 1000000000;\n\n  if (!ticks_per_second_valid) {\n    LARGE_INTEGER tps;\n    tps.QuadPart = 0;\n    QueryPerformanceFrequency(&tps);\n    ticks_per_second = tps.QuadPart;\n    // Performance counter might not be present. Then it returns 0.\n    if (!ticks_per_second) {\n      ticks_per_second = 1;\n    }\n    ticks_per_second_valid = 1;\n  }\n\n  QueryPerformanceCounter(&li);\n  ticks = li.QuadPart;\n  seconds = ticks / (double)ticks_per_second;\n  return (cl_ulong)(seconds * NS_PER_S + 0.5);\n}\n\n#else\n\n// Query the system timer, return a timer value in ns\ncl_ulong acl_get_time_ns() {\n  struct timespec a;\n  const cl_ulong NS_PER_S = 1000000000;\n  // Must use the MONOTONIC clock because the REALTIME clock\n  // can go backwards due to adjustments by NTP for clock drift.\n  // The MONOTONIC clock provides a timestamp since some fixed point in\n  // the past, which might be system boot time or the start of the Unix\n  // epoch.  This matches the Windows QueryPerformanceCounter semantics.\n  // The MONOTONIC clock is to be used for measuring time intervals, and\n  // fits the semantics of timestamps from the *device* perspective as defined\n  // in OpenCL for clGetEventProfilingInfo.\n  clock_gettime(CLOCK_MONOTONIC, &a);\n  return (cl_ulong)(a.tv_nsec) + (cl_ulong)(a.tv_sec * NS_PER_S);\n}\n#endif\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_printf.cpp",
        "data": "// Copyright (C) 2012-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// Printf\n// ========\n//\n// Handle the parsing and printing out of printf output from the kernel\n//\n// (see Trasforms/FPGATransforms/TransformPrintf.cpp for description\n//  of the printf data format in memory)\n//\n\n// System headers.\n#include <cassert>\n#include <sstream>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string>\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_kernel.h>\n#include <acl_printf.h>\n#include <acl_program.h>\n#include <acl_svm.h>\n#include <acl_types.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\nstatic size_t l_dump_printf_buffer(cl_event event, cl_kernel kernel,\n                                   unsigned size);\nstatic void decode_string(std::string &print_data);\n\n// stores to global memory are bound by smallest global memory bandwidth we\n// support (i.e 32-bytes) WARNING: this should match the value in\n// Trasforms/FPGATransforms/TransformPrintf.cpp\n#define SMALLEST_GLOBAL_MEMORY_BANDWIDTH 32\n\ntypedef enum {\n  LS_NONE = 0,\n  LS_HH,\n  LS_H,\n  LS_HL, // valid only for vector types\n  LS_L\n} Length_Specifier;\n\ntypedef enum {\n  PRINTABLE_TYPE_C = 0,\n  PRINTABLE_TYPE_D,\n  PRINTABLE_TYPE_I,\n  PRINTABLE_TYPE_O,\n  PRINTABLE_TYPE_U,\n  PRINTABLE_TYPE_x,\n  PRINTABLE_TYPE_X,\n  PRINTABLE_TYPE_s,\n  PRINTABLE_TYPE_f,\n  PRINTABLE_TYPE_F,\n  PRINTABLE_TYPE_e,\n  PRINTABLE_TYPE_E,\n  PRINTABLE_TYPE_g,\n  PRINTABLE_TYPE_G,\n  PRINTABLE_TYPE_a,\n  PRINTABLE_TYPE_A,\n  PRINTABLE_TYPE_P,\n  PRINTABLE_TYPE_LAST,\n} Printable_Type;\n\nstatic const char Printable_Type_Identifier[PRINTABLE_TYPE_LAST] = {\n    'c', 'd', 'i', 'o', 'u', 'x', 'X', 's', 'f',\n    'F', 'e', 'E', 'g', 'G', 'a', 'A', 'p',\n};\n\ntypedef struct {\n  Printable_Type _type;\n  std::string _conversion_string;\n} Printable_Data_Elem;\n\nstatic float convert_half_to_float(unsigned short half)\n// This method converts a 16-bit half-precision FP number to a\n// single-precision floating point number.\n// Copy/paste from /ip/test/common/fp_convert_from_half/fpgen#1.cpp\n{\n  unsigned int sign = (unsigned int)(half & 0x8000) << 16;\n  unsigned int exponent = (unsigned int)(half & 0x7C00) >> 10;\n  unsigned int mantissa = (unsigned int)(half & 0x03ff) << 13;\n  unsigned int result;\n\n  // Handle special case\n  if (exponent == 0x01f) // max exponent\n  {\n    exponent = 255; // Max exponent in the single-precision format.\n  } else if (exponent != 0) {\n    // exponent -15 removes the bias in half precision, and +127 adds the bias\n    // for single-precision.\n    exponent = (127 - 15) + exponent;\n  }\n\n  exponent = exponent << 23;\n  result = sign | exponent | mantissa;\n  return (*(float *)&result);\n}\n\n// find length specifier strings in the given conversion_string\nstatic Length_Specifier\nget_length_specifier(const std::string &conversion_string) {\n  for (unsigned i = 0; i < conversion_string.length(); ++i) {\n    if (conversion_string[i] == 'h') {\n      if (i + 1 == conversion_string.length())\n        return LS_H;\n      else if (conversion_string[i + 1] == 'l')\n        return LS_HL;\n      else if (conversion_string[i + 1] == 'h')\n        return LS_HH;\n      else\n        return LS_H;\n    } else if (conversion_string[i] == 'l') {\n      return LS_L;\n    }\n  }\n  return LS_NONE;\n}\n\nstatic unsigned get_llvm_data_size_char() {\n  return 4; // char's are always converted to i32 in IR\n}\n\n// these are the size of types in LLVM IR\nstatic unsigned get_llvm_data_size_int_uint(Length_Specifier length,\n                                            int vector_type) {\n\n  // inconsistent length specifier, HL valid only for vector types\n  if (length == LS_HL && !vector_type)\n    return 0;\n\n  if (length == LS_NONE || length == LS_HL)\n    return 4; // HL only in vector type\n  else if (!vector_type && length == LS_HH)\n    return 4; // char's are converted to i32 (if not in vector)\n  else if (vector_type && length == LS_HH)\n    return 1;\n  else if (length == LS_H)\n    return 2;\n  else if (length == LS_L)\n    return 8;\n  return 0;\n}\n\nstatic unsigned get_llvm_data_size_float_double(Length_Specifier length,\n                                                int vector_type) {\n\n  if (length == LS_HH)\n    return 0;\n\n  if (!vector_type && length == LS_H)\n    return 0;\n\n  // half float\n  if (vector_type && length == LS_H)\n    return 2;\n\n  // interpret as double\n  if (!vector_type || (vector_type && length == LS_L)) {\n    return 8;\n  }\n  return 4;\n}\n\nstatic unsigned get_llvm_data_size_ptr() {\n  return 8; // pointer's are i64\n}\n\nstatic unsigned get_llvm_data_size_string() {\n  return 4; // Transform printf pass inserts string index (i32)\n}\n\n// verify the size of various host types (used to read from printf buffer)\n// against the llvm IR types (used to fill the printf buffer)\nstatic int verify_types() {\n\n  if (\n      // char's are not converted in vector types\n      sizeof(char) == get_llvm_data_size_int_uint(LS_HH, 1) &&\n      sizeof(unsigned char) == get_llvm_data_size_int_uint(LS_HH, 1) &&\n\n      sizeof(int) == get_llvm_data_size_char() &&\n      sizeof(int) == get_llvm_data_size_int_uint(LS_HH, 0) &&\n      sizeof(short) == get_llvm_data_size_int_uint(LS_H, 0) &&\n      sizeof(int) == get_llvm_data_size_int_uint(LS_HL, 1) &&\n      sizeof(long long) == get_llvm_data_size_int_uint(LS_L, 0) &&\n\n      sizeof(unsigned int) == get_llvm_data_size_int_uint(LS_HH, 0) &&\n      sizeof(unsigned short) == get_llvm_data_size_int_uint(LS_H, 0) &&\n      sizeof(unsigned int) == get_llvm_data_size_int_uint(LS_HL, 1) &&\n      sizeof(unsigned long long) == get_llvm_data_size_int_uint(LS_L, 0) &&\n      // vector floats not are always converted to double\n      sizeof(float) == get_llvm_data_size_float_double(LS_NONE, 1) &&\n      // vector floats are converted to double if LS_L is given\n      sizeof(double) == get_llvm_data_size_float_double(LS_L, 1) &&\n      // vector half floats are 2-bytes, will be read as short\n      sizeof(unsigned short) == get_llvm_data_size_float_double(LS_H, 1) &&\n      // non-vector floats are always converted to double\n      sizeof(double) == get_llvm_data_size_float_double(LS_NONE, 0) &&\n      sizeof(unsigned long long) == get_llvm_data_size_ptr() &&\n      sizeof(unsigned int) == get_llvm_data_size_string()) {\n    return 1;\n  }\n  return 0;\n}\n\nstatic unsigned get_llvm_data_size(Printable_Type type, Length_Specifier length,\n                                   int vector_type) {\n\n  if (type == PRINTABLE_TYPE_C) {\n    return get_llvm_data_size_char();\n  } else if (type == PRINTABLE_TYPE_P) {\n    return get_llvm_data_size_ptr();\n  } else if (type == PRINTABLE_TYPE_s) {\n    return get_llvm_data_size_string();\n  } else if (type == PRINTABLE_TYPE_D || type == PRINTABLE_TYPE_I ||\n             type == PRINTABLE_TYPE_O || type == PRINTABLE_TYPE_U ||\n             type == PRINTABLE_TYPE_x || type == PRINTABLE_TYPE_X) {\n    return get_llvm_data_size_int_uint(length, vector_type);\n  } else if (type == PRINTABLE_TYPE_f || type == PRINTABLE_TYPE_F ||\n             type == PRINTABLE_TYPE_e || type == PRINTABLE_TYPE_E ||\n             type == PRINTABLE_TYPE_g || type == PRINTABLE_TYPE_G ||\n             type == PRINTABLE_TYPE_a || type == PRINTABLE_TYPE_A) {\n    // non-vector floats are always converted to double\n    return get_llvm_data_size_float_double(length, vector_type);\n  }\n  assert(0);\n  return 0;\n}\n\n// assign the format string from auto-discovery\n// return 1 if format string was successfully read\n// return 0 if format string was not successfully read\nstatic int\nget_format_string(const unsigned format_string_index,\n                  std::string &format_string,\n                  const std::vector<acl_printf_info_t> &printf_infos) {\n  for (const auto &p : printf_infos) {\n    if (p.index == format_string_index) {\n      format_string = p.format_string;\n      return 1;\n    }\n  }\n\n  return 0;\n}\n\n// read a single char at offset and print it\n// char's are extended to 4-bytes in kernel\nstatic void print_data_elem_char(char *buffer, unsigned offset,\n                                 const std::string &format_string) {\n\n  int data = *((int *)&buffer[offset]);\n#ifdef DEBUG\n  if (debug_mode > 0) {\n    printf(\" print_data_elem_char:\");\n    printf(\" format_string=%s\", format_string.c_str());\n    printf(\" data=%u\", data);\n    printf(\" output=\");\n  }\n#endif\n\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n  printf(format_string.c_str(), (char)data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n#ifdef DEBUG\n  acl_print_debug_msg(\"\\n\");\n#endif\n}\n\n// read a single int at offset and print it\nstatic void print_data_elem_int(char *buffer, unsigned offset,\n                                const std::string &format_string,\n                                Length_Specifier length, int vector_type) {\n\n#ifdef DEBUG\n  printf(\" print_data_elem_int:\");\n  printf(\" format_string=%s\", format_string.c_str());\n#endif\n\n  if (length == LS_HH && vector_type) {\n    char data = *((char *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%d\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  } else if (length == LS_NONE || length == LS_HH || length == LS_HL) {\n    int data = *((int *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%d\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  } else if (length == LS_H) {\n    short data = *((short *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%hd\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  } else if (length == LS_L) {\n    long long data = *((long long *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%lld\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  }\n\n#ifdef DEBUG\n  printf(\"\\n\");\n#endif\n}\n\n// read a single uint at offset and print it\nstatic void print_data_elem_uint(char *buffer, unsigned offset,\n                                 const std::string &format_string,\n                                 Length_Specifier length, int vector_type) {\n\n#ifdef DEBUG\n  printf(\" print_data_elem_int:\");\n  printf(\" format_string=%s\", format_string.c_str());\n#endif\n\n  if (length == LS_HH && vector_type) {\n    unsigned char data = *((unsigned char *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%d\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  } else if (length == LS_NONE || length == LS_HH || length == LS_HL) {\n    unsigned int data = *((unsigned int *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%d\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  } else if (length == LS_H) {\n    unsigned short data = *((unsigned short *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%hd\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  } else if (length == LS_L) {\n    unsigned long long data = *((unsigned long long *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" data=%lld\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  }\n\n#ifdef DEBUG\n  printf(\"\\n\");\n#endif\n}\n\n// read a single double at offset and print it\n// (NOTE: all floats (except float vectors) are converted to double in the IR)\nstatic void print_data_elem_float(char *buffer, unsigned offset,\n                                  const std::string &format_string,\n                                  Length_Specifier length, int vector_type) {\n\n  // print a double\n  if (!vector_type || (vector_type && length == LS_L)) {\n    double data = *((double *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" print_data_elem_double:\");\n    printf(\" format_string=%s\", format_string);\n    printf(\" data=%f\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  }\n  // print a half float\n  else if (vector_type && length == LS_H) {\n    unsigned short temp = *((unsigned short *)&buffer[offset]);\n    float data = convert_half_to_float(temp);\n#ifdef DEBUG\n    printf(\" print_data_elem_double:\");\n    printf(\" format_string=%s\", format_string);\n    printf(\" data=%f\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  } else {\n    float data = *((float *)&buffer[offset]);\n#ifdef DEBUG\n    printf(\" print_data_elem_float:\");\n    printf(\" format_string=%s\", format_string);\n    printf(\" data=%f\", data);\n    printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n    printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n  }\n\n#ifdef DEBUG\n  printf(\"\\n\");\n#endif\n}\n\n// read a single float at offset and print it\n// (NOTE: floats are converted to double in the IR)\nstatic void print_data_elem_ptr(char *buffer, unsigned offset,\n                                const std::string &format_string) {\n\n  unsigned long long data = *((unsigned long long *)&buffer[offset]);\n#ifdef DEBUG\n  printf(\" print_data_elem_ptr:\");\n  printf(\" format_string=%s\", format_string);\n  printf(\" data=%lld\", data);\n  printf(\" output=\");\n#endif\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n  printf(format_string.c_str(), data);\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n#ifdef DEBUG\n  printf(\"\\n\");\n#endif\n}\n\n// read the index of the literal string, get the string from printf infos, print\n// it\nstatic void\nprint_data_elem_string(char *buffer, unsigned offset,\n                       std::string &format_string,\n                       const std::vector<acl_printf_info_t> &printf_infos) {\n  unsigned int literal_string_index = *((unsigned int *)&buffer[offset]);\n  std::string literal_string;\n\n  int success =\n      get_format_string(literal_string_index, literal_string, printf_infos);\n\n  // corrupt data in memory, fail silently, may be able to print the remaining\n  // data\n  if (!success)\n    return;\n\n#ifdef DEBUG\n  printf(\" print_data_elem_string:\");\n  printf(\" format_string=%s\", format_string);\n  printf(\" literal_string_index=%d\", literal_string_index);\n  printf(\" literal_string=%s\", literal_string);\n  printf(\" output=\");\n#endif\n\n  // handle special characters\n  // before printing\n  decode_string(literal_string);\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4774) // warning C4774: 'printf' : format string is\n                                // not a string literal\n#endif\n  printf(format_string.c_str(), literal_string.c_str());\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n#ifdef DEBUG\n  printf(\"\\n\");\n#endif\n}\n\nstatic void print_data_elem(Printable_Type type, char *buffer, unsigned offset,\n                            std::string &format_string, Length_Specifier length,\n                            const std::vector<acl_printf_info_t> &printf_infos,\n                            int vector_type) {\n\n  if (type == PRINTABLE_TYPE_C) {\n    print_data_elem_char(buffer, offset, format_string);\n  } else if (type == PRINTABLE_TYPE_P) {\n    print_data_elem_ptr(buffer, offset, format_string);\n  } else if (type == PRINTABLE_TYPE_s) {\n    print_data_elem_string(buffer, offset, format_string, printf_infos);\n  } else if (type == PRINTABLE_TYPE_D || type == PRINTABLE_TYPE_I) {\n    print_data_elem_int(buffer, offset, format_string, length, vector_type);\n  } else if (type == PRINTABLE_TYPE_O || type == PRINTABLE_TYPE_U ||\n             type == PRINTABLE_TYPE_x || type == PRINTABLE_TYPE_X) {\n    print_data_elem_uint(buffer, offset, format_string, length, vector_type);\n  } else if (type == PRINTABLE_TYPE_f || type == PRINTABLE_TYPE_F ||\n             type == PRINTABLE_TYPE_e || type == PRINTABLE_TYPE_E ||\n             type == PRINTABLE_TYPE_g || type == PRINTABLE_TYPE_G ||\n             type == PRINTABLE_TYPE_a || type == PRINTABLE_TYPE_A) {\n    // (all floats (except float vectors) are converted to double in the IR)\n    print_data_elem_float(buffer, offset, format_string, length, vector_type);\n  } else {\n    assert(0);\n  }\n}\n\n// Handle vector types by replicating the conversion string for each\n// vector element\nstatic int\nmodify_conversion_string_for_vectors(std::string &conversion_string) {\n  // see if the conversion string contains v*, where * can be 2, 3, 4, 8, 16\n  int found_v = -1;\n  std::string vector_size_string(2, '\\0');\n  std::stringstream non_vector_ss;\n  for (unsigned i = 0; i < conversion_string.length(); i++) {\n    if (conversion_string[i] == 'v') {\n      // there is more than one v\n      if (found_v != -1)\n        return -1;\n      found_v = static_cast<int>(i);\n    } else if (found_v != -1 && i == static_cast<unsigned>((found_v + 1))) {\n      if (conversion_string[i] <= '9' && conversion_string[i] >= '0') {\n        vector_size_string[0] = conversion_string[i];\n      } else\n        return -1; // v followed by no integer\n    } else if (found_v != -1 && i == static_cast<unsigned>((found_v + 2))) {\n      if (conversion_string[i] <= '9' && conversion_string[i] >= '0') {\n        vector_size_string[1] = conversion_string[i];\n      } else\n        non_vector_ss << conversion_string[i];\n    } else {\n      non_vector_ss << conversion_string[i];\n    }\n  }\n\n  // no vector type\n  if (found_v == -1)\n    return 1;\n\n  int vector_size = std::stoi(vector_size_string);\n  std::string non_vector_string = non_vector_ss.str();\n\n#ifdef DEBUG\n  printf(\"vector_size=%d\\n\", vector_size);\n  printf(\"non_vector_string=%s\\n\", non_vector_string.c_str());\n  printf(\"non_vector_string_length=%d\\n\", non_vector_string.length());\n#endif\n\n  // copy non_vector_string into conversion_string\n  // remove length specifier \"hl\" from the string, this is special for opencl\n  // vector types and standard C printf does not understand it\n  std::stringstream conversion_ss;\n  for (unsigned i = 0; i < non_vector_string.length(); i++) {\n    if (non_vector_string[i] == 'h' && i != (non_vector_string.length() - 1) &&\n        non_vector_string[i + 1] == 'l') {\n      i++;\n      continue;\n    }\n    conversion_ss << non_vector_string[i];\n  }\n  conversion_string = conversion_ss.str();\n\n  return vector_size;\n}\n\n// Read the format string starting from location ptr, as long as\n// end_of_string is not reached.\n// Extract string to be printed out and the data that needs be to\n// be converted.\n// For instance, for \"mydata=%4.2f\", the non_conversion_string is \"mydata=\"\n// the data_elem is PRINTABLE_TYPE_FLOAT, its _conversion_string is \"%4.2f\"\nstatic std::string::const_iterator get_data_elem_at_offset(\n    std::string::const_iterator ptr, std::string::const_iterator end_of_string,\n    Printable_Data_Elem &data_elem, std::string &non_conversion_string) {\n\n  std::stringstream ss;\n\n  // each data conversion starts with \"%\", find the first one\n  while (ptr != end_of_string) {\n\n    // %% prints as %\n    if ((ptr + 1) != end_of_string && *ptr == '%' && *(ptr + 1) == '%') {\n      ss << '%';\n      ptr += 2;\n    } else {\n      if (*ptr == '%')\n        break;\n      ss << *ptr;\n      ptr++;\n    }\n  }\n  non_conversion_string = ss.str();\n  if (ptr == end_of_string)\n    return end_of_string; // end of parsing\n  assert(*ptr == '%');\n\n  // advance until we reach c, u, d, f, etc. or end_of_string\n  ss.str(\"\");\n  do {\n    ss << *ptr;\n    assert((int)PRINTABLE_TYPE_C == 0);\n    for (int current_type = (int)PRINTABLE_TYPE_C;\n         current_type != (int)PRINTABLE_TYPE_LAST; current_type++) {\n      if (*ptr == Printable_Type_Identifier[current_type]) {\n        data_elem._type = (Printable_Type)current_type;\n        data_elem._conversion_string = ss.str();\n        return ptr + 1;\n      }\n    }\n    ptr++;\n  } while (ptr != end_of_string);\n  data_elem._conversion_string = ss.str();\n\n  // we found % but not followed by c, u, d, f\n  return end_of_string;\n}\n\nstatic size_t l_dump_printf_buffer(cl_event event, cl_kernel kernel,\n                                   unsigned size) {\n  unsigned global_offset;        // the location in the printf buffer\n  unsigned single_printf_offset; // the offset of a single printf\n  void (*hal_dma_fn)(cl_event, const void *, void *, size_t) = 0;\n  int src_on_host = 1;\n  size_t dumped_buffer_size = 0;\n#ifdef _WIN32\n  __declspec(align(64)) char\n      buffer[ACL_PRINTF_BUFFER_TOTAL_SIZE]; // Aligned to 64, for dma transfers\n#else\n  char buffer[ACL_PRINTF_BUFFER_TOTAL_SIZE]\n      __attribute__((aligned(64))); // Aligned to 64, for dma transfers\n#endif\n  cl_bool device_supports_any_svm = acl_svm_device_supports_any_svm(\n      event->command_queue->device->def.physical_device_id);\n  cl_bool device_supports_physical_memory =\n      acl_svm_device_supports_physical_memory(\n          event->command_queue->device->def.physical_device_id);\n\n  const auto &printf_infos = kernel->accel_def->printf_format_info;\n\n  if (!verify_types()) {\n    printf(\"Host data types are incompatible with ACL compiler, ignoring \"\n           \"printfs...\\n\");\n    return dumped_buffer_size;\n  }\n\n  if (printf_infos.empty())\n    return dumped_buffer_size;\n\n  // Memory is on the device if all of these are true:\n  //   The memory is not SVM or the device does not support SVM.\n  //   The device support physical memory\n  //   The memory is not host accessible\n  // Note that a very similar check is made in l_get_devices_affected_for_op,\n  // l_mem_transfer_buffer_explicitly and l_dump_printf_buffer\n  if ((!kernel->printf_device_buffer->is_svm || !device_supports_any_svm) &&\n      (device_supports_physical_memory)) {\n    src_on_host = kernel->printf_device_buffer->block_allocation->region\n                      ->is_host_accessible;\n  }\n\n  // In shared memory system, device and host memory may be the same.\n  // So pick the right copy function here.\n  if (src_on_host) {\n    hal_dma_fn = acl_get_hal()->copy_hostmem_to_hostmem;\n  } else {\n    hal_dma_fn = acl_get_hal()->copy_globalmem_to_hostmem;\n  }\n\n  // It needs the context from ACL_HAL_DEBUG instead of ACL_DEBUG\n  if (acl_get_hal()->get_debug_verbosity &&\n      acl_get_hal()->get_debug_verbosity() > 0) {\n    printf(\"Previously processed buffer size is %zu \\n\",\n           kernel->processed_printf_buffer_size);\n  }\n\n  // Check if we have already processed all the printf buffer\n  if (size > (unsigned int)kernel->processed_printf_buffer_size) {\n    void *unprocessed_begin = (void *)((char *)kernel->printf_device_buffer\n                                           ->block_allocation->range.begin +\n                                       kernel->processed_printf_buffer_size);\n    assert(size >= kernel->processed_printf_buffer_size);\n    dumped_buffer_size = size - kernel->processed_printf_buffer_size;\n    hal_dma_fn(NULL, unprocessed_begin, buffer, dumped_buffer_size);\n  } else {\n    if (acl_get_hal()->get_debug_verbosity &&\n        acl_get_hal()->get_debug_verbosity() > 0) {\n      printf(\"All Printf() buffer has already been dumped \\n\");\n    }\n    return dumped_buffer_size;\n  }\n\n#ifdef DEBUG\n  if (debug_mode > 0) {\n    printf(\"acl_dump_printf_buffer at %p size=%d\\n\", buffer, size);\n    printf(\"num_prints=%u\\n\", printf_infos.size());\n  }\n#endif\n\n#ifdef DEBUG\n  // dump the buffer contents\n  {\n    unsigned i;\n    for (i = 0; i < size; i += 4) {\n      unsigned format_string_index = *((unsigned *)&buffer[i]);\n      acl_print_debug_msg(\"%u: %x\\n\", i, format_string_index);\n    }\n  }\n#endif\n  // always 32-byte aligned address (this may change if printf chunks can be\n  // of different sizes )\n  // process all the printfs as long as there is data\n  for (global_offset = 0, single_printf_offset = 0;\n       global_offset < dumped_buffer_size;\n       global_offset += single_printf_offset) {\n\n    // the first 4-bytes is the index of the format string\n    unsigned local_offset;\n    int success;\n    unsigned size_of_data = 0;\n    // read the index of the format string from memory (i.e. first 4-bytes of\n    // the printf data)\n    unsigned format_string_index = *((unsigned *)&buffer[global_offset]);\n\n#ifdef DEBUG\n    if (debug_mode) {\n      printf(\"----- new printf data chunk -----\\n\");\n      printf(\"global_offset=%u\\n\", global_offset);\n      printf(\"format_string_index=%u\\n\", format_string_index);\n    }\n#endif\n\n    // get the format string using the index\n    std::string format_string;\n    success =\n        get_format_string(format_string_index, format_string, printf_infos);\n    // silently exit\n    if (!success) {\n      acl_print_debug_msg(\n          \"corrupt printf data, ignoring remaining printfs...\\n\");\n      return dumped_buffer_size;\n    }\n\n#ifdef DEBUG\n    acl_print_debug_msg(\"format_string=%s\\n\", format_string);\n#endif\n\n    // the address of the data in the buffer of the printf\n    local_offset = global_offset + 4;\n\n    // process the output of a single printf inside this loop\n    // loop as long as we dont reach the end of the format string\n\n    for (auto ptr = format_string.cbegin(); ptr != format_string.cend();) {\n\n      int vector_size;\n      int i;\n\n      // a single data conversion (e.g. %d, %f, etc.)\n      Printable_Data_Elem data_elem;\n      Length_Specifier length;\n\n      // this is format string without conversion, i.e. just a string to print\n      // out\n      std::string non_conversion_string;\n\n      // starting from the current location in the format string (i.e. ptr)\n      // read from format string, extract string to be printed out as well as\n      // the type of the data to be converted.\n      // (e.g. for \"globalid=%d\" non_conversion_string is \"globalid=\" and\n      // data_elem is integer)\n      ptr = get_data_elem_at_offset(ptr, format_string.cend(), data_elem,\n                                    non_conversion_string);\n\n      //\n      // Print non_conversion_string first\n      //\n\n      // handle special characters\n      // before printing\n      decode_string(non_conversion_string);\n\n#ifdef DEBUG\n      printf(\"non conversion string=%s\\n\", non_conversion_string.c_str());\n#else\n      printf(\"%s\", non_conversion_string.c_str());\n#endif\n#ifdef DEBUG\n      printf(\"\\n\");\n#endif\n\n      // no data to be printed, end of printing for this printf\n      if (ptr == format_string.cend() && data_elem._conversion_string.empty())\n        break;\n\n      // Handle vector types by replicating the conversion string for each\n      // vector element\n      vector_size =\n          modify_conversion_string_for_vectors(data_elem._conversion_string);\n\n      if (vector_size == -1) {\n        acl_print_debug_msg(\"wrong vector specifier in printf call, ignoring \"\n                            \"remaining printfs...\\n\");\n        return dumped_buffer_size;\n      }\n\n      // get the length specifier\n      length = get_length_specifier(data_elem._conversion_string);\n\n#ifdef DEBUG\n      printf(\"length specifier=%s\\n\", length == LS_HL   ? \"LS_HL\"\n                                      : length == LS_HH ? \"LS_HH\"\n                                      : length == LS_H  ? \"LS_H\"\n                                      : length == LS_L  ? \"LS_L\"\n                                                        : \"NONE\");\n#endif\n\n      // the size of the data we are converting right now\n      size_of_data =\n          get_llvm_data_size(data_elem._type, length, (vector_size > 1));\n\n      if (size_of_data == 0) {\n        acl_print_debug_msg(\"wrong length modifier in printf call, ignoring \"\n                            \"remaining printfs...\\n\");\n        return dumped_buffer_size;\n      }\n\n      for (i = 0; i < vector_size; i++) {\n        //\n        // read size_of_data bytes from buffer at an aligned address\n        // WARNING: this should match the aligned address calculation in\n        // TransformPrintf pass.\n        // WARNING: vectors are scalarized before TransformPrintf pass,\n        // hence, data is NOT aligned at vector size, but at vector element size\n        // e.g. if we are printing float16, data would be aligned at\n        // sizeof(float) not sizeof(float16).\n        {\n          // this is the offset we need to add to read the data at an aligned\n          // offset\n          unsigned unalignment_size = (local_offset % size_of_data);\n          if (unalignment_size != 0) {\n            unsigned unalignment_offset = size_of_data - unalignment_size;\n            local_offset += unalignment_offset;\n          }\n        }\n\n        //\n        // read and print size_of_data at address local_offset\n        //\n        print_data_elem(data_elem._type, buffer, local_offset,\n                        data_elem._conversion_string, length, printf_infos,\n                        (vector_size > 1));\n\n        // printf comma (i.e. ',') between vector elements\n        if (i != (vector_size - 1))\n          printf(\",\");\n\n        local_offset += size_of_data;\n      }\n    }\n\n    // how much data did we process for this printf?\n    single_printf_offset = local_offset - global_offset;\n\n    // make it power-of-2, at least SMALLEST_GLOBAL_MEMORY_BANDWIDTH\n    // WARNING: this should match the sizes printf data chunks in\n    // TransformPrintf pass\n    if (single_printf_offset < SMALLEST_GLOBAL_MEMORY_BANDWIDTH)\n      single_printf_offset = SMALLEST_GLOBAL_MEMORY_BANDWIDTH;\n    else {\n      unsigned power_of_2_offset = SMALLEST_GLOBAL_MEMORY_BANDWIDTH;\n      while (single_printf_offset > power_of_2_offset)\n        power_of_2_offset *= 2;\n      single_printf_offset = power_of_2_offset;\n    }\n  }\n\n#ifdef DEBUG\n  printf(\"exiting acl_dump_buffer...\\n\");\n#endif\n  return dumped_buffer_size;\n}\n\n//\n// Schedule enqueue read buffer to read printf buffer\n// The activation ID is the device op ID.\nvoid acl_schedule_printf_buffer_pickup(int activation_id, int size,\n                                       int debug_dump_printf) {\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n\n  // This function can potentially be called by a HAL that does not use the\n  // ACL global lock, so we need to use acl_lock() instead of\n  // acl_assert_locked(). However, the MMD HAL calls this function from a unix\n  // signal handler, which can't lock mutexes, so we don't lock in that case.\n  // All functions called from this one therefore have to use\n  // acl_assert_locked_or_sig() instead of just acl_assert_locked().\n  std::unique_lock lock{acl_mutex_wrapper, std::defer_lock};\n  if (!acl_is_inside_sig()) {\n    lock.lock();\n  }\n\n#ifdef DEBUG\n  printf(\"printf pickup %d %d\\n\", activation_id, size);\n  fflush(stdout);\n#endif\n  if (activation_id >= 0 && activation_id < doq->max_ops) {\n    // This address is stable, given a fixed activation_id.\n    // So we don't run into race conditions.\n    acl_device_op_t *op = doq->op + activation_id;\n    op->info.num_printf_bytes_pending = (cl_uint)size;\n\n    // Propagate the operation info\n    op->info.debug_dump_printf = debug_dump_printf ? 1 : 0;\n  }\n  // Signal all waiters.\n  acl_signal_device_update();\n}\n\nvoid acl_process_printf_buffer(void *user_data, acl_device_op_t *op) {\n  user_data = user_data; // Only used by the test mock\n\n  if (op) {\n    cl_event event = op->info.event;\n    cl_kernel kernel = event->cmd.info.ndrange_kernel.kernel;\n\n    // Grab the printf data and emit it.\n    cl_uint num_bytes = op->info.num_printf_bytes_pending;\n    size_t dumped_buffer_size = l_dump_printf_buffer(event, kernel, num_bytes);\n\n    if (op->info.debug_dump_printf) {\n      // Update the already processed buffer size\n      kernel->processed_printf_buffer_size += dumped_buffer_size;\n    } else {\n      // Full dump, reset this variable\n      kernel->processed_printf_buffer_size = 0;\n    }\n\n    // Mark this printf work as done.  Must do this *before* unstalling\n    // the kernel, to avoid a race against the kernel filling up the\n    // buffer again.\n    op->info.num_printf_bytes_pending = 0;\n\n    // Ensure kernel IRQ doesn't race with us to update the\n    // num_printf_bytes_pending.\n    acl_memory_barrier();\n\n    // Allow the kernel to continue running.\n    // We don't need to unstall the kernel during the early flushing during\n    // debug.\n    if (!op->info.debug_dump_printf) {\n      acl_get_hal()->unstall_kernel(\n          event->cmd.info.ndrange_kernel.device->def.physical_device_id,\n          op->id);\n    }\n  }\n}\n\nstatic void decode_string(std::string &print_data) {\n  auto src = print_data;\n  auto len = src.length();\n\n  auto src_i = 0U;\n  std::stringstream dst;\n  while (src_i < len) {\n    // check for special chars\n    if ((src_i + 1) < len && src[src_i] == '\\\\' && src[src_i + 1] == '\\\\') {\n      // unwrap slash char\n      dst << '\\\\';\n      src_i += 2;\n    } else if ((src_i + 2) < len && src[src_i] == '\\\\' &&\n               src[src_i + 1] == '2' && src[src_i + 2] == '0') {\n      // replace with space\n      dst << ' ';\n      src_i += 3;\n    } else if ((src_i + 2) < len && src[src_i] == '\\\\' &&\n               src[src_i + 1] == '0' && src[src_i + 2] == '9') {\n      // replace with horizontal tab\n      dst << '\\t';\n      src_i += 3;\n    } else if ((src_i + 2) < len && src[src_i] == '\\\\' &&\n               src[src_i + 1] == '0' && src[src_i + 2] == 'b') {\n      // replace with vertical tab\n      dst << '\\v';\n      src_i += 3;\n    } else if ((src_i + 2) < len && src[src_i] == '\\\\' &&\n               src[src_i + 1] == '0' && src[src_i + 2] == 'd') {\n      // replace with carriage return\n      dst << '\\r';\n      src_i += 3;\n    } else if ((src_i + 2) < len && src[src_i] == '\\\\' &&\n               src[src_i + 1] == '0' && src[src_i + 2] == 'a') {\n      // replace new line char\n      dst << '\\n';\n      src_i += 3;\n    } else if ((src_i + 2) < len && src[src_i] == '\\\\' &&\n               src[src_i + 1] == '0' && src[src_i + 2] == 'c') {\n      // replace with feed\n      dst << '\\f';\n      src_i += 3;\n    } else if ((src_i + 2) < len && src[src_i] == '\\\\' &&\n               src[src_i + 1] == '0' && src[src_i + 2] == '0') {\n      // replace with null char\n      dst << '\\0';\n      src_i += 3;\n    } else {\n      // if this is a normal char\n      dst << src[src_i++];\n    }\n  }\n\n  print_data = dst.str();\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_device_binary.cpp",
        "data": "// Copyright (C) 2019-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n// To determine endianness of Linux host system\n#ifdef __linux__\n#include <endian.h>\n#endif\n#include <fstream>\n#include <sstream>\n\n// External library headers.\n#include <pkg_editor/pkg_editor.h>\n\n// Internal headers.\n#include <acl_auto_configure.h>\n#include <acl_device_binary.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_kernel.h>\n#include <acl_support.h>\n#include <acl_util.h>\n\n// Use endian.h to determine system endianness under Linux.  Assume that\n// other platforms (Windows) are always little endian.\nstatic unsigned l_is_host_big_endian(void) {\n#ifdef _WIN32 // Not Linux - we don't support big endian hosts on other\n              // platforms\n  return 0;\n#else // We only have endian.h under Linux\n#if __BYTE_ORDER == __LITTLE_ENDIAN\n  return 0;\n#elif __BYTE_ORDER == __BIG_ENDIAN\n  return 1;\n#endif\n#endif\n}\n\n// Compares global memory definition of two devices.\n// Returns CL_SUCCESS if they are exactly the same.\n// Returns CL_INVALID_BINARY if any parameter are different.\nstatic cl_int l_compare_device_global_mem_defs(\n    cl_context context, const acl_device_def_autodiscovery_t *orig_device_def,\n    const acl_device_def_autodiscovery_t *new_device_def) {\n  cl_int status = CL_SUCCESS;\n  // The hal printf will only result in output if ACL_DEBUG is set to > 0\n#define AND_CHECK_GLOBAL_MEM_DEF(CONDITION, ERRCODE, FAILMSG)                  \\\n  if (status == CL_SUCCESS && !(CONDITION)) {                                  \\\n    status = ERRCODE;                                                          \\\n    acl_print_debug_msg(FAILMSG);                                              \\\n    acl_context_callback(context, FAILMSG);                                    \\\n  }\n  const char *incompatible_msg = \"Device memory layout changes across binaries\";\n  AND_CHECK_GLOBAL_MEM_DEF(orig_device_def->num_global_mem_systems ==\n                               new_device_def->num_global_mem_systems,\n                           CL_INVALID_BINARY, incompatible_msg);\n\n  // Loop over all memory systems and check compatibility\n  for (unsigned i = 0; i < orig_device_def->num_global_mem_systems; i++) {\n    incompatible_msg = \"Device memory bank count changes across binaries\";\n    AND_CHECK_GLOBAL_MEM_DEF(\n        orig_device_def->global_mem_defs[i].num_global_banks ==\n            new_device_def->global_mem_defs[i].num_global_banks,\n        CL_INVALID_BINARY, incompatible_msg);\n    incompatible_msg = \"Device memory type changes across binaries\";\n    AND_CHECK_GLOBAL_MEM_DEF(orig_device_def->global_mem_defs[i].type ==\n                                 new_device_def->global_mem_defs[i].type,\n                             CL_INVALID_BINARY, incompatible_msg);\n    incompatible_msg = \"Device memory range changes across binaries\";\n    AND_CHECK_GLOBAL_MEM_DEF(orig_device_def->global_mem_defs[i].range.begin ==\n                                 new_device_def->global_mem_defs[i].range.begin,\n                             CL_INVALID_BINARY, incompatible_msg);\n    AND_CHECK_GLOBAL_MEM_DEF(orig_device_def->global_mem_defs[i].range.next ==\n                                 new_device_def->global_mem_defs[i].range.next,\n                             CL_INVALID_BINARY, incompatible_msg);\n\n    if (new_device_def->global_mem_defs[i].num_global_banks > 1) {\n      incompatible_msg = \"Device memory config address changes across binaries\";\n      AND_CHECK_GLOBAL_MEM_DEF(\n          orig_device_def->global_mem_defs[i].config_addr ==\n              new_device_def->global_mem_defs[i].config_addr,\n          CL_INVALID_BINARY, incompatible_msg);\n    }\n\n    if (!orig_device_def->global_mem_defs[i].name.empty() &&\n        !new_device_def->global_mem_defs[i].name.empty()) {\n      incompatible_msg = \"Device memory name changes across binaries\";\n      AND_CHECK_GLOBAL_MEM_DEF(orig_device_def->global_mem_defs[i].name ==\n                                   new_device_def->global_mem_defs[i].name,\n                               CL_INVALID_BINARY, incompatible_msg);\n    } else {\n      incompatible_msg =\n          \"Device memory is heterogeneous but doesn't have a name\";\n      // Name can be not set if there's only 1 memory\n      AND_CHECK_GLOBAL_MEM_DEF(new_device_def->num_global_mem_systems == 1,\n                               CL_INVALID_BINARY, incompatible_msg);\n    }\n  }\n\n#undef AND_CHECK_GLOBAL_MEM_DEF\n\n  return status;\n}\n\nvoid acl_device_binary_t::load_content(const std::string &filename) {\n  unload_content();\n\n  m_filename = acl_realpath_existing(filename);\n\n  reload_content();\n  if (m_binary_pkg) {\n    m_binary_type = CL_PROGRAM_BINARY_TYPE_EXECUTABLE;\n  } else {\n    m_binary_type = CL_PROGRAM_BINARY_TYPE_NONE;\n  }\n}\n\nvoid acl_device_binary_t::load_content(const unsigned char *binary_content,\n                                       const std::size_t binary_len) {\n  unload_content();\n\n  m_binary_storage = acl_shared_aligned_ptr(binary_len);\n  safe_memcpy(get_content(), binary_content, binary_len,\n              m_binary_storage.get_requested_size(), binary_len);\n\n  m_binary_type = CL_PROGRAM_BINARY_TYPE_EXECUTABLE;\n  m_binary_pkg = acl_pkg_open_file_from_memory(\n      reinterpret_cast<char *>(get_content()), binary_len, 0);\n  if (m_binary_pkg) {\n    m_binary_type = CL_PROGRAM_BINARY_TYPE_EXECUTABLE;\n  } else {\n    m_binary_type = CL_PROGRAM_BINARY_TYPE_NONE;\n  }\n}\n\nvoid acl_device_binary_t::unload_content() const {\n  if (m_binary_pkg) {\n    acl_pkg_close_file(m_binary_pkg);\n    m_binary_pkg = nullptr;\n  }\n  m_binary_storage = acl_shared_aligned_ptr();\n}\n\nvoid acl_device_binary_t::reload_content() const {\n  if (get_content() || m_filename == \"\") {\n    // Binary is loaded in memory so no need to reload it.\n    return;\n  }\n\n  std::ifstream binfile{m_filename, std::ios::binary | std::ios::ate};\n  auto size = binfile.tellg();\n  binfile.seekg(0);\n  m_binary_storage = acl_shared_aligned_ptr(static_cast<size_t>(size));\n  binfile.read(reinterpret_cast<char *>(m_binary_storage.get_aligned_ptr()),\n               static_cast<std::streamsize>(size));\n\n  m_binary_pkg = acl_pkg_open_file_from_memory(\n      reinterpret_cast<char *>(m_binary_storage.get_aligned_ptr()),\n      m_binary_storage.get_requested_size(), 0);\n}\n\n#ifdef _MSC_VER\n#pragma warning(push)\n// assignment in conditional expression.\n#pragma warning(disable : 4706)\n#endif\n\n// Load binary_pkg and validate it.\n// It must meet certain minimum checks: have the right sections,\n// and the content of some of them must match the device program hash, etc.\ncl_int acl_device_binary_t::load_binary_pkg(int validate_compile_options,\n                                            int validate_memory_layout) {\n  cl_int is_simulator;\n  auto context = get_dev_prog()->program->context;\n#define FAILREAD_MSG \"Could not read parts of the program binary.\"\n  size_t data_len = 0;\n\n  acl_assert_locked();\n\n  if (acl_platform.offline_mode == ACL_CONTEXT_MPSIM &&\n      !validate_compile_options &&\n      context->compiler_mode != CL_CONTEXT_COMPILER_MODE_OFFLINE_INTELFPGA &&\n      get_binary_len() < 1024) {\n    // IF the binary is ridiculously small (arbitrary number) we are going\n    // to assume it is just a name string so we are going to fake preloaded\n    // for emulator, by picking the first aocx we can find.\n    // IF you set mode=3 and have more than one aocx file this might not\n    // give you what you expect.\n\n    auto result = acl_glob(\"*.aocx\");\n    if (result.empty()) {\n      result = acl_glob(\"../*.aocx\");\n      if (result.empty()) {\n        ERR_RET(CL_INVALID_BINARY, context,\n                \"Can't find any aocx file in . or ..\\n\");\n      }\n    }\n\n    load_content(result[0]);\n    if (!get_content()) {\n      ERR_RET(CL_INVALID_BINARY, context, \"Can't read aocx file\\n\");\n    }\n  }\n\n  // Open the package from the memory image.\n  // Use 0 for show_mode, i.e. no user messages for anything.\n  const auto pkg = get_binary_pkg();\n  if (!pkg)\n    ERR_RET(CL_INVALID_BINARY, context, \"Binary file is malformed\");\n\n  auto status = CL_SUCCESS;\n\n  // The hal printf will only result in output if ACL_DEBUG is set to > 0\n#define AND_CHECK(CONDITION, ERRCODE, FAILMSG)                                 \\\n  if (status == CL_SUCCESS && !(CONDITION)) {                                  \\\n    status = ERRCODE;                                                          \\\n    acl_print_debug_msg(FAILMSG);                                              \\\n    acl_context_callback(context, FAILMSG);                                    \\\n  }\n\n  // When extracting text sections, always allocate one more byte for the\n  // terminating NUL.\n\n  // Check board.\n  // Must always be present, and match dev_prog->device\n  AND_CHECK(acl_pkg_section_exists(pkg, \".acl.board\", &data_len),\n            CL_INVALID_BINARY,\n            \"Malformed program binary: missing .acl.board section\");\n  std::vector<char> pkg_board(data_len + 1);\n  AND_CHECK(\n      acl_pkg_read_section(pkg, \".acl.board\", pkg_board.data(), data_len + 1),\n      CL_INVALID_BINARY, FAILREAD_MSG \" (board)\");\n  std::string pkg_board_str{pkg_board.data()};\n\n  // Check board name matches - first create error message, then do check\n  std::stringstream errmsg;\n  auto *dev_prog = get_dev_prog();\n  errmsg\n      << \"The current binary is not compatible with the one presently loaded \"\n         \"on the FPGA.\\n\"\n      << \"This binary targets the BSP variant \" << pkg_board_str\n      << \", while the FPGA has the BSP\\n\"\n      << \"variant \" << dev_prog->device->def.autodiscovery_def.name << \".\\n\"\n      << \"Use the command 'aocl initialize' to load the matching BSP\\n\"\n      << \"variant prior to invoking the host executable.\";\n  if (!acl_getenv(\"ACL_PCIE_PROGRAM_SKIP_BOARDNAME_CHECK\")) {\n    AND_CHECK(pkg_board_str == dev_prog->device->def.autodiscovery_def.name,\n              CL_INVALID_BINARY,\n              errmsg.str().c_str()); // perform the board name check\n  }\n\n  // Check random hash.\n  // Must always be present, and if matches - skip first reprogram\n  // Note that this step must be done before new autodiscovery is loaded in the\n  // runtime.\n  if (acl_pkg_section_exists(pkg, \".acl.rand_hash\", &data_len) &&\n      dev_prog->device->loaded_bin == nullptr &&\n      acl_platform.offline_mode != ACL_CONTEXT_MPSIM) {\n    std::vector<char> pkg_rand_hash(data_len + 1);\n    AND_CHECK(acl_pkg_read_section(pkg, \".acl.rand_hash\", pkg_rand_hash.data(),\n                                   data_len + 1),\n              CL_INVALID_BINARY, FAILREAD_MSG \" (rand_hash)\");\n    // Note that we use dev_prog->device when checking for device global\n    // Having the same binary suggest that the aocx on the device currently is\n    // the same as the aocx used to create program, so we can peek the device\n    // global setup now instead of later after acl_load_device_def_from_str\n    if (dev_prog->device->def.autodiscovery_def.binary_rand_hash ==\n            std::string(pkg_rand_hash.data()) &&\n        (!acl_device_has_reprogram_device_globals(dev_prog->device))) {\n      dev_prog->device->last_bin = this;\n      dev_prog->device->loaded_bin = this;\n    }\n  }\n\n  // Check autodiscovery.\n  AND_CHECK(acl_pkg_section_exists(pkg, \".acl.autodiscovery\", &data_len),\n            CL_INVALID_BINARY,\n            \"Malformed program binary: missing .acl.autodiscovery section\");\n  std::vector<char> pkg_autodiscovery(data_len + 1);\n  AND_CHECK(acl_pkg_read_section(pkg, \".acl.autodiscovery\",\n                                 pkg_autodiscovery.data(), data_len + 1),\n            CL_INVALID_BINARY, FAILREAD_MSG \" (ad)\");\n  std::string pkg_autodiscovery_str{pkg_autodiscovery.data()};\n  // A trivial check on autodiscovery.\n  // Really, it should parse too, but we don't check that here.\n  AND_CHECK(pkg_autodiscovery_str.length() > 4, CL_INVALID_BINARY,\n            \"Invalid .acl.autodiscovery section in program binary\");\n  // Load the system configuration, so we get the kernel interfaces.\n  if (status == CL_SUCCESS) {\n    std::string err;\n    auto ok = acl_load_device_def_from_str(pkg_autodiscovery_str,\n                                           get_devdef().autodiscovery_def, err);\n    if (!ok) {\n      acl_context_callback(\n          context, \"Malformed program interface definition found in binary: \");\n      acl_context_callback(context, err.c_str());\n      status = CL_INVALID_BINARY;\n    }\n\n    // Check that memory layout does not change across reprograms.\n    if (status == CL_SUCCESS && ok) {\n      // For simulator flow, we treat as if the device has already been\n      // programmed and check device global memory layout against\n      // dev_prog->device->last_bin\n      if (acl_platform.offline_mode == ACL_CONTEXT_MPSIM) {\n        if (validate_memory_layout && dev_prog->device->last_bin) {\n          AND_CHECK(get_devdef().autodiscovery_def.num_global_mem_systems <=\n                            1 ||\n                        (context->eagerly_program_device_with_first_binary &&\n                         context->uses_dynamic_sysdef),\n                    CL_INVALID_BINARY,\n                    \"Binary's memory defintions could not be verified \"\n                    \"against device\");\n          status = l_compare_device_global_mem_defs(\n              context,\n              &(dev_prog->device->last_bin->get_devdef().autodiscovery_def),\n              &(get_devdef().autodiscovery_def));\n        }\n      } else {\n        if (get_devdef().autodiscovery_def.num_global_mem_systems == 0) {\n          // Trying to create program with pre-19.3 aocx, which is no longer\n          // supported The runtime should not get to this point\n          acl_context_callback(context, \"Program version is incompatible with \"\n                                        \"this version of the runtime\");\n          status = CL_INVALID_BINARY;\n        } else if (dev_prog->device->def.autodiscovery_def\n                       .num_global_mem_systems == 0) {\n          // The target device is programmed with pre-19.3 aocx, the runtime\n          // should also not get to this point\n          acl_context_callback(context, \"The binary currently loaded on the \"\n                                        \"device has an unsupported version!\");\n          status = CL_INVALID_BINARY;\n        } else {\n          // Check that this binary is compatible with the heterogeneous\n          // memories that are already on the device.\n          // ** IMPORTANT **\n          // 1. For now, we will only support the case where swapping between\n          // programs preserves the logical memory layout. If this\n          //    is NOT the case, then we may have to remap and reallocate\n          //    buffers everytime we report. While this is possible, it is\n          //    complex and does it offer that much value?\n          // 2. ACL is still single threaded. This means that if last_bin is NOT\n          // set, then this current dev_bin will be loaded onto the device.\n          if (validate_memory_layout) {\n            status = l_compare_device_global_mem_defs(\n                context, &(dev_prog->device->def.autodiscovery_def),\n                &(get_devdef().autodiscovery_def));\n          }\n        }\n      }\n    }\n  }\n\n  is_simulator = 0;\n  if (status == CL_SUCCESS &&\n      acl_pkg_section_exists(pkg, \".acl.simulator_object\", &data_len)) {\n    if (acl_platform.offline_mode != ACL_CONTEXT_MPSIM) {\n      acl_context_callback(\n          context,\n          \"aocx contains simulated kernel, but simulation mode not set!\");\n      status = CL_INVALID_BINARY;\n    } else {\n      is_simulator = 1;\n      // Don't validate the compiler options for simulator.\n      // This is equivalent to having a pre-loaded aocx.\n      validate_compile_options = 0;\n    }\n  } else if (status == CL_SUCCESS &&\n             acl_pkg_section_exists(pkg, \".acl.emulator_object.linux\",\n                                    &data_len)) {\n    acl_context_callback(\n        context,\n        \"aocx contains unsupported legacy opencl emulated kernel for linux!\");\n    status = CL_INVALID_BINARY;\n  } else if (status == CL_SUCCESS &&\n             acl_pkg_section_exists(pkg, \".acl.emulator_object.windows\",\n                                    &data_len)) {\n    acl_context_callback(\n        context,\n        \"aocx contains unsupported legacy opencl emulated kernel for windows!\");\n  }\n  if (status == CL_SUCCESS && acl_platform.offline_mode == ACL_CONTEXT_MPSIM &&\n      !is_simulator) {\n    acl_context_callback(context,\n                         \"Simulation mode set but aocx is for hardware!\");\n    status = CL_INVALID_BINARY;\n  }\n\n  if (validate_compile_options) {\n    // Check autodiscovery.\n    AND_CHECK(acl_pkg_section_exists(pkg, \".acl.compileoptions\", &data_len),\n              CL_INVALID_BINARY,\n              \"Malformed program binary: missing .acl.compileoptions section\");\n    std::vector<char> pkg_compileoptions(data_len + 1);\n    AND_CHECK(acl_pkg_read_section(pkg, \".acl.compileoptions\",\n                                   pkg_compileoptions.data(), data_len + 1),\n              CL_INVALID_BINARY, FAILREAD_MSG \" (compile options)\");\n    AND_CHECK(std::string(pkg_compileoptions.data()) == dev_prog->build_options,\n              CL_INVALID_BINARY,\n              \"Program was built with different compile options\");\n\n    // Check hash of source+options. Don't do so for binaries loaded by the\n    // simulator.\n    if (!dev_prog->hash.empty()) {\n      // The binary itself doesn't contain the source, because we don't by\n      // default keep customer source IP in the binary.\n      AND_CHECK(acl_pkg_section_exists(pkg, ACL_PKG_SECTION_HASH, &data_len),\n                CL_INVALID_BINARY,\n                \"Malformed program binary: missing \" ACL_PKG_SECTION_HASH\n                \" section\");\n      std::vector<char> pkg_hash(data_len + 1);\n      AND_CHECK(acl_pkg_read_section(pkg, ACL_PKG_SECTION_HASH, pkg_hash.data(),\n                                     data_len + 1),\n                CL_INVALID_BINARY, FAILREAD_MSG \" (hash)\");\n      AND_CHECK(std::string(pkg_hash.data()) == dev_prog->hash,\n                CL_INVALID_BINARY,\n                \"Program was built with different source contents (checked via \"\n                \"hashing)\");\n    }\n  }\n\n  // Validate that kernel endianness matches host system\n  if (status == CL_SUCCESS) {\n    if (l_is_host_big_endian() !=\n        get_devdef().autodiscovery_def.is_big_endian) {\n      acl_context_callback(\n          context, \"Endianness mismatch between host system and device binary\");\n      if (l_is_host_big_endian()) {\n        acl_context_callback(context, \"Host is big endian\");\n      } else {\n        acl_context_callback(context, \"Host is little endian\");\n      }\n      if (get_devdef().autodiscovery_def.is_big_endian) {\n        acl_context_callback(context, \"Device is big endian\");\n      } else {\n        acl_context_callback(context, \"Device is little endian\");\n      }\n\n      status = CL_INVALID_BINARY; // Fail on the endianness mismatch\n    }\n  }\n\n#undef AND_CHECK\n\n  return status;\n#undef FAILREAD_MSG\n}\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n"
    },
    {
        "label": "acl_pll.cpp",
        "data": "// Copyright (C) 2013-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <assert.h>\n#include <sstream>\n\n// Internal headers.\n#include <acl_pll.h>\n\n#define PLL_VERSION_ID (0xa0c00001)\n#define PLL_VERSION_ID_20NM (0xa0c00002)\n\n#define ACL_PLL_DEBUG_MSG_VERBOSE(p, verbosity, m, ...)                        \\\n  if (p->io.printf && (p->io.debug_verbosity) >= verbosity)                    \\\n    do {                                                                       \\\n      p->io.printf((m), ##__VA_ARGS__);                                        \\\n  } while (0)\n\n// PLL reconfig Register Address map (SV, CV)\nconst unsigned int PLL_MODE_REG = 0 * 4;\nconst unsigned int PLL_STATUS_REG = 1 * 4;\nconst unsigned int PLL_START_REG = 2 * 4;\nconst unsigned int PLL_N_REG = 3 * 4;\nconst unsigned int PLL_M_REG = 4 * 4;\nconst unsigned int PLL_C_REG = 5 * 4;\nconst unsigned int PLL_PHASE_REG = 6 * 4;\nconst unsigned int PLL_K_REG = 7 * 4;\nconst unsigned int PLL_R_REG = 8 * 4;\nconst unsigned int PLL_CP_REG = 9 * 4;\nconst unsigned int PLL_DIV_REG = 28 * 4;\nconst unsigned int PLL_C0_READ_REG = 10 * 4;\nconst unsigned int PLL_C1_READ_REG = 11 * 4;\n\n// PLL reconfig Register Address map (A10)\nconst unsigned int PLL_M_REG_20NM = 144 * 4;\nconst unsigned int PLL_N_REG_20NM = 160 * 4;\nconst unsigned int PLL_C0_REG_20NM = 192 * 4;\nconst unsigned int PLL_C1_REG_20NM = 193 * 4;\nconst unsigned int PLL_LF_REG_20NM = 64 * 4;\nconst unsigned int PLL_CP_REG_20NM = 32 * 4;\n\n/*************************** Function headers ***********************/\n\nstatic void acl_pll_read_from_elf(acl_pll *pll,\n                                  const std::string &pkg_pll_config,\n                                  pll_setting_t *P);\nstatic int get_version_id(acl_pll *pll);\nstatic int _acl_pll_read(acl_pll *pll, dev_addr_t addr, unsigned int *val);\nstatic int _acl_pll_write(acl_pll *pll, unsigned int addr, unsigned int val);\nint read_pll_settings(acl_pll *pll, unsigned int i, pll_setting_t *P,\n                      const std::string &pkg_pll_config);\nstatic float _acl_pll_get_kernel_freq(acl_pll *pll, unsigned clocksel);\nstatic int wait_on_lock(acl_pll *pll);\n\n/*************************** Utility Functions ***********************/\n\nvoid acl_pll_read_from_elf(acl_pll *pll, const std::string &pkg_pll_config,\n                           pll_setting_t *P) {\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1,\n                            \"PLL: Parsing pll_config.bin ELF string: %s\\n\",\n                            pkg_pll_config.c_str());\n\n  // Tokenize pkg_pll_config using spaces as delimiters.\n  std::vector<std::string> tokens;\n  size_t start = 0;\n  size_t end = 0;\n  while (end != std::string::npos) {\n    end = pkg_pll_config.find(' ', start);\n    tokens.push_back(pkg_pll_config.substr(\n        start, end == std::string::npos ? std::string::npos : end - start));\n    if (end != std::string::npos) {\n      start = end + 1;\n    }\n  }\n\n  assert(tokens.size() == 7);\n\n  auto *pll_settings = P;\n  pll_settings->freq_khz = (unsigned)atoi(tokens[0].c_str());\n  pll_settings->m = (unsigned)atoi(tokens[1].c_str());\n  pll_settings->n = (unsigned)atoi(tokens[2].c_str());\n  pll_settings->c0 = (unsigned)atoi(tokens[3].c_str());\n  pll_settings->c1 = (unsigned)atoi(tokens[4].c_str());\n  pll_settings->k = (unsigned)atoi(tokens[5].c_str());\n  pll_settings->cp = (unsigned)atoi(tokens[6].c_str());\n\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1,\n                            \"PLL: Done parsing pll_config.bin ELF string\\n\");\n}\n\n// Returns 0 on success, -1 on failure\nstatic int get_version_id(acl_pll *pll) {\n  unsigned int version = 0;\n  int r;\n\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"PLL: Reading PLL version ID\\n\");\n  r = _acl_pll_read(pll, OFFSET_PLL_VERSION_ID, &version);\n  if (r != 0 || (version != PLL_VERSION_ID && version != PLL_VERSION_ID_20NM)) {\n    pll->io.printf(\n        \"  PLL: Version mismatch! Expected 0x%x or 0x%x but read 0x%x\\n\",\n        PLL_VERSION_ID, PLL_VERSION_ID_20NM, version);\n    return -1;\n  } else {\n    pll->version_id = version;\n    return 0;\n  }\n}\n\nint acl_pll_is_valid(acl_pll *pll) {\n  if (pll->version_id == PLL_VERSION_ID) {\n    if (pll->num_known_settings > 0 && pll->known_settings != NULL &&\n        sizeof(*pll->known_settings) ==\n            (pll->num_known_settings * sizeof(pll_setting_t)) &&\n        acl_bsp_io_is_valid(&pll->io))\n      return 1;\n    else\n      return 0;\n  } else if (pll->version_id == PLL_VERSION_ID_20NM) {\n    if (acl_bsp_io_is_valid(&pll->io))\n      return 1;\n    else\n      return 0;\n  }\n  return 0;\n}\n\nint acl_pll_setting_is_valid(pll_setting_t ps) {\n  if ((ps.freq_khz > 0 && ps.freq_khz < MAX_POSSIBLE_FMAX) && (ps.m != 0) &&\n      (ps.cp > 0 && ps.cp < 200) && (ps.c0 == ps.c1 * 2))\n    return 1;\n  else\n    return 0;\n}\n\n// Read from pll with pll validation\n// Returns 0 on success, < 0 on error\nstatic int acl_pll_read(acl_pll *pll, unsigned int addr, unsigned int *val) {\n  if (!acl_pll_is_valid(pll)) {\n    pll->io.printf(\"PLL Error: Invalid pll handle used\");\n    return -1;\n  }\n\n  return _acl_pll_read(pll, addr, val);\n}\n\n// Write to pll without pll validation - needed to validate\n// Returns 0 on success, < 0 on error\nstatic int acl_pll_write(acl_pll *pll, unsigned int addr, unsigned int val) {\n  if (!acl_pll_is_valid(pll)) {\n    pll->io.printf(\"PLL Error: Invalid pll handle used\");\n    return -1;\n  }\n\n  return _acl_pll_write(pll, addr, val);\n}\n\n// Read from pll without pll validation - needed to validate\n// Returns 0 on success, < 0 on error\nstatic int _acl_pll_read(acl_pll *pll, dev_addr_t addr, unsigned int *val) {\n  size_t size = sizeof(unsigned int);\n  size_t r = pll->io.read(&pll->io, addr, (char *)val, size);\n  if (r < size) {\n    pll->io.printf(\"PLL Error: Read failed, %zu bytes read, %zu expected\\n\", r,\n                   size);\n    return -1;\n  }\n  return 0;\n}\n\n// Write to pll without pll validation - needed to validate\n// Returns 0 on success, < 0 on error\nstatic int _acl_pll_write(acl_pll *pll, unsigned int addr, unsigned int val) {\n  size_t size = sizeof(unsigned int);\n  size_t r = pll->io.write(&pll->io, (dev_addr_t)addr, (char *)&val, size);\n  if (r < size) {\n    pll->io.printf(\"PLL Error: Write failed, %zu bytes written, %zu expected\\n\",\n                   r, size);\n    return -1;\n  }\n  return 0;\n}\n\n// Need PLL ROM until we have ELF package.  Returns 0 if success, -ve on error\nint read_pll_settings(acl_pll *pll, unsigned int i, pll_setting_t *P,\n                      const std::string &pkg_pll_config) {\n  int r = 0;\n\n  if (pll->version_id == PLL_VERSION_ID) {\n    dev_addr_t addr = (dev_addr_t)i * sizeof(pll_setting_t);\n    P->freq_khz = 0;\n    P->m = 0;\n    P->n = 0;\n    P->k = 0;\n    P->c0 = 0;\n    P->c1 = 0;\n    P->r = 0;\n    P->cp = 0;\n    P->div = 0;\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->freq_khz);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->m);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->n);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->k);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->c0);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->c1);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->r);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->cp);\n    addr += sizeof(int);\n    r |= _acl_pll_read(pll, OFFSET_ROM + addr, &P->div);\n    addr += sizeof(int);\n    return r;\n  } else if (pll->version_id == PLL_VERSION_ID_20NM) {\n    // read from ELF package\n    acl_pll_read_from_elf(pll, pkg_pll_config, P);\n    return r;\n  } else {\n    return 1;\n  }\n}\n\n// Measures the kernel clock (clocksel==0) or 2x clock (clocksel==1) fmax\nstatic float _acl_pll_get_kernel_freq(acl_pll *pll, unsigned clocksel) {\n  time_ns start_time;\n  time_ns elapsed_time = 0;\n  unsigned int count = 0;\n\n  // Reset counter\n  acl_pll_write(pll, OFFSET_COUNTER, clocksel);\n\n  // Wait for timeout ns to pass\n  start_time = pll->io.get_time_ns();\n  while (elapsed_time < (time_ns)CLK_MEASUREMENT_PERIOD)\n    elapsed_time = (pll->io.get_time_ns() - start_time);\n\n  // Read counter and calculate freq\n  acl_pll_read(pll, OFFSET_COUNTER, &count);\n\n  return (float)count / (float)elapsed_time * 1000.0f;\n}\n\n// Return 1 if lock was achieved, 0 otherwise\nstatic int wait_on_lock(acl_pll *pll) {\n  time_ns reset_time;\n\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL: Waiting on lock signal\\n\");\n\n  // Wait for pll to reconfig itself by polling ready bit\n  reset_time = pll->io.get_time_ns();\n  while (!acl_pll_is_locked(pll)) {\n    if (pll->io.get_time_ns() - reset_time > (time_ns)(RECONFIG_TIMEOUT)) {\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"  PLL: FAILED to lock\\n\");\n      return 0;\n    }\n  }\n  return 1;\n}\n\n/*************************** Public Functions ************************/\n\nfloat acl_pll_get_kernel_freq(acl_pll *pll) {\n  return _acl_pll_get_kernel_freq(pll, 0);\n}\n\nfloat acl_pll_get_kernel2x_freq(acl_pll *pll) {\n  return _acl_pll_get_kernel_freq(pll, 1);\n}\n\nint acl_pll_init(acl_pll *pll, acl_bsp_io bsp_io,\n                 const std::string &pkg_pll_config) {\n  unsigned int i = 0;\n\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"PLL initializing\\n\");\n  pll->num_known_settings = 0;\n  pll->known_settings = NULL;\n  pll->curr_freq_khz = 0;\n\n  if (!acl_bsp_io_is_valid(&bsp_io))\n    return -1;\n  pll->io = bsp_io;\n\n  // 1. Make sure hw version matches expected sw version\n  assert(get_version_id(pll) == 0);\n\n  // Detected 20nm PLL and didn't supply PLL configuration string\n  // Not doing dynamic PLL reconfiguration\n  if (pll->version_id == PLL_VERSION_ID_20NM && pkg_pll_config == \"\") {\n    return 0;\n  };\n\n  if (pll->version_id == PLL_VERSION_ID) {\n    ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"PLL: found 28nm PLL (SV, CV)\\n\");\n    // -------------------------------------------------------------------\n    // 28nm PLL (SV, CV)\n    // -------------------------------------------------------------------\n    // 2. Find out how many known settings are compiled into the SOF\n    do {\n      pll_setting_t ps;\n      ps.freq_khz = 0;\n      if (read_pll_settings(pll, i, &ps, \"\") != 0)\n        return -1;\n      if (ps.freq_khz == 0 && i > 0)\n        break;\n      if (!acl_pll_setting_is_valid(ps)) {\n        pll->io.printf(\"PLL Error: Read invalid pll setting for %f MHz.  Make \"\n                       \"sure read access too acl_kernel_clk is functioning and \"\n                       \"the post-quartus-script succeeded\\n\",\n                       (float)ps.freq_khz / 1000.0);\n        pll->io.printf(\"  PLL: Invalid settings f=%d m=%d n=%d k=%d c0=%d \"\n                       \"c1=%d r=%d cp=%d div=%d\\n\",\n                       ps.freq_khz, ps.m, ps.n, ps.k, ps.c0, ps.c1, ps.r, ps.cp,\n                       ps.div);\n        return -1;\n      }\n\n    } while (i++ <= MAX_KNOWN_SETTINGS);\n\n    if (i > MAX_KNOWN_SETTINGS) {\n      pll->io.printf(\n          \"PLL Error: Found %d known pll settings.  Make sure PLL accesses are \"\n          \"functioning and the post-quartus-script succeeded\\n\",\n          i);\n      return -1;\n    }\n\n    pll->num_known_settings = i;\n\n    // 3. Load all the known settings\n\n    pll->known_settings = (pll_setting_t *)malloc(pll->num_known_settings *\n                                                  sizeof(pll_setting_t));\n    if (pll->known_settings == NULL) {\n      pll->io.printf(\" PLL Error: Ran out of memory\\n\");\n      return -1;\n    }\n    // Read these out of the pll_rom - hardcoded until we package with ELF\n    for (i = 0; i < pll->num_known_settings; i++) {\n      read_pll_settings(pll, i, &pll->known_settings[i], \"\");\n      ACL_PLL_DEBUG_MSG_VERBOSE(\n          pll, 2,\n          \"Read pll values f=%d m=%d n=%d k=%d c0=%d c1=%d r=%d cp=%d div=%d\\n\",\n          pll->known_settings[i].freq_khz, pll->known_settings[i].m,\n          pll->known_settings[i].n, pll->known_settings[i].k,\n          pll->known_settings[i].c0, pll->known_settings[i].c1,\n          pll->known_settings[i].r, pll->known_settings[i].cp,\n          pll->known_settings[i].div);\n    }\n\n    // 4. Configure the default settings\n\n    if (!acl_pll_setting_is_valid(pll->known_settings[0])) {\n      ACL_PLL_DEBUG_MSG_VERBOSE(\n          pll, 0,\n          \"Read pll values f=%d m=%d n=%d k=%d c0=%d c1=%d r=%d cp=%d div=%d\\n\",\n          pll->known_settings[0].freq_khz, pll->known_settings[0].m,\n          pll->known_settings[0].n, pll->known_settings[0].k,\n          pll->known_settings[0].c0, pll->known_settings[0].c1,\n          pll->known_settings[0].r, pll->known_settings[0].cp,\n          pll->known_settings[0].div);\n      pll->io.printf(\" PLL Error: Default pll settings are invalid\\n\");\n      return -1;\n    }\n  } else if (pll->version_id == PLL_VERSION_ID_20NM) {\n    ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"PLL: found 20nm PLL (A10)\\n\");\n    pll->num_known_settings = 1;\n    pll->known_settings = (pll_setting_t *)malloc(pll->num_known_settings *\n                                                  sizeof(pll_setting_t));\n    if (pll->known_settings == NULL) {\n      pll->io.printf(\" PLL Error: Ran out of memory\\n\");\n      return -1;\n    }\n    read_pll_settings(pll, 0, &pll->known_settings[0], pkg_pll_config);\n  };\n\n  // Reset the PLL on startup\n  acl_pll_reset(pll);\n  assert(wait_on_lock(pll));\n\n  ACL_PLL_DEBUG_MSG_VERBOSE(\n      pll, 1, \"PLL: Before reconfig, kernel clock set to %f MHz %f MHz\\n\",\n      acl_pll_get_kernel_freq(pll), acl_pll_get_kernel2x_freq(pll));\n\n  if (pll->known_settings == NULL) {\n    pll->io.printf(\" PLL Error: Ran out of memory\\n\");\n    return -1;\n  }\n\n  if (acl_pll_reconfigure(pll, pll->known_settings[0]) != 0) {\n    pll->io.printf(\" PLL Error: Failed to configure default settings\\n\");\n    return -1;\n  }\n\n  return 0;\n}\n\nvoid acl_pll_close(acl_pll *pll) {\n  if (pll == NULL) {\n    return;\n  }\n  if (pll->known_settings != NULL) {\n    free(pll->known_settings);\n  }\n  pll->curr_freq_khz = 0;\n  pll->num_known_settings = 0;\n}\n\nint acl_pll_is_locked(acl_pll *pll) {\n  unsigned int locked = 0;\n  acl_pll_read(pll, OFFSET_LOCK, &locked);\n  return (locked == 1) ? 1 : 0;\n}\n\nvoid acl_pll_reset(acl_pll *pll) {\n  time_ns reset_time;\n  unsigned int sw_resetn = 0; // Any write will cause reset - data is don't care\n\n  assert(pll);\n\n  acl_pll_write(pll, OFFSET_RESET, sw_resetn);\n\n  // This will stall while circuit is being reset.  Executing this read\n  // therefore ensures this circuit has come out of reset before proceeding.\n  acl_pll_read(pll, OFFSET_RESET, &sw_resetn);\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL: Reset issued\\n\");\n\n  // Just in case, verify that value read back shows reset deasserted\n  reset_time = pll->io.get_time_ns();\n  while (sw_resetn == 0) {\n    if (pll->io.get_time_ns() - reset_time > (time_ns)(RECONFIG_TIMEOUT)) {\n      pll->io.printf(\"PLL failed to come out of reset. Read 0x%x\\n\", sw_resetn);\n      assert(sw_resetn != 0);\n    }\n    acl_pll_read(pll, OFFSET_RESET, &sw_resetn);\n  }\n}\n\n// Reconfigure the PLL with the given settings\n// Returns 0 on success, -ve otherwise\nint acl_pll_reconfigure(acl_pll *pll, pll_setting_t pllsettings) {\n\n// If count is zero, bypass this counter (set bit 16 high)\n#define DIVUP50DUTY(x)                                                         \\\n  ((x <= 1) ? (1 << 16) : (((x % 2) << 17) | ((x / 2 + x % 2) << 8) | (x / 2)))\n\n  static int num_reconfigs = 0;\n  int num_retries = 0;\n  unsigned int r_val;\n  unsigned int cp_val;\n  time_ns reset_time;\n  int status = 0;\n  unsigned int x;\n\n  // only request a locked PLL for 28 nm PLL (SV, CV)\n  if (pll->version_id == PLL_VERSION_ID) {\n    if (!acl_pll_is_locked(pll)) {\n      ACL_PLL_DEBUG_MSG_VERBOSE(\n          pll, 1, \"  PLL: Reconfig requested without lock - resetting\\n\");\n      acl_pll_reset(pll);\n      assert(wait_on_lock(pll));\n    }\n  }\n\n  if (pll->io.debug_verbosity > 2) {\n    if (pll->version_id == PLL_VERSION_ID) {\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_M_REG, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback  M  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_N_REG, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback  N  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_C0_READ_REG, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback C0 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_C1_READ_REG, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback C1 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_R_REG, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback  R  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_CP_REG, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback CP  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_DIV_REG, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback DIV= %08x\\n\", x);\n    } else if (pll->version_id == PLL_VERSION_ID_20NM) {\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_M_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback  M  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_N_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback  N  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_C0_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback C0 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_C1_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback C1 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_CP_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback CP  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_LF_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback LF  = %08x\\n\", x);\n    } else {\n      return -1;\n    }\n  }\n\n  // Retry loop - sometimes the pll doesn't come out of reconfiguration\n  do {\n    if (pll->version_id == PLL_VERSION_ID) {\n      unsigned reconfig_status = 0;\n\n      ACL_PLL_DEBUG_MSG_VERBOSE(\n          pll, 1, \"Kernel pll reconfiguration parameters being set ... \\n\");\n\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \"  M = %08x\\n\",\n                                DIVUP50DUTY(pllsettings.m));\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \"  K = %08x\\n\", pllsettings.k);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \"  N = %08x\\n\",\n                                DIVUP50DUTY(pllsettings.n));\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \" C0 = %08x\\n\",\n                                DIVUP50DUTY(pllsettings.c0));\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \" C1 = %08x\\n\",\n                                DIVUP50DUTY(pllsettings.c1));\n\n      // Set to polling mode\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_MODE_REG, 1);\n\n      // M\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_M_REG,\n                    DIVUP50DUTY(pllsettings.m));\n\n      // K\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_K_REG, pllsettings.k);\n\n      // N\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_N_REG,\n                    DIVUP50DUTY(pllsettings.n));\n\n      // C0\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_C_REG,\n                    (0 << 18) | DIVUP50DUTY(pllsettings.c0));\n\n      // C1\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_C_REG,\n                    (1 << 18) | DIVUP50DUTY(pllsettings.c1));\n\n      // R - From the FD\n      switch (pllsettings.r) {\n      case 18000:\n        r_val = 0;\n        break;\n      case 16000:\n        r_val = 1;\n        break;\n      case 14000:\n        r_val = 2;\n        break;\n      case 12000:\n        r_val = 3;\n        break;\n      case 10000:\n        r_val = 4;\n        break;\n      case 8000:\n        r_val = 5;\n        break;\n      case 6000:\n        r_val = 6;\n        break;\n      case 4000:\n        r_val = 7;\n        break;\n      case 2000:\n        r_val = 8;\n        break;\n      case 1000:\n        r_val = 9;\n        break;\n      case 500:\n        r_val = 10;\n        break;\n      default:\n        pll->io.printf(\"PLL: Read invalid PLL setting - R = %d\\n\",\n                       pllsettings.r);\n        return -1;\n      }\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \"  R = %08x\\n\", r_val);\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_R_REG, r_val);\n\n      // CP\n      switch (pllsettings.cp) {\n      case 5:\n        cp_val = 0;\n        break;\n      case 10:\n        cp_val = 1;\n        break;\n      case 20:\n        cp_val = 2;\n        break;\n      case 30:\n        cp_val = 3;\n        break;\n      case 40:\n        cp_val = 4;\n        break;\n      default:\n        pll->io.printf(\"PLL: Read invalid PLL setting - CP = %d\\n\",\n                       pllsettings.cp);\n        return -1;\n      }\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \" CP = %08x\\n\", cp_val);\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_CP_REG, cp_val);\n\n      // DIV\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \"DIV = %08x\\n\",\n                                pllsettings.div == 2 ? 0 : 1);\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_DIV_REG,\n                    pllsettings.div == 2 ? 0 : 1u);\n\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2,\n                                \"Kernel pll reconfiguration starting ... \\n\");\n\n      // Start dynamic reconfig\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL + PLL_START_REG, 1);\n\n      // Wait for pll to reconfig itself by polling ready bit\n      reset_time = pll->io.get_time_ns();\n      while (status != 1) {\n        if (reconfig_status == 0)\n          acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_STATUS_REG,\n                       &reconfig_status);\n        status = acl_pll_is_locked(pll);\n\n        // Upon timeout, reset pll and try again.\n        if (pll->io.get_time_ns() - reset_time > (time_ns)(RECONFIG_TIMEOUT)) {\n          ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1,\n                                    \"  PLL: FAILED to configure kernel clock \"\n                                    \"after %d successful reconfigs\\n\",\n                                    num_reconfigs);\n          ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"       reconfig %d, lock %d\\n\",\n                                    reconfig_status, status);\n          // Reset just the PLL to try again\n          acl_pll_reset(pll);\n          break;\n        }\n        status &= reconfig_status;\n      }\n\n      if (status == 1)\n        break;\n    } else if (pll->version_id == PLL_VERSION_ID_20NM) {\n      ACL_PLL_DEBUG_MSG_VERBOSE(\n          pll, 1, \"Kernel pll reconfiguration parameters being set ... \\n\");\n\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \"  M = %08x\\n\", pllsettings.m);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \"  N = %08x\\n\", pllsettings.n);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \" C0 = %08x\\n\", pllsettings.c0);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \" C1 = %08x\\n\", pllsettings.c1);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \" CP = %08x\\n\", pllsettings.cp);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 2, \" LF = %08x\\n\", pllsettings.k);\n\n      // M\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_M_REG_20NM,\n                    pllsettings.m);\n\n      // N\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_N_REG_20NM,\n                    pllsettings.n);\n\n      // C0\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_C0_REG_20NM,\n                    pllsettings.c0);\n\n      // C1\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_C1_REG_20NM,\n                    pllsettings.c1);\n\n      // CP\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_CP_REG_20NM,\n                    pllsettings.cp);\n\n      // LF\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_LF_REG_20NM,\n                    pllsettings.k);\n\n      // submit all reconfiguration settings by writing to the start address\n      acl_pll_write(pll, OFFSET_RECONFIG_CTRL_20NM, 1);\n      assert(wait_on_lock(pll));\n\n      break;\n    }\n  } while (num_retries++ < MAX_RECONFIG_RETRIES);\n\n  if (num_retries > MAX_RECONFIG_RETRIES) {\n    pll->io.printf(\"  PLL: FAILED to configure kernel clock after %d reconfigs \"\n                   \"and %d retries\\n\",\n                   num_reconfigs, num_retries - 1);\n    if (pll->version_id == PLL_VERSION_ID) {\n      pll->io.printf(\"  PLL:    Settings attempted f=%d m=%d n=%d k=%d c0=%d \"\n                     \"c1=%d r=%d cp=%d div=%d\\n\",\n                     pllsettings.freq_khz, pllsettings.m, pllsettings.n,\n                     pllsettings.k, pllsettings.c0, pllsettings.c1,\n                     pllsettings.r, pllsettings.cp, pllsettings.div);\n\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_M_REG, &x);\n      pll->io.printf(\" PLL readback M  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_N_REG, &x);\n      pll->io.printf(\" PLL readback N  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_C0_READ_REG, &x);\n      pll->io.printf(\" PLL readback C0 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_C1_READ_REG, &x);\n      pll->io.printf(\" PLL readback C1 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_R_REG, &x);\n      pll->io.printf(\" PLL readback R  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_CP_REG, &x);\n      pll->io.printf(\" PLL readback CP  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL + PLL_DIV_REG, &x);\n      pll->io.printf(\" PLL readback DIV= %08x\\n\", x);\n      return -1;\n    } else if (pll->version_id == PLL_VERSION_ID_20NM) {\n      pll->io.printf(\"  PLL:    Settings attempted f=%d m=%d n=%d c0=%d c1=%d \"\n                     \"lf=%d cp=%d\\n\",\n                     pllsettings.freq_khz, pllsettings.m, pllsettings.n,\n                     pllsettings.c0, pllsettings.c1, pllsettings.k,\n                     pllsettings.cp);\n\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_M_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback  M  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_N_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback  N  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_C0_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback C0 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_C1_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback C1 = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_LF_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback LF  = %08x\\n\", x);\n      acl_pll_read(pll, OFFSET_RECONFIG_CTRL_20NM + PLL_CP_REG_20NM, &x);\n      ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \" PLL Readback CP  = %08x\\n\", x);\n      return -1;\n    } else {\n      return -1;\n    }\n  }\n\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"Kernel pll ready\\n\");\n\n  // The output counters can get out of phase, so always reset\n  acl_pll_reset(pll);\n  assert(wait_on_lock(pll));\n\n  num_reconfigs++;\n\n  ACL_PLL_DEBUG_MSG_VERBOSE(pll, 1, \"Kernel clock set to %f MHz %f MHz\\n\",\n                            acl_pll_get_kernel_freq(pll),\n                            acl_pll_get_kernel2x_freq(pll));\n\n  // Update internal tracking of currently set clock frequency\n  pll->curr_freq_khz = pllsettings.freq_khz;\n  return 0;\n}\n\npll_setting_t acl_pll_get_default_settings(acl_pll *pll) {\n  assert(pll && pll->num_known_settings > 0);\n  return pll->known_settings[0];\n}\n"
    },
    {
        "label": "acl_icd_dispatch.cpp",
        "data": "// Copyright (C) 2014-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <string.h>\n\n// External library headers.\n#include <CL/cl_ext_intelfpga.h>\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl_icd_dispatch.h>\n#include <acl_thread.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n#ifndef MAX_NAME_SIZE\n#define MAX_NAME_SIZE 1204\n#endif\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL\nclGetExtensionFunctionAddressIntelFPGA(const char *func_name) {\n// MSVC doesn't like the conversion from function pointer to data pointer,\n// but that is what the OpenCL function requires so we allow it\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4054)\n#endif\n#define ADDFUNCTIONLOOKUP(name)                                                \\\n  if (strncmp(func_name, #name, MAX_NAME_SIZE) == 0)                           \\\n  return (void *)name\n\n  ADDFUNCTIONLOOKUP(clIcdGetPlatformIDsKHR);\n  ADDFUNCTIONLOOKUP(clTrackLiveObjectsIntelFPGA);\n  ADDFUNCTIONLOOKUP(clReportLiveObjectsIntelFPGA);\n  ADDFUNCTIONLOOKUP(clGetProfileInfoIntelFPGA);\n  ADDFUNCTIONLOOKUP(clGetProfileDataDeviceIntelFPGA);\n  ADDFUNCTIONLOOKUP(clGetBoardExtensionFunctionAddressIntelFPGA);\n  ADDFUNCTIONLOOKUP(clReadPipeIntelFPGA);\n  ADDFUNCTIONLOOKUP(clWritePipeIntelFPGA);\n  ADDFUNCTIONLOOKUP(clMapHostPipeIntelFPGA);\n  ADDFUNCTIONLOOKUP(clUnmapHostPipeIntelFPGA);\n  ADDFUNCTIONLOOKUP(clSetDeviceExceptionCallbackIntelFPGA);\n  ADDFUNCTIONLOOKUP(clCreateProgramWithBinaryAndProgramDeviceIntelFPGA);\n  ADDFUNCTIONLOOKUP(clReconfigurePLLIntelFPGA);\n  ADDFUNCTIONLOOKUP(clResetKernelsIntelFPGA);\n  ADDFUNCTIONLOOKUP(clSetBoardLibraryIntelFPGA);\n  ADDFUNCTIONLOOKUP(clCreateBufferWithPropertiesINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueReadHostPipeINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueWriteHostPipeINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueReadGlobalVariableINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueWriteGlobalVariableINTEL);\n\n// USM APIs are not currently supported on 32bit devices\n#ifndef __arm__\n  // Add USM APIs\n  ADDFUNCTIONLOOKUP(clHostMemAllocINTEL);\n  ADDFUNCTIONLOOKUP(clDeviceMemAllocINTEL);\n  ADDFUNCTIONLOOKUP(clSharedMemAllocINTEL);\n  ADDFUNCTIONLOOKUP(clMemFreeINTEL);\n  ADDFUNCTIONLOOKUP(clMemBlockingFreeINTEL);\n  ADDFUNCTIONLOOKUP(clGetMemAllocInfoINTEL);\n  ADDFUNCTIONLOOKUP(clSetKernelArgMemPointerINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueMemsetINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueMemFillINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueMemcpyINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueMigrateMemINTEL);\n  ADDFUNCTIONLOOKUP(clEnqueueMemAdviseINTEL);\n#endif\n\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n  return NULL;\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clGetBoardExtensionFunctionAddressIntelFPGA(\n    const char *func_name, cl_device_id device) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  {\n    void *ret = acl_get_hal()->get_board_extension_function_address(\n        func_name, device->def.physical_device_id);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL\nclGetExtensionFunctionAddress(const char *func_name) {\n  return clGetExtensionFunctionAddressIntelFPGA(func_name);\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL\nclGetExtensionFunctionAddressForPlatformIntelFPGA(cl_platform_id platform,\n                                                  const char *func_name) {\n  // We currently only have one platform\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_platform_is_valid(platform)) {\n    return NULL;\n  }\n  return clGetExtensionFunctionAddressIntelFPGA(func_name);\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clGetExtensionFunctionAddressForPlatform(\n    cl_platform_id platform, const char *func_name) {\n  return clGetExtensionFunctionAddressForPlatformIntelFPGA(platform, func_name);\n}\n\ncl_icd_dispatch acl_icd_dispatch = {\n    // 0\n    clGetPlatformIDsIntelFPGA, clGetPlatformInfoIntelFPGA,\n    clGetDeviceIDsIntelFPGA, clGetDeviceInfoIntelFPGA, clCreateContextIntelFPGA,\n    // 5\n    clCreateContextFromTypeIntelFPGA, clRetainContextIntelFPGA,\n    clReleaseContextIntelFPGA, clGetContextInfoIntelFPGA,\n    clCreateCommandQueueIntelFPGA,\n    // 10\n    clRetainCommandQueueIntelFPGA, clReleaseCommandQueueIntelFPGA,\n    clGetCommandQueueInfoIntelFPGA,\n#ifdef CL_USE_DEPRECATED_OPENCL_1_0_APIS\n    clSetCommandQueuePropertyIntelFPGA,\n#else\n    NULL,\n#endif\n    clCreateBufferIntelFPGA,\n    // 15\n    clCreateImage2DIntelFPGA, clCreateImage3DIntelFPGA,\n    clRetainMemObjectIntelFPGA, clReleaseMemObjectIntelFPGA,\n    clGetSupportedImageFormatsIntelFPGA,\n    // 20\n    clGetMemObjectInfoIntelFPGA, clGetImageInfoIntelFPGA,\n    clCreateSamplerIntelFPGA, clRetainSamplerIntelFPGA,\n    clReleaseSamplerIntelFPGA,\n    // 25\n    clGetSamplerInfoIntelFPGA, clCreateProgramWithSourceIntelFPGA,\n    clCreateProgramWithBinaryIntelFPGA, clRetainProgramIntelFPGA,\n    clReleaseProgramIntelFPGA,\n    // 30\n    clBuildProgramIntelFPGA, clUnloadCompilerIntelFPGA,\n    clGetProgramInfoIntelFPGA, clGetProgramBuildInfoIntelFPGA,\n    clCreateKernelIntelFPGA,\n    // 35\n    clCreateKernelsInProgramIntelFPGA, clRetainKernelIntelFPGA,\n    clReleaseKernelIntelFPGA, clSetKernelArgIntelFPGA, clGetKernelInfoIntelFPGA,\n    // 40\n    clGetKernelWorkGroupInfoIntelFPGA, clWaitForEventsIntelFPGA,\n    clGetEventInfoIntelFPGA, clRetainEventIntelFPGA, clReleaseEventIntelFPGA,\n    // 45\n    clGetEventProfilingInfoIntelFPGA, clFlushIntelFPGA, clFinishIntelFPGA,\n    clEnqueueReadBufferIntelFPGA, clEnqueueWriteBufferIntelFPGA,\n    // 50\n    clEnqueueCopyBufferIntelFPGA, clEnqueueReadImageIntelFPGA,\n    clEnqueueWriteImageIntelFPGA, clEnqueueCopyImageIntelFPGA,\n    clEnqueueCopyImageToBufferIntelFPGA,\n    // 55\n    clEnqueueCopyBufferToImageIntelFPGA, clEnqueueMapBufferIntelFPGA,\n    clEnqueueMapImageIntelFPGA, clEnqueueUnmapMemObjectIntelFPGA,\n    clEnqueueNDRangeKernelIntelFPGA,\n    // 60\n    clEnqueueTaskIntelFPGA, clEnqueueNativeKernelIntelFPGA,\n    clEnqueueMarkerIntelFPGA, clEnqueueWaitForEventsIntelFPGA,\n    clEnqueueBarrierIntelFPGA,\n    // 65\n    clGetExtensionFunctionAddressIntelFPGA,\n\n    /* cl_khr_gl_sharing */\n    NULL, // clCreateFromGLBuffer;\n    NULL, // clCreateFromGLTexture2D;\n    NULL, // clCreateFromGLTexture3D;\n    NULL, // clCreateFromGLRenderbuffer;\n    NULL, // clGetGLObjectInfo;\n    NULL, // clGetGLTextureInfo;\n    NULL, // clEnqueueAcquireGLObjects;\n    NULL, // clEnqueueReleaseGLObjects;\n    NULL, // clGetGLContextInfoKHR;\n\n    /* cl_khr_d3d10_sharing */\n    NULL, // clGetDeviceIDsFromD3D10KHR;\n    NULL, // clCreateFromD3D10BufferKHR;\n    NULL, // clCreateFromD3D10Texture2DKHR;\n    NULL, // clCreateFromD3D10Texture3DKHR;\n    NULL, // clEnqueueAcquireD3D10ObjectsKHR;\n    NULL, // clEnqueueReleaseD3D10ObjectsKHR;\n\n    /* OpenCL 1.1 */\n    clSetEventCallbackIntelFPGA, clCreateSubBufferIntelFPGA,\n    clSetMemObjectDestructorCallbackIntelFPGA, clCreateUserEventIntelFPGA,\n    clSetUserEventStatusIntelFPGA, clEnqueueReadBufferRectIntelFPGA,\n    clEnqueueWriteBufferRectIntelFPGA, clEnqueueCopyBufferRectIntelFPGA,\n\n    /* cl_ext_device_fission */\n    NULL, // clCreateSubDevicesEXT;\n    NULL, // clRetainDeviceEXT;\n    NULL, // clReleaseDeviceEXT;\n\n    /* cl_khr_gl_event */\n    NULL, // clCreateEventFromGLsyncKHR;\n\n    /* OpenCL 1.2 */\n    clCreateSubDevicesIntelFPGA, // Not implemented\n    clRetainDeviceIntelFPGA,     // Not implemented\n    clReleaseDeviceIntelFPGA,    // Not implemented\n    clCreateImageIntelFPGA,\n    clCreateProgramWithBuiltInKernelsIntelFPGA, // Not implemented\n    clCompileProgramIntelFPGA,                  // Not implemented\n    clLinkProgramIntelFPGA,                     // Not implemented\n    clUnloadPlatformCompilerIntelFPGA,          // Not implemented\n    clGetKernelArgInfoIntelFPGA, clEnqueueFillBufferIntelFPGA,\n    clEnqueueFillImageIntelFPGA, clEnqueueMigrateMemObjectsIntelFPGA,\n    clEnqueueMarkerWithWaitListIntelFPGA,  // Not implemented\n    clEnqueueBarrierWithWaitListIntelFPGA, // Not implemented\n    clGetExtensionFunctionAddressForPlatformIntelFPGA,\n\n    /* cl_khr_gl_sharing */\n    NULL, // clCreateFromGLTexture;\n\n    /* cl_khr_d3d11_sharing */\n    NULL, // clGetDeviceIDsFromD3D11KHR;\n    NULL, // clCreateFromD3D11BufferKHR;\n    NULL, // clCreateFromD3D11Texture2DKHR;\n    NULL, // clCreateFromD3D11Texture3DKHR;\n    NULL, // clCreateFromDX9MediaSurfaceKHR;\n    NULL, // clEnqueueAcquireD3D11ObjectsKHR;\n    NULL, // clEnqueueReleaseD3D11ObjectsKHR;\n\n    /* cl_khr_dx9_media_sharing */\n    NULL, // clGetDeviceIDsFromDX9MediaAdapterKHR;\n    NULL, // clEnqueueAcquireDX9MediaSurfacesKHR;\n    NULL, // clEnqueueReleaseDX9MediaSurfacesKHR;\n\n    /* cl_khr_egl_image */\n    NULL, // clCreateFromEGLImageKHR;\n    NULL, // clEnqueueAcquireEGLObjectsKHR;\n    NULL, // clEnqueueReleaseEGLObjectsKHR;\n\n    /* cl_khr_egl_event */\n    NULL, // clCreateEventFromEGLSyncKHR;\n\n    /* OpenCL 2.0 */\n    clCreateCommandQueueWithPropertiesIntelFPGA, clCreatePipeIntelFPGA,\n    clGetPipeInfoIntelFPGA, clSVMAllocIntelFPGA, clSVMFreeIntelFPGA,\n    clEnqueueSVMFreeIntelFPGA, clEnqueueSVMMemcpyIntelFPGA,\n    clEnqueueSVMMemFillIntelFPGA, clEnqueueSVMMapIntelFPGA,\n    clEnqueueSVMUnmapIntelFPGA, clCreateSamplerWithPropertiesIntelFPGA,\n    clSetKernelArgSVMPointerIntelFPGA,\n    clSetKernelExecInfoIntelFPGA, // Not implemented\n\n    /* cl_khr_sub_groups */\n    NULL, // clGetKernelSubGroupInfoKHR;\n};\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_hostch.cpp",
        "data": "// Copyright (C) 2017-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n// External library headers.\n#include <CL/cl_ext_intelfpga.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_hostch.h>\n#include <acl_mem.h>\n#include <acl_platform.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n/* Local Functions */\nstatic cl_int l_push_packet(unsigned int physical_device_id, int channel_handle,\n                            const void *host_buffer, size_t write_size) {\n  size_t pushed_data;\n  int status = 0;\n\n  pushed_data = acl_get_hal()->hostchannel_push(\n      physical_device_id, channel_handle, host_buffer, write_size, &status);\n  assert(status == 0);\n  if (pushed_data == write_size) {\n    return CL_SUCCESS;\n  }\n  // If data type is not byte aligned, such as AC_INT56\n  // Pushed data can be smaller than the request write_size due to compiler\n  // padding. Runtime needs to check if the trailing bytes are all 0s.\n  else if ((pushed_data > 0) && (pushed_data < write_size)) {\n    for (size_t i = pushed_data; i < write_size; i++) {\n      unsigned char c = ((char *)host_buffer)[i];\n      if (c != 0) {\n        // This shouldn't happen. Needs to send out a warning to user rather\n        // than a silent function failure.\n        std::cerr << \"Error: Data is not fully written into the Hostpipe. \"\n                     \"None-0 bits have been cut off \\n\";\n        assert(0);\n      }\n    }\n    return CL_SUCCESS;\n  } else {\n    // Pipe is full in this case. Nothing is pushed in this case.\n    assert(pushed_data == 0);\n    return CL_PIPE_FULL;\n  }\n}\n\nstatic cl_int l_push_sideband_packet(unsigned int physical_device_id,\n                                     int channel_handle,\n                                     const void *host_buffer, size_t write_size,\n                                     host_pipe_t &host_pipe_info) {\n  // This pipe has sideband signals, need to break into data section and\n  // sideband signal sections\n  int status = 0;\n  bool final_status = true;\n  size_t total_pushed = 0;\n  for (auto const &sideband_signal_entry :\n       host_pipe_info.side_band_signals_vector) {\n    size_t pushed_data;\n    if (sideband_signal_entry.port_identifier ==\n        static_cast<unsigned>(AOCL_MMD_HOSTCHANNEL_PORT_DATA)) {\n      pushed_data = acl_get_hal()->hostchannel_push_no_ack(\n          host_pipe_info.m_physical_device_id, host_pipe_info.m_channel_handle,\n          (const void *)((char *)host_buffer +\n                         sideband_signal_entry.port_offset / 8),\n          sideband_signal_entry.side_band_size / 8, &status);\n    } else {\n      // this is sideband signal\n      pushed_data = acl_get_hal()->hostchannel_sideband_push_no_ack(\n          host_pipe_info.m_physical_device_id,\n          sideband_signal_entry.port_identifier,\n          host_pipe_info.m_channel_handle,\n          (const void *)((char *)host_buffer +\n                         sideband_signal_entry.port_offset / 8),\n          sideband_signal_entry.side_band_size / 8, &status);\n    }\n    final_status = final_status && status;\n    if (pushed_data == 0) {\n      return CL_PIPE_FULL;\n    }\n    total_pushed += pushed_data;\n  }\n  // Now need to acknowledge\n  size_t acked_size;\n\n  acked_size = acl_get_hal()->hostchannel_ack_buffer(\n      host_pipe_info.m_physical_device_id, host_pipe_info.m_channel_handle,\n      total_pushed, &status);\n\n  if (acked_size == 0) {\n    return CL_PIPE_FULL;\n  }\n\n  status = final_status && status;\n  return status;\n}\n\nstatic size_t l_pull_sideband_packet(unsigned int physical_device_id,\n                                     int channel_handle,\n                                     const void *host_buffer, size_t write_size,\n                                     host_pipe_t &host_pipe_info, int &status) {\n  bool final_status = true;\n  size_t total_pulled_data = 0;\n  for (auto const &sideband_signal_entry :\n       host_pipe_info.side_band_signals_vector) {\n    size_t pulled_data;\n    if (sideband_signal_entry.port_identifier ==\n        static_cast<unsigned>(AOCL_MMD_HOSTCHANNEL_PORT_DATA)) {\n      pulled_data = acl_get_hal()->hostchannel_pull_no_ack(\n          host_pipe_info.m_physical_device_id, host_pipe_info.m_channel_handle,\n          (void *)((char *)host_buffer + sideband_signal_entry.port_offset / 8),\n          sideband_signal_entry.side_band_size / 8, &status);\n    } else {\n      // this is sideband signal\n      pulled_data = acl_get_hal()->hostchannel_sideband_pull_no_ack(\n          host_pipe_info.m_physical_device_id,\n          sideband_signal_entry.port_identifier,\n          host_pipe_info.m_channel_handle,\n          (void *)((char *)host_buffer + sideband_signal_entry.port_offset / 8),\n          sideband_signal_entry.side_band_size / 8, &status);\n    }\n    final_status = final_status && status;\n    if (pulled_data == 0) {\n      return 0;\n    }\n    total_pulled_data += pulled_data;\n  }\n  // Now need to acknowledge\n  size_t acked_size;\n\n  acked_size = acl_get_hal()->hostchannel_ack_buffer(\n      host_pipe_info.m_physical_device_id, host_pipe_info.m_channel_handle,\n      total_pulled_data, &status);\n  if (acked_size == 0) {\n    return 0;\n  }\n  status = final_status && status;\n  return total_pulled_data;\n}\n\nstatic void l_clean_up_pending_pipe_ops(cl_mem pipe) {\n  size_t acked_size = 0;\n  int status = 0;\n\n  while (true) {\n    // Flush out any packets at the top of the queue\n    for (auto it = pipe->host_pipe_info->m_host_op_queue.begin();\n         it != pipe->host_pipe_info->m_host_op_queue.end() &&\n         it->m_op == PACKET;) {\n      int res = 0;\n      size_t available_sz;\n      void *mmd_buffer;\n\n      // Make sure the mmd_pointer is where you expect it be\n      mmd_buffer = acl_get_hal()->hostchannel_get_buffer(\n          pipe->host_pipe_info->m_physical_device_id,\n          pipe->host_pipe_info->m_channel_handle, &available_sz, &status);\n      assert(status == 0);\n      assert(mmd_buffer == it->m_mmd_buffer);\n\n      // We have to block on sending these packets which we claimed we have\n      // already sent to the mmd\n      while (res == 0) {\n        acked_size = acl_get_hal()->hostchannel_ack_buffer(\n            pipe->host_pipe_info->m_physical_device_id,\n            pipe->host_pipe_info->m_channel_handle,\n            pipe->fields.pipe_objs.pipe_packet_size, &status);\n        if (acked_size == pipe->fields.pipe_objs.pipe_packet_size) {\n          res = 1;\n        } else {\n          assert(status == 0);\n          assert(acked_size == 0);\n        }\n      }\n      pipe->host_pipe_info->size_buffered -= acked_size;\n\n      // Clean up the host pipe operation form the queue\n      it = pipe->host_pipe_info->m_host_op_queue.erase(it);\n    }\n\n    // If the operation queue is not empty, the next operation should be a MAP\n    // operation Process this map operation and see if there's any data to be\n    // acked\n    if (!pipe->host_pipe_info->m_host_op_queue.empty()) {\n      size_t available_sz;\n      void *mmd_buffer;\n\n      auto it = pipe->host_pipe_info->m_host_op_queue.begin();\n      assert(it->m_op == MAP);\n\n      // Make sure the mmd_pointer is where you expect it be\n      mmd_buffer = acl_get_hal()->hostchannel_get_buffer(\n          pipe->host_pipe_info->m_physical_device_id,\n          pipe->host_pipe_info->m_channel_handle, &available_sz, &status);\n      assert(status == 0);\n      assert(mmd_buffer == it->m_mmd_buffer);\n\n      // If the user has unmapped any part, send that part to the mmd\n      if (it->m_size_sent > 0) {\n        acked_size = 0;\n        while (acked_size != it->m_size_sent) {\n          acked_size += acl_get_hal()->hostchannel_ack_buffer(\n              pipe->host_pipe_info->m_physical_device_id,\n              pipe->host_pipe_info->m_channel_handle,\n              it->m_size_sent - acked_size, &status);\n          assert(status == 0);\n        }\n        pipe->host_pipe_info->size_buffered -= acked_size;\n      }\n\n      // If this operation is incomplete, it blocks the beginning of the queue\n      // and we stop\n      if (it->m_size_sent != it->m_op_size) {\n        return;\n      }\n\n      // This operation is done so remove it from the queue\n      pipe->host_pipe_info->m_host_op_queue.erase(it);\n    } else {\n      return;\n    }\n  }\n}\n\nstatic void l_move_ops_from_hostbuf_to_mmdbuf(cl_mem pipe) {\n  void *mmd_buffer;\n  size_t buffer_size;\n  int status = 0;\n\n  auto it = pipe->host_pipe_info->m_host_op_queue.begin();\n\n  // This function should only do work for write pipes since we can't buffer\n  // read operations\n  assert((pipe->flags & CL_MEM_HOST_WRITE_ONLY) ||\n         pipe->host_pipe_info->m_host_op_queue.empty());\n  // Make sure we didn't buffer more data than the width of the pipe\n  assert(pipe->host_pipe_info->size_buffered <=\n         (pipe->fields.pipe_objs.pipe_max_packets *\n          pipe->fields.pipe_objs.pipe_packet_size));\n\n  // Find space in the mmd buffer\n  mmd_buffer = acl_get_hal()->hostchannel_get_buffer(\n      pipe->host_pipe_info->m_physical_device_id,\n      pipe->host_pipe_info->m_channel_handle, &buffer_size, &status);\n  // What if we don't have enough mmd buffer space for some reason? Right now,\n  // we will only buffer upto the maximum depth of the channel\n  // (max_packets*packet_size). Thus we should always have enough space to\n  // transfer the data from m_host_buffer to m_mmd_buffer\n  assert(buffer_size >= pipe->host_pipe_info->size_buffered);\n\n  // Copy data from temporary host buffer to mmd buffer for every operation\n  while (it != pipe->host_pipe_info->m_host_op_queue.end()) {\n    it->m_mmd_buffer = mmd_buffer;\n    safe_memcpy(it->m_mmd_buffer, it->m_host_buffer, it->m_op_size, buffer_size,\n                it->m_op_size);\n\n    free(it->m_host_buffer);\n    it->m_host_buffer = NULL;\n\n    mmd_buffer = ((char *)mmd_buffer) + it->m_op_size;\n    ++it;\n  }\n}\n\ncl_int acl_bind_pipe_to_channel(cl_mem pipe, cl_device_id device,\n                                const acl_device_def_autodiscovery_t &devdef) {\n  int direction;\n\n  acl_assert_locked();\n\n  for (const auto &hostpipe : devdef.acl_hostpipe_info) {\n    if (hostpipe.name == pipe->host_pipe_info->host_pipe_channel_id) {\n      // Found the object, we can do checks on it now\n      // Check width\n      if (pipe->fields.pipe_objs.pipe_packet_size != hostpipe.data_width) {\n        return CL_INVALID_VALUE;\n      }\n      // Check direction\n      if ((pipe->flags & CL_MEM_HOST_READ_ONLY) && hostpipe.is_dev_to_host) {\n        direction = DEVICE_TO_HOST;\n      } else if ((pipe->flags & CL_MEM_HOST_WRITE_ONLY) &&\n                 hostpipe.is_host_to_dev) {\n        direction = HOST_TO_DEVICE;\n      } else {\n        return CL_INVALID_VALUE;\n      }\n      // Check max buffer size\n      if (pipe->fields.pipe_objs.pipe_max_packets > hostpipe.max_buffer_depth) {\n        return CL_INVALID_VALUE;\n      }\n\n      pipe->host_pipe_info->m_physical_device_id =\n          device->def.physical_device_id;\n      pipe->host_pipe_info->m_channel_handle =\n          acl_get_hal()->hostchannel_create(\n              pipe->host_pipe_info->m_physical_device_id,\n              (char *)pipe->host_pipe_info->host_pipe_channel_id.c_str(),\n              pipe->fields.pipe_objs.pipe_max_packets,\n              pipe->fields.pipe_objs.pipe_packet_size, direction);\n      size_t buffer_size;\n      int status = 0;\n      acl_get_hal()->hostchannel_get_buffer(\n          pipe->host_pipe_info->m_physical_device_id,\n          pipe->host_pipe_info->m_channel_handle, &buffer_size, &status);\n      if (pipe->host_pipe_info->m_channel_handle <= 0) {\n        return CL_INVALID_VALUE;\n      }\n      pipe->host_pipe_info->binded = true;\n      return CL_SUCCESS;\n    }\n  }\n  // No matching hostpipe channel id(should not happen)\n  return CL_INVALID_VALUE;\n}\n\nvoid acl_process_pipe_transactions(cl_mem pipe) {\n  l_move_ops_from_hostbuf_to_mmdbuf(pipe);\n  l_clean_up_pending_pipe_ops(pipe);\n}\n\nvoid acl_bind_and_process_all_pipes_transactions(\n    cl_context context, cl_device_id device,\n    const acl_device_def_autodiscovery_t &devdef) {\n  for (const auto &pipe : context->pipe_vec) {\n    if (device->loaded_bin && pipe->host_pipe_info &&\n        pipe->host_pipe_info->m_binded_kernel &&\n        device->loaded_bin->get_dev_prog()->program ==\n            pipe->host_pipe_info->m_binded_kernel->program) {\n      acl_bind_pipe_to_channel(pipe, device, devdef);\n      l_move_ops_from_hostbuf_to_mmdbuf(pipe);\n      l_clean_up_pending_pipe_ops(pipe);\n    }\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReadPipeIntelFPGA(cl_mem pipe, void *ptr) {\n  void *mmd_buffer;\n  size_t pulled_data;\n  size_t buffer_size;\n  cl_int status = 0;\n\n  {\n    std::scoped_lock lock{acl_mutex_wrapper};\n    acl_idle_update(pipe->context);\n  }\n\n  if (pipe->host_pipe_info == NULL) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, pipe->context,\n            \"This pipe is not a host pipe\");\n  }\n\n  acl_mutex_lock(&(pipe->host_pipe_info->m_lock));\n\n  // Error checking\n  if (!(pipe->flags & CL_MEM_HOST_READ_ONLY)) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_MEM_OBJECT, pipe->context,\n            \"This host pipe is not read-only pipe\");\n  }\n  if (!pipe->host_pipe_info->m_binded_kernel) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_KERNEL, pipe->context,\n            \"This host pipe has not been bound to a kernel yet\");\n  }\n  if (ptr == NULL) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_VALUE, pipe->context,\n            \"Invalid pointer was provided to host data\");\n  }\n\n  // Is the pipe bound to a channel yet? If not then return unsuccessfully\n  if (!pipe->host_pipe_info->binded) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    return CL_PIPE_EMPTY;\n  }\n\n  // Are there any operations queued up on this pipe? Is there any data still\n  // available to be read? If yes return that data. Else return 0\n  if (!pipe->host_pipe_info->m_host_op_queue.empty()) {\n    // Find space in the mmd buffer\n    mmd_buffer = acl_get_hal()->hostchannel_get_buffer(\n        pipe->host_pipe_info->m_physical_device_id,\n        pipe->host_pipe_info->m_channel_handle, &buffer_size, &status);\n    assert(status == 0);\n    // Size of buffered space should never exceed actual available space in the\n    // mmd\n    assert(buffer_size >= pipe->host_pipe_info->size_buffered);\n\n    // If there's no space left, return unsuccessfully\n    if (buffer_size < pipe->host_pipe_info->size_buffered +\n                          pipe->fields.pipe_objs.pipe_packet_size) {\n      acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n      return CL_PIPE_EMPTY;\n    }\n\n    mmd_buffer = ((char *)mmd_buffer) + pipe->host_pipe_info->size_buffered;\n    pipe->host_pipe_info->size_buffered +=\n        pipe->fields.pipe_objs.pipe_packet_size;\n\n    // Create the host operation data structure\n    host_op_t host_op;\n    host_op.m_op = PACKET;\n    host_op.m_mmd_buffer = mmd_buffer;\n    host_op.m_host_buffer = NULL;\n    host_op.m_op_size = pipe->fields.pipe_objs.pipe_packet_size;\n    host_op.m_size_sent = 0;\n\n    safe_memcpy(ptr, host_op.m_mmd_buffer, host_op.m_op_size, buffer_size,\n                buffer_size);\n\n    // Save the host operation for later in the operation queue\n    pipe->host_pipe_info->m_host_op_queue.push_back(host_op);\n\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    return CL_SUCCESS;\n  }\n\n  pulled_data = acl_get_hal()->hostchannel_pull(\n      pipe->host_pipe_info->m_physical_device_id,\n      pipe->host_pipe_info->m_channel_handle, ptr,\n      pipe->fields.pipe_objs.pipe_packet_size, &status);\n  assert(status == 0);\n\n  if (pulled_data == pipe->fields.pipe_objs.pipe_packet_size) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    return CL_SUCCESS;\n  } else {\n    // A packet of data is the smallest size this channel can receive. If it\n    // didn't receive a packet, it shouldn't have received anything at all.\n    assert(pulled_data == 0);\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    return CL_PIPE_EMPTY;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clWritePipeIntelFPGA(cl_mem pipe, void *ptr) {\n  size_t buffer_size;\n  void *buffer = 0;\n  cl_int status = 0;\n  cl_int ret;\n\n  {\n    std::scoped_lock lock{acl_mutex_wrapper};\n    acl_idle_update(pipe->context);\n  }\n\n  // Error checking\n  if (pipe->host_pipe_info == NULL) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, pipe->context,\n            \"This pipe is not a host pipe\");\n  }\n\n  acl_mutex_lock(&(pipe->host_pipe_info->m_lock));\n\n  if (!(pipe->flags & CL_MEM_HOST_WRITE_ONLY)) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_MEM_OBJECT, pipe->context,\n            \"This host pipe is not write-only pipe\");\n  }\n  if (!pipe->host_pipe_info->m_binded_kernel) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_KERNEL, pipe->context,\n            \"This host pipe has not been bound to a kernel yet\");\n  }\n  if (ptr == NULL) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_VALUE, pipe->context,\n            \"Invalid pointer was provided to host data\");\n  }\n\n  // If there is no queued op and the pipe is binded\n  // packet should be send right here\n  if (pipe->host_pipe_info->m_host_op_queue.empty() &&\n      pipe->host_pipe_info->binded) {\n    ret = l_push_packet(pipe->host_pipe_info->m_physical_device_id,\n                        pipe->host_pipe_info->m_channel_handle, ptr,\n                        pipe->fields.pipe_objs.pipe_packet_size);\n\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    return ret;\n  }\n\n  // Queue this operation because we can't do it now\n\n  // Find space in the mmd buffer\n  if (pipe->host_pipe_info->binded) {\n    buffer = acl_get_hal()->hostchannel_get_buffer(\n        pipe->host_pipe_info->m_physical_device_id,\n        pipe->host_pipe_info->m_channel_handle, &buffer_size, &status);\n    assert(status == 0);\n  }\n  // If the pipe is not bound to a channel, then we need\n  // to create a temporary buffer for it in the host\n  else {\n    buffer_size = pipe->fields.pipe_objs.pipe_packet_size *\n                  pipe->fields.pipe_objs.pipe_max_packets;\n  }\n\n  // Size of buffered space should never exceed actual available space\n  assert(buffer_size >= pipe->host_pipe_info->size_buffered);\n\n  // If there's no space left, return unsuccessfully\n  if (buffer_size < pipe->host_pipe_info->size_buffered +\n                        pipe->fields.pipe_objs.pipe_packet_size) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    return CL_PIPE_FULL;\n  }\n\n  // Create the host operation data structure\n  host_op_t host_op;\n  host_op.m_op = PACKET;\n  host_op.m_op_size = pipe->fields.pipe_objs.pipe_packet_size;\n  host_op.m_size_sent = 0;\n\n  if (pipe->host_pipe_info->binded) {\n    buffer = ((char *)buffer) + pipe->host_pipe_info->size_buffered;\n\n    host_op.m_mmd_buffer = buffer;\n    host_op.m_host_buffer = NULL;\n\n    safe_memcpy(host_op.m_mmd_buffer, ptr, host_op.m_op_size,\n                pipe->host_pipe_info->size_buffered, host_op.m_op_size);\n  } else {\n    buffer = malloc(pipe->fields.pipe_objs.pipe_packet_size);\n    if (buffer == NULL) {\n      acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n      ERR_RET(CL_OUT_OF_HOST_MEMORY, pipe->context,\n              \"Could not allocate memory for internal data structure\");\n    }\n\n    host_op.m_mmd_buffer = NULL;\n    host_op.m_host_buffer = buffer;\n\n    safe_memcpy(host_op.m_host_buffer, ptr, host_op.m_op_size,\n                pipe->fields.pipe_objs.pipe_packet_size, host_op.m_op_size);\n  }\n\n  pipe->host_pipe_info->size_buffered +=\n      pipe->fields.pipe_objs.pipe_packet_size;\n\n  // Save the host operation for later in the operation queue\n  pipe->host_pipe_info->m_host_op_queue.push_back(host_op);\n\n  acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clMapHostPipeIntelFPGA(cl_mem pipe,\n                                                      cl_map_flags map_flags,\n                                                      size_t requested_size,\n                                                      size_t *mapped_size,\n                                                      cl_int *errcode_ret) {\n  size_t buffer_size;\n  void *buffer = 0;\n  int status = 0;\n\n  {\n    std::scoped_lock lock{acl_mutex_wrapper};\n    acl_idle_update(pipe->context);\n  }\n\n  if (pipe->host_pipe_info == NULL) {\n    BAIL_INFO(CL_INVALID_MEM_OBJECT, pipe->context,\n              \"This pipe is not a host pipe\");\n  }\n\n  acl_mutex_lock(&(pipe->host_pipe_info->m_lock));\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  // Error checking\n  if (mapped_size == NULL) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    BAIL_INFO(CL_INVALID_VALUE, pipe->context,\n              \"Invalid pointer was provided for mapped_size argument\");\n  }\n  *mapped_size = 0;\n\n  if (!pipe->host_pipe_info->m_binded_kernel) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    BAIL_INFO(CL_INVALID_KERNEL, pipe->context,\n              \"This host pipe has not been bound to a kernel yet\");\n  }\n  if (map_flags != 0) {\n    acl_context_callback(pipe->context,\n                         \"map_flags value other than 0 is not supported in the \"\n                         \"Runtime. Ignoring it...\");\n  }\n\n  // Find space in the mmd\n  if (pipe->host_pipe_info->binded) {\n    buffer = acl_get_hal()->hostchannel_get_buffer(\n        pipe->host_pipe_info->m_physical_device_id,\n        pipe->host_pipe_info->m_channel_handle, &buffer_size, &status);\n    assert(status == 0);\n  }\n  // If the pipe is not bound to a channel, then we need to\n  // create a temporary buffer for it in the host\n  else {\n    // Obviously can't buffer read operations\n    if (pipe->flags & CL_MEM_HOST_READ_ONLY) {\n      acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n      BAIL_INFO(CL_OUT_OF_RESOURCES, pipe->context,\n                \"No buffer space for the map operation\");\n    }\n\n    buffer_size = pipe->fields.pipe_objs.pipe_packet_size *\n                  pipe->fields.pipe_objs.pipe_max_packets;\n  }\n\n  // Size of buffered space should never exceed actual available space in the\n  // mmd\n  assert(buffer_size >= pipe->host_pipe_info->size_buffered);\n\n  // If there's no space left, return unsuccessfully\n  if (buffer_size == pipe->host_pipe_info->size_buffered) {\n    if (pipe->flags & CL_MEM_HOST_READ_ONLY) {\n      acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n      BAIL_INFO(CL_OUT_OF_RESOURCES, pipe->context,\n                \"No buffer space for the read map operation\");\n    } else {\n      acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n      BAIL_INFO(CL_OUT_OF_RESOURCES, pipe->context,\n                \"No buffer space for the write map operation\");\n    }\n  }\n\n  // Figure out how much space we can give\n  buffer_size -= pipe->host_pipe_info->size_buffered;\n  if (requested_size != 0) {\n    buffer_size = (buffer_size < requested_size) ? buffer_size : requested_size;\n  }\n  *mapped_size = buffer_size;\n\n  // Create the host pipe operation data structure\n  host_op_t host_op;\n  host_op.m_op = MAP;\n  host_op.m_size_sent = 0;\n  host_op.m_op_size = buffer_size;\n\n  if (pipe->host_pipe_info->binded) {\n    buffer = ((char *)buffer) + pipe->host_pipe_info->size_buffered;\n\n    host_op.m_mmd_buffer = buffer;\n    host_op.m_host_buffer = NULL;\n  } else {\n    buffer = malloc(buffer_size);\n    if (buffer == NULL) {\n      acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, pipe->context,\n                \"Could not allocate memory for internal data structure\");\n    }\n\n    host_op.m_mmd_buffer = NULL;\n    host_op.m_host_buffer = buffer;\n  }\n\n  pipe->host_pipe_info->size_buffered += buffer_size;\n\n  // Save the host pipe operation in the operation queue\n  pipe->host_pipe_info->m_host_op_queue.push_back(host_op);\n\n  if (pipe->host_pipe_info->binded) {\n    buffer = host_op.m_mmd_buffer;\n  } else {\n    buffer = host_op.m_host_buffer;\n  }\n\n  acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n\n  return buffer;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclUnmapHostPipeIntelFPGA(cl_mem pipe, void *mapped_ptr, size_t size_to_unmap,\n                         size_t *unmapped_size) {\n  void *mmd_buffer;\n  size_t acked_size;\n  size_t buffer_size;\n  int status = 0;\n  int first = 1;\n\n  {\n    std::scoped_lock lock{acl_mutex_wrapper};\n    acl_idle_update(pipe->context);\n  }\n\n  if (pipe->host_pipe_info == NULL) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, pipe->context,\n            \"This pipe is not a host pipe\");\n  }\n\n  acl_mutex_lock(&(pipe->host_pipe_info->m_lock));\n\n  // Error checking\n  if (!pipe->host_pipe_info->m_binded_kernel) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_KERNEL, pipe->context,\n            \"This host pipe has not been bound to a kernel yet\");\n  }\n\n  assert(pipe->host_pipe_info->m_host_op_queue.size());\n  auto it = pipe->host_pipe_info->m_host_op_queue.begin();\n  while (it != pipe->host_pipe_info->m_host_op_queue.end()) {\n    if (pipe->host_pipe_info->binded) {\n      if (it->m_mmd_buffer == mapped_ptr) {\n        break;\n      }\n    } else {\n      if (it->m_host_buffer == mapped_ptr) {\n        break;\n      }\n    }\n    first = 0;\n    ++it;\n  }\n  if (it == pipe->host_pipe_info->m_host_op_queue.end()) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(CL_INVALID_VALUE, pipe->context,\n            \"This is not a valid mapped pointer\");\n  }\n  assert(it->m_op == MAP);\n\n  // You shouldn't be trying to send over more data than you have mapped\n  if (size_to_unmap > (it->m_op_size - it->m_size_sent)) {\n    acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n    ERR_RET(\n        CL_INVALID_VALUE, pipe->context,\n        \"You are trying to upmap more mapped buffer space than you have left\");\n  }\n\n  if (first == 0 || !pipe->host_pipe_info->binded) {\n    // This is a queued map operation or we haven't binded to the channel yet.\n    // Just fake the upmap and leave it for later\n    it->m_size_sent += size_to_unmap;\n    if (unmapped_size != NULL) {\n      *unmapped_size = size_to_unmap;\n    }\n  } else {\n    // Checking to make sure the mmd buffer is where it's supposed to be\n    mmd_buffer = acl_get_hal()->hostchannel_get_buffer(\n        pipe->host_pipe_info->m_physical_device_id,\n        pipe->host_pipe_info->m_channel_handle, &buffer_size, &status);\n    assert(status == 0);\n    assert(mmd_buffer == (void *)((char *)mapped_ptr + it->m_size_sent));\n\n    // This is the first operation in the queue. Send it to the mmd for transfer\n    // to the device\n    assert(pipe->host_pipe_info->m_host_op_queue.begin() == it);\n    acked_size = acl_get_hal()->hostchannel_ack_buffer(\n        pipe->host_pipe_info->m_physical_device_id,\n        pipe->host_pipe_info->m_channel_handle, size_to_unmap, &status);\n    assert(status == 0);\n    if (unmapped_size != NULL) {\n      *unmapped_size = acked_size;\n    }\n    it->m_size_sent += acked_size;\n    assert(it->m_size_sent <= it->m_op_size);\n    pipe->host_pipe_info->size_buffered -= acked_size;\n\n    if (it->m_size_sent == it->m_op_size) {\n      // This map operation is done now. Clean up.\n      assert(acked_size == size_to_unmap);\n      pipe->host_pipe_info->m_host_op_queue.erase(it);\n\n      // Go through the rest of the queue and flush out any operations\n      // that were blocked by this MAP operation\n      l_clean_up_pending_pipe_ops(pipe);\n    }\n  }\n  acl_mutex_unlock(&(pipe->host_pipe_info->m_lock));\n\n  return CL_SUCCESS;\n}\n\n// Ideally this should be passed from the autodiscovery string.\nstatic constexpr unsigned csr_pipe_address_offet = 8;\n\nvoid acl_read_program_hostpipe(void *user_data, acl_device_op_t *op) {\n\n  cl_event event = op->info.event;\n  cl_int status = 0;\n  size_t pulled_data = 0;\n  bool blocking = event->cmd.info.host_pipe_dynamic_info.blocking;\n  acl_assert_locked();\n\n  if (!acl_event_is_valid(event) ||\n      !acl_command_queue_is_valid(event->command_queue)) {\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n\n  // Difference between event->cmd.info.host_pipe_dynamic_info\n  // And host_pipe_info.\n  // Event member contains dynamic information like data and size\n  // The host_pipe_info stored in the dev_prog->program_hostpipe_map\n  // Contains the static information of the pipe, like protocol\n\n  assert(event->command_queue->device->loaded_bin != NULL &&\n         \"No loaded binary for read hostpipe\");\n  acl_device_program_info_t *dev_prog =\n      event->command_queue->device->loaded_bin->get_dev_prog();\n  auto host_pipe_info = dev_prog->program_hostpipe_map.at(\n      std::string(event->cmd.info.host_pipe_dynamic_info.logical_name));\n  acl_mutex_lock(&(host_pipe_info.m_lock));\n  acl_set_device_op_execution_status(op, CL_SUBMITTED);\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  if (host_pipe_info.implement_in_csr) {\n    // Here is the logic for CSR pipe read\n    // Compiler initializes ready register to 1, if ready register exist\n    // Non-Blocking uses_ready<true>\n    //   1. if ready == 1, fail.\n    //   2. Read data.\n    //   3. write 1 to ready.\n\n    // Blocking uses_ready<true>\n    //   1. wait until ready = 0.\n    //   2. read data.\n    //   3. write 1 to ready.\n\n    // uses_ready<false>\n    // Both Blocking and NonBlocking\n    //   1. Read data (always succeeds)\n\n    unsigned long long parsed;\n    uintptr_t data_reg, ready_reg;\n    // Convert the CSR address to a pointer\n    try {\n      parsed = std::stoull(host_pipe_info.csr_address, nullptr);\n    } catch (const std::exception &) {\n\n      acl_set_device_op_execution_status(op, -1);\n      return;\n    }\n\n    data_reg = static_cast<uintptr_t>(parsed);\n    ready_reg = static_cast<uintptr_t>(\n        parsed +\n        csr_pipe_address_offet); // ready reg is data reg shift by 8 byte\n    unsigned ready = 1;\n    unsigned ready_value;\n    unsigned *ready_value_pointer = &ready_value;\n\n    if (host_pipe_info.is_stall_free == 0) {\n      // If Blocking, wait until the ready register = 0\n      // If Non-blocking, just read once and report failure if ready == 1\n      do {\n        acl_get_hal()->read_csr(host_pipe_info.m_physical_device_id, ready_reg,\n                                (void *)ready_value_pointer,\n                                (size_t)sizeof(uintptr_t));\n      } while (blocking && ready_value != 0);\n\n      // If non-blocking and ready bit is 1, set the op to fail.\n      if (!blocking && ready_value == 1) {\n        acl_mutex_unlock(&(host_pipe_info.m_lock));\n        acl_set_device_op_execution_status(op, -1);\n        return;\n      }\n    }\n    // start the CSR read\n    auto status =\n        acl_get_hal()->read_csr(host_pipe_info.m_physical_device_id, data_reg,\n                                event->cmd.info.host_pipe_dynamic_info.ptr,\n                                event->cmd.info.host_pipe_dynamic_info.size);\n    if (status != 0) {\n      acl_mutex_unlock(&(host_pipe_info.m_lock));\n      acl_set_device_op_execution_status(op, -1);\n      return;\n    }\n    // Tell CSR it's ready if ready register exist\n    if (host_pipe_info.is_stall_free == 0) {\n      acl_get_hal()->write_csr(host_pipe_info.m_physical_device_id, ready_reg,\n                               (void *)&ready, (size_t)sizeof(uintptr_t));\n    }\n  } else {\n    // Non CSR Case\n    if (host_pipe_info.num_side_band_signals == 0) {\n      pulled_data = acl_get_hal()->hostchannel_pull(\n          host_pipe_info.m_physical_device_id, host_pipe_info.m_channel_handle,\n          event->cmd.info.host_pipe_dynamic_info.ptr,\n          event->cmd.info.host_pipe_dynamic_info.size, &status);\n    } else {\n      // This pipe has sideband signals, need to break into data section and\n      // sideband signal sections\n      pulled_data = l_pull_sideband_packet(\n          host_pipe_info.m_physical_device_id, host_pipe_info.m_channel_handle,\n          event->cmd.info.host_pipe_dynamic_info.ptr,\n          event->cmd.info.host_pipe_dynamic_info.size, host_pipe_info, status);\n    }\n\n    if (!blocking) {\n      // If it is non-blocking read, we return with the success code right away\n      // TODO: Change to pulled_data != pipe.width when sideband signals pipe\n      // are fully implemented. Right now we consider the result is good as long\n      // as pulled_data > 0.\n      if (status != 0 || pulled_data == 0) {\n        acl_mutex_unlock(&(host_pipe_info.m_lock));\n        acl_set_device_op_execution_status(op, -1);\n        return;\n      }\n    } else {\n      // If it is a blocking read, this call won't return until the kernel\n      // writes the data into the pipe.\n      // TODO: Change to pulled_data == pipe.width when sideband signals pipe\n      // are fully implemented. Right now we consider the result is good as long\n      // as pulled_data > 0.\n      while (status != 0 || pulled_data == 0) {\n        if (host_pipe_info.num_side_band_signals == 0) {\n          pulled_data = acl_get_hal()->hostchannel_pull(\n              host_pipe_info.m_physical_device_id,\n              host_pipe_info.m_channel_handle,\n              event->cmd.info.host_pipe_dynamic_info.ptr,\n              event->cmd.info.host_pipe_dynamic_info.size, &status);\n        } else {\n          pulled_data = l_pull_sideband_packet(\n              host_pipe_info.m_physical_device_id,\n              host_pipe_info.m_channel_handle,\n              event->cmd.info.host_pipe_dynamic_info.ptr,\n              event->cmd.info.host_pipe_dynamic_info.size, host_pipe_info,\n              status);\n        }\n        acl_update_device_op_queue(&(acl_platform.device_op_queue));\n      }\n    }\n  }\n\n  acl_mutex_unlock(&(host_pipe_info.m_lock));\n  acl_set_device_op_execution_status(op, CL_COMPLETE);\n}\n\nvoid acl_write_program_hostpipe(void *user_data, acl_device_op_t *op) {\n\n  cl_int status;\n  cl_event event = op->info.event;\n  bool blocking = event->cmd.info.host_pipe_dynamic_info.blocking;\n  acl_assert_locked();\n\n  if (!acl_event_is_valid(event) ||\n      !acl_command_queue_is_valid(event->command_queue)) {\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n\n  // Difference between event->cmd.info.host_pipe_dynamic_info\n  // And host_pipe_info.\n  // Event member contains dynamic information like data and size\n  // The host_pipe_info stored in the dev_prog->program_hostpipe_map\n  // Contains the static information of the pipe, like protocol\n\n  assert(event->command_queue->device->loaded_bin != NULL &&\n         \"No loaded binary for write hostpipe\");\n  acl_device_program_info_t *dev_prog =\n      event->command_queue->device->loaded_bin->get_dev_prog();\n  auto host_pipe_info = dev_prog->program_hostpipe_map.at(\n      std::string(event->cmd.info.host_pipe_dynamic_info.logical_name));\n  acl_mutex_lock(&(host_pipe_info.m_lock));\n  acl_set_device_op_execution_status(op, CL_SUBMITTED);\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  if (host_pipe_info.implement_in_csr) {\n    // Get CSR address\n\n    // Here is the logic for CSR pipe write\n    // Blocking uses_valid<true>:\n    //   1. read valid reg, wait until valid is 0\n    //   2. write to the pipe.\n    //   3. write 1 to the valid.\n\n    // Non-blocking uses_valid<true>\n    //   1. read valid reg once ->return failure if valid is 1\n    //   2. write to the pipe.\n    //   3. write 1 to the valid.\n\n    // uses_valid<false>\n    // Both Blocking and NonBlocking\n    //   1. Write data (always succeeds)\n\n    unsigned long long parsed;\n    uintptr_t data_reg, valid_reg;\n    try {\n      parsed = std::stoull(host_pipe_info.csr_address, nullptr);\n    } catch (const std::exception &) {\n      acl_set_device_op_execution_status(op, -1);\n      return;\n    }\n    data_reg = static_cast<uintptr_t>(parsed);\n    valid_reg = static_cast<uintptr_t>(\n        parsed +\n        csr_pipe_address_offet); // valid reg is data reg shift by 8 byte, move\n                                 // this to the autodiscovery string maybe\n\n    unsigned valid_value = 1;\n    unsigned *valid_value_pointer = &valid_value;\n\n    if (host_pipe_info.is_stall_free == 0) {\n      if (blocking) {\n        while (valid_value != 0) {\n          acl_get_hal()->read_csr(host_pipe_info.m_physical_device_id,\n                                  valid_reg, (void *)valid_value_pointer,\n                                  (size_t)sizeof(uintptr_t));\n        }\n      } else {\n        // Non-blocking, if valid reg is 1, return failure.\n        acl_get_hal()->read_csr(host_pipe_info.m_physical_device_id, valid_reg,\n                                (void *)valid_value_pointer,\n                                (size_t)sizeof(uintptr_t));\n\n        if (valid_value == 1) {\n          acl_mutex_unlock(&(host_pipe_info.m_lock));\n          acl_set_device_op_execution_status(op, -1);\n          return;\n        }\n      }\n    }\n\n    // start the write\n    auto status = acl_get_hal()->write_csr(\n        host_pipe_info.m_physical_device_id, data_reg,\n        event->cmd.info.host_pipe_dynamic_info.write_ptr,\n        event->cmd.info.host_pipe_dynamic_info.size);\n\n    if (status != 0) {\n      acl_mutex_unlock(&(host_pipe_info.m_lock));\n      acl_set_device_op_execution_status(op, -1);\n      return;\n    }\n\n    if (host_pipe_info.is_stall_free == 0) {\n      const unsigned valid = 1;\n      acl_get_hal()->write_csr(host_pipe_info.m_physical_device_id, valid_reg,\n                               (void *)&valid, (size_t)sizeof(uintptr_t));\n    }\n\n  } else {\n    // Regular hostpipe\n    // Attempt to write once\n    if (host_pipe_info.num_side_band_signals == 0) {\n      status = l_push_packet(host_pipe_info.m_physical_device_id,\n                             host_pipe_info.m_channel_handle,\n                             event->cmd.info.host_pipe_dynamic_info.write_ptr,\n                             event->cmd.info.host_pipe_dynamic_info.size);\n    } else {\n      status = l_push_sideband_packet(\n          host_pipe_info.m_physical_device_id, host_pipe_info.m_channel_handle,\n          event->cmd.info.host_pipe_dynamic_info.write_ptr,\n          event->cmd.info.host_pipe_dynamic_info.size, host_pipe_info);\n    }\n    if (!blocking) {\n      // If it is non-blocking write, we return with the success/failure code\n      // right away\n      if (status != CL_SUCCESS) {\n        acl_mutex_unlock(&(host_pipe_info.m_lock));\n        acl_set_device_op_execution_status(op, -1);\n        return;\n      }\n    } else {\n      // If it's a blocking write, this function won't return until the write\n      // success.\n      while (status != CL_SUCCESS) {\n        if (host_pipe_info.num_side_band_signals == 0) {\n          status =\n              l_push_packet(host_pipe_info.m_physical_device_id,\n                            host_pipe_info.m_channel_handle,\n                            event->cmd.info.host_pipe_dynamic_info.write_ptr,\n                            event->cmd.info.host_pipe_dynamic_info.size);\n        } else {\n          status = l_push_sideband_packet(\n              host_pipe_info.m_physical_device_id,\n              host_pipe_info.m_channel_handle,\n              event->cmd.info.host_pipe_dynamic_info.write_ptr,\n              event->cmd.info.host_pipe_dynamic_info.size, host_pipe_info);\n        }\n        acl_update_device_op_queue(&(acl_platform.device_op_queue));\n      }\n    }\n  }\n  acl_mutex_unlock(&(host_pipe_info.m_lock));\n  acl_set_device_op_execution_status(op, CL_COMPLETE);\n}\n\n// Submit an op to the device op queue to read hostpipe.\n// Return 1 if we made forward progress, 0 otherwise.\ncl_int acl_submit_read_program_hostpipe_device_op(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n\n  // No user-level scheduling blocks this hostpipe read\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  // Already enqueued.\n  if (event->last_device_op) {\n    return result;\n  }\n\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n  acl_device_op_t *last_op = 0;\n\n  // Precautionary, but it also nudges the device scheduler to try\n  // to free up old operation slots.\n  acl_forget_proposed_device_ops(doq);\n\n  last_op = acl_propose_device_op(doq, ACL_DEVICE_OP_HOSTPIPE_READ,\n                                  event); // TODO Change this to the READ op\n\n  if (last_op) {\n    // We managed to enqueue everything.\n    event->last_device_op = last_op;\n    acl_commit_proposed_device_ops(doq);\n    result = 1;\n  } else {\n    // Back off, and wait until later when we have more space in the\n    // device op queue.\n    acl_forget_proposed_device_ops(doq);\n  }\n  return result;\n}\n\n// Submit an op to the device op queue to write hostpipe.\n// Return 1 if we made forward progress, 0 otherwise.\ncl_int acl_submit_write_program_hostpipe_device_op(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n\n  // No user-level scheduling blocks this hostpipe write op\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  // Already enqueued.\n  if (event->last_device_op) {\n    return result;\n  }\n\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n  acl_device_op_t *last_op = 0;\n\n  // Precautionary, but it also nudges the device scheduler to try\n  // to free up old operation slots.\n  acl_forget_proposed_device_ops(doq);\n\n  last_op = acl_propose_device_op(doq, ACL_DEVICE_OP_HOSTPIPE_WRITE, event);\n\n  if (last_op) {\n    // We managed to enqueue everything.\n    event->last_device_op = last_op;\n    acl_commit_proposed_device_ops(doq);\n    result = 1;\n  } else {\n    // Back off, and wait until later when we have more space in the\n    // device op queue.\n    acl_forget_proposed_device_ops(doq);\n  }\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueReadHostPipeINTEL(\n    cl_command_queue command_queue, cl_program program, const char *pipe_symbol,\n    cl_bool blocking_read, void *ptr, size_t size,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n\n  cl_int status = 0;\n\n  // Get context from program, command_queue and event\n  cl_context context = program->context;\n  cl_device_id device = command_queue->device;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid pointer was provided to host data\");\n  }\n\n  if (pipe_symbol == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid Pipe Symbol\");\n  }\n\n  assert(device->loaded_bin != NULL &&\n         \"No loaded binary for enqueue hostpipe read\");\n  acl_device_program_info_t *dev_prog = device->loaded_bin->get_dev_prog();\n\n  auto search = dev_prog->program_hostpipe_map.find(std::string(pipe_symbol));\n\n  if (search == dev_prog->program_hostpipe_map.end()) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Pipe Symbol is not found in the device\");\n  }\n\n  if (search == dev_prog->program_hostpipe_map.end()) {\n\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Pipe Symbol is not found in the device\");\n  }\n\n  cl_event local_event = 0; // used for blocking\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_READ_HOST_PIPE_INTEL, &local_event);\n\n  if (status != CL_SUCCESS)\n    return status;\n\n  local_event->cmd.info.host_pipe_dynamic_info.size = size;\n  local_event->cmd.info.host_pipe_dynamic_info.ptr = ptr;\n  local_event->cmd.info.host_pipe_dynamic_info.blocking = blocking_read;\n  local_event->cmd.info.host_pipe_dynamic_info.logical_name = pipe_symbol;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (blocking_read) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWriteHostPipeINTEL(\n    cl_command_queue command_queue, cl_program program, const char *pipe_symbol,\n    cl_bool blocking_write, const void *ptr, size_t size,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n\n  cl_int status = 0;\n  // Get context from program, command_queue and event\n  cl_context context = program->context;\n  cl_device_id device = command_queue->device;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid pointer was provided to host data\");\n  }\n\n  if (pipe_symbol == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid Pipe Symbol\");\n  }\n\n  assert(device->loaded_bin != NULL &&\n         \"No loaded binary for enqueue hostpipe write\");\n  acl_device_program_info_t *dev_prog = device->loaded_bin->get_dev_prog();\n\n  auto search = dev_prog->program_hostpipe_map.find(std::string(pipe_symbol));\n\n  if (search == dev_prog->program_hostpipe_map.end()) {\n\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Pipe Symbol is not found in the device\");\n  }\n\n  cl_event local_event = 0; // used for blocking\n\n  // Create an event/command to actually move the data at the appropriate time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_WRITE_HOST_PIPE_INTEL, &local_event);\n\n  if (status != CL_SUCCESS)\n    return status;\n\n  local_event->cmd.info.host_pipe_dynamic_info.size = size;\n  local_event->cmd.info.host_pipe_dynamic_info.write_ptr = ptr;\n  local_event->cmd.info.host_pipe_dynamic_info.blocking = blocking_write;\n  local_event->cmd.info.host_pipe_dynamic_info.logical_name = pipe_symbol;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (blocking_write) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n\n  return CL_SUCCESS;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_device_op.cpp",
        "data": "// Copyright (C) 2012-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <assert.h>\n#include <stdio.h>\n#include <string.h>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl_command_queue.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_hostch.h>\n#include <acl_kernel.h>\n#include <acl_mem.h>\n#include <acl_printf.h>\n#include <acl_profiler.h>\n#include <acl_program.h>\n#include <acl_support.h>\n#include <acl_svm.h>\n#include <acl_types.h>\n#include <acl_usm.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n#ifdef ACL_DEVICE_OP_STATS\n#define STATS(X) X\n#else\n#define STATS(X)\n#endif\n\n//\n// Device operations\n// =================\n//\n// All interactions with the device should occur through a scheduled device\n// operation.  We use a single platform-wide queue to manage these\n// operations, to make it simpler to manage exclusion.\n//\n// For example, running a kernel could imply reprogramming the device, and\n// not other activity can occur during reprogramming.\n//\n// Also, it's platform-wide because must support multiple contexts at the\n// same time, all of which can use the same device.\n//\n// An operation is added to the device operation queue when a\n// user-level event/command is *submitted*.\n// That is, no user-level event dependencies are blocking\n// the execution of the device operation.  This means that only device\n// operation blocks forward progress on the device operation queue.\n//\nstatic unsigned l_update_device_op_queue_once(acl_device_op_queue_t *doq);\nstatic void l_record_milestone(acl_device_op_t *op,\n                               cl_profiling_info milestone);\nstatic int l_first_proposed(acl_device_op_queue_t *doq);\nstatic int l_is_noop_migration(acl_device_op_t *op);\nstatic int l_is_mem_in_use(acl_device_op_t *op, acl_device_op_queue_t *doq);\n\n// We must support a mixture of concurrent and serialized device\n// operations:\n//\n//    - Memory transfers must be serialized with respect to each other\n//    due to limitations in our PCIe driver.\n//\n//    - We should support multiple kernels in flight from different user\n//    command queues.\n//\n//    - We must serialize device reprogramming with all other operations.\n//\n//    - Some kernels may have printf calls, and they may stall during\n//    kernel execution.  The stall is resolved by transferring the printf\n//    buffer from the device and printing it out.  No other memory\n//    operations may be in flight during this time.\n//\n// We have several ways to control concurrency:\n//\n// 1. We use the concept of an \"exclusion group\".\n//    Sometimes a single user-level command requires a sequence of\n//    device operations, which we can think of as a transaction.\n//    The operations in a group must occur in sequence, and not overlap.\n//    They appear on the device queue in sequence (without other operations\n//    in between) and are all marked with an identifying exclusion group id.\n//    The group id is just the id of the last device operation in the\n//    group.\n//\n// 2. We use a \"conflict matrix\" to tell us whether a pair of device\n//    operations are forbidden to run concurrently.\n//    The indices are the device operation conflict type, which depends\n//    only on the operation type, and the printf-stall state if it's a\n//    kernel.\n//\n// 3. The reprogramming operation is very SPECIAL.\n//    It can move unbounded numbers of buffers to and from the device, and\n//    it must reprogram the device.\n//    We don't bother to make micro-operations for the buffer moves: it's\n//    just too much bookkeeping, for no benefit.\n//\n//\n\nstatic unsigned char conflict_matrix_half_duplex\n    [ACL_NUM_CONFLICT_TYPES][ACL_NUM_CONFLICT_TYPES] = {\n        // NONE, MEM_READ, MEM_WRITE, MEM_RW, KERNEL,\n        //   PROGRAM, HOSTPIPE_READ, HOSTPIPE_WRITE,\n        //     DEVICE_GLOBAL_READ, DEVICE_GLOBAL_WRITE\n        {0, 0, 0, 0, 0, 1, 0, 0, 0, 0}, // vs. None\n        {0, 1, 1, 1, 0, 1, 1, 1, 1, 1}, // vs. MEM_READ\n        {0, 1, 1, 1, 0, 1, 1, 1, 1, 1}, // vs. MEM_WRITE\n        {0, 1, 1, 1, 0, 1, 1, 1, 1, 1}, // vs. MEM_RW\n        {0, 0, 0, 0, 0, 1, 0, 0, 0, 0}, // vs. KERNEL\n        {1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, // vs. PROGRAM\n        {0, 1, 1, 1, 0, 1, 0, 0, 1, 1}, // vs. HOSTPIPE_READ\n        {0, 1, 1, 1, 0, 1, 0, 0, 1, 1}, // vs. HOSTPIPE_WRITE\n        {0, 1, 1, 1, 0, 1, 1, 1, 1, 1}, // vs. DEVICE_GLOBAL_READ\n        {0, 1, 1, 1, 0, 1, 1, 1, 1, 1}  // vs. DEVICE_GLOBAL_WRITE\n};\n\nstatic unsigned char conflict_matrix_full_duplex\n    [ACL_NUM_CONFLICT_TYPES][ACL_NUM_CONFLICT_TYPES] = {\n\n        // NONE, MEM_READ, MEM_WRITE, MEM_RW, KERNEL,\n        //   PROGRAM, HOSTPIPE_READ, HOSTPIPE_WRITE\n        //     DEVICE_GLOBAL_READ, DEVICE_GLOBAL_WRITE\n        {0, 0, 0, 0, 0, 1, 0, 0, 0, 0}, // vs. None\n        {0, 1, 0, 1, 0, 1, 1, 1, 1, 0}, // vs. MEM_READ\n        {0, 0, 1, 1, 0, 1, 1, 1, 0, 1}, // vs. MEM_WRITE\n        {0, 1, 1, 1, 0, 1, 1, 1, 1, 1}, // vs. MEM_RW\n        {0, 0, 0, 0, 0, 1, 0, 0, 0, 0}, // vs. KERNEL\n        {1, 1, 1, 1, 1, 1, 1, 1, 1, 1}, // vs. PROGRAM\n        {0, 1, 1, 1, 0, 1, 0, 0, 0, 0}, // vs. HOSTPIPE_READ\n        {0, 1, 1, 1, 0, 1, 0, 0, 0, 0}, // vs. HOSTPIPE_WRITE\n        {0, 1, 0, 1, 0, 1, 1, 1, 1, 0}, // vs. DEVICE_GLOBAL_READ\n        {0, 0, 1, 1, 0, 1, 1, 1, 0, 1}  // vs. DEVICE_GLOBAL_WRITE\n};\n\nstatic const char *l_type_name(int op_type) {\n  switch (op_type) {\n  case ACL_DEVICE_OP_NONE:\n    return \"NONE\";\n    break;\n  case ACL_DEVICE_OP_KERNEL:\n    return \"KERNEL\";\n    break;\n  case ACL_DEVICE_OP_MEM_TRANSFER_READ:\n    return \"MEM_TRANSFER_READ\";\n    break;\n  case ACL_DEVICE_OP_MEM_TRANSFER_WRITE:\n    return \"MEM_TRANSFER_WRITE\";\n    break;\n  case ACL_DEVICE_OP_MEM_TRANSFER_COPY:\n    return \"MEM_TRANSFER_COPY\";\n    break;\n  case ACL_DEVICE_OP_REPROGRAM:\n    return \"REPROGRAM\";\n    break;\n  case ACL_DEVICE_OP_MEM_MIGRATION:\n    return \"MIGRATE_MEM_OBJECT\";\n    break;\n  case ACL_DEVICE_OP_USM_MEMCPY:\n    return \"USM_MEMCPY\";\n    break;\n  case ACL_DEVICE_OP_HOSTPIPE_READ:\n    return \"HOSTPIPE_READ\";\n    break;\n  case ACL_DEVICE_OP_HOSTPIPE_WRITE:\n    return \"HOSTPIPE_WRITE\";\n    break;\n  case ACL_DEVICE_OP_DEVICE_GLOBAL_READ:\n    return \"DEVICE_GLOBAL_READ\";\n    break;\n  case ACL_DEVICE_OP_DEVICE_GLOBAL_WRITE:\n    return \"DEVICE_GLOBAL_WRITE\";\n    break;\n  default:\n    return \"<err>\";\n    break;\n  }\n}\n\n// Return the index of the first proposed operation, if any.\nstatic int l_first_proposed(acl_device_op_queue_t *doq) {\n  acl_assert_locked();\n  if (!doq)\n    return ACL_OPEN;\n  if (doq->last_committed != ACL_OPEN)\n    return doq->op[doq->last_committed].link;\n  return doq->first_live; // This is ACL_OPEN if there are no proposed ops.\n}\n\nint acl_first_proposed_device_op_idx(acl_device_op_queue_t *doq) {\n  return l_first_proposed(doq);\n}\n\nstatic void l_dump_op(const char *str, acl_device_op_t *op) {\n  acl_assert_locked();\n\n  if (debug_mode > 0) {\n    static const char *status_name[] = {\"CL_COMPLETE\", \"CL_RUNNING\",\n                                        \"CL_SUBMITTED\", \"CL_QUEUED\",\n                                        \"ACL_PROPOSED\"};\n\n    if (op) {\n      acl_print_debug_msg(\n          \"%s op[%d] { type:%s status:%s(%d) execStatus:%s(%d) group:%d \"\n          \"first:%d last:%d link:%d printfBytes:%d }\\n\",\n          (str ? str : \"\"), op->id, l_type_name(op->info.type),\n          (op->status >= 0 ? status_name[op->status] : \"ERR\"), op->status,\n          (op->execution_status >= 0 ? status_name[op->execution_status]\n                                     : \"ERR\"),\n          op->execution_status, op->group_id, op->first_in_group,\n          op->last_in_group, op->link, op->info.num_printf_bytes_pending);\n    }\n  }\n}\n\n///////////////////////\n// External functions\n\nvoid acl_init_device_op_queue(acl_device_op_queue_t *doq) {\n  acl_init_device_op_queue_limited(doq, ACL_MAX_DEVICE_OPS);\n}\n\nvoid acl_init_device_op_queue_limited(acl_device_op_queue_t *doq,\n                                      int max_allowed) {\n  int i;\n  acl_assert_locked();\n\n  if (!doq)\n    return;\n  doq->num_committed = 0;\n  doq->num_proposed = 0;\n  doq->max_ops = max_allowed;\n  for (i = 0; i < max_allowed; i++) {\n    doq->op[i].id = i;\n    acl_device_op_reset_device_op(doq->op + i);\n  }\n\n  // The live lists are all empty.\n  doq->first_live = ACL_OPEN;\n  doq->last_committed = ACL_OPEN;\n  doq->last_live = ACL_OPEN;\n\n  // Thread the free list\n  doq->first_free = 0;\n  for (i = 0; i < max_allowed - 1; i++) {\n    doq->op[i].link = i + 1;\n  }\n  doq->op[max_allowed - 1].link = ACL_OPEN;\n\n  // Set callback info.\n  // Set up the standard functions.\n  // These are overridden during mock testing.\n  doq->user_data = 0;\n  doq->launch_kernel = acl_launch_kernel;\n  doq->transfer_buffer = acl_mem_transfer_buffer;\n  doq->process_printf = acl_process_printf_buffer;\n  doq->program_device = acl_program_device;\n  doq->migrate_buffer = acl_mem_migrate_buffer;\n  doq->usm_memcpy = acl_usm_memcpy;\n  doq->hostpipe_read = acl_read_program_hostpipe;\n  doq->hostpipe_write = acl_write_program_hostpipe;\n  doq->device_global_read = acl_read_device_global;\n  doq->device_global_write = acl_write_device_global;\n  doq->log_update = 0;\n\n  for (i = 0; i < ACL_MAX_DEVICE; i++) {\n    doq->devices[i] = NULL;\n  }\n\n  STATS(doq->stats = acl_device_op_stats_t());\n}\n\nacl_device_op_conflict_type_t acl_device_op_conflict_type(acl_device_op_t *op) {\n  acl_device_op_conflict_type_t result = ACL_CONFLICT_NONE;\n  cl_event event = NULL;\n  acl_assert_locked();\n\n  if (op) {\n    switch (op->info.type) {\n    case ACL_DEVICE_OP_KERNEL:\n      result = op->info.num_printf_bytes_pending ? ACL_CONFLICT_MEM\n                                                 : ACL_CONFLICT_KERNEL;\n      break;\n    case ACL_DEVICE_OP_MEM_TRANSFER_READ:\n      result = ACL_CONFLICT_MEM_READ;\n      break;\n    case ACL_DEVICE_OP_MEM_TRANSFER_WRITE:\n      result = ACL_CONFLICT_MEM_WRITE;\n      break;\n    case ACL_DEVICE_OP_MEM_TRANSFER_COPY:\n      result = ACL_CONFLICT_MEM;\n      break;\n    case ACL_DEVICE_OP_MEM_MIGRATION:\n      result = ACL_CONFLICT_MEM;\n      break;\n    case ACL_DEVICE_OP_USM_MEMCPY:\n      event = op->info.event;\n      if (acl_event_is_valid(event)) {\n        if (event->cmd.info.usm_xfer.src_on_host) {\n          result = (event->cmd.info.usm_xfer.dst_on_host)\n                       ? ACL_CONFLICT_NONE\n                       : ACL_CONFLICT_MEM_WRITE;\n        } else {\n          result = (event->cmd.info.usm_xfer.dst_on_host)\n                       ? ACL_CONFLICT_MEM_READ\n                       : ACL_CONFLICT_MEM;\n        }\n      }\n      break;\n    case ACL_DEVICE_OP_REPROGRAM:\n      result = ACL_CONFLICT_REPROGRAM;\n      break;\n    case ACL_DEVICE_OP_HOSTPIPE_READ:\n      result = ACL_CONFLICT_HOSTPIPE_READ;\n      break;\n    case ACL_DEVICE_OP_HOSTPIPE_WRITE:\n      result = ACL_CONFLICT_HOSTPIPE_WRITE;\n      break;\n    case ACL_DEVICE_OP_DEVICE_GLOBAL_READ:\n      result = ACL_CONFLICT_DEVICE_GLOBAL_READ;\n      break;\n    case ACL_DEVICE_OP_DEVICE_GLOBAL_WRITE:\n      result = ACL_CONFLICT_DEVICE_GLOBAL_WRITE;\n      break;\n    case ACL_DEVICE_OP_NONE:\n    case ACL_NUM_DEVICE_OP_TYPES:\n      result = ACL_CONFLICT_NONE;\n      break;\n\n      // Intentionally omit the default clause so the compiler\n      // enforces adding a case for new op types\n    }\n  }\n  return result;\n}\n\nacl_device_op_t *acl_propose_device_op(acl_device_op_queue_t *doq,\n                                       acl_device_op_type_t type,\n                                       cl_event event) {\n  return acl_propose_indexed_device_op(doq, type, event, 0);\n}\n\nacl_device_op_t *acl_propose_indexed_device_op(acl_device_op_queue_t *doq,\n                                               acl_device_op_type_t type,\n                                               cl_event event,\n                                               unsigned int index) {\n  acl_device_op_t *result;\n  int idx = 0;\n  acl_assert_locked();\n  if (!doq)\n    return 0;\n\n  // Determine where we'll write this one.\n  if (doq->first_free == ACL_OPEN) {\n    // No space.\n    return 0;\n  }\n  // Use the first node from the free list.\n  idx = doq->first_free;\n  result = doq->op + idx;\n  doq->first_free = result->link;\n  result->link = ACL_OPEN;\n  if (doq->first_live == ACL_OPEN) {\n    doq->first_live = idx;\n  }\n  if (doq->last_live != ACL_OPEN) {\n    doq->op[doq->last_live].link = idx;\n  }\n  doq->last_live = idx;\n  if ((doq->last_committed != ACL_OPEN) &&\n      (doq->op[doq->last_committed].link == ACL_OPEN)) {\n    doq->op[doq->last_committed].link = idx;\n  }\n\n  doq->num_proposed++;\n\n  acl_device_op_reset_device_op(result);\n\n  result->status = ACL_PROPOSED;\n  result->execution_status = ACL_PROPOSED;\n  result->info.type = type;\n  result->info.event = event;\n  result->info.index = index;\n  result->conflict_type = acl_device_op_conflict_type(result);\n\n  return result;\n}\n\nvoid acl_forget_proposed_device_ops(acl_device_op_queue_t *doq) {\n  // Forget all proposed operations.\n  // Two things to do:\n  //    - reset first_proposed\n  //    - roll back num_ops\n\n  acl_assert_locked();\n\n  if (!doq)\n    return;\n  doq->num_proposed = 0;\n  if ((doq->last_live != ACL_OPEN) && (doq->last_live != doq->last_committed)) {\n    // There are proposed operations.\n    // Put them back on the free list.\n    // Connect the end of the proposed list to start of the free list.\n    doq->op[doq->last_live].link = doq->first_free;\n    // Connect the free list head to the start of the proposed list.\n    doq->first_free = l_first_proposed(doq);\n\n    doq->last_live = doq->last_committed;\n    if (doq->last_committed == ACL_OPEN) {\n      doq->first_live = ACL_OPEN;\n    } else {\n      doq->op[doq->last_committed].link = ACL_OPEN;\n    }\n  }\n\n  // Nudge the scheduler.\n  // Free up any completed commands from the front of the queue.\n  (void)acl_update_device_op_queue(doq);\n}\n\nvoid acl_commit_proposed_device_ops(acl_device_op_queue_t *doq) {\n  // Convert proposed operations into committed operations.\n  int idx, i;\n  int group_id;\n  acl_assert_locked();\n\n  if (!doq)\n    return;\n  if (!doq->num_proposed)\n    return;\n\n  // All the device ops get a group id that is the same as the op id of\n  // the last op.\n  group_id = doq->last_live;\n\n  // Now sweep through all proposed operations and update status and group id.\n  for (idx = l_first_proposed(doq), i = 0; idx != ACL_OPEN;\n       idx = doq->op[idx].link, i++) {\n    acl_device_op_t *op = doq->op + idx;\n\n    op->status = CL_QUEUED;\n    op->execution_status = CL_QUEUED;\n    op->group_id = group_id;\n    op->first_in_group = (i == 0);\n    op->last_in_group = (i == doq->num_proposed - 1);\n    l_dump_op(\"commit\", op);\n  }\n  doq->num_committed += doq->num_proposed;\n  doq->num_proposed = 0;\n\n  doq->last_committed = doq->last_live;\n\n  // Nudge the scheduler.\n  acl_update_device_op_queue(doq);\n}\n\nunsigned acl_update_device_op_queue(acl_device_op_queue_t *doq) {\n  unsigned num_updates = 0;\n  unsigned local_num_updates = 0;\n  acl_assert_locked();\n  do {\n    local_num_updates = l_update_device_op_queue_once(doq);\n    num_updates += local_num_updates;\n  } while (local_num_updates);\n  return num_updates;\n}\n\n// Find out which devices are affected by a device op\n// The device op must still be active, or you could dereference an invalid\n// pointer through a dead or recycled cl_event object.\nstatic unsigned\nl_get_devices_affected_for_op(acl_device_op_t *op, unsigned int physical_ids[],\n                              acl_device_op_conflict_type_t conflicts[]) {\n  unsigned int num_devices_affected = 0;\n  // The precondition of the function is device op must be active\n  assert(op);\n  cl_event event = op->info.event;\n  acl_assert_locked();\n\n  if (op) {\n    switch (op->info.type) {\n    case ACL_DEVICE_OP_KERNEL: {\n      if (acl_event_is_valid(event)) {\n        physical_ids[0] =\n            event->cmd.info.ndrange_kernel.device->def.physical_device_id;\n        conflicts[0] = ACL_CONFLICT_KERNEL;\n        num_devices_affected = 1;\n      }\n    } break;\n    case ACL_DEVICE_OP_REPROGRAM: {\n      if (acl_event_is_valid(event)) {\n        auto *dev_prog =\n            event->cmd.type == CL_COMMAND_PROGRAM_DEVICE_INTELFPGA\n                ? event->cmd.info.eager_program->get_dev_prog()\n                : event->cmd.info.ndrange_kernel.dev_bin->get_dev_prog();\n\n        physical_ids[0] = dev_prog->device->def.physical_device_id;\n        conflicts[0] = ACL_CONFLICT_REPROGRAM;\n        num_devices_affected = 1;\n      }\n    } break;\n    case ACL_DEVICE_OP_MEM_TRANSFER_READ:\n    case ACL_DEVICE_OP_MEM_TRANSFER_WRITE:\n    case ACL_DEVICE_OP_MEM_TRANSFER_COPY:\n      if (acl_event_is_valid(event) &&\n          acl_command_queue_is_valid(event->command_queue)) {\n        // Check the IDs of devices from the memory locations associated with\n        // the transfer\n        cl_mem src_mem = event->cmd.info.mem_xfer.src_mem;\n        cl_mem dst_mem = event->cmd.info.mem_xfer.dst_mem;\n        cl_bool device_supports_any_svm = acl_svm_device_supports_any_svm(\n            event->command_queue->device->def.physical_device_id);\n        cl_bool device_supports_physical_memory =\n            acl_svm_device_supports_physical_memory(\n                event->command_queue->device->def.physical_device_id);\n        num_devices_affected = 0;\n\n        // Memory is on the device if all of these are true:\n        //   The memory is not SVM or the device does not support SVM.\n        //   The device support physical memory\n        //   The memory is in the global memory region\n        // Note that a very similar check is made in\n        // l_get_devices_affected_for_op, l_mem_transfer_buffer_explicitly and\n        // l_dump_printf_buffer\n        if ((!src_mem->is_svm || !device_supports_any_svm) &&\n            (device_supports_physical_memory) &&\n            (src_mem->block_allocation->region == &(acl_platform.global_mem))) {\n          // src is on host\n          assert(!src_mem->allocation_deferred);\n          physical_ids[num_devices_affected] =\n              ACL_GET_PHYSICAL_ID(src_mem->block_allocation->range.begin);\n          conflicts[num_devices_affected] = ACL_CONFLICT_MEM_WRITE;\n          num_devices_affected++;\n        }\n        if ((!dst_mem->is_svm || !device_supports_any_svm) &&\n            (device_supports_physical_memory) &&\n            (dst_mem->block_allocation->region == &(acl_platform.global_mem))) {\n          // dst is on host\n          assert(!dst_mem->allocation_deferred);\n          physical_ids[num_devices_affected] =\n              ACL_GET_PHYSICAL_ID(dst_mem->block_allocation->range.begin);\n          conflicts[num_devices_affected] = ACL_CONFLICT_MEM_READ;\n          num_devices_affected++;\n        }\n        // intra device copy\n        if (num_devices_affected == 2 && physical_ids[0] == physical_ids[1]) {\n          num_devices_affected = 1;\n          conflicts[0] = ACL_CONFLICT_MEM;\n        }\n      }\n      break;\n    case ACL_DEVICE_OP_MEM_MIGRATION: {\n      if (acl_event_is_valid(event) &&\n          acl_command_queue_is_valid(event->command_queue)) {\n        cl_bool physical_id_affected[ACL_MAX_DEVICE];\n        acl_mem_migrate_t memory_migration;\n        // Check the IDs of devices from the memory locations associated with\n        // the transfer\n        acl_mem_migrate_wrapper_t *src_mem_list;\n        cl_bool device_supports_any_svm = acl_svm_device_supports_any_svm(\n            event->command_queue->device->def.physical_device_id);\n        cl_bool device_supports_physical_memory =\n            acl_svm_device_supports_physical_memory(\n                event->command_queue->device->def.physical_device_id);\n        unsigned int i;\n        num_devices_affected = 0;\n\n        if (event->cmd.type == CL_COMMAND_MIGRATE_MEM_OBJECTS) {\n          memory_migration = event->cmd.info.memory_migration;\n        } else {\n          memory_migration = event->cmd.info.ndrange_kernel.memory_migration;\n        }\n\n        src_mem_list = memory_migration.src_mem_list;\n        for (i = 0; i < ACL_MAX_DEVICE; ++i) {\n          physical_id_affected[i] = CL_FALSE;\n        }\n\n        for (i = 0; i < memory_migration.num_mem_objects; ++i) {\n          // Source memory is on a device so that device is affected\n          if ((!src_mem_list[i].src_mem->is_svm || !device_supports_any_svm) &&\n              device_supports_physical_memory &&\n              src_mem_list[i].src_mem->block_allocation->region ==\n                  &(acl_platform.global_mem)) {\n            physical_id_affected[ACL_GET_PHYSICAL_ID(\n                src_mem_list[i].src_mem->block_allocation->range.begin)] =\n                CL_TRUE;\n          }\n\n          // destination memory is affected:\n          physical_id_affected[src_mem_list[i].destination_physical_device_id] =\n              CL_TRUE;\n        }\n\n        for (i = 0; i < ACL_MAX_DEVICE; ++i) {\n          if (physical_id_affected[i]) {\n            physical_ids[num_devices_affected] = i;\n            conflicts[num_devices_affected] = ACL_CONFLICT_MEM;\n            num_devices_affected++;\n          }\n        }\n      }\n    } break;\n    case ACL_DEVICE_OP_USM_MEMCPY:\n      if (acl_event_is_valid(event) &&\n          acl_command_queue_is_valid(event->command_queue)) {\n        physical_ids[0] = event->command_queue->device->def.physical_device_id;\n        conflicts[0] = acl_device_op_conflict_type(op);\n        num_devices_affected = 1;\n      }\n      break;\n    case ACL_DEVICE_OP_HOSTPIPE_READ:\n    case ACL_DEVICE_OP_HOSTPIPE_WRITE:\n      if (acl_event_is_valid(event) &&\n          acl_command_queue_is_valid(event->command_queue)) {\n        physical_ids[0] = event->command_queue->device->def.physical_device_id;\n        conflicts[0] = acl_device_op_conflict_type(op);\n        num_devices_affected = 1;\n      }\n      break;\n    case ACL_DEVICE_OP_DEVICE_GLOBAL_READ:\n    case ACL_DEVICE_OP_DEVICE_GLOBAL_WRITE:\n      if (acl_event_is_valid(event) &&\n          acl_command_queue_is_valid(event->command_queue)) {\n        physical_ids[0] = event->command_queue->device->def.physical_device_id;\n        conflicts[0] = acl_device_op_conflict_type(op);\n        num_devices_affected = 1;\n      }\n      break;\n    case ACL_DEVICE_OP_NONE:\n    case ACL_NUM_DEVICE_OP_TYPES:\n      break;\n    }\n  }\n  if (num_devices_affected == 0) {\n    // This case is only valid for unit tests\n    // Make assumptions on which devices are affected\n    // Possible TODO to add for Hostpipe read and write\n    if (event && event->context && op) {\n      if (event->context->num_devices >= 2 &&\n          op->info.type != ACL_DEVICE_OP_KERNEL &&\n          op->info.type != ACL_DEVICE_OP_REPROGRAM) {\n        switch (op->info.type) {\n        case ACL_DEVICE_OP_MEM_TRANSFER_READ:\n          conflicts[0] = ACL_CONFLICT_MEM_READ;\n          conflicts[1] = ACL_CONFLICT_MEM_WRITE;\n          break;\n        case ACL_DEVICE_OP_MEM_TRANSFER_WRITE:\n          conflicts[0] = ACL_CONFLICT_MEM_WRITE;\n          conflicts[1] = ACL_CONFLICT_MEM_READ;\n          break;\n        default:\n          conflicts[0] = acl_device_op_conflict_type(op);\n          conflicts[1] = conflicts[0];\n          break;\n        }\n        physical_ids[0] = event->context->device[0]->def.physical_device_id;\n        physical_ids[1] = event->context->device[1]->def.physical_device_id;\n        num_devices_affected = 2;\n      } else {\n        physical_ids[0] = event->context->device[0]->def.physical_device_id;\n        conflicts[0] = acl_device_op_conflict_type(op);\n        num_devices_affected = 1;\n      }\n    } else {\n      physical_ids[0] = 0;\n      conflicts[0] = acl_device_op_conflict_type(op);\n      num_devices_affected = 1;\n    }\n  }\n  return num_devices_affected;\n}\n\n/**\n * Follows the same checking pattern as acl_mem.c:acl_mem_migrate_buffer(),\n * except doesn't execute the memory operation. This check is used to tell if a\n * migration associated with an op is safe to execute at the current time. It is\n * safe to execute the memory operation if memory is already at destination, or\n * no other running kernel is using that memory reagion.\n */\nstatic int l_is_noop_migration(acl_device_op_t *op) {\n  cl_event event = op->info.event;\n  acl_assert_locked();\n\n  assert(acl_event_is_valid(event));\n  assert(acl_command_queue_is_valid(event->command_queue));\n  acl_mem_migrate_t memory_migration;\n  unsigned int physical_id =\n      event->command_queue->device->def.physical_device_id;\n  cl_bool device_supports_any_svm =\n      acl_svm_device_supports_any_svm(physical_id);\n  cl_bool device_supports_physical_memory =\n      acl_svm_device_supports_physical_memory(physical_id);\n  unsigned int index = op->info.index;\n\n  assert(op->info.type == ACL_DEVICE_OP_MEM_MIGRATION);\n  assert(event->cmd.type !=\n         CL_COMMAND_MIGRATE_MEM_OBJECTS); // This is a task or NDRange\n  memory_migration = event->cmd.info.ndrange_kernel.memory_migration;\n  assert(index < memory_migration.num_mem_objects);\n\n  const cl_mem src_mem = memory_migration.src_mem_list[index].src_mem;\n  acl_block_allocation_t *src_mem_block;\n  const unsigned int dest_device =\n      memory_migration.src_mem_list[index].destination_physical_device_id;\n  const unsigned int dest_mem_id =\n      memory_migration.src_mem_list[index].destination_mem_id;\n\n  assert(src_mem->reserved_allocations[dest_device].size() > dest_mem_id);\n  assert(src_mem->reserved_allocations[dest_device][dest_mem_id] != NULL);\n  src_mem_block = src_mem->block_allocation;\n\n  if (src_mem->allocation_deferred) {\n    src_mem_block = src_mem->reserved_allocations[dest_device][dest_mem_id];\n  }\n\n  // Explicitly not checking if memory is mapped or unmapped. CommandQueue\n  // deals with it is safe to execute migrates with subbuffers or not in case of\n  // fast kernel relaunch, otherwise it is the users responsibility\n  if ((src_mem->is_svm && device_supports_any_svm) ||\n      (!device_supports_physical_memory)) {\n    return 1; // Do nothing for SVM\n  } else if (src_mem_block ==\n                 src_mem->reserved_allocations[dest_device][dest_mem_id] &&\n             src_mem->mem_cpy_host_ptr_pending != 1) {\n    // If memory is already at the destination, do nothing\n    // Make sure any pending migrations have happened!\n    return 1;\n  }\n\n  return 0;\n}\n\n/**\n * Iterates over all kernel_ops in the device_op_queue and returns true if any\n * of them are operating on the same memory regions as the provided MEM_MIGRATE.\n */\nstatic int l_is_mem_in_use(acl_device_op_t *base_op,\n                           acl_device_op_queue_t *doq) {\n  // Get memory blocks\n  acl_assert_locked();\n  cl_event event = base_op->info.event;\n  assert(acl_event_is_valid(event));\n  acl_mem_migrate_t memory_migration_base;\n  unsigned int index = base_op->info.index;\n  assert(base_op->info.type == ACL_DEVICE_OP_MEM_MIGRATION);\n  memory_migration_base = event->cmd.info.ndrange_kernel.memory_migration;\n  assert(index < memory_migration_base.num_mem_objects);\n\n  const cl_mem src_mem = memory_migration_base.src_mem_list[index].src_mem;\n  acl_block_allocation_t *src_mem_block = src_mem->block_allocation;\n  const unsigned int dest_device =\n      memory_migration_base.src_mem_list[index].destination_physical_device_id;\n  const unsigned int dest_mem_id =\n      memory_migration_base.src_mem_list[index].destination_mem_id;\n  acl_block_allocation_t *dst_mem_block =\n      src_mem->reserved_allocations[dest_device][dest_mem_id];\n\n  // Iterate over ops, comparing memory blocks\n  acl_mem_migrate_t memory_migration;\n  acl_device_op_t *op;\n  for (int i = doq->first_live; i != ACL_OPEN && i != l_first_proposed(doq);\n       i = doq->op[i].link) {\n    op = doq->op + i;\n\n    if (op->status > CL_SUBMITTED || op->status <= CL_COMPLETE ||\n        op->info.type != ACL_DEVICE_OP_KERNEL) {\n      continue;\n    }\n\n    memory_migration = op->info.event->cmd.info.ndrange_kernel.memory_migration;\n    for (unsigned j = 0; j < memory_migration.num_mem_objects; j++) {\n      if (memory_migration.src_mem_list[j].src_mem->block_allocation ==\n              src_mem_block ||\n          memory_migration.src_mem_list[j].src_mem->block_allocation ==\n              dst_mem_block) {\n        return 1;\n      }\n    }\n  }\n  return 0;\n}\n\n// Update the device operation queue.\n//\n// Perform several tasks:\n//\n// - Clean up complete operations.\n//\n// - Start (or restart) queued operations if they don't conflict\n// with running operations.\n//\n//\n// Note that the conflict type of an operation is dynamic.  So we must take\n// special care when deciding to start an operation (or unstall a printf\n// kernel).\n//\n// The algorithm is as follows:\n//    - Scan all operations on the queue, looking for pending operations,\n//    and noting their conflict types.\n//    - Then kick off (or restart) the earliest unsubmitted operation which\n//    does not conflict with a pending operation.\n//    (And that kicked off operation now becomes in flight.)\nunsigned l_update_device_op_queue_once(acl_device_op_queue_t *doq) {\n  unsigned num_updates = 0;\n  unsigned some_ops_need_submission = 0;\n  int idx;\n  unsigned k;\n  int last_live_group;\n  int keep_going;\n  acl_assert_locked();\n\n  // We need to know whether any operations are pending, or waiting to\n  // run, organized by conflict type.\n  // This is the id of the earliest pending operation, by conflict\n  // type.\n  int conflicts_on_device[ACL_MAX_DEVICE][ACL_NUM_CONFLICT_TYPES];\n\n  STATS(acl_device_op_stats_t *stats = 0;)\n  acl_assert_locked();\n\n  // Reset pending IDs, and full duplex matrix\n  memset(conflicts_on_device, 0,\n         ACL_MAX_DEVICE * ACL_NUM_CONFLICT_TYPES * sizeof(int));\n\n  if (!doq)\n    return 0;\n\n  STATS(stats = &(doq->stats);)\n  STATS(stats->num_queue_updates++;)\n\n#define FOREACH_OP                                                             \\\n  for (idx = doq->first_live, keep_going = 1;                                  \\\n       keep_going && idx != ACL_OPEN && idx != l_first_proposed(doq);          \\\n       idx = doq->op[idx].link) {                                              \\\n    acl_device_op_t *op = doq->op + idx;\n\n#define FOREACH_OP_END }\n\n  // First pass.\n  // Submit (or unstall) operations as possible.\n  //\n  // For speed, we do a prescan to see if any operations *might* need\n  // submitting (or printf processing).  In the common case there are none\n  // and we can skip the relatively expensive checks for conflicts.\n  //\n  // Determine the earliest operation that needs something done but\n  // which does not conflict with a pending operation.\n  // There are two cases:\n  //    The operation is CL_QUEUED and it does not conflict with a\n  //    pending operation.\n  //\n  //    The operation is a CL_RUNNING kernel that is stalled on printf.\n  //    It notionally conflicts with *itself*, but we can detect that\n  //    case\n\n  // See if any operations need to be submitted (or have printf bytes\n  // picked up).\n  // It's tempting to just keep a count of number of operations in\n  // CL_QUEUED state.  That's easy.\n  // But the tricky part is knowing how many require printf processing.\n  // The num_printf_bytes_pending field is updated during an IRQ and in an\n  // IRQ we can't grab a mutex to protect a *shared* counter variable.\n  // Also, the device op queue needs to handle operations for multiple device\n  // binaries so we can't easily short circuit the check for the common\n  // case where there are no printfs in a *particular* binary.\n  // So just do the basic scan.\n  some_ops_need_submission = 0;\n  FOREACH_OP {\n    if ((CL_QUEUED == op->status) || op->info.num_printf_bytes_pending) {\n      some_ops_need_submission = 1;\n      keep_going = 0;\n    }\n  }\n  FOREACH_OP_END\n\n  if (some_ops_need_submission) {\n    // What is the group id of the last live operation we've looked at?\n    // This is used to make sure only one operation is active per group.\n    last_live_group = -1; // Initialize with an invalid group id.\n    unsigned block_noop_pruning = 0;\n\n    FOREACH_OP {\n      // Check exclusion groups. Only one operation from a group may be in\n      // flight at a time.\n      unsigned j;\n      int is_conflicting = (last_live_group == op->group_id);\n      unsigned int num_devices_affected, physical_ids[ACL_MAX_DEVICE];\n      acl_device_op_conflict_type_t conflicts[ACL_MAX_DEVICE];\n      STATS(stats->num_exclusion_checks++;)\n\n      if (op->status <= CL_QUEUED) {\n        num_devices_affected =\n            l_get_devices_affected_for_op(op, physical_ids, conflicts);\n\n        // cache the devices to check their support for concurrent operations\n        for (k = 0; k < num_devices_affected && !is_conflicting; k++) {\n          if (!doq->devices[physical_ids[k]] && op->info.event &&\n              op->info.event->context) {\n            for (j = 0; j < op->info.event->context->num_devices; j++) {\n              if (op->info.event->context->device[j]->def.physical_device_id ==\n                  physical_ids[k]) {\n                doq->devices[physical_ids[k]] =\n                    &(op->info.event->context->device[j]->def);\n              }\n            }\n          }\n        }\n\n        if (!is_conflicting &&\n            (CL_QUEUED == op->status || op->info.num_printf_bytes_pending)) {\n          // Potentially need to submit it now.\n          STATS(stats->num_conflict_checks++;)\n\n          // See if it conflicts with an pending operation.\n          for (k = 0; k < num_devices_affected && !is_conflicting; k++) {\n            for (j = 0; j < ACL_NUM_CONFLICT_TYPES && !is_conflicting; j++) {\n              unsigned int physical_id = physical_ids[k];\n              if (conflicts_on_device[physical_id][j] > 0) {\n                const acl_device_op_conflict_type_t conflict_type =\n                    conflicts[k];\n                if (doq->devices[physical_id] &&\n                    doq->devices[physical_id]->max_inflight_mem_ops == 2) {\n                  is_conflicting =\n                      is_conflicting ||\n                      conflict_matrix_full_duplex[conflict_type][j];\n                } else {\n                  is_conflicting =\n                      is_conflicting ||\n                      conflict_matrix_half_duplex[conflict_type][j];\n                }\n              }\n            }\n          }\n\n          if (op->info.event &&\n              (op->info.event->cmd.type == CL_COMMAND_TASK ||\n               op->info.event->cmd.type == CL_COMMAND_NDRANGE_KERNEL)) {\n            const cl_kernel kernel =\n                op->info.event->cmd.info.ndrange_kernel.kernel;\n            const cl_device_id device =\n                op->info.event->cmd.info.ndrange_kernel.device;\n            unsigned int fast_launch_depth =\n                kernel->accel_def->fast_launch_depth;\n            if (op->info.type == ACL_DEVICE_OP_MEM_MIGRATION) {\n              if (l_is_noop_migration(op) && !block_noop_pruning) {\n                // ignore if we conflict on the conflict matrix\n                // this is a noop and safe to execute\n                is_conflicting = 0;\n              } else {\n                // Prevents moving memory that is currently being used\n                is_conflicting |= l_is_mem_in_use(op, doq);\n                block_noop_pruning = 1;\n              }\n            } else if (op->info.type == ACL_DEVICE_OP_KERNEL &&\n                       op->status == CL_QUEUED) {\n              // If Kernel-op make sure the relaunch buffer is not full\n              // Ignore this check if this is getting resubmitted due to a\n              // printf (status will not be QUEUED)\n\n              // Check all the submitted and running kernels if they are running\n              // on the same accelerator if they are make sure we don't surpass\n              // the fast relaunch depth.\n              unsigned num_on_device = 0;\n              for (int i = doq->first_live;\n                   i != ACL_OPEN && i != l_first_proposed(doq);\n                   i = doq->op[i].link) {\n                acl_device_op_t *searched_op = doq->op + i;\n                if (searched_op->id == op->id) {\n                  break; // reached the current op\n                }\n                if (searched_op->info.type == ACL_DEVICE_OP_KERNEL &&\n                    (searched_op->status == CL_SUBMITTED ||\n                     searched_op->status == CL_RUNNING)) {\n                  const cl_kernel searched_kernel =\n                      searched_op->info.event->cmd.info.ndrange_kernel.kernel;\n                  const cl_device_id searched_device =\n                      searched_op->info.event->cmd.info.ndrange_kernel.device;\n                  if (searched_kernel->accel_def->id == kernel->accel_def->id &&\n                      searched_device->id == device->id) {\n                    // same accelerator, same device, different op\n                    num_on_device++;\n                  }\n                }\n              }\n              if (num_on_device > fast_launch_depth) {\n                is_conflicting = 1;\n              }\n            }\n          } else if (op->info.event &&\n                     (op->info.type == ACL_DEVICE_OP_MEM_TRANSFER_READ ||\n                      op->info.type == ACL_DEVICE_OP_MEM_TRANSFER_WRITE ||\n                      op->info.type == ACL_DEVICE_OP_MEM_TRANSFER_COPY ||\n                      op->info.type == ACL_DEVICE_OP_HOSTPIPE_READ ||\n                      op->info.type == ACL_DEVICE_OP_HOSTPIPE_WRITE ||\n                      op->info.type == ACL_DEVICE_OP_DEVICE_GLOBAL_READ ||\n                      op->info.type == ACL_DEVICE_OP_DEVICE_GLOBAL_WRITE)) {\n            if (!acl_mem_op_requires_transfer(op->info.event->cmd)) {\n              is_conflicting = 0;\n            }\n          }\n\n          if (!is_conflicting) {\n            // Good to go (or go again).\n            acl_submit_device_op(doq, op);\n            num_updates++;\n            STATS(stats->num_submits++;)\n\n            // Don't try to submit another command on this round.\n            // I don't want to have to bother with updating the\n            // inflight_id.  ( Could just set inflight_id[ conflict_type ] = idx\n            // ? )\n            keep_going = 0;\n            last_live_group = op->group_id;\n          }\n        }\n        if (CL_RUNNING <= op->status) {\n          // It's pending\n          last_live_group = op->group_id;\n          STATS(stats->num_live_op_pending_calcs++;)\n\n          // Force this operation to potentially conflict with any later\n          // operation. This is done to prevent kernels executing before a\n          // reprogram\n          for (k = 0; k < num_devices_affected; k++) {\n            conflicts_on_device[physical_ids[k]][conflicts[k]]++;\n          }\n        }\n      }\n    }\n    FOREACH_OP_END\n  }\n\n  // Second pass.\n  // Process asynchronous status updates from the device.\n  FOREACH_OP {\n    switch (op->status) {\n    default:\n      keep_going = 0;\n      break;\n\n    case CL_QUEUED:\n      STATS(stats->num_queued++;)\n      break; // No status updates possible.\n\n    case CL_SUBMITTED:\n      // Check for status transition\n      STATS(stats->num_submitted++;)\n      if (op->execution_status <= CL_RUNNING) {\n        op->status = CL_RUNNING;\n        l_dump_op(\"status\", op);\n\n        // Running state transition happens when the first op in the group\n        // signals that it has execution state of RUNNING\n        // EXCEPT(!) for groups that contain kernel-ops. Those get only marked\n        // as running when the kernel-op itself is running. This is done so that\n        // only a single event in the command_queue is ever in a running state\n        // event if the next kernel is doing its setup tasks (MEM_MIGRATEs)\n        // in parallel with the previous kernel invocation. -Fast Kernel\n        // Relaunch Feature\n        if (op->info.event) { // protects against segfaults in unit tests, as no\n                              // events are attached\n          if (op->info.type == ACL_DEVICE_OP_KERNEL ||\n              (op->first_in_group &&\n               op->info.event->cmd.type != CL_COMMAND_TASK &&\n               op->info.event->cmd.type != CL_COMMAND_NDRANGE_KERNEL)) {\n            acl_post_status_to_owning_event(op, CL_RUNNING);\n          }\n        }\n\n        if (op->info.type == ACL_DEVICE_OP_KERNEL && op->info.event) {\n          acl_command_info_t cmd = op->info.event->cmd;\n          int can_reset_invocation_wrapper = 1;\n          // Release the invocation image.\n          // But just once, in case someone else grabs it soon after the first\n          // update. The event cmd.info should still be valid since only our own\n          // host thread can release it.\n          if (cmd.info.ndrange_kernel.serialization_wrapper) {\n            // Only release if this event is not part of kernel serialized\n            // execution, or it is the last time the event is submitted as part\n            // of serialized exec. Otherwise, the event will be resubmitted with\n            // the same invocation wrapper, so it is still needed.\n            acl_dev_kernel_invocation_image_t *invocation =\n                cmd.info.ndrange_kernel.serialization_wrapper->image;\n            cl_ulong x = invocation->global_work_offset[0],\n                     y = invocation->global_work_offset[1],\n                     z = invocation->global_work_offset[2];\n            if (x == invocation->global_work_size[0] - 1 &&\n                y == invocation->global_work_size[1] - 1 &&\n                z == invocation->global_work_size[2] - 1)\n              can_reset_invocation_wrapper = 0;\n          }\n          if (can_reset_invocation_wrapper) {\n            acl_kernel_invocation_wrapper_t *invocation_wrapper =\n                cmd.info.ndrange_kernel.invocation_wrapper;\n            if (invocation_wrapper->image->arg_value) {\n              acl_delete_arr(invocation_wrapper->image->arg_value);\n              invocation_wrapper->image->arg_value = 0;\n            }\n            acl_reset_ref_count(invocation_wrapper);\n          }\n        }\n\n        // For test purposes.\n        if (doq->log_update)\n          doq->log_update(doq->user_data, op, CL_RUNNING);\n\n        num_updates++;\n      }\n      break;\n\n    case CL_RUNNING:\n      // Check for status transition\n      STATS(stats->num_running++;)\n      if (op->execution_status <= CL_COMPLETE) {\n        cl_event event = op->info.event;\n        if (op->execution_status == CL_COMPLETE &&\n            op->info.type == ACL_DEVICE_OP_KERNEL && event) {\n          // If this is a kernel event, we might need to resubmit the kernel as\n          // a part of serialization of workitem invariant ndranges.\n          if (op->info.event->cmd.info.ndrange_kernel.serialization_wrapper) {\n            acl_dev_kernel_invocation_image_t *invocation =\n                op->info.event->cmd.info.ndrange_kernel.serialization_wrapper\n                    ->image;\n            cl_ulong x = invocation->global_work_offset[0],\n                     y = invocation->global_work_offset[1],\n                     z = invocation->global_work_offset[2];\n            int resubmit_needed = 0;\n            acl_print_debug_msg(\"global_work_offset = (%lu, %lu, %lu)\\n\", x, y,\n                                z);\n            assert(x < invocation->global_work_size[0] &&\n                   y < invocation->global_work_size[1] &&\n                   z < invocation->global_work_size[2]);\n            if (z < invocation->global_work_size[2] - 1) {\n              z++;\n              resubmit_needed = 1;\n            } else if (y < invocation->global_work_size[1] - 1) {\n              z = 0;\n              y++;\n              resubmit_needed = 1;\n            } else if (x < invocation->global_work_size[0] - 1) {\n              z = 0;\n              y = 0;\n              x++;\n              resubmit_needed = 1;\n            }\n            // else, no resubmit_needed since x ==\n            // invocation->global_work_size[0]-1 && y == x <\n            // invocation->global_work_size[1]-1 && z <\n            // invocation->global_work_size[2]-1\n            if (resubmit_needed) {\n              //  - Keep track of how many times we have relaunched so far\n              //  - Set the execution_status back to CL_RUNNING\n              //  - Relaunch the kernel.\n              invocation->global_work_offset[0] = x;\n              invocation->global_work_offset[1] = y;\n              invocation->global_work_offset[2] = z;\n              // No changes to dependencies or conflicts are expected since the\n              // event was already running.\n              op->execution_status = CL_RUNNING;\n              doq->launch_kernel(doq->user_data, op);\n              break;\n            } else {\n              acl_kernel_invocation_wrapper_t *serialization_wrapper =\n                  op->info.event->cmd.info.ndrange_kernel.serialization_wrapper;\n              if (serialization_wrapper->image->arg_value) {\n                acl_delete_arr(serialization_wrapper->image->arg_value);\n                serialization_wrapper->image->arg_value = 0;\n              }\n              acl_reset_ref_count(serialization_wrapper);\n            }\n          }\n        }\n        // Get profiler data from performance counters in the kernel.\n        // This must come before posting status to the owning event, or\n        // we will never get the profile data out.\n        (void)acl_process_profiler_scan_chain(op);\n\n        if (op->last_in_group) {\n          acl_post_status_to_owning_event(op, op->execution_status);\n        }\n\n        if (op->info.type == ACL_DEVICE_OP_KERNEL && event) {\n          // Mark this accelerator as no longer being occupied.\n          event->cmd.info.ndrange_kernel.dev_bin->get_dev_prog()->current_event\n              [event->cmd.info.ndrange_kernel.kernel->accel_def] = nullptr;\n        }\n\n        // Cut the backpointer. No more interrupts will come.\n        if (event)\n          event->current_device_op = 0;\n\n        // Make sure we don't post again.\n        // This must come after reading profile data since it checks\n        // the event status.\n        op->status = CL_COMPLETE;\n        l_dump_op(\"status\", op);\n\n        // For test purposes.\n        if (doq->log_update)\n          doq->log_update(doq->user_data, op, CL_COMPLETE);\n\n        num_updates++;\n      }\n      break;\n\n    case CL_COMPLETE:\n      // Just waiting to be pruned.\n      // Continue looking at the events that follow.\n      STATS(stats->num_complete++;)\n      break;\n    }\n  }\n  FOREACH_OP_END\n\n  // Third pass:  Prune events.\n  //\n  // A kernel can be in CL_COMPLETE state *and* have printf bytes pending.\n  // (This is the normal case of a kernel that doesn't print much, and so\n  // it doesn't overfill the printf buffer.)\n  // Do not prune the kernel in that case. Wait for the next schedule\n  // update loop to flush that printf buffer out.\n  //\n  // Need to iterate differently since we will be removing ops from the\n  // committed list while we iterate.\n  {\n    int prev_live = ACL_OPEN;\n    idx = doq->first_live;\n    acl_print_debug_msg(\"   pruning: (%d) %d fp %d\\n\", prev_live, idx,\n                        l_first_proposed(doq));\n    while (idx != ACL_OPEN && idx != l_first_proposed(doq)) {\n      acl_device_op_t *op = doq->op + idx;\n      int nxt_idx =\n          op->link; // Need to get this before mutating the operation link.\n\n      if (debug_mode > 0)\n        l_dump_op(\"   consider\", op);\n      if (op->status <= CL_COMPLETE &&\n          (op->info.num_printf_bytes_pending == 0)) {\n        // This operation is finished.  We can prune it.\n\n        if (debug_mode > 0)\n          l_dump_op(\"    pruning\", op);\n\n        // Tricky case. Propagate an error condition from one op to all\n        // the remaining ones in the group.\n        if (op->execution_status < 0 && !op->last_in_group &&\n            op->id != doq->last_committed) {\n          // Just update the next one.  It in turn will keep on propagating.\n          int next_idx = op->link;\n          doq->op[next_idx].execution_status = op->execution_status;\n          doq->op[next_idx].timestamp[CL_COMPLETE] = op->timestamp[CL_COMPLETE];\n          acl_print_debug_msg(\" PROP op[%d] exec_status <-- %d\\n\", next_idx,\n                              op->execution_status);\n        }\n\n        // Now pull this op off the committed list and put it back on the\n        // free list.\n        if (doq->first_live == idx) {\n          doq->first_live = op->link;\n        }\n        if (doq->last_committed == idx) {\n          doq->last_committed = prev_live;\n        }\n        if (doq->last_live == idx) {\n          doq->last_live = prev_live;\n        }\n\n        if (prev_live != ACL_OPEN) {\n          doq->op[prev_live].link = op->link;\n        }\n        op->link = doq->first_free;\n        doq->first_free = idx;\n\n        if (debug_mode > 0)\n          l_dump_op(\"    pruned\", op);\n\n        doq->num_committed--;\n        num_updates++;\n      } else {\n        prev_live = idx;\n      }\n\n      // Advance\n      idx = nxt_idx;\n\n      acl_print_debug_msg(\"   pruning: (%d) %d fp %d\\n\", prev_live, idx,\n                          l_first_proposed(doq));\n    }\n  }\n  return num_updates;\n#undef FOREACH_OP\n#undef FOREACH_OP_END\n}\n\nvoid acl_submit_device_op(acl_device_op_queue_t *doq, acl_device_op_t *op) {\n  acl_assert_locked();\n\n  if (!doq || !op)\n    return;\n\n  if (op->status == CL_QUEUED) {\n    // The regular case.\n    cl_event event = op->info.event;\n    op->status = CL_SUBMITTED;\n    op->timestamp[CL_SUBMITTED] = acl_get_hal()->get_timestamp();\n    l_dump_op(\"status\", op);\n    // Only submit if not an error already.\n    if (op->execution_status >= 0) {\n      op->execution_status = CL_SUBMITTED;\n      if (op->first_in_group) {\n        acl_post_status_to_owning_event(op, CL_SUBMITTED);\n      }\n\n      // Establish the backpointer before actually starting the operation.\n      // This backpointer is used to route HAL interrupt-based status\n      // updates, so it must be in place before those interrupts can be\n      // triggered.\n      if (event)\n        event->current_device_op = op;\n\n#define DOIT(CALL, X)                                                          \\\n  if (doq->CALL)                                                               \\\n  doq->CALL(doq->user_data, X)\n      switch (op->info.type) {\n      case ACL_DEVICE_OP_KERNEL:\n        DOIT(launch_kernel, op);\n        break;\n      case ACL_DEVICE_OP_MEM_TRANSFER_READ:\n      case ACL_DEVICE_OP_MEM_TRANSFER_WRITE:\n      case ACL_DEVICE_OP_MEM_TRANSFER_COPY:\n        DOIT(transfer_buffer, op);\n        break;\n      case ACL_DEVICE_OP_REPROGRAM:\n        DOIT(program_device, op);\n        break;\n      case ACL_DEVICE_OP_MEM_MIGRATION:\n        DOIT(migrate_buffer, op);\n        break;\n      case ACL_DEVICE_OP_USM_MEMCPY:\n        DOIT(usm_memcpy, op);\n        break;\n      case ACL_DEVICE_OP_HOSTPIPE_READ:\n        DOIT(hostpipe_read, op);\n        break;\n      case ACL_DEVICE_OP_HOSTPIPE_WRITE:\n        DOIT(hostpipe_write, op);\n        break;\n      case ACL_DEVICE_OP_DEVICE_GLOBAL_READ:\n        DOIT(device_global_read, op);\n        break;\n      case ACL_DEVICE_OP_DEVICE_GLOBAL_WRITE:\n        DOIT(device_global_write, op);\n        break;\n      default:\n        break;\n      }\n    }\n  } else if (op->info.num_printf_bytes_pending) {\n    // Special case for a kernel stalled on printf, or just completed but\n    // with some printf material to write out.\n    // This entails a blocking memory transfer deep down the callstack.\n    DOIT(process_printf, op);\n    // Can't clear num_printf_bytes_pending here because that sets up a\n    // race with the kernel.\n    // Why? The process_printf call will unstall the kernel, which could\n    // then very quickly block again and issue an IRQ.\n    // So we have to clear the printf byte count before unstalling the\n    // kernel. So that has to happen at a deeper level.\n  }\n#undef DOIT\n}\n\nvoid acl_set_device_op_execution_status(acl_device_op_t *op,\n                                        cl_int new_status) {\n  acl_assert_locked_or_sig();\n\n  if (op) {\n    cl_int effective_status = new_status;\n\n    switch (new_status) {\n    case CL_QUEUED:\n    case CL_SUBMITTED:\n    case CL_RUNNING:\n    case CL_COMPLETE:\n      l_record_milestone(op, (cl_profiling_info)new_status);\n      break;\n    default:\n      if (new_status >= 0) {\n        // Not given a valid status.  Internal error?\n        // Change to a negative status so command queue processing works.\n\n        // we can't call a user callback from inside a signal handler\n        if (!acl_is_inside_sig()) {\n          cl_context context = op->info.event->command_queue->context;\n          if (context) {\n            acl_context_callback(\n                context, \"Internal error: Setting invalid operation status \"\n                         \"with positive value\");\n          }\n        }\n        effective_status = ACL_INVALID_EXECUTION_STATUS; // this is negative\n      }\n      if (effective_status < 0) {\n        // An error condition.  Record as complete.\n        l_record_milestone(op, CL_COMPLETE);\n      }\n      break;\n    }\n\n    op->execution_status = effective_status;\n  }\n}\n\nvoid acl_post_status_to_owning_event(acl_device_op_t *op, int new_status) {\n  acl_assert_locked();\n  if (!op) {\n    return;\n  }\n  cl_event event = op->info.event;\n  if (!acl_event_is_valid(event)) {\n    return;\n  }\n  if (!acl_command_queue_is_valid(event->command_queue)) {\n    return;\n  }\n  cl_command_queue command_queue = event->command_queue;\n\n  // Send the lower level execution_status because it might be\n  // negative, to signal error.\n  event->execution_status = new_status;\n  if (new_status <= CL_COMPLETE) {\n    event->timestamp[CL_COMPLETE] = op->timestamp[CL_COMPLETE];\n\n    if (command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n      command_queue->completed_commands.push_back(event);\n    }\n    command_queue->num_commands_submitted--;\n  } else {\n    event->timestamp[new_status] = op->timestamp[new_status];\n  }\n}\n\n///////////////////////\n// Internal\n\nvoid acl_device_op_reset_device_op(acl_device_op_t *op) {\n  acl_assert_locked();\n\n  op->status = CL_COMPLETE;\n  op->execution_status = CL_COMPLETE;\n  op->timestamp[CL_SUBMITTED] = 0;\n  op->timestamp[CL_RUNNING] = 0;\n  op->timestamp[CL_COMPLETE] = 0;\n  op->timestamp[CL_QUEUED] = 0;\n  op->first_in_group = 1;\n  op->last_in_group = 1;\n  op->group_id = op->id; // Normally each operation is in its own group.\n  op->info.type = ACL_DEVICE_OP_NONE;\n  op->info.event = 0;\n  op->info.index = 0;\n  op->info.num_printf_bytes_pending = 0;\n}\n\nstatic void l_record_milestone(acl_device_op_t *op,\n                               cl_profiling_info milestone) {\n  acl_assert_locked_or_sig();\n\n  if (op) {\n    cl_ulong ts = acl_get_hal()->get_timestamp();\n    switch (milestone) {\n    case CL_QUEUED:\n    case CL_SUBMITTED:\n    case CL_RUNNING:\n    case CL_COMPLETE:\n      acl_print_debug_msg(\" devop[%d]->timestamp[%d] = %lu\\n\", op->id,\n                          (int)(milestone), (unsigned long)(ts));\n      op->timestamp[milestone] = ts;\n      break;\n    default:\n      break;\n    }\n  }\n}\n\nACL_EXPORT\nvoid acl_device_op_dump_stats(void) {\n#ifdef ACL_DEVICE_OP_STATS\n  acl_device_op_stats_t *stats = &(acl_platform.device_op_queue.stats);\n  acl_assert_locked();\n  printf(\"Device op stats:\\n\");\n\n#define PF(X)                                                                  \\\n  printf(\"  %-25s %12u %12.6f\\n\", #X, stats->num_##X,                          \\\n         ((float)stats->num_##X / (float)stats->num_queue_updates));\n  PF(queue_updates)\n  PF(exclusion_checks)\n  PF(conflict_checks)\n  PF(submits)\n  PF(live_op_pending_calcs)\n  PF(queued)\n  PF(running)\n  PF(complete)\n  fflush(stdout);\n#undef PF\n#else\n  printf(\"Device op stats are not available\\n\");\n#endif\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_device.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <assert.h>\n#include <string.h>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_globals.h>\n#include <acl_mem.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_types.h>\n#include <acl_util.h>\n#include <acl_visibility.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\nsize_t l_get_device_official_name(unsigned int physical_device_id, char *name,\n                                  size_t size, const char *raw_name);\nsize_t l_get_device_vendor_name(unsigned int physical_device_id, char *name,\n                                size_t size);\nstatic cl_int l_get_device_core_temperature(unsigned int physical_device_id);\n\n//////////////////////////////\n// OpenCL API\n\n// Return info devices available to the user.\n// **Include predefined devices like DMA**\n// Can be used to query the number of available devices, or to get their\n// ids, or both.\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetDeviceIDsIntelFPGA(\n    cl_platform_id platform, cl_device_type device_type, cl_uint num_entries,\n    cl_device_id *devices, cl_uint *num_devices) {\n  cl_int status = CL_SUCCESS;\n  cl_uint num_matched = 0;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_platform_is_valid(platform)) {\n    return CL_INVALID_PLATFORM;\n  }\n  VALIDATE_ARRAY_OUT_ARGS(num_entries, devices, num_devices, 0);\n\n  switch (device_type) {\n  case CL_DEVICE_TYPE_CPU:\n  case CL_DEVICE_TYPE_GPU:\n    // We don't have these.\n    // At least, we don't support sending Kernels to the host CPU.\n    break;\n\n  case CL_DEVICE_TYPE_ALL:\n  case CL_DEVICE_TYPE_DEFAULT:\n  case CL_DEVICE_TYPE_ACCELERATOR:\n  case 0xFFFFFFFFFFFFFFFF: {\n    unsigned int idev;\n    // Include the predefined devices.\n    for (idev = 0; idev < acl_platform.num_devices; idev++) {\n      if (devices && num_matched < num_entries) {\n        devices[num_matched] = &(acl_platform.device[idev]);\n      }\n      num_matched++;\n    }\n  } break;\n\n  default:\n    return CL_INVALID_DEVICE_TYPE;\n    break;\n  }\n\n  if (num_matched == 0) {\n    status = CL_DEVICE_NOT_FOUND;\n  }\n  if (num_devices) {\n    *num_devices = num_matched;\n  }\n\n  return status;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetDeviceIDs(cl_platform_id platform,\n                                               cl_device_type device_type,\n                                               cl_uint num_entries,\n                                               cl_device_id *devices,\n                                               cl_uint *num_devices) {\n  return clGetDeviceIDsIntelFPGA(platform, device_type, num_entries, devices,\n                                 num_devices);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetDeviceInfoIntelFPGA(\n    cl_device_id device, cl_device_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  char name_buf[MAX_NAME_SIZE];\n  acl_result_t result;\n  cl_context context = 0;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n#ifndef REMOVE_VALID_CHECKS\n  if (!acl_device_is_valid_ptr(device)) {\n    return CL_INVALID_DEVICE;\n  }\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          0);\n#endif\n\n  RESULT_INIT;\n\n  // For these properties, the device needs to have been opened before (but not\n  // necessarily be currently opened) In case the device has never been opened\n  // before, we need to open it by\n  //  creating a context, then read the info, then release the context after.\n  //  This step is to create a context. If that fails, then just return the\n  //  error status.\n  if (!device->has_been_opened) {\n    cl_int status;\n    switch (param_name) {\n    case CL_DEVICE_ENDIAN_LITTLE:\n    case CL_DEVICE_GLOBAL_MEM_SIZE:\n    case CL_DEVICE_LOCAL_MEM_SIZE:\n    case CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE:\n    case CL_DEVICE_MAX_MEM_ALLOC_SIZE:\n    case CL_DEVICE_VENDOR:\n      context = clCreateContext(0, 1, &device, NULL, NULL, &status);\n      if (status != CL_SUCCESS) {\n        return status;\n      }\n      break;\n    }\n  }\n  // else, the devices has been opened before, we just need to read the info. No\n  // extra work (e.g. creating the context) needed\n\n  // For these properties, the device needs to be currently opened.\n  // In case the device is not currently opened, we need to open it by\n  //  creating a context, then read the info, then release the context after.\n  //  This step is to create a context. If that fails, then just return the\n  //  error status.\n  //    - CL_DEVICE_AVAILABLE is a special case, we confirm we can open the\n  //    device,\n  //      but if we fail we simply set the result to 0, not returning an error\n  std::string builtinkernels;\n  if (!device->opened_count) {\n    cl_int status;\n    switch (param_name) {\n    case CL_DEVICE_NAME:\n    case CL_DEVICE_CORE_TEMPERATURE_INTELFPGA:\n    case CL_DEVICE_SVM_CAPABILITIES:\n    case CL_DEVICE_AVAILABLE:\n      context = clCreateContext(0, 1, &device, NULL, NULL, &status);\n      if (status != CL_SUCCESS) {\n        if (param_name == CL_DEVICE_AVAILABLE) { // special case\n          RESULT_BOOL(0);                        // it must not be available\n        } else {\n          return status;\n        }\n      } else if (param_name == CL_DEVICE_AVAILABLE) {\n        RESULT_BOOL(\n            1); // successfully create a context means device is available\n      }\n      break;\n    }\n  } else { // the device is currently opened. No extra work (e.g. creating the\n           // context) needed\n    // For CL_DEVICE_AVAILABLE, if the device is currently opened, then the\n    // device is available.\n    if (param_name == CL_DEVICE_AVAILABLE) {\n      RESULT_BOOL(1);\n    }\n    // For other properties, we just need to read the info.\n  }\n\n  // Read the info and set the result value\n  switch (param_name) {\n  case CL_DEVICE_ADDRESS_BITS:\n    RESULT_UINT(device->address_bits);\n    break;\n  // Assume all devices are EMBEDDED_PROFILE, with no kernel compiler\n  case CL_DEVICE_COMPILER_AVAILABLE:\n  case CL_DEVICE_LINKER_AVAILABLE:\n    RESULT_BOOL(0);\n    break;\n  case CL_DEVICE_ENDIAN_LITTLE:\n    RESULT_BOOL(device->def.autodiscovery_def.is_big_endian ? (cl_bool)CL_FALSE\n                                                            : (cl_bool)CL_TRUE);\n    break;\n  case CL_DEVICE_ERROR_CORRECTION_SUPPORT:\n    RESULT_BOOL(0);\n    break;\n\n  case CL_DEVICE_EXTENSIONS:\n    RESULT_STR(acl_platform.extensions);\n    break;\n\n  case CL_DEVICE_GLOBAL_MEM_CACHE_TYPE:\n    RESULT_INT(CL_READ_ONLY_CACHE);\n    break;\n  case CL_DEVICE_GLOBAL_MEM_CACHE_SIZE:\n    RESULT_ULONG(32768);\n    break;\n  case CL_DEVICE_GLOBAL_MEM_CACHELINE_SIZE:\n    RESULT_INT(0);\n    break;\n  case CL_DEVICE_GLOBAL_MEM_SIZE: {\n#ifdef __arm__\n    // TODO: legacy code here, need to verify correctness with ARM board\n    auto gmem_id = acl_get_default_device_global_memory(device->def);\n    if (gmem_id < 0) {\n      RESULT_INT(0);\n      break;\n    }\n    cl_ulong size =\n        ACL_RANGE_SIZE(device->def.autodiscovery_def.global_mem_defs[gmem_id]\n                           .get_usable_range());\n    // on SoC board, two DDR systems are not equivalent\n    // so only half can be accessed with a single alloc.\n    size /= 2;\n#else\n    cl_ulong size = 0;\n    for (unsigned gmem_idx = 0;\n         gmem_idx < device->def.autodiscovery_def.num_global_mem_systems;\n         gmem_idx++) {\n      if (device->def.autodiscovery_def.global_mem_defs[gmem_idx].type ==\n          ACL_GLOBAL_MEM_DEVICE_PRIVATE) {\n        size += ACL_RANGE_SIZE(\n            device->def.autodiscovery_def.global_mem_defs[gmem_idx].range);\n      }\n    }\n#endif\n    RESULT_ULONG(size);\n    break;\n  }\n\n  case CL_DEVICE_IMAGE_SUPPORT:\n#if ACL_SUPPORT_IMAGES == 1\n    RESULT_BOOL(CL_TRUE);\n#else\n    RESULT_BOOL(CL_FALSE);\n#endif\n    break;\n\n  case CL_DEVICE_LOCAL_MEM_TYPE:\n    RESULT_ENUM(CL_LOCAL);\n    break;\n  case CL_DEVICE_LOCAL_MEM_SIZE:\n    RESULT_ULONG(device->min_local_mem_size);\n    break;\n\n  case CL_DEVICE_MAX_CLOCK_FREQUENCY:\n    RESULT_INT(1000);\n    break; // A gigahertz...\n  case CL_DEVICE_MAX_COMPUTE_UNITS:\n    RESULT_INT(acl_platform.max_compute_units);\n    break;\n  case CL_DEVICE_MAX_CONSTANT_ARGS:\n    RESULT_UINT(acl_platform.max_constant_args);\n    break;\n\n  case CL_DEVICE_MAX_CONSTANT_BUFFER_SIZE: {\n#ifdef __arm__\n    // TODO: legacy code here, need to verify correctness with ARM board\n    auto gmem_id = acl_get_default_device_global_memory(device->def);\n    if (gmem_id < 0) {\n      RESULT_INT(0);\n      break;\n    }\n    cl_ulong size =\n        ACL_RANGE_SIZE(device->def.autodiscovery_def.global_mem_defs[gmem_id]\n                           .get_usable_range()) /\n        4;\n    // Cut by 2 again, see comment for CL_DEVICE_GLOBAL_MEM_SIZE\n    size /= 2;\n#else\n    // Return the maximum size of a single allocation to the constant memory\n    // (i.e., global memory)\n    cl_ulong size = 0;\n    for (unsigned gmem_idx = 0;\n         gmem_idx < device->def.autodiscovery_def.num_global_mem_systems;\n         gmem_idx++) {\n      if (device->def.autodiscovery_def.global_mem_defs[gmem_idx].type ==\n          ACL_GLOBAL_MEM_DEVICE_PRIVATE) {\n        cl_ulong curr_size = 0;\n        // TODO: investigate if ACL_MEM_ALIGN of 0x400 is still required to\n        //       perform device allocations to memory with 0 starting address\n        acl_system_global_mem_allocation_type_t alloc_type =\n            device->def.autodiscovery_def.global_mem_defs[gmem_idx]\n                .allocation_type;\n        if (!alloc_type || (alloc_type & ACL_GLOBAL_MEM_DEVICE_ALLOCATION)) {\n          curr_size = ACL_RANGE_SIZE(\n              device->def.autodiscovery_def.global_mem_defs[gmem_idx]\n                  .get_usable_range());\n        } else {\n          curr_size = ACL_RANGE_SIZE(\n              device->def.autodiscovery_def.global_mem_defs[gmem_idx].range);\n        }\n        if (curr_size > size) {\n          size = curr_size;\n        }\n      }\n    }\n    // Note: devices not of type CL_DEVICE_TYPE_CUSTOM and conformant\n    // to OpenCL 1.2 spec will return size at least of 64KB here\n#endif\n    RESULT_ULONG(size);\n  } break;\n  case CL_DEVICE_MAX_MEM_ALLOC_SIZE: {\n#ifdef __arm__\n    // TODO: legacy code here, need to verify correctness with ARM board\n    auto gmem_id = acl_get_default_device_global_memory(device->def);\n    if (gmem_id < 0) {\n      RESULT_INT(0);\n      break;\n    }\n    cl_ulong size =\n        ACL_RANGE_SIZE(device->def.autodiscovery_def.global_mem_defs[gmem_id]\n                           .get_usable_range());\n    // on SoC board, two DDR systems are not equivalent\n    // so only half can be accessed with a single alloc.\n\n    // If user uses only device memory on 2DDR board, they can use size/2 mem\n    const char *override =\n        acl_getenv(\"CL_DEVICE_DISABLE_ARM_MEMSIZE_SAFEGUARD\");\n    if (override) {\n      size = size / 2;\n    } else {\n      size = size / 8;\n    }\n#else\n    cl_ulong size = 0;\n    for (unsigned gmem_idx = 0;\n         gmem_idx < device->def.autodiscovery_def.num_global_mem_systems;\n         gmem_idx++) {\n      if (device->def.autodiscovery_def.global_mem_defs[gmem_idx].type ==\n          ACL_GLOBAL_MEM_DEVICE_PRIVATE) {\n        cl_ulong curr_size = 0;\n        // TODO: investigate if ACL_MEM_ALIGN of 0x400 is still required to\n        //       perform device allocations to memory with 0 starting address\n        acl_system_global_mem_allocation_type_t alloc_type =\n            device->def.autodiscovery_def.global_mem_defs[gmem_idx]\n                .allocation_type;\n        if (!alloc_type || (alloc_type & ACL_GLOBAL_MEM_DEVICE_ALLOCATION)) {\n          curr_size = ACL_RANGE_SIZE(\n              device->def.autodiscovery_def.global_mem_defs[gmem_idx]\n                  .get_usable_range());\n        } else {\n          curr_size = ACL_RANGE_SIZE(\n              device->def.autodiscovery_def.global_mem_defs[gmem_idx].range);\n        }\n        if (curr_size > size) {\n          size = curr_size;\n        }\n      }\n    }\n    // Note: devices not of type CL_DEVICE_TYPE_CUSTOM and\n    // conformant to OpenCL 1.2 spec will return size at least of\n    // max(CL_DEVICE_GLOBAL_MEM_SIZE/4, 1*1024*1024) here\n#endif\n    RESULT_ULONG(size);\n  } break;\n\n  case CL_DEVICE_MAX_PARAMETER_SIZE:\n    RESULT_SIZE_T(acl_platform.max_param_size);\n    break;\n  // Constant memory is global memory, so just consider all of global mem.\n  case CL_DEVICE_MAX_WORK_GROUP_SIZE:\n    RESULT_SIZE_T(acl_platform.max_work_group_size);\n    break;\n  case CL_DEVICE_MAX_WORK_ITEM_DIMENSIONS:\n    RESULT_INT(acl_platform.max_work_item_dimensions);\n    break;\n  case CL_DEVICE_MAX_WORK_ITEM_SIZES:\n    RESULT_SIZE_T3(acl_platform.max_work_item_sizes,\n                   acl_platform.max_work_item_sizes,\n                   acl_platform.max_work_item_sizes);\n    break;\n  case CL_DEVICE_MEM_BASE_ADDR_ALIGN:\n    RESULT_UINT(acl_platform.mem_base_addr_align);\n    break;\n  case CL_DEVICE_MIN_DATA_TYPE_ALIGN_SIZE:\n    RESULT_INT(acl_platform.min_data_type_align_size);\n    break;\n  case CL_DEVICE_NAME:\n    l_get_device_official_name(device->def.physical_device_id, name_buf,\n                               MAX_NAME_SIZE,\n                               device->def.autodiscovery_def.name.c_str());\n    RESULT_STR(name_buf);\n    break;\n  case CL_DEVICE_BUILT_IN_KERNELS: {\n    for (acl_accel_def_t accel : device->def.autodiscovery_def.accel) {\n      builtinkernels.append(accel.iface.name);\n      builtinkernels.append(\";\");\n    }\n    if (builtinkernels.length() > 0) {\n      builtinkernels.pop_back();\n    } // remove trailing semicolon\n    RESULT_STR(builtinkernels.c_str());\n  } break;\n  case CL_DEVICE_PLATFORM:\n    RESULT_PTR(&acl_platform);\n    break;\n  case CL_DEVICE_PRINTF_BUFFER_SIZE:\n    RESULT_SIZE_T(acl_platform.printf_buffer_size);\n    break;\n  case CL_DEVICE_PREFERRED_INTEROP_USER_SYNC:\n    RESULT_BOOL(CL_TRUE);\n    break;\n  case CL_DEVICE_PARENT_DEVICE:\n    RESULT_PTR(NULL);\n    break;\n  case CL_DEVICE_PARTITION_MAX_SUB_DEVICES:\n    RESULT_UINT(0);\n    break;\n  case CL_DEVICE_PARTITION_PROPERTIES:\n    RESULT_INT(0);\n    break;\n  case CL_DEVICE_PARTITION_AFFINITY_DOMAIN:\n    RESULT_BITFIELD(0);\n    break;\n  case CL_DEVICE_PARTITION_TYPE:\n    RESULT_INT(0);\n    break;\n  case CL_DEVICE_REFERENCE_COUNT:\n    RESULT_UINT(1);\n    break;\n  // Assume 32 bits is good\n  case CL_DEVICE_PREFERRED_VECTOR_WIDTH_CHAR:\n    RESULT_INT(4);\n    break;\n  case CL_DEVICE_PREFERRED_VECTOR_WIDTH_SHORT:\n    RESULT_INT(2);\n    break;\n  case CL_DEVICE_PREFERRED_VECTOR_WIDTH_INT:\n    RESULT_INT(1);\n    break;\n  case CL_DEVICE_PREFERRED_VECTOR_WIDTH_LONG:\n    RESULT_INT(1);\n    break;\n  case CL_DEVICE_PREFERRED_VECTOR_WIDTH_FLOAT:\n    RESULT_INT(1);\n    break;\n  case CL_DEVICE_PREFERRED_VECTOR_WIDTH_DOUBLE:\n    RESULT_INT(ACL_SUPPORT_DOUBLE != 0);\n    break; // We support cl_khr_fp64\n  case CL_DEVICE_PREFERRED_VECTOR_WIDTH_HALF:\n    RESULT_INT(0);\n    break; // We don't support half\n  case CL_DEVICE_NATIVE_VECTOR_WIDTH_CHAR:\n    RESULT_INT(4);\n    break;\n  case CL_DEVICE_NATIVE_VECTOR_WIDTH_SHORT:\n    RESULT_INT(2);\n    break;\n  case CL_DEVICE_NATIVE_VECTOR_WIDTH_INT:\n    RESULT_INT(1);\n    break;\n  case CL_DEVICE_NATIVE_VECTOR_WIDTH_LONG:\n    RESULT_INT(1);\n    break;\n  case CL_DEVICE_NATIVE_VECTOR_WIDTH_FLOAT:\n    RESULT_INT(1);\n    break;\n  case CL_DEVICE_NATIVE_VECTOR_WIDTH_DOUBLE:\n    RESULT_INT(ACL_SUPPORT_DOUBLE != 0);\n    break; // We support cl_khr_fp64\n  case CL_DEVICE_NATIVE_VECTOR_WIDTH_HALF:\n    RESULT_INT(0);\n    break; // We don't support half\n\n  case CL_DEVICE_PROFILE:\n    RESULT_STR(\"EMBEDDED_PROFILE\");\n    break; // no kernel compiler\n  case CL_DEVICE_VENDOR:\n    l_get_device_vendor_name(device->def.physical_device_id, name_buf,\n                             MAX_NAME_SIZE);\n    RESULT_STR(name_buf);\n    break;\n\n  case CL_DEVICE_PROFILING_TIMER_RESOLUTION:\n    RESULT_SIZE_T(1);\n    break; // guess\n  case CL_DEVICE_MAX_SAMPLERS:\n    RESULT_INT(ACL_MAX_SAMPLER);\n    break;\n  case CL_DEVICE_HALF_FP_CONFIG:\n    RESULT_INT(0);\n    break;\n  case CL_DEVICE_EXECUTION_CAPABILITIES:\n    RESULT_BITFIELD(CL_EXEC_KERNEL);\n    break;\n\n  // CL_DEVICE_QUEUE_ON_HOST_PROPERTIES replaces CL_DEVICE_QUEUE_PROPERTIES in\n  // OpenCL 2.0. Both macros are the same value so this is backwards compatible.\n  case CL_DEVICE_QUEUE_ON_HOST_PROPERTIES:\n    RESULT_BITFIELD(acl_platform.queue_properties);\n    break;\n  case CL_DEVICE_SINGLE_FP_CONFIG:\n    RESULT_BITFIELD(acl_platform.single_fp_config);\n    break;\n  case CL_DEVICE_DOUBLE_FP_CONFIG:\n    RESULT_BITFIELD(acl_platform.double_fp_config);\n    break;\n  case CL_DEVICE_TYPE:\n    RESULT_BITFIELD(device->type);\n    break;\n  case CL_DEVICE_VENDOR_ID:\n    RESULT_UINT(device->vendor_id);\n    break;\n  case CL_DEVICE_VERSION:\n    RESULT_STR(device->version);\n    break;\n  case CL_DRIVER_VERSION:\n    RESULT_STR(device->driver_version);\n    break;\n\n  case CL_DEVICE_HOST_UNIFIED_MEMORY:\n    RESULT_BOOL(0);\n    break;\n  case CL_DEVICE_OPENCL_C_VERSION:\n    // The OpenCL 1.2 api conformance test forces there to be a space after the\n    // version number.\n#ifdef ACL_120\n    RESULT_STR(\"OpenCL C 1.2 \");\n    break;\n#else\n    RESULT_STR(\"OpenCL C 1.0 \");\n    break;\n#endif\n  // Image stuff. Conformance tests still query these\n  case CL_DEVICE_MAX_READ_IMAGE_ARGS:\n  case CL_DEVICE_MAX_WRITE_IMAGE_ARGS: // This option is only for backward\n                                       // compatibility, MAX_READ_WRITE should\n                                       // be used instead.\n  case CL_DEVICE_MAX_READ_WRITE_IMAGE_ARGS:\n    RESULT_INT(128);\n    break;\n  case CL_DEVICE_IMAGE2D_MAX_WIDTH:\n  case CL_DEVICE_IMAGE2D_MAX_HEIGHT:\n    RESULT_SIZE_T(16384);\n    break;\n  case CL_DEVICE_IMAGE3D_MAX_WIDTH:\n  case CL_DEVICE_IMAGE3D_MAX_HEIGHT:\n  case CL_DEVICE_IMAGE3D_MAX_DEPTH:\n    RESULT_SIZE_T(2048);\n    break;\n  case CL_DEVICE_IMAGE_MAX_BUFFER_SIZE:\n    RESULT_SIZE_T(65536);\n    break; // Minimum is 65536.\n  case CL_DEVICE_IMAGE_MAX_ARRAY_SIZE:\n    RESULT_SIZE_T(2048);\n    break;\n  case CL_DEVICE_IMAGE_PITCH_ALIGNMENT:\n    RESULT_UINT(2);\n    break; // must be a power of 2.\n  case CL_DEVICE_IMAGE_BASE_ADDRESS_ALIGNMENT:\n    RESULT_UINT(acl_platform.mem_base_addr_align);\n    break;\n  case CL_DEVICE_CORE_TEMPERATURE_INTELFPGA:\n    RESULT_INT(l_get_device_core_temperature(device->def.physical_device_id));\n    break;\n\n  // Returns a bit field listing all of the supported types of SVM memory\n  // suppoted by this device\n  case CL_DEVICE_SVM_CAPABILITIES: {\n    int memories_supported;\n    acl_get_hal()->has_svm_memory_support(device->def.physical_device_id,\n                                          &memories_supported);\n    RESULT_INT(memories_supported);\n  } break;\n  case CL_DEVICE_PREFERRED_PLATFORM_ATOMIC_ALIGNMENT:\n    RESULT_UINT(0);\n    break; // Aligned to the natural size of the type\n  case CL_DEVICE_PREFERRED_GLOBAL_ATOMIC_ALIGNMENT:\n    RESULT_UINT(0);\n    break; // Aligned to the natural size of the type\n  case CL_DEVICE_PREFERRED_LOCAL_ATOMIC_ALIGNMENT:\n    RESULT_UINT(0);\n    break; // Aligned to the natural size of the type\n  case CL_DEVICE_MAX_PIPE_ARGS:\n    RESULT_UINT(acl_platform.max_pipe_args);\n    break;\n  case CL_DEVICE_PIPE_MAX_ACTIVE_RESERVATIONS:\n    RESULT_UINT(acl_platform.pipe_max_active_reservations);\n    break;\n  case CL_DEVICE_PIPE_MAX_PACKET_SIZE:\n    RESULT_UINT(acl_platform.pipe_max_packet_size);\n    break;\n\n  // USM properties\n  case CL_DEVICE_HOST_MEM_CAPABILITIES_INTEL:\n  case CL_DEVICE_DEVICE_MEM_CAPABILITIES_INTEL:\n  case CL_DEVICE_SINGLE_DEVICE_SHARED_MEM_CAPABILITIES_INTEL: {\n    unsigned int capabilities;\n    if (param_name == CL_DEVICE_HOST_MEM_CAPABILITIES_INTEL) {\n      capabilities = device->def.host_capabilities;\n    } else if (param_name == CL_DEVICE_DEVICE_MEM_CAPABILITIES_INTEL) {\n      if (acl_platform.offline_mode == ACL_CONTEXT_MPSIM) {\n        // Device allocations are not supported in IPA flow which\n        // currently runs in simulation only. Return true device\n        // allocation capabilities in this case.\n        capabilities = device->def.device_capabilities;\n      } else {\n        // IPA is not supported with hardware flow currently, and\n        // device allocations are supported for all legacy devices\n        capabilities = ACL_MEM_CAPABILITY_SUPPORTED;\n      }\n    } else {\n      capabilities = device->def.shared_capabilities;\n    }\n\n    bool supported = capabilities & ACL_MEM_CAPABILITY_SUPPORTED;\n    bool concurrent = capabilities & ACL_MEM_CAPABILITY_CONCURRENT;\n    bool atomic = capabilities & ACL_MEM_CAPABILITY_ATOMIC;\n\n    if (!supported) {\n      RESULT_BITFIELD(0);\n    }\n    if (atomic && concurrent) {\n      RESULT_BITFIELD(CL_UNIFIED_SHARED_MEMORY_CONCURRENT_ATOMIC_ACCESS_INTEL);\n    }\n    if (concurrent) {\n      RESULT_BITFIELD(CL_UNIFIED_SHARED_MEMORY_CONCURRENT_ACCESS_INTEL);\n    }\n    if (atomic) {\n      RESULT_BITFIELD(CL_UNIFIED_SHARED_MEMORY_ATOMIC_ACCESS_INTEL);\n    }\n    if (supported) {\n      RESULT_BITFIELD(CL_UNIFIED_SHARED_MEMORY_ACCESS_INTEL);\n    }\n  } break;\n  case CL_DEVICE_CROSS_DEVICE_SHARED_MEM_CAPABILITIES_INTEL: {\n    RESULT_BITFIELD(0);\n  } break;\n\n  case CL_DEVICE_SHARED_SYSTEM_MEM_CAPABILITIES_INTEL: {\n    RESULT_BITFIELD(0);\n  } break;\n\n  case CL_DEVICE_ATOMIC_FENCE_CAPABILITIES: {\n    cl_bitfield res = 0;\n    res = res | CL_DEVICE_ATOMIC_SCOPE_WORK_ITEM;\n    res = res | CL_DEVICE_ATOMIC_SCOPE_WORK_GROUP;\n    res = res | CL_DEVICE_ATOMIC_SCOPE_DEVICE;\n    RESULT_BITFIELD(res);\n  } break;\n\n  default:\n    break;\n  }\n  if (context) {\n    clReleaseContext(context);\n  }\n\n  if (result.size == 0) {\n    // We didn't implement the enum. Error out semi-gracefully.\n    return CL_INVALID_VALUE;\n  }\n\n  if (param_value) {\n    // Actually try to return the string.\n    if (param_value_size < result.size) {\n      // Buffer is too small to hold the return value.\n      return CL_INVALID_VALUE;\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetDeviceInfo(cl_device_id device,\n                                                cl_device_info param_name,\n                                                size_t param_value_size,\n                                                void *param_value,\n                                                size_t *param_value_size_ret) {\n  return clGetDeviceInfoIntelFPGA(device, param_name, param_value_size,\n                                  param_value, param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clCreateSubDevicesIntelFPGA(\n    cl_device_id in_device,\n    const cl_device_partition_property *partition_properties,\n    cl_uint num_entries, cl_device_id *out_devices, cl_uint *num_devices) {\n  // Spec says:\n  // clCreateSubDevices returns CL_SUCCESS if the partition is created\n  // successfully. Otherwise, it returns a NULL value with the following error\n  // values returned in errcode_ret:\n  // - CL_INVALID_DEVICE if in_device is not valid.\n  // - CL_INVALID_VALUE if values specified in properties are not valid or if\n  // values specified in properties are valid but not supported by the device.\n  //  etc.\n\n  // Since we don't support creating sub devices, we should follow the first\n  // case if in_device is not valid, and the second case if it is.\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n  // Suppress compiler warnings.\n  partition_properties = partition_properties;\n  num_entries = num_entries;\n  out_devices = out_devices;\n  num_devices = num_devices;\n\n  if (!acl_device_is_valid(in_device)) {\n    return CL_INVALID_DEVICE;\n  }\n\n  return CL_INVALID_VALUE;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clCreateSubDevices(\n    cl_device_id in_device,\n    const cl_device_partition_property *partition_properties,\n    cl_uint num_entries, cl_device_id *out_devices, cl_uint *num_devices) {\n  return clCreateSubDevicesIntelFPGA(in_device, partition_properties,\n                                     num_entries, out_devices, num_devices);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainDeviceIntelFPGA(cl_device_id device) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Spec says:\n  // \"increments the device reference count if device is a valid sub-device\n  // created by a call to\n  //  clCreateSubDevices. If device is a root level device i.e. a cl_device_id\n  //  returned by clGetDeviceIDs, the device reference count remains unchanged.\n  //  clRetainDevice returns CL_SUCCESS if the function is executed successfully\n  //  or the device is a root-level device. Otherwise, it returns one of the\n  //  following errors:\n  //  - CL_INVALID_DEVICE if device is not a valid sub-device created by a call\n  //  to\n  //    clCreateSubDevices.\"\n  //\n  // It appears to contradict itself (?):\n  // \"clRetainDevice returns CL_SUCCESS if ... the device is a root-level\n  // device.\"\n  //   and\n  // \"CL_INVALID_DEVICE if device is not a valid sub-device\"\n  //\n  // But we'll go with the first statement for root-level devices (return\n  // CL_SUCCESS and do nothing)\n\n  // Since we don't (currently) support sub-devices, valid devices must be\n  // root-level:\n  if (acl_device_is_valid(device)) {\n    return CL_SUCCESS;\n  } else {\n    return CL_INVALID_DEVICE;\n  }\n}\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainDevice(cl_device_id device) {\n  return clRetainDeviceIntelFPGA(device);\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL\nclReleaseDeviceIntelFPGA(cl_device_id device) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Spec says:\n  // \"decrements the device reference count if device is a valid sub-device\n  // created by a call to\n  //  clCreateSubDevices. If device is a root level device i.e. a cl_device_id\n  //  returned by clGetDeviceIDs, the device reference count remains unchanged.\n  //  clReleaseDevice returns CL_SUCCESS if the function is executed\n  //  successfully. Otherwise, it returns one of the following errors:\n  //  - CL_INVALID_DEVICE if device is not a valid sub-device created by a call\n  //  to\n  //    clCreateSubDevices.\"\n  //\n  // As with clRetainDevice, we'll go with the interpretation that for\n  // root-level devices we return CL_SUCCESS and do nothing\n\n  // Since we don't (currently) support sub-devices, valid devices must be\n  // root-level:\n  if (acl_device_is_valid(device)) {\n    return CL_SUCCESS;\n  } else {\n    return CL_INVALID_DEVICE;\n  }\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL\nclReleaseDevice(cl_device_id device) {\n  return clReleaseDeviceIntelFPGA(device);\n}\n\nACL_EXPORT CL_API_ENTRY cl_int\nclReconfigurePLLIntelFPGA(cl_device_id device, const char *pll_settings_str) {\n  // To get the format of the second string argument please refer to the code\n  // comments specified for struct pll_setting_t in include/acl_pll.\n  const acl_hal_t *hal;\n  cl_int configure_status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_device_is_valid(device)) {\n    return CL_INVALID_DEVICE;\n  }\n  if (!pll_settings_str) {\n    return CL_INVALID_VALUE;\n  }\n\n  hal = acl_get_hal();\n  configure_status =\n      hal->pll_reconfigure(device->def.physical_device_id, pll_settings_str);\n  if (configure_status == 0)\n    return CL_SUCCESS;\n  else\n    return CL_INVALID_OPERATION;\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL clSetDeviceExceptionCallback(\n    cl_uint num_devices, const cl_device_id *devices,\n    CL_EXCEPTION_TYPE_INTEL listen_mask,\n    acl_exception_notify_fn_t pfn_exception_notify, void *user_data) {\n  return clSetDeviceExceptionCallbackIntelFPGA(\n      num_devices, devices, listen_mask, pfn_exception_notify, user_data);\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL\nclSetDeviceExceptionCallbackIntelFPGA(\n    cl_uint num_devices, const cl_device_id *devices,\n    CL_EXCEPTION_TYPE_INTEL listen_mask,\n    acl_exception_notify_fn_t pfn_exception_notify, void *user_data) {\n  unsigned i;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!pfn_exception_notify)\n    return CL_INVALID_VALUE;\n  if (!listen_mask)\n    return CL_INVALID_VALUE;\n  if (!devices && num_devices > 0)\n    return CL_INVALID_VALUE;\n  if (devices && num_devices == 0)\n    return CL_INVALID_VALUE;\n\n  for (i = 0; i < num_devices; ++i) {\n    devices[i]->exception_notify_fn = pfn_exception_notify;\n    devices[i]->exception_notify_user_data = user_data;\n    devices[i]->listen_mask = listen_mask;\n  }\n\n  return CL_SUCCESS;\n}\n\n//////////////////////////////\n// Internals\n\nint acl_device_is_valid(cl_device_id device) {\n  acl_assert_locked();\n\n  if (!acl_device_is_valid_ptr(device)) {\n    return 0;\n  }\n\n  return 1;\n}\n\nsize_t l_get_device_official_name(unsigned int physical_device_id, char *name,\n                                  size_t size, const char *raw_name) {\n  const acl_hal_t *hal;\n  size_t raw_name_strlen;\n  acl_assert_locked();\n\n  hal = acl_get_hal();\n  assert(hal);\n\n  if (size == 0)\n    return 0;\n\n  raw_name_strlen = strnlen(raw_name, MAX_NAME_SIZE);\n  strncpy(name, raw_name, raw_name_strlen);\n  name[raw_name_strlen] = '\\0';\n\n  if (hal->get_device_official_name == NULL)\n    return raw_name_strlen;\n\n  if (size > raw_name_strlen + 4) {\n    strncat(name, \" : \", size - raw_name_strlen - 1);\n    raw_name_strlen += 3;\n  }\n  if (size > raw_name_strlen)\n    hal->get_device_official_name(physical_device_id, &name[raw_name_strlen],\n                                  size - raw_name_strlen);\n  return strnlen(name, MAX_NAME_SIZE);\n}\n\nsize_t l_get_device_vendor_name(unsigned int physical_device_id, char *name,\n                                size_t size) {\n  const acl_hal_t *hal;\n  acl_assert_locked();\n\n  hal = acl_get_hal();\n  assert(hal);\n\n  if (size == 0)\n    return 0;\n\n  if (hal->get_device_vendor_name == NULL) {\n    strncpy(name, \"Unknown\", size);\n    name[size - 1] = '\\0';\n  } else\n    hal->get_device_vendor_name(physical_device_id, name, size);\n  return strnlen(name, MAX_NAME_SIZE);\n}\n\n// ACL utility to get on-chip temperature.  Only the PCIE HAL does anything\n// during a call, and not all ifaces have the hardware.  Currently only SV PCIe\n// returns an actual temperature (the rest return failure).\n// This function is exposed to users through clGetDeviceInfo with an IntelFPGA\n// extension query.  See example use in p4/tools/query_temperature\nstatic cl_int l_get_device_core_temperature(unsigned int physical_device_id) {\n  cl_bool return_val;\n  cl_int temp; // Degrees C\n  const acl_hal_t *hal;\n  acl_assert_locked();\n\n  hal = acl_get_hal();\n  assert(hal);\n\n  return_val = hal->query_temperature(physical_device_id, &temp);\n\n  if (return_val) {\n    return temp;\n  } else {\n    return 0; // 0 degrees on unsupported read\n  }\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_usm.cpp",
        "data": "// Copyright (C) 2020-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <algorithm>\n#include <array>\n#include <cassert>\n#include <cstdio>\n#include <iostream>\n#include <optional>\n#include <sstream>\n#include <string>\n#include <unordered_set>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_mem.h>\n#include <acl_platform.h>\n#include <acl_support.h>\n#include <acl_svm.h>\n#include <acl_types.h>\n#include <acl_usm.h>\n#include <acl_util.h>\n\n#include <MMD/aocl_mmd.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n// Forward declaration\nstatic void l_add_usm_alloc_to_context(cl_context context,\n                                       acl_usm_allocation_t *usm_alloc);\nstatic void l_remove_usm_alloc_from_context(cl_context context,\n                                            acl_usm_allocation_t *usm_alloc);\nstatic cl_bool l_ptr_in_usm_alloc_range(acl_usm_allocation_t *usm_alloc,\n                                        const void *dst_ptr, size_t size);\nstatic void *l_set_dev_alloc_bit(const void *ptr);\nstatic void l_cl_mem_blocking_free(cl_context context, void *ptr);\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clHostMemAllocINTEL(\n    cl_context context, const cl_mem_properties_intel *properties, size_t size,\n    cl_uint alignment, cl_int *errcode_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  if (!acl_context_is_valid(context)) {\n    BAIL(CL_INVALID_CONTEXT);\n  }\n\n  if (size == 0) {\n    BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n              \"Memory buffer cannot be of size zero\");\n  }\n\n  // Spec only allows for power of 2 allignment.\n  // Alignment of '0' means use the default\n  if (alignment & (alignment - 1)) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"alignment must be power of 2\");\n  }\n\n  // Spec specifies that alignment is no bigger than the largest supported data\n  // type\n  if (alignment > sizeof(cl_long16)) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"Requested alignment greater than largest data type \"\n              \"supported by device (long16)\");\n  }\n\n  std::vector<cl_device_id> devices = std::vector<cl_device_id>(\n      context->device, context->device + context->num_devices);\n  if (alignment == 0) {\n    bool same_device = true;\n    // In a system of different devices use the biggest min_alignment out of the\n    // lot, this guarantees that the alignment will work for all devices. If the\n    // min alignment of one device is greater that the max alignment of another,\n    // an error will be generated during allocation.\n    for (const auto &dev : devices) {\n      alignment = std::max<cl_uint>(alignment,\n                                    (cl_uint)dev->def.min_host_mem_alignment);\n      same_device &= dev->def.autodiscovery_def.name ==\n                     devices[0]->def.autodiscovery_def.name;\n    }\n\n    if (same_device) {\n      // if the context contains only the same device, let the MMD decide what\n      // the default alignment should be\n      alignment = 0;\n    }\n  }\n\n  // Iterate over properties.\n  // The end of the properties list is specified with a zero.\n  cl_mem_alloc_flags_intel alloc_flags = 0;\n  std::optional<cl_uint> mem_id;\n  while (properties != NULL && *properties != 0) {\n    switch (*properties) {\n    case CL_MEM_ALLOC_FLAGS_INTEL: {\n      alloc_flags = *(properties + 1);\n    } break;\n    case CL_MEM_ALLOC_BUFFER_LOCATION_INTEL: {\n      mem_id = (cl_uint) * (properties + 1);\n    } break;\n    default: {\n      BAIL_INFO(CL_INVALID_PROPERTY, context, \"Invalid properties\");\n    }\n    }\n    properties += 2;\n  }\n\n  for (const auto &dev : devices) {\n    if (!acl_usm_has_access_capability(dev,\n                                       CL_DEVICE_HOST_MEM_CAPABILITIES_INTEL)) {\n      BAIL_INFO(\n          CL_INVALID_OPERATION, context,\n          \"Device does not support host Unified Shared Memory allocations: \" +\n              dev->def.autodiscovery_def.name);\n    }\n    // Ensure requested size is valid and supported by the specified device\n    cl_ulong max_alloc = 0;\n    cl_int ret = clGetDeviceInfo(dev, CL_DEVICE_MAX_MEM_ALLOC_SIZE,\n                                 sizeof(max_alloc), &max_alloc, 0);\n    if (ret) {\n      BAIL_INFO(ret, context,\n                \"Failed to query CL_DEVICE_MAX_MEM_ALLOC_SIZE for device: \" +\n                    dev->def.autodiscovery_def.name);\n    }\n    if (size > max_alloc) {\n      BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n                \"Size larger than allocation size supported by device: \" +\n                    dev->def.autodiscovery_def.name);\n    }\n  }\n\n  bool track_mem_id = false;\n  if (acl_get_hal()->host_alloc) {\n    std::array<mem_properties_t, 3> mmd_properties;\n    {\n      auto mmd_properties_it = mmd_properties.begin();\n      if (mem_id) {\n        if (acl_get_hal()->support_buffer_location(devices)) {\n          track_mem_id = true;\n          *mmd_properties_it++ = AOCL_MMD_MEM_PROPERTIES_BUFFER_LOCATION;\n          *mmd_properties_it++ = *mem_id;\n        }\n      }\n      *mmd_properties_it++ = 0;\n    }\n\n    acl_usm_allocation_t *usm_alloc =\n        (acl_usm_allocation_t *)acl_malloc(sizeof(acl_usm_allocation_t));\n\n    if (!usm_alloc) {\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context, \"Out of host memory\");\n    }\n\n    int error = 0;\n    void *mem = acl_get_hal()->host_alloc(\n        devices, size, alignment,\n        mmd_properties[0] ? mmd_properties.data() : nullptr, &error);\n    if (error) {\n      acl_free(usm_alloc);\n      switch (error) {\n      case CL_OUT_OF_HOST_MEMORY:\n        BAIL_INFO(error, context,\n                  \"Error: Unable to allocate \" + std::to_string(size) +\n                      \" bytes\");\n        break;\n      case CL_INVALID_VALUE:\n        BAIL_INFO(error, context,\n                  \"Error: Unsupported alignment of \" +\n                      std::to_string(alignment));\n        break;\n      case CL_INVALID_PROPERTY:\n        BAIL_INFO(error, context, \"Error: Unsuported properties\");\n        break;\n      default:\n        BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                  \"Error: Unable to allocate memory\");\n        break;\n      }\n    }\n\n    usm_alloc->mem = NULL;\n    usm_alloc->device = NULL; // All device in context\n    usm_alloc->range.begin = mem;\n    usm_alloc->range.next = (void *)((size_t)mem + size);\n    usm_alloc->alloc_flags = alloc_flags;\n    usm_alloc->type = CL_MEM_TYPE_HOST_INTEL;\n    usm_alloc->alignment = alignment;\n    usm_alloc->host_shared_mem_id = 0; // Initialize to 0\n    if (track_mem_id) {\n      usm_alloc->host_shared_mem_id = *mem_id;\n    }\n\n    l_add_usm_alloc_to_context(context, usm_alloc);\n    return mem;\n  }\n\n  BAIL_INFO(CL_INVALID_VALUE, context,\n            \"Host allocation is not supported for devices in this context\");\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL\nclDeviceMemAllocINTEL(cl_context context, cl_device_id device,\n                      const cl_mem_properties_intel *properties, size_t size,\n                      cl_uint alignment, cl_int *errcode_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Valid argument check\n  if (!acl_context_is_valid(context)) {\n    BAIL(CL_INVALID_CONTEXT);\n  }\n  if (!acl_device_is_valid(device)) {\n    BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid device\");\n  }\n  if (!acl_context_uses_device(context, device)) {\n    BAIL_INFO(CL_INVALID_DEVICE, context,\n              \"Device is not associated with the context\");\n  }\n  if (size == 0) {\n    BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n              \"Memory buffer cannot be of size zero\");\n  }\n\n  cl_ulong max_alloc = 0;\n  clGetDeviceInfo(device, CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof(max_alloc),\n                  &max_alloc, 0);\n\n  if (size > max_alloc) {\n    BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n              \"Memory buffer size is larger than max size supported by device\");\n  }\n\n  if (!acl_usm_has_access_capability(device,\n                                     CL_DEVICE_DEVICE_MEM_CAPABILITIES_INTEL)) {\n    BAIL_INFO(\n        CL_INVALID_OPERATION, context,\n        \"Device does not support device Unified Shared Memory allocations: \" +\n            device->def.autodiscovery_def.name);\n  }\n\n  // Spec allows for power of 2 allignment.\n  // For now, we only allow alignment to ACL_MEM_ALIGN.\n  // Over align all allocations to ACL_MEM_ALIGN.\n  // Alignment of '0' means use the default\n  if (alignment == 0) {\n    alignment = ACL_MEM_ALIGN;\n  }\n  if (alignment & (alignment - 1)) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"alignment must be power of 2\");\n  }\n  if (alignment > ACL_MEM_ALIGN) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Alignment value is not supported\");\n  }\n  alignment = ACL_MEM_ALIGN;\n\n  // Iterate over properties.\n  // The end of the properties list is specified with a zero.\n  cl_mem_alloc_flags_intel alloc_flags = 0;\n  cl_uint mem_id = acl_get_default_memory(device->def);\n  while (properties != NULL && *properties != 0) {\n    switch (*properties) {\n    case CL_MEM_ALLOC_FLAGS_INTEL: {\n      alloc_flags = *(properties + 1);\n    } break;\n    case CL_MEM_ALLOC_BUFFER_LOCATION_INTEL: {\n      mem_id = (cl_uint) * (properties + 1);\n    } break;\n    default: {\n      BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid properties\");\n    }\n    }\n    properties += 2;\n  }\n\n  cl_int status;\n\n  // Use cl_mem for convenience\n  cl_mem_properties_intel props[] = {CL_MEM_ALLOC_BUFFER_LOCATION_INTEL, mem_id,\n                                     0};\n  cl_mem usm_device_buffer = clCreateBufferWithPropertiesINTEL(\n      context, props, CL_MEM_READ_WRITE, size, NULL, &status);\n  if (status != CL_SUCCESS) {\n    BAIL_INFO(status, context, \"Failed to allocate device memory\");\n  }\n  // Runtime will do device allocation on bind to device\n  if (!acl_bind_buffer_to_device(device, usm_device_buffer)) {\n    clReleaseMemObjectIntelFPGA(usm_device_buffer);\n    BAIL_INFO(CL_OUT_OF_RESOURCES, context, \"Failed to allocate device memory\");\n  }\n  acl_usm_allocation_t *usm_alloc =\n      (acl_usm_allocation_t *)acl_malloc(sizeof(acl_usm_allocation_t));\n\n  if (!usm_alloc) {\n    clReleaseMemObjectIntelFPGA(usm_device_buffer);\n    BAIL_INFO(CL_OUT_OF_RESOURCES, context, \"Out of host memory\");\n  }\n\n  void *ptr = acl_get_physical_address(usm_device_buffer, device);\n  ptr = l_set_dev_alloc_bit(ptr);\n\n  usm_alloc->mem = usm_device_buffer;\n  usm_alloc->device = device;\n  usm_alloc->range.begin = ptr;\n  usm_alloc->range.next = (void *)((size_t)ptr + size);\n  usm_alloc->alloc_flags = alloc_flags;\n  usm_alloc->type = CL_MEM_TYPE_DEVICE_INTEL;\n  usm_alloc->alignment = alignment;\n\n  l_add_usm_alloc_to_context(context, usm_alloc);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  return ptr;\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL\nclSharedMemAllocINTEL(cl_context context, cl_device_id device,\n                      const cl_mem_properties_intel *properties, size_t size,\n                      cl_uint alignment, cl_int *errcode_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  if (!acl_context_is_valid(context)) {\n    BAIL(CL_INVALID_CONTEXT);\n  }\n  if (device != nullptr && !acl_device_is_valid(device)) {\n    BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid device\");\n  }\n  if (device != nullptr && !acl_context_uses_device(context, device)) {\n    BAIL_INFO(CL_INVALID_DEVICE, context,\n              \"Device is not associated with the context\");\n  }\n  if (size == 0) {\n    BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n              \"Allocation cannot be of size zero\");\n  }\n  // USM spec allows only power-of-2 alignment, or 0 (default alignment)\n  if (alignment & (alignment - 1)) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"alignment must be power of 2\");\n  }\n\n  // Ensure the specified device, or at least one of the devices in the context\n  // supports shared allocations.\n  std::vector<cl_device_id> devices;\n  if (device != nullptr) {\n    devices.push_back(device);\n  } else {\n    devices = std::vector<cl_device_id>(context->device,\n                                        context->device + context->num_devices);\n  }\n  for (const auto &dev : devices) {\n    if (!acl_usm_has_access_capability(\n            dev, CL_DEVICE_SINGLE_DEVICE_SHARED_MEM_CAPABILITIES_INTEL)) {\n      BAIL_INFO(\n          CL_INVALID_OPERATION, context,\n          \"Device does not support shared Unified Shared Memory allocations\");\n    }\n  }\n\n  // Spec specifies that alignment is no bigger than the largest supported data\n  // type\n  if (alignment > sizeof(cl_long16)) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"Requested alignment greater than largest data type \"\n              \"supported by device (long16)\");\n  }\n\n  // Ensure requested size is valid and supported by the specified device, or at\n  // least one of the devices in the context.\n  if (device != nullptr) {\n    cl_ulong dev_alloc = 0;\n    cl_int ret = clGetDeviceInfo(device, CL_DEVICE_MAX_MEM_ALLOC_SIZE,\n                                 sizeof(dev_alloc), &dev_alloc, 0);\n    if (ret) {\n      BAIL_INFO(ret, context,\n                \"Failed to query CL_DEVICE_MAX_MEM_ALLOC_SIZE for device\");\n    }\n    if (size > dev_alloc) {\n      BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n                \"Size larger than allocation size supported by device\");\n    }\n  }\n  if (device == nullptr && (size > context->max_mem_alloc_size)) {\n    BAIL_INFO(\n        CL_INVALID_BUFFER_SIZE, context,\n        \"Size larger than allocation size supported by any device in context\");\n  }\n\n  // Iterate over properties.\n  // The end of the properties list is specified with a zero.\n  cl_mem_alloc_flags_intel alloc_flags = 0;\n  std::unordered_set<unsigned long long> seen_flags;\n  std::optional<cl_uint> mem_id;\n  while (properties != NULL && *properties != 0) {\n    switch (*properties) {\n    case CL_MEM_ALLOC_FLAGS_INTEL: {\n      if (seen_flags.insert(CL_MEM_ALLOC_FLAGS_INTEL).second == false) {\n        BAIL_INFO(CL_INVALID_PROPERTY, context,\n                  \"Property specified multiple times\");\n      }\n      switch (*(properties + 1)) {\n      case CL_MEM_ALLOC_WRITE_COMBINED_INTEL:\n        break;\n      default:\n        BAIL_INFO(CL_INVALID_PROPERTY, context, \"Invalid value for property\");\n      }\n      alloc_flags = *(properties + 1);\n    } break;\n    case CL_MEM_ALLOC_BUFFER_LOCATION_INTEL: {\n      if (seen_flags.insert(CL_MEM_ALLOC_BUFFER_LOCATION_INTEL).second ==\n          false) {\n        BAIL_INFO(CL_INVALID_PROPERTY, context,\n                  \"Property specified multiple times\");\n      }\n      mem_id = (cl_uint) * (properties + 1);\n    } break;\n    default: {\n      BAIL_INFO(CL_INVALID_PROPERTY, context, \"Invalid properties\");\n    }\n    }\n    properties += 2;\n  }\n\n  bool track_mem_id = false;\n  if (acl_get_hal()->shared_alloc) {\n    std::array<mem_properties_t, 3> mmd_properties;\n    {\n      auto mmd_properties_it = mmd_properties.begin();\n      if (mem_id) {\n        if (acl_get_hal()->support_buffer_location(\n                std::vector<cl_device_id>{device})) {\n          track_mem_id = true;\n          *mmd_properties_it++ = AOCL_MMD_MEM_PROPERTIES_BUFFER_LOCATION;\n          *mmd_properties_it++ = *mem_id;\n        }\n      }\n      *mmd_properties_it++ = 0;\n    }\n\n    acl_usm_allocation_t *usm_alloc =\n        (acl_usm_allocation_t *)acl_malloc(sizeof(acl_usm_allocation_t));\n\n    if (!usm_alloc) {\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context, \"Out of host memory\");\n    }\n\n    int error;\n    void *mem = acl_get_hal()->shared_alloc(\n        device, size, alignment,\n        mmd_properties[0] ? mmd_properties.data() : nullptr, &error);\n    if (mem == NULL) {\n      acl_free(usm_alloc);\n      switch (error) {\n      case CL_OUT_OF_HOST_MEMORY:\n        BAIL_INFO(error, context,\n                  \"Error: Unable to allocate \" + std::to_string(size) +\n                      \" bytes\");\n        break;\n      case CL_INVALID_VALUE:\n        BAIL_INFO(error, context,\n                  \"Error: Unsupported alignment of \" +\n                      std::to_string(alignment));\n        break;\n      case CL_INVALID_PROPERTY:\n        BAIL_INFO(error, context, \"Error: Unsuported properties\");\n        break;\n      default:\n        BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                  \"Error: Unable to allocate memory\");\n        break;\n      }\n    }\n\n    usm_alloc->mem = NULL;\n    usm_alloc->device = device; // May be NULL\n    usm_alloc->range.begin = mem;\n    usm_alloc->range.next = (void *)((size_t)mem + size);\n    usm_alloc->alloc_flags = alloc_flags;\n    usm_alloc->type = CL_MEM_TYPE_SHARED_INTEL;\n    usm_alloc->alignment = alignment;\n    usm_alloc->host_shared_mem_id = 0; // Initialize to 0\n    if (track_mem_id) {\n      usm_alloc->host_shared_mem_id = *mem_id;\n    }\n\n    l_add_usm_alloc_to_context(context, usm_alloc);\n    return mem;\n  }\n\n  // After all the error check, still error out\n  // Shared allocation is not supported yet\n  BAIL_INFO(CL_INVALID_VALUE, context,\n            \"Shared allocation is not supported for devices in this context\");\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clMemFreeINTEL(cl_context context, void *ptr) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n\n  // NULL is valid input where nothing happens\n  if (ptr == NULL) {\n    return CL_SUCCESS;\n  }\n\n  acl_usm_allocation_t *usm_alloc = acl_get_usm_alloc_from_ptr(context, ptr);\n  if (!usm_alloc) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Memory must be USM allocation in context\");\n  }\n  if (usm_alloc->range.begin != ptr) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Pointer must be exact value returned by allocation\");\n  }\n\n  switch (usm_alloc->type) {\n  case CL_MEM_TYPE_HOST_INTEL: {\n    if (acl_get_hal()->free) {\n      if (acl_get_hal()->free(context, const_cast<void *>(ptr))) {\n        ERR_RET(CL_INVALID_VALUE, context, \"Failed to free host allocation\");\n      }\n    }\n    break;\n  }\n  case CL_MEM_TYPE_DEVICE_INTEL: {\n    cl_int status = clReleaseMemObjectIntelFPGA(usm_alloc->mem);\n    if (status != CL_SUCCESS) {\n      return status;\n    }\n    break;\n  }\n  case CL_MEM_TYPE_SHARED_INTEL: {\n    if (acl_get_hal()->free) {\n      if (acl_get_hal()->free(context, const_cast<void *>(ptr))) {\n        ERR_RET(CL_INVALID_VALUE, context, \"Failed to free shared allocation\");\n      }\n    }\n    break;\n  }\n  default: {\n    ERR_RET(CL_INVALID_VALUE, context, \"Pointer must be from USM allocation\");\n    break;\n  }\n  }\n\n  l_remove_usm_alloc_from_context(context, usm_alloc);\n  acl_free(usm_alloc);\n  usm_alloc = nullptr;\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clMemBlockingFreeINTEL(cl_context context,\n                                                       void *ptr) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n\n  // NULL is valid input where nothing happens\n  if (ptr == NULL) {\n    return CL_SUCCESS;\n  }\n\n  acl_usm_allocation_t *usm_alloc = acl_get_usm_alloc_from_ptr(context, ptr);\n  if (!usm_alloc) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Memory must be USM allocation in context\");\n  }\n  if (usm_alloc->range.begin != ptr) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Pointer must be exact value returned by allocation\");\n  }\n\n  // wait for enqueued commands that uses ptr to finish before free\n  l_cl_mem_blocking_free(context, ptr);\n\n  switch (usm_alloc->type) {\n  case CL_MEM_TYPE_HOST_INTEL: {\n    if (acl_get_hal()->free) {\n      if (acl_get_hal()->free(context, const_cast<void *>(ptr))) {\n        ERR_RET(CL_INVALID_VALUE, context, \"Failed to free host allocation\");\n      }\n    }\n    break;\n  }\n  case CL_MEM_TYPE_DEVICE_INTEL: {\n    cl_int status = clReleaseMemObjectIntelFPGA(usm_alloc->mem);\n    if (status != CL_SUCCESS) {\n      return status;\n    }\n    break;\n  }\n  case CL_MEM_TYPE_SHARED_INTEL: {\n    if (acl_get_hal()->free) {\n      if (acl_get_hal()->free(context, const_cast<void *>(ptr))) {\n        ERR_RET(CL_INVALID_VALUE, context, \"Failed to free shared allocation\");\n      }\n    }\n    break;\n  }\n  default: {\n    ERR_RET(CL_INVALID_VALUE, context, \"Pointer must be from USM allocation\");\n    break;\n  }\n  }\n\n  l_remove_usm_alloc_from_context(context, usm_alloc);\n  acl_free(usm_alloc);\n  usm_alloc = nullptr;\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetMemAllocInfoINTEL(\n    cl_context context, const void *ptr, cl_mem_info_intel param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          context);\n\n  // Get USM allocation associated with ptr\n  acl_usm_allocation_t *usm_alloc = acl_get_usm_alloc_from_ptr(context, ptr);\n\n  acl_result_t result;\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_MEM_ALLOC_TYPE_INTEL: {\n    if (usm_alloc) {\n      RESULT_UINT(usm_alloc->type);\n    } else {\n      RESULT_UINT(CL_MEM_TYPE_UNKNOWN_INTEL);\n    }\n  } break;\n\n  case CL_MEM_ALLOC_BUFFER_LOCATION_INTEL: {\n    if (usm_alloc) {\n      if (usm_alloc->mem) {\n        RESULT_UINT(usm_alloc->mem->mem_id);\n      } else {\n        RESULT_UINT(usm_alloc->host_shared_mem_id);\n      }\n    } else {\n      RESULT_UINT(0);\n    }\n  } break;\n\n  case CL_MEM_ALLOC_BASE_PTR_INTEL: {\n    void *base_ptr = NULL;\n    if (usm_alloc) {\n      base_ptr = usm_alloc->range.begin;\n    }\n    RESULT_PTR(base_ptr);\n  } break;\n\n  case CL_MEM_ALLOC_SIZE_INTEL: {\n    size_t size = 0;\n    if (usm_alloc) {\n      size = (size_t)((unsigned long long)usm_alloc->range.next -\n                      (unsigned long long)usm_alloc->range.begin);\n    }\n    RESULT_SIZE_T(size);\n  } break;\n\n  case CL_MEM_ALLOC_DEVICE_INTEL: {\n    cl_device_id device = NULL;\n    if (usm_alloc) {\n      device = usm_alloc->device;\n    }\n    RESULT_PTR(device);\n  } break;\n\n  case CL_MEM_ALLOC_FLAGS_INTEL: {\n    cl_mem_alloc_flags_intel alloc_flags = 0;\n    if (usm_alloc) {\n      alloc_flags = usm_alloc->alloc_flags;\n    }\n    RESULT_BITFIELD(alloc_flags);\n  } break;\n\n  default: {\n    ERR_RET(CL_INVALID_VALUE, context, \"Param name is not a valid query\");\n  } break;\n  }\n\n  if (param_value) {\n    // Try to return the param value.\n    if (param_value_size < result.size) {\n      // Buffer is too small to hold the return value.\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Param value size is smaller than query return type\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\n// clEnqueueMemsetINTEL has been removed in the latest OpenCL spec, but SYCl\n// runtime hasn't been updated yet.\n// Keep it around until it is not used by SYCL.\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclEnqueueMemsetINTEL(cl_command_queue command_queue, void *dst_ptr,\n                     cl_int value, size_t size, cl_uint num_events_in_wait_list,\n                     const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueMemFillINTEL(command_queue, dst_ptr, &value, sizeof(char),\n                               size, num_events_in_wait_list, event_wait_list,\n                               event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueMemFillINTEL(\n    cl_command_queue command_queue, void *dst_ptr, const void *pattern,\n    size_t pattern_size, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  char *ptr;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (dst_ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (((uintptr_t)dst_ptr) % (pattern_size) != 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer not aligned with pattern size\");\n  }\n  if (pattern == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pattern argument cannot be NULL\");\n  }\n  if (pattern_size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pattern size argument cannot be 0\");\n  }\n  // Pattern size must be less than largest supported int/float vec type\n  if (pattern_size > sizeof(double) * 16) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Patern size must be less than double16\");\n  }\n  // Pattern size can only be power of 2\n  if (pattern_size & (pattern_size - 1)) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Patern size must be power of 2\");\n  }\n  if (size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context, \"Size cannot be 0\");\n  }\n  if (size % pattern_size != 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Size must be multiple of pattern size\");\n  }\n\n  // This array is passed to clSetEventCallback for releasing the\n  // allocated memory and releasing the event, if *event is null.\n  void **callback_data = (void **)acl_malloc(sizeof(void *) * 2);\n  if (!callback_data) {\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  acl_aligned_ptr_t *aligned_ptr =\n      (acl_aligned_ptr_t *)acl_malloc(sizeof(acl_aligned_ptr_t));\n  if (!aligned_ptr) {\n    acl_free(callback_data);\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  // Replicating the value, size times.\n  *aligned_ptr = acl_mem_aligned_malloc(size);\n  ptr = (char *)(aligned_ptr->aligned_ptr);\n  if (!ptr) {\n    acl_free(aligned_ptr);\n    acl_free(callback_data);\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  for (cl_uint i = 0; i < size / pattern_size; i++) {\n    safe_memcpy(&(ptr[i * pattern_size]), pattern, pattern_size, pattern_size,\n                pattern_size);\n  }\n\n  acl_usm_allocation_t *usm_alloc =\n      acl_get_usm_alloc_from_ptr(command_queue->context, dst_ptr);\n\n  bool dst_on_host = true;\n  cl_device_id dst_device = NULL;\n  if (usm_alloc) {\n    if (l_ptr_in_usm_alloc_range(usm_alloc, dst_ptr, size) != CL_TRUE) {\n      acl_mem_aligned_free(command_queue->context, aligned_ptr);\n      acl_free(aligned_ptr);\n      acl_free(callback_data);\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Size accesses outside of USM allocation dst_ptr range\");\n    }\n    if (usm_alloc->type == CL_MEM_TYPE_DEVICE_INTEL) {\n      dst_device = usm_alloc->device;\n      dst_on_host = false;\n    }\n  }\n\n  // Even if dst_ptr is not USM allocation, continue assuming it is system mem.\n  // If it is USM allocation though, it needs to be on same dev as queue.\n  if (dst_device && dst_device->id != command_queue->device->id) {\n    // Cleaning up before failing.\n    acl_mem_aligned_free(command_queue->context, aligned_ptr);\n    acl_free(aligned_ptr);\n    acl_free(callback_data);\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Memory allocation needs to be on command queue device\");\n  }\n\n  cl_event tmp_event = NULL;\n\n  // Create an event to fill the data after events in wait list complete\n  cl_int status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_MEMFILL_INTEL, &tmp_event);\n  if (status != CL_SUCCESS) {\n    // Cleaning up before failing.\n    acl_mem_aligned_free(command_queue->context, aligned_ptr);\n    acl_free(aligned_ptr);\n    acl_free(callback_data);\n    return status; // already signalled callback\n  }\n  tmp_event->cmd.info.usm_xfer.src_ptr = ptr;\n  tmp_event->cmd.info.usm_xfer.dst_ptr = dst_ptr;\n  tmp_event->cmd.info.usm_xfer.size = size;\n  tmp_event->cmd.info.usm_xfer.src_on_host = true;\n  tmp_event->cmd.info.usm_xfer.dst_on_host = dst_on_host;\n\n  // If nothing's blocking, then complete right away\n  acl_idle_update(command_queue->context);\n\n  callback_data[0] = (void *)(aligned_ptr);\n  if (event) {\n    *event = tmp_event;\n    // User needs the event, so we shouldn't release it after the event\n    // completion.\n    callback_data[1] = NULL;\n  } else {\n    // Passing the event to release it when the event is done.\n    callback_data[1] = tmp_event;\n  }\n  clSetEventCallback(tmp_event, CL_COMPLETE,\n                     acl_free_allocation_after_event_completion,\n                     (void *)callback_data);\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueMemcpyINTEL(\n    cl_command_queue command_queue, cl_bool blocking, void *dst_ptr,\n    const void *src_ptr, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (dst_ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (src_ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer size cannot be 0\");\n  }\n\n  if (((char *)src_ptr < (char *)dst_ptr &&\n       (char *)src_ptr + size > (char *)dst_ptr) ||\n      ((char *)dst_ptr < (char *)src_ptr &&\n       (char *)dst_ptr + size > (char *)src_ptr)) {\n    ERR_RET(CL_MEM_COPY_OVERLAP, command_queue->context,\n            \"Source and destination memory overlaps\");\n  }\n\n  acl_usm_allocation_t *dst_usm_alloc =\n      acl_get_usm_alloc_from_ptr(command_queue->context, dst_ptr);\n\n  bool dst_on_host = true;\n  cl_device_id dst_device = NULL;\n  if (dst_usm_alloc) {\n    if (l_ptr_in_usm_alloc_range(dst_usm_alloc, dst_ptr, size) != CL_TRUE) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Size accesses outside of USM allocation dst_ptr range\");\n    }\n    if (dst_usm_alloc->type == CL_MEM_TYPE_DEVICE_INTEL) {\n      dst_device = dst_usm_alloc->device;\n      dst_on_host = false;\n    }\n  }\n\n  acl_usm_allocation_t *src_usm_alloc =\n      acl_get_usm_alloc_from_ptr(command_queue->context, src_ptr);\n\n  bool src_on_host = true;\n  cl_device_id src_device = NULL;\n  // Even if src_ptr is not USM pointer, continue assuming it's system mem\n  if (src_usm_alloc) {\n    if (l_ptr_in_usm_alloc_range(src_usm_alloc, src_ptr, size) != CL_TRUE) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Size accesses outside of USM allocation src_ptr range\");\n    }\n    if (src_usm_alloc->type == CL_MEM_TYPE_DEVICE_INTEL) {\n      src_device = src_usm_alloc->device;\n      src_on_host = false;\n    }\n  }\n\n  if ((dst_device && dst_device->id != command_queue->device->id) ||\n      (src_device && src_device->id != command_queue->device->id)) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Memory allocation needs to be on command queue's device\");\n  }\n\n  cl_event tmp_event = NULL;\n\n  // Create an event to fill the data after events in wait list complete\n  cl_int status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_MEMCPY_INTEL, &tmp_event);\n  if (status != CL_SUCCESS) {\n    return status; // already signalled callback\n  }\n  tmp_event->cmd.info.usm_xfer.src_ptr = src_ptr;\n  tmp_event->cmd.info.usm_xfer.dst_ptr = dst_ptr;\n  tmp_event->cmd.info.usm_xfer.size = size;\n  tmp_event->cmd.info.usm_xfer.src_on_host = src_on_host;\n  tmp_event->cmd.info.usm_xfer.dst_on_host = dst_on_host;\n\n  // If nothing's blocking, then complete right away\n  acl_idle_update(command_queue->context);\n\n  if (blocking) {\n    status = clWaitForEvents(1, &tmp_event);\n  }\n\n  if (event) {\n    *event = tmp_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(tmp_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n\n  if (blocking && status == CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST) {\n    return status;\n  }\n\n  return CL_SUCCESS;\n}\n\n// Unused argument names are commented out to avoid Windows compile warning:\n// unreferenced formal parameter\n// Uncomment when the API is fully implemented\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueMigrateMemINTEL(\n    cl_command_queue command_queue, const void *ptr, size_t /* size */,\n    cl_mem_migration_flags /* flags */, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument can not be NULL\");\n  }\n\n  // Migrate currently doesn't do anything\n  // Treat it like clEnqueueMarkerWithWaitList, but we won't wait for all events\n  // in command queue to finish for OOO queues.\n  cl_event local_event = NULL;\n  cl_int result =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_MIGRATEMEM_INTEL, &local_event);\n\n  if (result != CL_SUCCESS) {\n    return result;\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    clReleaseEvent(local_event);\n  }\n  return CL_SUCCESS;\n}\n\n// Unused argument names are commented out to avoid Windows compile warning:\n// unreferenced formal parameter\n// Uncomment when the API is implemented\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueMemAdviseINTEL(\n    cl_command_queue command_queue, const void *ptr, size_t /* size */,\n    cl_mem_advice_intel /* advice */, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument can not be NULL\");\n  }\n\n  // MemAdvise currently doesn't do anything\n  // Treat it like clEnqueueMarkerWithWaitList, but we won't wait for all events\n  // in command queue to finish for OOO queues.\n  cl_event local_event = NULL;\n  cl_int result =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_MEMADVISE_INTEL, &local_event);\n\n  if (result != CL_SUCCESS) {\n    return result;\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    clReleaseEvent(local_event);\n  }\n  return CL_SUCCESS;\n}\n\nvoid acl_usm_memcpy(void *, acl_device_op_t *op) {\n  cl_event event = op->info.event;\n  acl_assert_locked();\n\n  if (!acl_event_is_valid(event) ||\n      !acl_command_queue_is_valid(event->command_queue)) {\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n\n  acl_set_device_op_execution_status(op, CL_SUBMITTED);\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  const acl_hal_t *const hal = acl_get_hal();\n  if (event->cmd.info.usm_xfer.src_on_host) {\n    if (event->cmd.info.usm_xfer.dst_on_host) {\n      hal->copy_hostmem_to_hostmem(event, event->cmd.info.usm_xfer.src_ptr,\n                                   event->cmd.info.usm_xfer.dst_ptr,\n                                   event->cmd.info.usm_xfer.size);\n    } else {\n      hal->copy_hostmem_to_globalmem(event, event->cmd.info.usm_xfer.src_ptr,\n                                     event->cmd.info.usm_xfer.dst_ptr,\n                                     event->cmd.info.usm_xfer.size);\n    }\n  } else {\n    if (event->cmd.info.usm_xfer.dst_on_host) {\n      hal->copy_globalmem_to_hostmem(event, event->cmd.info.usm_xfer.src_ptr,\n                                     event->cmd.info.usm_xfer.dst_ptr,\n                                     event->cmd.info.usm_xfer.size);\n    } else {\n      hal->copy_globalmem_to_globalmem(event, event->cmd.info.usm_xfer.src_ptr,\n                                       event->cmd.info.usm_xfer.dst_ptr,\n                                       event->cmd.info.usm_xfer.size);\n    }\n  }\n}\n\n// Called by acl_submit_command in acl_command.cpp once all events in\n// event_wait_list of clEnqueueMemFill/clEnqueueMemcpy are complete.\n// If transfer is from host-to-host, host-to-shared or shared-to-host,\n// do system memcpy. Otherwise, queue device op to do copy.\nint acl_submit_usm_memcpy(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n\n  // No user-level scheduling blocks this memory transfer.\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  if (event->last_device_op) {\n    return result;\n  }\n\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n  acl_device_op_t *last_op = 0;\n\n  // Precautionary, but it also nudges the device scheduler to try\n  // to free up old operation slots.\n  acl_forget_proposed_device_ops(doq);\n\n  // Check if both src and dst on are device and do system memcpy instead.\n  // clEnqueueMemcpy and clEnqueueMemFill only considers device allocation\n  // to be not \"on_host\". It may be more optimal for BSP to do copy for all\n  // USM allocations. However, that will mean copy from host to host USM\n  // allocation has acl_device_op_conflict_type_t of ACL_CONFLICT_MEM.\n  if (event->cmd.info.usm_xfer.src_on_host) {\n    if (event->cmd.info.usm_xfer.dst_on_host) {\n      // Do memcpy and set event to complete\n      safe_memcpy(event->cmd.info.usm_xfer.dst_ptr,\n                  event->cmd.info.usm_xfer.src_ptr,\n                  event->cmd.info.usm_xfer.size, event->cmd.info.usm_xfer.size,\n                  event->cmd.info.usm_xfer.size);\n      acl_set_execution_status(event, CL_SUBMITTED);\n      acl_set_execution_status(event, CL_RUNNING);\n      acl_set_execution_status(event, CL_COMPLETE);\n      return 1;\n    }\n  }\n\n  last_op = acl_propose_device_op(doq, ACL_DEVICE_OP_USM_MEMCPY, event);\n\n  if (last_op) {\n    // We managed to enqueue everything.\n    event->last_device_op = last_op;\n    acl_commit_proposed_device_ops(doq);\n    result = 1;\n  } else {\n    // Back off, and wait until later when we have more space in the\n    // device op queue.\n    acl_forget_proposed_device_ops(doq);\n  }\n  return result;\n}\n\nbool acl_usm_ptr_belongs_to_context(cl_context context, const void *ptr) {\n  if (!acl_get_usm_alloc_from_ptr(context, ptr)) {\n    return false;\n  }\n\n  return true;\n}\n\nacl_usm_allocation_t *acl_get_usm_alloc_from_ptr(cl_context context,\n                                                 const void *ptr) {\n  for (auto it : context->usm_allocation) {\n    if (l_ptr_in_usm_alloc_range(it, ptr, 1) == CL_TRUE) {\n      return it;\n    }\n  }\n  return NULL;\n}\n\ncl_bool l_ptr_in_usm_alloc_range(acl_usm_allocation_t *usm_alloc,\n                                 const void *ptr, size_t size) {\n  unsigned long long start = (unsigned long long)usm_alloc->range.begin;\n  unsigned long long end = (unsigned long long)usm_alloc->range.next;\n  if ((unsigned long long)ptr >= start &&\n      (unsigned long long)ptr + size <= end) {\n    return CL_TRUE;\n  } else {\n    return CL_FALSE;\n  }\n}\n\nvoid l_add_usm_alloc_to_context(cl_context context,\n                                acl_usm_allocation_t *usm_alloc) {\n  for (auto it : context->usm_allocation) {\n    if (it == usm_alloc) {\n      return;\n    }\n  }\n  context->usm_allocation.insert(context->usm_allocation.begin(), usm_alloc);\n}\n\nvoid l_remove_usm_alloc_from_context(cl_context context,\n                                     acl_usm_allocation_t *usm_alloc) {\n  context->usm_allocation.remove(usm_alloc);\n}\n\nvoid *l_set_dev_alloc_bit(const void *ptr) {\n  void *ptr_val =\n      (void *)((unsigned long long)ptr | ((unsigned long long)1 << 61));\n  return ptr_val;\n}\n\nvoid l_cl_mem_blocking_free(cl_context context, void *ptr) {\n  int num_command_queues = context->num_command_queues;\n  acl_usm_allocation_t *src_usm_alloc = NULL;\n  acl_usm_allocation_t *dst_usm_alloc = NULL;\n\n  for (int i = 0; i < num_command_queues; i++) {\n    cl_command_queue current_cq = context->command_queue[i];\n    // Set a flag to indicate the command set of this command queue is being\n    // traversed, and any event deletion should be deferred\n    current_cq->waiting_for_events = true;\n\n    if (current_cq->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n      // Current queue is ooo queue, check events in commands\n      for (auto it = current_cq->commands.begin();\n           it != current_cq->commands.end();) {\n        auto event = *it;\n        if (event->execution_status != CL_COMPLETE) {\n          // check if ptr is used by kernels when we submit to queue\n          if (event->ptr_hashtable.find(ptr) != event->ptr_hashtable.end()) {\n            clWaitForEvents(1, &event);\n          }\n          // check if ptr is used in queues\n          if ((event->cmd.type == CL_COMMAND_MEMCPY_INTEL) ||\n              (event->cmd.type == CL_COMMAND_MEMFILL_INTEL)) {\n            src_usm_alloc = acl_get_usm_alloc_from_ptr(\n                context, event->cmd.info.usm_xfer.src_ptr);\n            dst_usm_alloc = acl_get_usm_alloc_from_ptr(\n                context, event->cmd.info.usm_xfer.dst_ptr);\n            if ((src_usm_alloc && (src_usm_alloc->range.begin == ptr)) ||\n                (dst_usm_alloc && (dst_usm_alloc->range.begin == ptr))) {\n              clWaitForEvents(1, &event);\n            }\n          }\n        }\n        if (event->defer_removal) {\n          it = current_cq->commands.erase(it);\n          event->defer_removal = false; // Reset as this event might get reused\n        } else {\n          ++it;\n        }\n      }\n    } else {\n      // Current queue is inorder queue, check events in inorder commands\n      for (auto it = current_cq->inorder_commands.begin();\n           it != current_cq->inorder_commands.end();) {\n        auto event = *it;\n        if (event->execution_status != CL_COMPLETE) {\n          // check if ptr is used by kernels when we submit to queue\n          if (event->ptr_hashtable.find(ptr) != event->ptr_hashtable.end()) {\n            clWaitForEvents(1, &event);\n          }\n          // check if ptr is used in queues\n          if ((event->cmd.type == CL_COMMAND_MEMCPY_INTEL) ||\n              (event->cmd.type == CL_COMMAND_MEMFILL_INTEL)) {\n            src_usm_alloc = acl_get_usm_alloc_from_ptr(\n                context, event->cmd.info.usm_xfer.src_ptr);\n            dst_usm_alloc = acl_get_usm_alloc_from_ptr(\n                context, event->cmd.info.usm_xfer.dst_ptr);\n            if ((src_usm_alloc && (src_usm_alloc->range.begin == ptr)) ||\n                (dst_usm_alloc && (dst_usm_alloc->range.begin == ptr))) {\n              clWaitForEvents(1, &event);\n            }\n          }\n        }\n        if (event->defer_removal) {\n          it = current_cq->inorder_commands.erase(it);\n          event->defer_removal = false; // Reset as this event might get reused\n        } else {\n          ++it;\n        }\n      }\n    }\n    current_cq->waiting_for_events = false;\n  }\n}\n\nbool acl_usm_has_access_capability(cl_device_id device, cl_device_info query) {\n  cl_bitfield capabilities = 0;\n  cl_int ret =\n      clGetDeviceInfo(device, query, sizeof(capabilities), &capabilities, 0);\n  if (ret)\n    return false;\n\n  return capabilities & CL_UNIFIED_SHARED_MEMORY_ACCESS_INTEL;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_program.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <algorithm>\n#include <cassert>\n#include <fstream>\n#include <sstream>\n#include <stdio.h>\n#include <string>\n#include <utility>\n#include <vector>\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_hash/acl_hash.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_auto.h>\n#include <acl_command_queue.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_hostch.h>\n#include <acl_icd_dispatch.h>\n#include <acl_mem.h>\n#include <acl_program.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_types.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n#define MAX_STRING_LENGTH 100000\n\n// Programs\n// ========\n//\n// Lifecycle of cl_program:\n//    States are:\n//       \"new\"    - initial state. no program is built, and kernels can't be\n//       loaded \"built\"  - Program has been built, and kernels can be loaded.\n//\n//\n// Faking the build process:\n//\n//    Note: Because we don't really have a CL compiler, we're\n//    faking *everything* about program builds.  We only store\n//    a \"built\" boolean flag, and that's it.\n//\n//    The only interesting bit comes a the time we enqueue a\n//    kernel.\n//    At that time we trace:\n//       kernel -> program -> context -> devices\n//    and\n//       command_queue -> device\n//    Once we verify a common device is present, we can look\n//    up the kernel interface in the list of accelerator\n//    definitions inside the device definition.\n//\n//\n// Data model:\n//\n//    cl_program has:\n//       - reference to context\n//       - reference to list of devices\n//          - For now, we punt on this.\n//       - a representation:\n//          - Normally source binaries are stored here\n//          - But we're implementing an OpenCL embedded profile, so we\n//          don't have a compiler.\n//          - We're likely going to shortchange things here, i.e. store\n//          nothing\n//       - build info:\n//          - info to support clGetProgramBuildInfo\n//          - For now, this is just an overall build_status\n//       - kernels\n//          - instantiated kernels\n//          - Each kernel has:\n//             - reference to interface\n//             - argument values\n\nACL_DEFINE_CL_OBJECT_ALLOC_FUNCTIONS(cl_program);\n\n//////////////////////////////\n// Local functions\n\nstatic void l_init_program(cl_program program, cl_context context);\nstatic void l_free_program(cl_program program);\nstatic acl_device_program_info_t *\nl_create_dev_prog(cl_program program, cl_device_id device, size_t binary_len,\n                  const unsigned char *binary);\nstatic cl_int l_build_program_for_device(cl_program program,\n                                         unsigned int dev_idx,\n                                         const char *options);\nstatic void l_compute_hash(cl_program program,\n                           acl_device_program_info_t *dev_prog);\nstatic cl_int l_build_from_source(acl_device_program_info_t *dev_prog);\nstatic cl_int l_build_from_source_in_dir(acl_device_program_info_t *dev_prog,\n                                         const char *dir);\nstatic void l_try_to_eagerly_program_device(cl_program program);\nstatic void\nl_device_memory_definition_copy(acl_device_def_autodiscovery_t *dest_dev,\n                                acl_device_def_autodiscovery_t *src_dev);\nstatic cl_int\nl_register_hostpipes_to_program(acl_device_program_info_t *dev_prog,\n                                unsigned int physical_device_id,\n                                cl_context context);\n//////////////////////////////\n// OpenCL API\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainProgramIntelFPGA(cl_program program) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_program_is_valid(program)) {\n    return CL_INVALID_PROGRAM;\n  }\n  acl_retain(program);\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainProgram(cl_program program) {\n  return clRetainProgramIntelFPGA(program);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseProgramIntelFPGA(cl_program program) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_program_is_valid(program)) {\n    return CL_INVALID_PROGRAM;\n  }\n  acl_release(program);\n  if (!acl_ref_count(program)) {\n    acl_release(program->context);\n    acl_untrack_object(program);\n    // Make sure we clean up the elf files\n    for (unsigned i = 0; i < program->num_devices; i++) {\n      if (program->device[i]->loaded_bin != nullptr)\n        program->device[i]->loaded_bin->unload_content();\n      if (program->device[i]->last_bin != nullptr)\n        program->device[i]->last_bin->unload_content();\n    }\n    l_free_program(program);\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseProgram(cl_program program) {\n  return clReleaseProgramIntelFPGA(program);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clUnloadCompilerIntelFPGA(void) {\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clUnloadCompiler(void) {\n  return clUnloadCompilerIntelFPGA();\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clCreateProgramWithSourceIntelFPGA(\n    cl_context context, cl_uint count, const char **strings,\n    const size_t *lengths, cl_int *errcode_ret) {\n  cl_uint i;\n  cl_uint idev;\n  int pass;\n  cl_program program = 0;\n  struct acl_file_handle_t *capture_fp = NULL;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context))\n    BAIL(CL_INVALID_CONTEXT);\n\n  if (count == 0) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Count parameter is zero\");\n  }\n  if (strings == 0) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"No source strings specified\");\n  }\n  for (i = 0; i < count; i++) {\n    if (strings[i] == 0) {\n      BAIL_INFO(CL_INVALID_VALUE, context, \"A string pointers is NULL\");\n    }\n  }\n\n  // Go ahead and allocate it.\n  program = acl_alloc_cl_program();\n  if (program == 0) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a program object\");\n  }\n\n  l_init_program(program, context);\n\n  // Just copy devices straight over from the context.\n  program->num_devices = context->num_devices;\n  for (idev = 0; idev < context->num_devices; idev++) {\n    program->device[idev] = context->device[idev];\n  }\n\n  // Now process the source text.\n  // We have to store it, for later access via clGetProgramInfo.\n  // We do two passes: first to determine the text size, the second\n  // to actually store and possibly output the text.\n\n  // Capture the source if we've been asked.\n  capture_fp = NULL;\n  if (acl_get_platform()->next_capture_id != ACL_OPEN) {\n    std::stringstream name;\n    name << acl_get_platform()->capture_base_path << \".\"\n         << acl_get_platform()->next_capture_id << \"\\n\";\n    capture_fp = acl_fopen(name.str().c_str(), \"w\");\n    acl_get_platform()->next_capture_id++;\n  }\n\n  // Use two passes:\n  //    First pass computes the source size in bytes.\n  //    Second pass allocates memory and stores the data.\n  for (pass = 0; pass < 2; ++pass) {\n    size_t offset = 0;\n    if (pass == 1) {\n      unsigned char *buffer;\n      program->source_len++; // Must also reserve space for the terminating NUL.\n      buffer = (unsigned char *)acl_malloc(program->source_len);\n      if (buffer == 0) {\n        acl_free_cl_program(program);\n        if (capture_fp) {\n          acl_fclose(capture_fp);\n        }\n        BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                  \"Could not allocate memory to store program source\");\n      }\n      program->source_text = buffer;\n    }\n    for (i = 0; i < count; i++) {\n      size_t len = lengths ? lengths[i] : 0;\n      if (len == 0)\n        len = strnlen(strings[i], MAX_STRING_LENGTH);\n      switch (pass) {\n      case 0:\n        program->source_len += len;\n        break;\n      case 1:\n        safe_memcpy(program->source_text + offset, strings[i], len, len, len);\n        if (capture_fp) {\n          acl_fwrite(strings[i], sizeof(char), len, capture_fp);\n        }\n        offset += len;\n        break;\n      }\n    }\n    if (pass == 1)\n      program->source_text[offset] = 0; // Terminating NUL\n  }\n\n  if (capture_fp) {\n    acl_fclose(capture_fp);\n  }\n\n  acl_retain(program->context);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_PROGRAM, program);\n\n  return program;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clCreateProgramWithSource(\n    cl_context context, cl_uint count, const char **strings,\n    const size_t *lengths, cl_int *errcode_ret) {\n  return clCreateProgramWithSourceIntelFPGA(context, count, strings, lengths,\n                                            errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clCreateProgramWithBinaryIntelFPGA(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list,\n    const size_t *lengths, const unsigned char **binaries,\n    cl_int *binary_status, cl_int *errcode_ret) {\n  cl_uint i;\n  cl_uint idev;\n  cl_program program = 0;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context))\n    BAIL(CL_INVALID_CONTEXT);\n\n  if (num_devices == 0 || device_list == 0) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid device list\");\n  }\n\n  for (i = 0; i < num_devices; i++) {\n    if (!acl_device_is_valid(device_list[i])) {\n      BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid device\");\n    }\n    if (!acl_context_uses_device(context, device_list[i])) {\n      BAIL_INFO(CL_INVALID_DEVICE, context,\n                \"Device is not associated with the context\");\n    }\n    if (lengths[i] == 0 || binaries[i] == 0) {\n      if (binary_status) {\n        binary_status[i] = CL_INVALID_VALUE;\n      }\n      BAIL_INFO(CL_INVALID_VALUE, context,\n                lengths[i] == 0 ? \"A binary length is zero\"\n                                : \"A binary pointer is NULL\");\n    }\n  }\n\n  // Go ahead and allocate it.\n  program = acl_alloc_cl_program();\n  if (program == 0) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a program object\");\n  }\n\n  l_init_program(program, context);\n\n  // Copy devices from arguments and set status.\n  program->num_devices = num_devices;\n  for (idev = 0; idev < num_devices; idev++) {\n    program->device[idev] = device_list[idev];\n\n    // Save the binary in a new acl_device_program_info_t\n    program->dev_prog[idev] = l_create_dev_prog(program, device_list[idev],\n                                                lengths[idev], binaries[idev]);\n    if (program->dev_prog[idev]) {\n      if (context->programs_devices || context->uses_dynamic_sysdef) {\n        if (!context->split_kernel) {\n          // Load and validate the ELF package form.\n          auto status =\n              program->dev_prog[idev]->device_binary.load_binary_pkg(0, 1);\n          if (status != CL_SUCCESS) {\n            l_free_program(program);\n            if (binary_status) {\n              binary_status[idev] = CL_INVALID_BINARY;\n            }\n            BAIL_INFO(CL_INVALID_BINARY, context, \"Invalid binary\");\n          }\n        } else {\n          assert(context->uses_dynamic_sysdef);\n          // Allow disabling preloading of split binaries if the user requests\n          // it. This is required in cases where there are many aocx files in\n          // the directory and each cl_program that is created only uses a small\n          // subset of them. Note that when preloading is disabled we load the\n          // kernels when clCreateKernel is called. Also queries to\n          // CL_PROGRAM_NUM_KERNELS and CL_PROGRAM_KERNEL_NAMES in\n          // clGetProgramInfo will return inaccurate results unless all kernels\n          // in the program are created. Preloading is enabled by default.\n          const char *preload =\n              acl_getenv(\"CL_PRELOAD_SPLIT_BINARIES_INTELFPGA\");\n          if (!preload || std::string(preload) != \"0\") {\n            // In split_kernel mode we have to load all aocx files\n            // in the specified directory which cumulatively contain all the\n            // kernels.\n            auto result = acl_glob(std::string(context->program_library_root) +\n                                   std::string(\"/kernel_*.aocx\"));\n            for (const auto &filename : result) {\n              // Trim \".aocx\" file extension.\n              auto kernel_name = filename.substr(0, filename.length() - 5);\n              auto l = kernel_name.find_last_of(\"/\");\n              if (l != std::string::npos) {\n                kernel_name = kernel_name.substr(l + 1);\n              }\n\n              auto &dev_bin =\n                  program->dev_prog[idev]->add_split_binary(kernel_name);\n              dev_bin.load_content(filename);\n              auto status = dev_bin.load_binary_pkg(0, 1);\n              if (status != CL_SUCCESS) {\n                l_free_program(program);\n                if (binary_status) {\n                  binary_status[idev] = CL_INVALID_BINARY;\n                }\n                BAIL_INFO(CL_INVALID_BINARY, context, \"Invalid binary\");\n              }\n\n              // Need to unload the binary and only load it on an as needed\n              // basis due to high memory usage when there are many split\n              // binaries.\n              dev_bin.unload_content();\n            }\n          }\n        }\n      } else {\n        assert(!context->split_kernel);\n        // Copy memory definition from initial device def to program in\n        // CL_CONTEXT_COMPILER_MODE_INTELFPGA mode.\n        l_device_memory_definition_copy(\n            &(program->dev_prog[idev]\n                  ->device_binary.get_devdef()\n                  .autodiscovery_def),\n            &(program->device[idev]->def.autodiscovery_def));\n      }\n    } else {\n      // Release all the memory we've allocated.\n      l_free_program(program);\n      if (binary_status) {\n        binary_status[idev] = CL_INVALID_VALUE;\n      }\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not allocate memory to store program binaries\");\n    }\n\n    // Wait to set status until after failures may have occurred for this\n    // device.\n    if (binary_status) {\n      binary_status[idev] = CL_SUCCESS;\n    }\n  }\n\n  acl_retain(program->context);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_PROGRAM, program);\n\n  l_try_to_eagerly_program_device(program);\n\n  // Register the program scoped hostpipe to each dev_prog\n  for (idev = 0; idev < num_devices; idev++) {\n    l_register_hostpipes_to_program(program->dev_prog[idev], idev, context);\n  }\n\n  return program;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clCreateProgramWithBinary(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list,\n    const size_t *lengths, const unsigned char **binaries,\n    cl_int *binary_status, cl_int *errcode_ret) {\n  return clCreateProgramWithBinaryIntelFPGA(context, num_devices, device_list,\n                                            lengths, binaries, binary_status,\n                                            errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL\nclCreateProgramWithBinaryAndProgramDeviceIntelFPGA(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list,\n    const size_t *lengths, const unsigned char **binaries,\n    cl_int *binary_status, cl_int *errcode_ret) {\n  cl_uint i;\n  cl_uint idev;\n  cl_program program = 0;\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_context_is_valid(context))\n    BAIL(CL_INVALID_CONTEXT);\n\n  // split_kernel mode is not supported in this special extension API which is\n  // not part of the OpenCL standard.\n  assert(context->split_kernel == 0);\n\n  if (num_devices == 0 || device_list == 0) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid device list\");\n  }\n\n  for (i = 0; i < num_devices; i++) {\n    if (!acl_device_is_valid(device_list[i])) {\n      BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid device\");\n    }\n    if (!acl_context_uses_device(context, device_list[i])) {\n      BAIL_INFO(CL_INVALID_DEVICE, context,\n                \"Device is not associated with the context\");\n    }\n    if (lengths[i] == 0 || binaries[i] == 0) {\n      if (binary_status) {\n        binary_status[i] = CL_INVALID_VALUE;\n      }\n      BAIL_INFO(CL_INVALID_VALUE, context,\n                lengths[i] == 0 ? \"A binary length is zero\"\n                                : \"A binary pointer is NULL\");\n    }\n  }\n\n  program = acl_alloc_cl_program();\n  if (program == 0) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a program object\");\n  }\n\n  l_init_program(program, context);\n\n  // Copy devices from arguments and set status.\n  program->num_devices = num_devices;\n  for (idev = 0; idev < num_devices; idev++) {\n    program->device[idev] = device_list[idev];\n\n    // Save the binary in a new acl_device_program_info_t\n    program->dev_prog[idev] = l_create_dev_prog(program, device_list[idev],\n                                                lengths[idev], binaries[idev]);\n    if (program->dev_prog[idev]) {\n      if (context->programs_devices || context->uses_dynamic_sysdef) {\n        // Load and validate the ELF package form.\n        auto status =\n            program->dev_prog[idev]->device_binary.load_binary_pkg(0, 0);\n        if (status != CL_SUCCESS) {\n          l_free_program(program);\n          if (binary_status) {\n            binary_status[idev] = CL_INVALID_BINARY;\n          }\n          BAIL_INFO(CL_INVALID_BINARY, context, \"Invalid binary\");\n        }\n      } else {\n        // Copy memory definition from initial device def to program in\n        // CL_CONTEXT_COMPILER_MODE_INTELFPGA mode.\n        l_device_memory_definition_copy(\n            &(program->dev_prog[idev]\n                  ->device_binary.get_devdef()\n                  .autodiscovery_def),\n            &(program->device[idev]->def.autodiscovery_def));\n      }\n    } else {\n      // Release all the memory we've allocated.\n      l_free_program(program);\n      if (binary_status) {\n        binary_status[idev] = CL_INVALID_VALUE;\n      }\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not allocate memory to store program binaries\");\n    }\n\n    // Wait to set status until after failures may have occurred for this\n    // device.\n    if (binary_status) {\n      binary_status[idev] = CL_SUCCESS;\n    }\n  }\n\n  acl_retain(program->context);\n\n  acl_track_object(ACL_OBJ_PROGRAM, program);\n\n  for (idev = 0; idev < program->num_devices; idev++) {\n    acl_device_program_info_t *dev_prog = program->dev_prog[idev];\n\n    if (dev_prog && dev_prog->build_status == CL_BUILD_SUCCESS) {\n      cl_int result;\n\n      // Use a regular event with an internal command type.\n      // We want a regular event so we get profiling info, so that we\n      // have the infrastructure to measure delays, and the user\n      // could in principle optimize this delay away.\n      cl_event reprogram_event = 0;\n\n      // Just use the auto_queue. We really only support one device\n      // anyway.\n      cl_command_queue cq = program->context->auto_queue;\n\n      // Schedule an eager programming of the device.\n      acl_print_debug_msg(\n          \"Device is not yet programmed: plan to eagerly program it\\n\");\n\n      result = acl_create_event(cq, 0, 0, // Don't wait on other events\n                                CL_COMMAND_PROGRAM_DEVICE_INTELFPGA,\n                                &reprogram_event);\n\n      if (result == CL_SUCCESS) {\n        acl_device_op_t reprogram_op;\n        reprogram_event->cmd.info.eager_program = &(dev_prog->device_binary);\n\n        // Try scheduling it.\n        reprogram_op.link = ACL_OPEN;\n        acl_device_op_reset_device_op(&reprogram_op);\n\n        reprogram_op.status = ACL_PROPOSED;\n        reprogram_op.execution_status = ACL_PROPOSED;\n        reprogram_op.info.type = ACL_DEVICE_OP_REPROGRAM;\n        reprogram_op.info.event = reprogram_event;\n        reprogram_op.info.index = 0;\n        reprogram_op.conflict_type = acl_device_op_conflict_type(&reprogram_op);\n\n        acl_program_device(NULL, &reprogram_op);\n\n        if (reprogram_op.execution_status != CL_SUCCESS) {\n          BAIL_INFO(CL_DEVICE_NOT_AVAILABLE, context,\n                    \"Reprogram of device failed\");\n        }\n\n      } else {\n        BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context, \"Invalid binary\");\n      }\n    } else {\n      BAIL_INFO(CL_BUILD_PROGRAM_FAILURE, context,\n                \"Program is not built correctly\");\n    }\n  }\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  return program;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clCreateProgramWithBuiltInKernelsIntelFPGA(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list,\n    const char *kernel_names, cl_int *errcode_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context))\n    BAIL(CL_INVALID_CONTEXT);\n\n  if (num_devices == 0 || device_list == 0) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid device list\");\n  }\n\n  if (kernel_names == NULL) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"kernel_names is NULL\");\n  }\n\n  if (num_devices >= ACL_MAX_DEVICE) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"num_dives specified is great thatn ACL_MAX_DEVICES\");\n  }\n\n  // list of semicolon delimited string of kernel names\n  std::set<std::string> kernel_names_split;\n  std::string token;\n  std::istringstream tokenStream(kernel_names);\n  while (std::getline(tokenStream, token, ';')) {\n    kernel_names_split.insert(token);\n  };\n\n  for (cl_uint i = 0; i < num_devices; i++) {\n    if (!acl_device_is_valid(device_list[i])) {\n      BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid device\");\n    }\n\n    if (!acl_context_uses_device(context, device_list[i])) {\n      BAIL_INFO(CL_INVALID_DEVICE, context,\n                \"Device is not associated with the context\");\n    }\n\n    // make sure current device contains all the builtin kernels\n    size_t find_count = kernel_names_split.size();\n    for (acl_accel_def_t accel : device_list[i]->def.autodiscovery_def.accel) {\n\n      if (kernel_names_split.count(accel.iface.name)) {\n        // found one of the kernels the device needs to have\n        find_count--;\n      }\n\n      if (find_count == 0)\n        break;\n    }\n    if (find_count != 0) {\n      BAIL_INFO(CL_INVALID_VALUE, context,\n                \"kernel_names contains a kernel name that is not \"\n                \"supported by all of the devices in device_list\");\n    }\n  }\n\n  // Go ahead and allocate it.\n  cl_program program = acl_alloc_cl_program();\n  if (program == 0) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a program object\");\n  }\n\n  l_init_program(program, context);\n\n  // Copy devices from arguments and set status.\n  program->num_devices = num_devices;\n  for (cl_uint idev = 0; idev < num_devices; idev++) {\n    program->device[idev] = device_list[idev];\n\n    // Save the binary in a new acl_device_program_info_t\n    program->dev_prog[idev] =\n        l_create_dev_prog(program, device_list[idev], 0, NULL);\n    if (program->dev_prog[idev]) {\n      if (context->programs_devices || context->uses_dynamic_sysdef) {\n        BAIL_INFO(CL_INVALID_VALUE, context, \"No builtin kernels available\\n\");\n      } else {\n\n        // i put this here since dla flow makes call to clGetProgramInfo which\n        // requires CL_BUILD_SUCCESS\n        program->dev_prog[idev]->build_status = CL_BUILD_SUCCESS;\n\n        // Copy memory definition from initial device def to program in\n        // CL_CONTEXT_COMPILER_MODE_INTELFPGA mode.\n        l_device_memory_definition_copy(\n            &(program->dev_prog[idev]\n                  ->device_binary.get_devdef()\n                  .autodiscovery_def),\n            &(program->device[idev]->def.autodiscovery_def));\n      }\n    } else {\n      // Release all the memory we've allocated.\n      l_free_program(program);\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not allocate memory to store program binaries\");\n    }\n  }\n\n  program->uses_builtin_kernels = CL_TRUE;\n\n  acl_retain(program->context);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_PROGRAM, program);\n\n  return program;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clCreateProgramWithBuiltInKernels(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list,\n    const char *kernel_names, cl_int *errcode_ret) {\n  return clCreateProgramWithBuiltInKernelsIntelFPGA(\n      context, num_devices, device_list, kernel_names, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clCompileProgramIntelFPGA(\n    cl_program program, cl_uint num_devices, const cl_device_id *device_list,\n    const char *options, cl_uint num_input_headers,\n    const cl_program *input_headers, const char **header_include_names,\n    acl_program_build_notify_fn_t pfn_notify, void *user_data) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_program_is_valid(program))\n    return CL_INVALID_PROGRAM;\n\n  // Suppress compiler warnings.\n  num_devices = num_devices;\n  device_list = device_list;\n  options = options;\n  num_input_headers = num_input_headers;\n  input_headers = input_headers;\n  header_include_names = header_include_names;\n  pfn_notify = pfn_notify;\n  user_data = user_data;\n\n  ERR_RET(CL_COMPILER_NOT_AVAILABLE, program->context,\n          \"Device compiler is not available\");\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clCompileProgram(\n    cl_program program, cl_uint num_devices, const cl_device_id *device_list,\n    const char *options, cl_uint num_input_headers,\n    const cl_program *input_headers, const char **header_include_names,\n    acl_program_build_notify_fn_t pfn_notify, void *user_data) {\n  return clCompileProgramIntelFPGA(program, num_devices, device_list, options,\n                                   num_input_headers, input_headers,\n                                   header_include_names, pfn_notify, user_data);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clLinkProgramIntelFPGA(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list,\n    const char *options, cl_uint num_input_programs,\n    const cl_program *input_programs, acl_program_build_notify_fn_t pfn_notify,\n    void *user_data, cl_int *errcode_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_context_is_valid(context))\n    BAIL(CL_INVALID_CONTEXT);\n  // For the sake of MSVC compiler warnings.\n  num_devices = num_devices;\n  device_list = device_list;\n  options = options;\n  num_input_programs = num_input_programs;\n  input_programs = input_programs;\n  pfn_notify = pfn_notify;\n  user_data = user_data;\n\n  BAIL_INFO(CL_LINKER_NOT_AVAILABLE, context, \"Device linker is not available\");\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_program CL_API_CALL clLinkProgram(\n    cl_context context, cl_uint num_devices, const cl_device_id *device_list,\n    const char *options, cl_uint num_input_programs,\n    const cl_program *input_programs, acl_program_build_notify_fn_t pfn_notify,\n    void *user_data, cl_int *errcode_ret) {\n  return clLinkProgramIntelFPGA(context, num_devices, device_list, options,\n                                num_input_programs, input_programs, pfn_notify,\n                                user_data, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetProgramInfoIntelFPGA(\n    cl_program program, cl_program_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  cl_context context;\n  acl_result_t result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_program_is_valid(program)) {\n    return CL_INVALID_PROGRAM;\n  }\n  context = program->context;\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          context);\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_PROGRAM_REFERENCE_COUNT:\n    RESULT_UINT(acl_ref_count(program));\n    break;\n  case CL_PROGRAM_CONTEXT:\n    RESULT_PTR(program->context);\n    break;\n  case CL_PROGRAM_NUM_DEVICES:\n    RESULT_UINT(program->num_devices);\n    break;\n  case CL_PROGRAM_DEVICES:\n    RESULT_BUF(program->device,\n               program->num_devices * sizeof(program->device[0]));\n    break;\n\n  case CL_PROGRAM_SOURCE:\n    // Program source could be empty if the program was created from\n    // binary.\n    RESULT_BUF(program->source_text, program->source_len);\n    break;\n\n  case CL_PROGRAM_BINARY_SIZES: {\n    // Special case to copy the binary sizes.\n    // We don't store it in the shape that this query wants it.\n    // This returns early!\n    if (param_value_size_ret) {\n      *param_value_size_ret = program->num_devices * sizeof(size_t);\n    }\n    if (param_value) {\n      // They actually want the values\n\n      if (param_value_size < (program->num_devices * sizeof(size_t))) {\n        ERR_RET(CL_INVALID_VALUE, context,\n                \"Parameter return buffer is too small\");\n      }\n      for (unsigned i = 0; i < program->num_devices; i++) {\n        // program->dev_prog[] could be NULL if a compile failed.\n        auto *dev_prog = program->dev_prog[i];\n        ((size_t *)param_value)[i] =\n            dev_prog ? dev_prog->device_binary.get_binary_len() : 0;\n      }\n    }\n    return CL_SUCCESS;\n  }\n\n  case CL_PROGRAM_BINARIES: {\n    // Special case to copy a sequence of buffers.\n    // This returns early!\n\n    // Put the size copyback first in case we error out later.\n    if (param_value_size_ret) {\n      *param_value_size_ret = program->num_devices * sizeof(char *);\n    }\n    if (param_value) {\n      // They actually want the values\n      unsigned char **dest = (unsigned char **)param_value;\n      if (param_value_size < (program->num_devices * sizeof(char *))) {\n        ERR_RET(CL_INVALID_VALUE, context,\n                \"Parameter return buffer is too small\");\n      }\n      for (unsigned i = 0; i < program->num_devices; ++i) {\n        auto *dev_prog = program->dev_prog[i];\n        if (dest[i] == nullptr) {\n          // Spec says:\n          // If an entry value in the array is NULL, the implementation skips\n          // copying the program binary for the specific device identified by\n          // the array index.\n          continue;\n        }\n        // The dev_prog or binary could be NULL if an attempted compile failed.\n        // But the call should still succeed.\n        if (dev_prog && dev_prog->device_binary.get_binary_len() > 0) {\n          const auto &db = dev_prog->device_binary;\n          // The OpenCL spec implies that the user must ensure dest[i] has\n          // enough allocated space to store the entire contents of the binary.\n          std::copy(db.get_content(), db.get_content() + db.get_binary_len(),\n                    dest[i]);\n        }\n      }\n    }\n    return CL_SUCCESS;\n  }\n\n  case CL_PROGRAM_NUM_KERNELS: {\n    size_t kernel_cnt = 0;\n    char exists_built_dev_prog =\n        0; // a flag to indicate if any built dev_prog exists.\n    for (cl_uint idev = 0; idev < program->num_devices; ++idev) {\n      acl_device_program_info_t *dev_prog = program->dev_prog[idev];\n      if (dev_prog && dev_prog->build_status == CL_BUILD_SUCCESS) {\n        // We need to find the number of kernels on one successfully built\n        // dev_prog.\n        kernel_cnt =\n            context->uses_dynamic_sysdef\n                ? program->dev_prog[idev]->get_num_kernels()\n                : program->device[idev]->def.autodiscovery_def.accel.size();\n\n        exists_built_dev_prog = 1;\n        break; // the rest, if any, will be repetitive\n      }\n    }\n    if (!exists_built_dev_prog)\n      ERR_RET(CL_INVALID_PROGRAM_EXECUTABLE, context,\n              \"A successfully built program executable was not found for any \"\n              \"device in the list of devices associated with program\");\n\n    RESULT_SIZE_T(kernel_cnt);\n    break;\n  }\n  case CL_PROGRAM_KERNEL_NAMES: {\n    // Special case to copy the name of all kernels.\n    // This returns early!\n    size_t total_ret_len = 0; // we don't know the param_Value_size_ret yet.\n    bool exists_built_dev_prog =\n        0; // a flag to indicate if any built dev_prog exists.\n\n    // Go through devices in this program for which the build status is\n    // successful.\n\n    // First, find the total return size:\n    std::set<std::string> names;\n    for (cl_uint idev = 0; idev < program->num_devices; idev++) {\n      acl_device_program_info_t *dev_prog = program->dev_prog[idev];\n      // finding a dev_prog that is built sucessfully.\n      if (dev_prog && dev_prog->build_status == CL_BUILD_SUCCESS) {\n        if (context->uses_dynamic_sysdef) {\n          names = dev_prog->get_all_kernel_names();\n        } else {\n          for (const auto &a :\n               program->device[idev]->def.autodiscovery_def.accel) {\n            names.insert(a.iface.name);\n          }\n        }\n\n        exists_built_dev_prog = true;\n        for (const auto &n : names) {\n          total_ret_len +=\n              n.length() +\n              1; //+1 is for the extra semi-colon to separate names.\n        }\n        break; // The rest, if any, will be repetitive.\n      }\n    }\n\n    if (!exists_built_dev_prog)\n      ERR_RET(CL_INVALID_PROGRAM_EXECUTABLE, context,\n              \"A successfully built program executable was not \"\n              \"found for any device in the list of devices \"\n              \"associated with program\");\n\n    // Based on the OpenCL 1.2 CTS api test, total_ret_len must include the\n    // space for the null terminator.\n    total_ret_len = total_ret_len > 0 ? total_ret_len : 1;\n    if (param_value_size_ret) {\n      *param_value_size_ret = total_ret_len;\n    }\n\n    if (param_value) {\n      if (total_ret_len > param_value_size) {\n        ERR_RET(CL_INVALID_VALUE, context,\n                \"Parameter return buffer is too small\");\n      }\n\n      std::stringstream ss;\n      size_t i = 0;\n      for (const auto &n : names) {\n        ss << n;\n        if (i < names.size() - 1)\n          ss << \";\";\n        ++i;\n      }\n\n      auto result_str = ss.str();\n      assert(result_str.length() == total_ret_len - 1);\n      safe_memcpy(param_value, result_str.c_str(), total_ret_len, total_ret_len,\n                  total_ret_len);\n    }\n  }\n    return CL_SUCCESS;\n\n  default:\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid program info query\");\n  }\n  // zero size result is valid!\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetProgramInfo(cl_program program,\n                                                 cl_program_info param_name,\n                                                 size_t param_value_size,\n                                                 void *param_value,\n                                                 size_t *param_value_size_ret) {\n  return clGetProgramInfoIntelFPGA(program, param_name, param_value_size,\n                                   param_value, param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetProgramBuildInfoIntelFPGA(\n    cl_program program, cl_device_id device, cl_program_build_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  cl_uint dev_idx;\n  cl_context context;\n  acl_device_program_info_t *dev_prog;\n  acl_result_t result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_program_is_valid(program)) {\n    return CL_INVALID_PROGRAM;\n  }\n  context = program->context;\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          context);\n\n  RESULT_INIT;\n\n  for (dev_idx = 0; dev_idx < program->num_devices; dev_idx++) {\n    if (program->device[dev_idx] == device) {\n      break;\n    }\n  }\n  if (dev_idx >= program->num_devices) {\n    ERR_RET(CL_INVALID_DEVICE, context,\n            \"The specified device is not associated with the program\");\n  }\n  dev_prog = program->dev_prog[dev_idx];\n\n  // The 1.0 conformance tests query CL_PROGRAM_BUILD_STATUS even if the build\n  // failed.  It will ask the other queries only if the build succeeded.\n  // So it's ok to return empty string for options and log if the build failed.\n  switch (param_name) {\n  case CL_PROGRAM_BUILD_STATUS:\n    RESULT_ENUM(dev_prog ? dev_prog->build_status : CL_BUILD_NONE);\n    break;\n  case CL_PROGRAM_BUILD_OPTIONS: {\n    const char *options = dev_prog ? dev_prog->build_options.c_str() : \"\";\n    RESULT_STR(options);\n  } break;\n  case CL_PROGRAM_BUILD_LOG: {\n    const char *log = dev_prog ? dev_prog->build_log.c_str() : \"\";\n    RESULT_STR(log);\n  } break;\n  case CL_PROGRAM_BINARY_TYPE: {\n    // If we don't have dev_prog, we just return CL_PROGRAM_BINARY_TYPE_NONE.\n    RESULT_ENUM(dev_prog ? dev_prog->device_binary.get_binary_type()\n                         : CL_PROGRAM_BINARY_TYPE_NONE);\n  } break;\n\n  default:\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid program build info query\");\n  }\n\n  if (result.size == 0) {\n    return CL_INVALID_VALUE;\n  } // should already have signalled\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetProgramBuildInfo(\n    cl_program program, cl_device_id device, cl_program_build_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  return clGetProgramBuildInfoIntelFPGA(program, device, param_name,\n                                        param_value_size, param_value,\n                                        param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clBuildProgramIntelFPGA(\n    cl_program program, cl_uint num_devices, const cl_device_id *device_list,\n    const char *options, acl_program_build_notify_fn_t pfn_notify,\n    void *user_data) {\n  cl_context context;\n  cl_int status = CL_SUCCESS;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_program_is_valid(program)) {\n    return CL_INVALID_PROGRAM;\n  }\n  context = program->context;\n  acl_print_debug_msg(\"Building program...\\n\");\n\n  if (program->num_kernels > 0) {\n    ERR_RET(CL_INVALID_OPERATION, context,\n            \"At least one kernel is still attached to the program\");\n  }\n  if (device_list && num_devices == 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid device list: num_devices is zero but device_list \"\n            \"is specified\");\n  }\n  if (0 == device_list && num_devices > 0) {\n    ERR_RET(\n        CL_INVALID_VALUE, context,\n        \"Invalid device list: num_devices is non-zero but device_list is NULL\");\n  }\n\n  if (pfn_notify == 0 && user_data != 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"user_data is set but pfn_notify is not\");\n  }\n\n  if (device_list) {\n    // The supplied devices must be associated with the program.\n    cl_uint idev, iprogdev;\n    for (idev = 0; idev < num_devices; idev++) {\n      // Optimize the common case: the caller is just using the same\n      // device list as passed in to the program creation (or context\n      // creation).\n      int saw_it = idev < program->num_devices &&\n                   program->device[idev] == device_list[idev];\n      for (iprogdev = 0; iprogdev < program->num_devices && !saw_it;\n           iprogdev++) {\n        saw_it = (program->device[iprogdev] == device_list[idev]);\n      }\n      if (!saw_it) {\n        ERR_RET(CL_INVALID_DEVICE, context,\n                \"A specified device is not associated with the program\");\n      }\n    }\n    // Ok, each device is associated with the program.\n  } else {\n    // Build for all devices in the program.\n    num_devices = program->num_devices;\n    device_list = program->device;\n  }\n\n  acl_print_debug_msg(\"Building program for each device\\n\");\n  // Actually build the program for each device.\n  // But we have to use the indexing of program->device[]\n  {\n    cl_uint idev, iprogdev;\n    for (idev = 0; idev < num_devices && status == CL_SUCCESS; idev++) {\n      // Optimize the common case: the caller is just using the same\n      // device list as passed in to the program creation (or context\n      // creation).\n      if (idev < program->num_devices &&\n          program->device[idev] == device_list[idev]) {\n        status = l_build_program_for_device(program, idev, options);\n      } else {\n        for (iprogdev = 0; iprogdev < program->num_devices; iprogdev++) {\n          if (program->device[iprogdev] == device_list[idev]) {\n            status = l_build_program_for_device(program, iprogdev, options);\n            break;\n          }\n        }\n      }\n    }\n  }\n  acl_print_debug_msg(\"Building program...status is %d\\n\", status);\n\n  if (status == CL_SUCCESS)\n    l_try_to_eagerly_program_device(program);\n\n  // Call the notification callback.\n  if (pfn_notify)\n    pfn_notify(program, user_data);\n  return status;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclBuildProgram(cl_program program, cl_uint num_devices,\n               const cl_device_id *device_list, const char *options,\n               acl_program_build_notify_fn_t pfn_notify, void *user_data) {\n  return clBuildProgramIntelFPGA(program, num_devices, device_list, options,\n                                 pfn_notify, user_data);\n}\n//////////////////////////////\n// Internal\n\nstatic void l_free_program(cl_program program) {\n  unsigned i;\n  kernel_list_t *curr_kernel_list;\n  kernel_list_t *next_kernel_list;\n  acl_assert_locked();\n\n  acl_free(program->source_text);\n  for (i = 0; i < sizeof(program->dev_prog) / sizeof(program->dev_prog[0]);\n       i++) {\n    if (program->dev_prog[i]) {\n      acl_delete(program->dev_prog[i]);\n    }\n  }\n\n  curr_kernel_list = program->kernel_list;\n  while (curr_kernel_list != NULL) {\n    next_kernel_list = curr_kernel_list->next;\n    acl_free_cl_kernel(curr_kernel_list->kernel);\n    acl_free(curr_kernel_list);\n    curr_kernel_list = next_kernel_list;\n  }\n  curr_kernel_list = program->kernel_free_list;\n  while (curr_kernel_list != NULL) {\n    next_kernel_list = curr_kernel_list->next;\n    acl_free_cl_kernel(curr_kernel_list->kernel);\n    acl_free(curr_kernel_list);\n    curr_kernel_list = next_kernel_list;\n  }\n  acl_free_cl_program(program);\n}\n\n// Just a regular old reference counted object.\nint acl_program_is_valid(cl_program program) {\n  acl_assert_locked();\n\n  if (!acl_is_valid_ptr(program))\n    return 0;\n  if (!acl_ref_count(program))\n    return 0;\n  if (!acl_context_is_valid(program->context))\n    return 0;\n  return 1;\n}\n\nstatic void l_init_program(cl_program program, cl_context context) {\n  acl_assert_locked();\n\n  acl_retain(program);\n  program->dispatch = &acl_icd_dispatch;\n  program->context = context;\n  program->kernel_list = NULL;\n  program->kernel_free_list = NULL;\n  program->num_kernels = 0;\n  program->uses_builtin_kernels = CL_FALSE;\n}\n\nvoid acl_program_invalidate_builds(cl_program program) {\n  acl_assert_locked();\n\n  // Mark all builds as invalid, and delete them.\n  // This is used for testing.\n  if (acl_program_is_valid(program)) {\n    cl_uint idev;\n    for (idev = 0; idev < ACL_MAX_DEVICE; idev++) {\n      if (program->dev_prog[idev])\n        program->dev_prog[idev]->build_status = CL_BUILD_NONE;\n    }\n  }\n}\n\n// Create dev_prog.\n// Use NULL for binary if you don't have one yet.\nstatic acl_device_program_info_t *\nl_create_dev_prog(cl_program program, cl_device_id device, size_t binary_len,\n                  const unsigned char *binary) {\n  acl_assert_locked();\n\n  auto *result = acl_new<acl_device_program_info_t>();\n  if (result == nullptr) {\n    return nullptr;\n  }\n  result->program = program;\n  result->device = device;\n  result->build_status = CL_BUILD_NONE;\n\n  if (binary) {\n    result->device_binary.load_content(binary, binary_len);\n\n    // The OpenCL 1.2 spec doesn't say whether the user has to call\n    // clBuildProgram or clCompileProgram after loading from binary. The 1.2\n    // conformance tests never build or compile after loading from binary. And\n    // the goHDR example code does not do clBuildProgram.\n    //\n    // We don't do anything on \"build\" action.\n    // So enhance usability and user expectations by automatically setting\n    // build status to success.\n    result->build_status = CL_BUILD_SUCCESS;\n  }\n\n  return result;\n}\n\n// Loop through auto-discovery string and store program scope hostpipe\n// information in the device program info\nstatic cl_int\nl_register_hostpipes_to_program(acl_device_program_info_t *dev_prog,\n                                unsigned int physical_device_id,\n                                cl_context context) {\n\n  host_pipe_t host_pipe_info;\n\n  for (const auto &hostpipe : dev_prog->device_binary.get_devdef()\n                                  .autodiscovery_def.hostpipe_mappings) {\n    // Skip if the hostpipe doesn't have a logical name.\n    // It's not the program scoped hostpipe.\n    if (hostpipe.logical_name == \"-\") {\n      continue;\n    }\n    // Skip if the hostpipe is already registered in the program\n    auto search = dev_prog->program_hostpipe_map.find(hostpipe.logical_name);\n    if (search != dev_prog->program_hostpipe_map.end()) {\n      continue;\n    }\n    host_pipe_t host_pipe_info;\n    host_pipe_info.m_physical_device_id = physical_device_id;\n    if (hostpipe.is_read && hostpipe.is_write) {\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"Hostpipes don't allow both read and write operations from the \"\n              \"host.\");\n    }\n    if (!hostpipe.is_read && !hostpipe.is_write) {\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"The hostpipe direction is not set.\");\n    }\n\n    if (hostpipe.implement_in_csr) {\n      // CSR hostpipe read and write from the given CSR address directly\n      host_pipe_info.implement_in_csr = true;\n      host_pipe_info.csr_address = hostpipe.csr_address;\n      // CSR pipe doesn't use m_channel_handle but we want to have it\n      // initialized.\n      host_pipe_info.m_channel_handle = -1;\n    } else {\n      host_pipe_info.implement_in_csr = false;\n      host_pipe_info.m_channel_handle = acl_get_hal()->hostchannel_create(\n          physical_device_id, (char *)hostpipe.physical_name.c_str(),\n          hostpipe.pipe_depth, hostpipe.pipe_width,\n          hostpipe.is_read); // If it's a read pipe, pass 1 to the\n                             // hostchannel_create, which is HOST_TO_DEVICE\n      if (host_pipe_info.m_channel_handle <= 0) {\n        return CL_INVALID_VALUE;\n      }\n    }\n    host_pipe_info.protocol = hostpipe.protocol;\n    host_pipe_info.is_stall_free = hostpipe.is_stall_free;\n    acl_mutex_init(&(host_pipe_info.m_lock), NULL);\n    // The following property is not used by the program scoped hostpipe but we\n    // don't want to leave it uninitialized\n    host_pipe_info.binded = false;\n    host_pipe_info.m_binded_kernel = NULL;\n    host_pipe_info.size_buffered = 0;\n\n    dev_prog->program_hostpipe_map[hostpipe.logical_name] =\n        std::move(host_pipe_info);\n  }\n\n  // Start from 2024.1, Runtime receives sideband signals information\n  for (const auto &sideband_signal_mapping :\n       dev_prog->device_binary.get_devdef()\n           .autodiscovery_def.sideband_signal_mappings) {\n\n    // Skip if the sideband_signal doesn't have a logical name.\n    // It's not the program scoped hostpipe.\n    if (sideband_signal_mapping.logical_name == \"-\") {\n      continue;\n    }\n\n    // The hostpipe hostpipe logical name must be found in the program\n    auto search = dev_prog->program_hostpipe_map.find(\n        sideband_signal_mapping.logical_name);\n    if (search == dev_prog->program_hostpipe_map.end()) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Sideband signal is binded to non-exist hostpipe\");\n    }\n\n    auto &host_pipe_info =\n        dev_prog->program_hostpipe_map.at(sideband_signal_mapping.logical_name);\n    if (sideband_signal_mapping.port_identifier !=\n        static_cast<unsigned>(AOCL_MMD_HOSTCHANNEL_PORT_DATA)) {\n      host_pipe_info.num_side_band_signals++;\n    }\n\n    // Store the sideband info into the hostpipe info.\n    sideband_signal_t sideband_signal_info;\n    sideband_signal_info.port_identifier =\n        sideband_signal_mapping.port_identifier;\n    sideband_signal_info.port_offset = sideband_signal_mapping.port_offset;\n    sideband_signal_info.side_band_size = sideband_signal_mapping.sideband_size;\n\n    host_pipe_info.side_band_signals_vector.emplace_back(sideband_signal_info);\n  }\n\n  return CL_SUCCESS;\n}\n\nstatic cl_int l_build_program_for_device(cl_program program,\n                                         unsigned int dev_idx,\n                                         const char *options) {\n\n  acl_device_program_info_t *dev_prog = 0;\n  cl_context context;\n  int build_status; // CL_BUILD_IN_PROGRESS, CL_BUILD_ERROR, or\n                    // CL_BUILD_SUCCESS.\n  cl_int status = CL_SUCCESS;\n  acl_assert_locked();\n\n  context = program->context;\n\n  if (!program->source_text) {\n    // Program was created from binary.\n    dev_prog = program->dev_prog[dev_idx];\n    // User might have provided a bad binary (e.g. random bytes).\n    // Need to check that once we can.\n    // So we can only do a NULL check, but\n    // clCreateProgramWithBinary would already have failed due to NULL binary.\n    if (!dev_prog)\n      ERR_RET(CL_INVALID_BINARY, context, \"No binary loaded for device\");\n\n    // Prep for re-build.\n    dev_prog->build_status = CL_BUILD_ERROR;\n  } else {\n    // Program was created from source.\n    assert(context->split_kernel == 0);\n\n    // clBuildProgram should succeed for all but the offline case.\n    // Easiest to check the enum instead of creating a new context\n    // attribute.\n    if (context->compiler_mode == CL_CONTEXT_COMPILER_MODE_OFFLINE_INTELFPGA) {\n      ERR_RET(CL_COMPILER_NOT_AVAILABLE, context,\n              \"Device code compiler is not available\");\n    }\n    acl_delete(program->dev_prog[dev_idx]);\n    program->dev_prog[dev_idx] = nullptr;\n\n    dev_prog = l_create_dev_prog(program, program->device[dev_idx], 0, 0);\n    if (!dev_prog) {\n      ERR_RET(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate memory to store the program binary\");\n    }\n    // num_global_mem_systems of device binary autodiscovery def will remain 0\n    // only when CL_CONTEXT_COMPILER_MODE_INTELFPGA is\n    // ACL_COMPILER_MODE_PRELOADED_BINARY_ONLY,\n    // in these situations we should only use num_global_memory_system of device\n    // autodiscovery def which would be initialized with acl_kernel_if_init by\n    // reading the autodiscovery string of the bitstream currently programmed on\n    // the board.\n    dev_prog->device_binary.get_devdef()\n        .autodiscovery_def.num_global_mem_systems = 0;\n    program->dev_prog[dev_idx] = dev_prog;\n  }\n  // At this point dev_prog is a valid pointer, and an alias to\n  // program->dev_prog[dev_idx]\n  build_status = CL_BUILD_IN_PROGRESS;\n\n  dev_prog->build_options = options ? options : \"\";\n  auto last_pos = dev_prog->build_options.find_last_not_of(\" \");\n  if (last_pos != std::string::npos) {\n    // Trim trailing spaces.\n    dev_prog->build_options = dev_prog->build_options.substr(0, last_pos + 1);\n  }\n\n  if (program->source_text) {\n    // Compiling from source.  Need a hash.\n    l_compute_hash(program, dev_prog);\n  } else {\n    // Just keep what we loaded before.\n    // Even if options changed, we're not changing the binary.\n    // So the hash stays the same!\n  }\n  if (context->uses_program_library && program->source_text) {\n    // Compile from source.\n    acl_print_debug_msg(\"Compile from source\\n\");\n\n    // In failure case, this will signal error.\n    status = l_build_from_source(dev_prog);\n    if (status != CL_SUCCESS) {\n      build_status = CL_BUILD_ERROR;\n    }\n    if (dev_prog->build_log == \"\") {\n      dev_prog->build_log = \"Build failed, without log\";\n    }\n  } else {\n    dev_prog->build_log = \"Trivial build\";\n  }\n\n  // If no errors, then all is ok!\n  if (build_status == CL_BUILD_IN_PROGRESS)\n    build_status = CL_BUILD_SUCCESS;\n\n  dev_prog->build_status = build_status;\n  return status;\n}\n\nstatic void l_compute_hash(cl_program program,\n                           acl_device_program_info_t *dev_prog) {\n  acl_hash_context_t ctx;\n  acl_assert_locked();\n\n  acl_hash_init_sha1(&ctx);\n\n  // Depends on board name.\n  acl_hash_add(&ctx, dev_prog->device->def.autodiscovery_def.name.c_str(),\n               dev_prog->device->def.autodiscovery_def.name.length());\n  acl_hash_add(&ctx, (const void *)'\\0', 1);\n\n  // And compile options.\n  acl_hash_add(&ctx, dev_prog->build_options.c_str(),\n               dev_prog->build_options.length());\n  acl_hash_add(&ctx, (const void *)'\\0', 1);\n\n  // And the source\n  acl_hash_add(&ctx, program->source_text, program->source_len);\n\n  // Finalize and store the result.\n  char hash[ACL_HASH_SHA1_DIGEST_BUFSIZE];\n  acl_hash_hexdigest(&ctx, hash, sizeof(hash));\n  dev_prog->hash = std::string(hash);\n}\n\ncl_kernel acl_program_alloc_kernel(cl_program program) {\n  cl_kernel result = 0;\n  acl_assert_locked();\n\n  if (acl_program_is_valid(program)) {\n    kernel_list_t *tmp_kernel_list;\n    if (program->kernel_free_list != NULL) {\n      tmp_kernel_list = program->kernel_free_list;\n      program->kernel_free_list = program->kernel_free_list->next;\n    } else {\n      cl_kernel kernel = acl_alloc_cl_kernel();\n      if (!kernel) {\n        return 0;\n      }\n      tmp_kernel_list = (kernel_list_t *)acl_malloc(sizeof(kernel_list_t));\n      if (!tmp_kernel_list) {\n        acl_free_cl_kernel(kernel);\n        return 0;\n      }\n      tmp_kernel_list->kernel = kernel;\n    }\n    tmp_kernel_list->next = program->kernel_list;\n    program->kernel_list = tmp_kernel_list;\n    ++program->num_kernels;\n    result = tmp_kernel_list->kernel;\n  }\n\n  return result;\n}\n\nvoid acl_program_forget_kernel(cl_program program, cl_kernel kernel) {\n  acl_assert_locked();\n\n  if (acl_program_is_valid(program)) {\n    kernel_list_t *search_kernel_list = program->kernel_list;\n    kernel_list_t *prev_kernel_list = NULL;\n    while (search_kernel_list != NULL) {\n      if (search_kernel_list->kernel == kernel) {\n        if (prev_kernel_list == NULL) {\n          program->kernel_list = search_kernel_list->next;\n        } else {\n          prev_kernel_list->next = search_kernel_list->next;\n        }\n        search_kernel_list->next = program->kernel_free_list;\n        program->kernel_free_list = search_kernel_list;\n        --program->num_kernels;\n        break;\n      }\n      prev_kernel_list = search_kernel_list;\n      search_kernel_list = search_kernel_list->next;\n    }\n  }\n}\n\n// This is the behaviour for the ACL_DEVICE_OP_REPROGRAM device operation.\n// The device op queue guarantees that the device is not servicing any\n// requests right now.\nACL_EXPORT\nvoid acl_program_device(void *user_data, acl_device_op_t *op) {\n  cl_event event = op->info.event;\n  cl_context context = event->command_queue->context;\n  int status = 0;\n\n  // The underlying command is either:\n  //    an eager programming command:  CL_COMMAND_PROGRAM_DEVICE_INTELFPGA,\n  // or a kernel enqueue: CL_COMMAND_NDRANGE_KERNEL, or CL_COMMAND_TASK\n  auto *dev_bin = event->cmd.type == CL_COMMAND_PROGRAM_DEVICE_INTELFPGA\n                      ? event->cmd.info.eager_program\n                      : event->cmd.info.ndrange_kernel.dev_bin;\n  auto *dev_prog = dev_bin->get_dev_prog();\n\n  acl_assert_locked();\n\n  user_data = user_data; // Only used by the test mock.\n\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  acl_print_debug_msg(\n      \"Reprogram %s with program containing %d accelerators: %s ...\\n\",\n      dev_prog->device->def.autodiscovery_def.name.c_str(),\n      dev_prog->program->num_kernels,\n      (dev_prog->program->num_kernels ? dev_prog->program->kernel_list->kernel\n                                            ->accel_def->iface.name.c_str()\n                                      : \"<none>\"));\n\n  // the program flow will first attempt memory preserved programming, then\n  // falls back to memory unpreserved programming if it fails the data save and\n  // restore between device to host can be avoid if memory preserved programming\n  // succeeds otherwise it requires to save and restore buffer before and after\n  // the memory unpreserved programming\n  if (context->programs_devices) {\n    int interleave_changed = 0;\n    acl_device_def_t current_devdef =\n        dev_prog->device->loaded_bin\n            ? dev_prog->device->loaded_bin->get_devdef()\n            : dev_prog->device->def;\n\n    for (unsigned int i = 0;\n         i < current_devdef.autodiscovery_def.num_global_mem_systems; i++) {\n      // as long as one mem system change interleave setting, save and restore\n      // all mem objects\n      if (current_devdef.autodiscovery_def.global_mem_defs[i]\n              .burst_interleaved != dev_bin->get_devdef()\n                                        .autodiscovery_def.global_mem_defs[i]\n                                        .burst_interleaved) {\n        interleave_changed = 1;\n        break;\n      }\n    }\n    if (interleave_changed &&\n        context->saves_and_restores_buffers_for_reprogramming) {\n      acl_copy_device_buffers_to_host_before_programming(\n          context, dev_prog->device->def.physical_device_id,\n          context->reprogram_buf_read_callback);\n    }\n\n    acl_print_debug_msg(\"Runtime program_device: Programming device with \"\n                        \"memory preservation\\n\");\n\n    status = acl_get_hal()->program_device(\n        dev_prog->device->def.physical_device_id, &(dev_bin->get_devdef()),\n        dev_bin->get_binary_pkg(), ACL_PROGRAM_PRESERVE_MEM);\n    if (dev_prog->program->context->split_kernel) {\n      // In split kernel mode we need to ensure that binaries are only loaded\n      // in memory when they are needed because of high memory usage when many\n      // split binaries are present.\n      dev_bin->unload_content();\n    }\n\n    if (interleave_changed &&\n        context->saves_and_restores_buffers_for_reprogramming &&\n        status != ACL_PROGRAM_CANNOT_PRESERVE_GLOBAL_MEM) {\n      acl_copy_device_buffers_from_host_after_programming(\n          context, dev_prog->device->def.physical_device_id,\n          context->reprogram_buf_write_callback);\n    }\n\n    if (status && status != ACL_PROGRAM_CANNOT_PRESERVE_GLOBAL_MEM) {\n      // program failed, either because of the failure of memory unpreserved\n      // programming, or the invalid binary. no need to try reprogram again\n      acl_print_debug_msg(\n          \"Runtime program_device: Programming the device failed. Exited.\\n\");\n      if (status > 0)\n        status = -status;\n      acl_set_device_op_execution_status(op, status);\n      return;\n    } else if (status == ACL_PROGRAM_CANNOT_PRESERVE_GLOBAL_MEM) {\n      // program failed because of the failure of memory preserved programming.\n      // need to firstly enable data saving from device to host\n      // and secondly reprogram the full chip\n      acl_print_debug_msg(\n          \"Runtime program_device: Trying memory preserved programming failed, \"\n          \"reprogramming the full device\\n\");\n      if (!interleave_changed &&\n          context->saves_and_restores_buffers_for_reprogramming) {\n        // Important!  This will transfer all memory buffers on the device,\n        // regardless of whether they belong to the same context.\n        acl_print_debug_msg(\"Runtime program_device: Preserving memory before \"\n                            \"the reprogramming\\n\");\n        acl_copy_device_buffers_to_host_before_programming(\n            context, dev_prog->device->def.physical_device_id,\n            context->reprogram_buf_read_callback);\n      }\n      // Delegate to HAL.\n      // It blocks.  We only support one device at a time anyway.\n      // acl_get_hal()->program_device( event, <image> )\n      acl_print_debug_msg(\"Reprogram it!\\n\");\n      status = acl_get_hal()->program_device(\n          dev_prog->device->def.physical_device_id, &(dev_bin->get_devdef()),\n          dev_bin->get_binary_pkg(), ACL_PROGRAM_UNPRESERVE_MEM);\n      if (dev_prog->program->context->split_kernel) {\n        // In split kernel mode we need to ensure that binaries are only loaded\n        // in memory when they are needed because of high memory usage when many\n        // split binaries are present.\n        dev_bin->unload_content();\n      }\n\n      if (status) {\n        // It failed.  Bail out early and mark the operation as failed.\n        // Ensure the status we pass back is negative, so it's seen as an\n        // error.\n        if (status > 0)\n          status = -status;\n        acl_set_device_op_execution_status(op, status);\n        return;\n      } else {\n        // reprogram succeeded.\n        acl_print_debug_msg(\"Runtime program_device: Memory unpreserved \"\n                            \"programming succeeded.\\n\");\n      }\n      // and thirdly enable data restoring from host to device\n      if (context->saves_and_restores_buffers_for_reprogramming) {\n        // Important!  This will transfer all memory buffers on the device,\n        // regardless of whether they belong to the same context.\n        acl_print_debug_msg(\"Runtime program_device: Restoring memory after \"\n                            \"the reprogramming\\n\");\n        acl_copy_device_buffers_from_host_after_programming(\n            context, dev_prog->device->def.physical_device_id,\n            context->reprogram_buf_write_callback);\n      }\n    } else {\n      // memory preserved programming successed.\n      acl_print_debug_msg(\n          \"Runtime program_device: Memory preserved programming succeeded.\\n\");\n    }\n    dev_prog->device->def.autodiscovery_def =\n        dev_bin->get_devdef().autodiscovery_def;\n\n    if (acl_platform.offline_mode == ACL_CONTEXT_MPSIM) {\n      // Override the device name to the simulator.\n      // In function acl_device_binary_t::load_binary_pkg, the name member will\n      // be checked against the .acl.board section of the aocx file, which would\n      // contain ACL_MPSIM_DEVICE_NAME when the binary is compiled with\n      // -march=simulator flag\n      dev_prog->device->def.autodiscovery_def.name = ACL_MPSIM_DEVICE_NAME;\n    }\n  } else {\n    acl_print_debug_msg(\n        \"Reprogram it... but context does not program devices, so skipping\\n\");\n  }\n\n  // Keep track of what binary is currently loaded.\n  dev_prog->device->loaded_bin = dev_bin;\n  acl_set_device_op_execution_status(op, CL_COMPLETE);\n  dev_prog->device->def.autodiscovery_def.binary_rand_hash =\n      dev_bin->get_devdef().autodiscovery_def.binary_rand_hash;\n\n  if (context->programs_devices) {\n    acl_bind_and_process_all_pipes_transactions(\n        context, dev_prog->device, dev_prog->device->def.autodiscovery_def);\n  }\n}\n\n// Build from \"source\", and load binary, and binary_pkg.\nstatic cl_int l_build_from_source(acl_device_program_info_t *dev_prog) {\n  cl_context context;\n  cl_int status = CL_SUCCESS;\n\n  acl_assert_locked();\n\n  context = dev_prog->program->context;\n  // split kernel mode not supported when building from source.\n  assert(context->split_kernel == 0);\n\n  auto hash_dir = acl_compute_hash_dir_name(context, dev_prog->hash);\n  if (hash_dir.empty())\n    ERR_RET(CL_BUILD_PROGRAM_FAILURE, context,\n            \"Internal error: needed a hash, but it doesn't exist.\");\n\n  if (!acl_make_path_to_dir(hash_dir)) {\n    ERR_RET(CL_BUILD_PROGRAM_FAILURE, context,\n            \"Can't make path to offline compilation directory.\");\n  }\n\n  status = l_build_from_source_in_dir(dev_prog, hash_dir.c_str());\n  if (status == CL_SUCCESS &&\n      (context->programs_devices || context->uses_dynamic_sysdef)) {\n    status = dev_prog->device_binary.load_binary_pkg(1, 1);\n    // Upon success we can say, there is a binary associated with the device.\n    // The only type we have for now is CL_PROGRAM_BINARY_TYPE_EXECUTABLE.\n    if (status == CL_SUCCESS)\n      assert(dev_prog->device_binary.get_binary_type() ==\n             CL_PROGRAM_BINARY_TYPE_EXECUTABLE);\n  }\n\n  return status;\n}\n\nstd::string acl_compute_hash_dir_name(cl_context context,\n                                      const std::string &hash) {\n  acl_assert_locked();\n\n  // We want to handle tens of thousands of files in the library.\n  // For hash:\n  //    abcdefg...z\n  // We compute result:\n  //    <prefix>/ab/cd/efg...z\n  // So we need at least 4 characters in the hash\n  if (hash.empty() || hash.length() < 4) {\n    // (Internal error: needed a hash, but it doesn't exist.)\n    return \"\";\n  } else {\n    std::stringstream ss;\n    ss << context->program_library_root << \"/\" << hash[0] << hash[1] << \"/\"\n       << hash[2] << hash[3] << \"/\" << hash.substr(4);\n    return ss.str();\n  }\n}\n\nstatic cl_int l_build_from_source_in_dir(acl_device_program_info_t *dev_prog,\n                                         const char *dir) {\n  // Remember the current directory, change to the given directory, and\n  // run the compile command with -initial-dir set to the current\n  // directory, then come back to the current directory.\n  //\n  // We change into the given (hash) directory because we don't trust the\n  // compile command to keep tidy.  Too bad, eh?\n  // Besides, it meshes better with manual runs of the kernel compiler.\n  //\n  // We need to use -initial-dir just in case the user's code does\n  //    #include \"foo.h\"\n  // and foo.h is in the current directory.  Or if the user supplies\n  // include directory arguments that are relative paths, e.g.\n  //    -I../foo\n  // See the OpenCL conformance \"compiler\" test.\n\n  cl_context context = dev_prog->program->context;\n  // split_kernel mode is not supported when building from source.\n  assert(context->split_kernel == 0);\n  cl_int status = CL_SUCCESS;\n  const char *clfile = \"kernels.cl\";\n  // We define two kinds of command files.\n  //    0 - Compile the program.\n  //    1 - Compile the program for the current mode, if the current mode\n  //        only builds a stub.\n  // We only need the second one if the compiler mode compiles programs\n  // incompletely.\n  const char *cmdfile[2] = {\"build.cmd\", \"build.stub.cmd\"};\n  unsigned num_build_cmds = context->compiles_programs_incompletely ? 2U : 1U;\n\n  acl_assert_locked();\n\n  // Save current working directory\n  auto cur_dir = acl_realpath_existing(\".\");\n  if (cur_dir == \"\") {\n    acl_context_callback(context, \"Allocation failure\");\n    status = CL_OUT_OF_HOST_MEMORY;\n  }\n\n  // Change to program library directory.\n  if (status == CL_SUCCESS && !acl_chdir(dir)) {\n    acl_context_callback(context,\n                         \"Can't change to program library directory: \");\n    acl_context_callback(context, dir);\n    acl_context_callback(context, acl_support_last_error());\n    status = CL_BUILD_PROGRAM_FAILURE;\n  }\n\n  // Write source to file, in binary mode.\n  // We want binary mode so that the compile hashes and results are\n  // cross-OS compatible.\n  if (status == CL_SUCCESS && context->compiles_programs) {\n    struct acl_file_handle_t *clfp;\n    if (0 != (clfp = acl_fopen(clfile, \"wb\"))) {\n      cl_program program = dev_prog->program;\n      size_t bytes_write = program->source_len - 1; // skip terminating NUL.\n      size_t bytes_written =\n          acl_fwrite(program->source_text, 1, bytes_write, clfp);\n      if (bytes_written != bytes_write) {\n        status = CL_BUILD_PROGRAM_FAILURE;\n        acl_context_callback(context, \"Can't write source file: \");\n        acl_context_callback(context, acl_support_last_error());\n      }\n      acl_fclose(clfp); // close in any case.\n    } else {\n      status = CL_BUILD_PROGRAM_FAILURE;\n      acl_context_callback(context, \"Can't write source file: \");\n      acl_context_callback(context, acl_support_last_error());\n    }\n  }\n  if (status == CL_SUCCESS && context->compiles_programs) {\n    auto absolute_clfile = acl_realpath_existing(clfile);\n    if (absolute_clfile == \"\") {\n      status = CL_BUILD_PROGRAM_FAILURE;\n      acl_context_callback(context, \"Can't find source file\");\n    }\n  }\n\n  // Construct the compile command.\n  std::array<std::string, 2> cmd;\n  if (status == CL_SUCCESS && context->compiles_programs) {\n    // Need the absolute path to the kernels file because aoc will\n    // chdir to -initial-dir before reading the .cl file.\n    for (unsigned i = 0; i < num_build_cmds; i++) {\n      std::stringstream ss;\n      auto compile_command = context->compile_command;\n      if (num_build_cmds == 2 && i == 0) {\n        compile_command = \"aoc -tidy\"; // What we need to fully compile to .aocx\n      }\n\n      ss << compile_command << \" >build.log 2>&1 -board=\"\n         << dev_prog->device->def.autodiscovery_def.name\n         << \" -hash=\" << dev_prog->hash << \" -initial-dir=\" << cur_dir << \" \"\n         << dir << \"/\" << clfile << \" \" << dev_prog->build_options << \"\\n\";\n      cmd[i] = ss.str();\n    }\n  }\n\n  // Write the compile script.\n  if (status == CL_SUCCESS && context->compiles_programs) {\n    for (unsigned i = 0; i < num_build_cmds; i++) {\n      struct acl_file_handle_t *cmdfp = acl_fopen(cmdfile[i], \"wb\");\n      if (debug_mode > 0) {\n        printf(\" cmdfp %p\\n\", cmdfp);\n      }\n      size_t bytes_written = 0;\n      if (cmdfp) {\n        bytes_written = (size_t)acl_fprintf(cmdfp, \"%s\", cmd[i].c_str());\n        acl_fclose(cmdfp); // close in any case.\n      }\n      if (!cmdfp || (cmd[i].length() != bytes_written)) {\n        if (debug_mode > 0) {\n          printf(\" cmdlen %d  bw %d\\n\", (int)cmd[i].length(),\n                 (int)bytes_written);\n        }\n        status = CL_BUILD_PROGRAM_FAILURE;\n        acl_context_callback(context, \"Can't write command file: \");\n        acl_context_callback(context, acl_support_last_error());\n      }\n    }\n  }\n\n  if (status == CL_SUCCESS && context->compiles_programs) {\n    // Run the compile command for the current mode.\n    auto cmd_status = acl_system(cmd[num_build_cmds - 1].c_str());\n    if (cmd_status != 0) {\n      status = CL_BUILD_PROGRAM_FAILURE;\n      acl_context_callback(context, \"Build failed\");\n      if (acl_has_error())\n        acl_context_callback(context, acl_support_last_error());\n    }\n\n    // Always load the build log, whether or not the build worked.\n    if (std::ifstream log{\"build.log\", std::ios::ate}) {\n      auto size = log.tellg();\n      log.seekg(0);\n      dev_prog->build_log.resize(static_cast<size_t>(size), '\\0');\n      log.read(&(dev_prog->build_log[0]), static_cast<std::streamsize>(size));\n    } else {\n      status = CL_BUILD_PROGRAM_FAILURE;\n      acl_context_callback(context, \"Could not capture build log: \");\n      acl_context_callback(context, dir);\n      acl_context_callback(context, \"build.log\");\n      if (acl_has_error())\n        acl_context_callback(context, acl_support_last_error());\n    }\n  }\n\n  if (status == CL_SUCCESS &&\n      (context->programs_devices || context->uses_dynamic_sysdef)) {\n    // Load the build output!\n    // (compiles_program should imply we get into this case too).\n    //\n    // If possible, load the full binary (ends in \"x\").\n    // Otherwise fall back to the partially compiled binary (ends in \"r\").\n\n    if (context->programs_devices) {\n      // If programming the device, then we truly need the full .aocx binary\n      // because that has all the programming info we need.\n      dev_prog->device_binary.load_content(\"kernels.aocx\");\n    } else if (context->uses_dynamic_sysdef) {\n      // But if we only need the dynamic sysdef, then we can fall back on\n      // using the partial compilation result.\n      dev_prog->device_binary.load_content(\"kernels.aocr\");\n    }\n\n    if (!dev_prog->device_binary.get_content())\n      status = CL_BUILD_PROGRAM_FAILURE;\n\n    if (status != CL_SUCCESS) {\n      // Unload the binary, because we allocated it.\n      dev_prog->device_binary.unload_content();\n    }\n\n    if (!dev_prog->device_binary.get_content()) {\n      status = CL_BUILD_PROGRAM_FAILURE;\n      acl_context_callback(context, \"Could not load the binary: \");\n      acl_context_callback(context, acl_support_last_error());\n      acl_context_callback(context, dir);\n      acl_context_callback(\n          context,\n          (context->programs_devices ? \"kernels.aocx\" : \"kernels.aocr\"));\n    }\n  }\n\n  // Change back to original directory.\n  if (!acl_chdir(cur_dir.c_str())) {\n    if (status == CL_SUCCESS) {\n      status = CL_BUILD_PROGRAM_FAILURE;\n      acl_context_callback(context,\n                           \"Could not change back into source directory\");\n    }\n  }\n\n  return status;\n}\n\nvoid acl_program_dump_dev_prog(acl_device_program_info_t *dev_prog) {\n  acl_assert_locked();\n\n  acl_print_debug_msg(\"dev_prog: %p {\\n\", dev_prog);\n\n  if (dev_prog && (debug_mode > 0)) {\n    printf(\"        program[%p]\\n\", dev_prog->program);\n    printf(\"        device [%d] %s\\n\", dev_prog->device->id,\n           dev_prog->device->def.autodiscovery_def.name.c_str());\n    printf(\"        status %d\\n\", dev_prog->build_status);\n    printf(\"        options '%s'\\n\",\n           (!dev_prog->build_options.empty() ? dev_prog->build_options.c_str()\n                                             : \"(nil)\"));\n    printf(\"        bin_len %lu\\n\",\n           (unsigned long)dev_prog->device_binary.get_binary_len());\n    printf(\"        bin     %p\\n\", dev_prog->device_binary.get_content());\n    printf(\"        bin_pkg %p\\n\", dev_prog->device_binary.get_binary_pkg());\n    printf(\"        hash    %s\\n\", dev_prog->hash.c_str());\n  }\n\n  acl_print_debug_msg(\"       }\\n\");\n}\n\n// Eagerly program the device.\n// In most cases, we'll have only one program, and in fact, the first program\n// to be created from binary, or built from source is the first one to be run.\nstatic void l_try_to_eagerly_program_device(cl_program program) {\n  cl_uint idev;\n  acl_assert_locked();\n\n  if (!program->context->eagerly_program_device_with_first_binary)\n    return;\n  for (idev = 0; idev < program->num_devices; idev++) {\n    acl_device_program_info_t *dev_prog = program->dev_prog[idev];\n    if (dev_prog && dev_prog->build_status == CL_BUILD_SUCCESS) {\n      cl_device_id device = dev_prog->device;\n      if (!device->last_bin) {\n        // Nothing program is scheduled to be loaded onto the device.\n        cl_int result;\n\n        // Use a regular event with an internal command type.\n        // We want a regular event so we get profiling info, so that we\n        // have the infrastructure to measure delays, and the user\n        // could in principle optimize this delay away.\n        cl_event reprogram_event = 0;\n\n        // Just use the auto_queue. We really only support one device\n        // anyway.\n        cl_command_queue cq = program->context->auto_queue;\n\n        // Schedule an eager programming of the device.\n        acl_print_debug_msg(\n            \"Device is not yet programmed: plan to eagerly program it\\n\");\n\n        result = acl_create_event(cq, 0, 0, // Don't wait on other events\n                                  CL_COMMAND_PROGRAM_DEVICE_INTELFPGA,\n                                  &reprogram_event);\n\n        if (result == CL_SUCCESS) {\n\n          reprogram_event->cmd.info.eager_program = &(dev_prog->device_binary);\n\n          // Need to retain the program until we've finished this\n          // programming command.\n          clRetainProgram(dev_prog->program);\n          // Now prod the scheduler.\n          // This should cause reprogramming to occur.\n          // And that should update device->last_bin to be updated.\n          acl_idle_update_queue(cq);\n          // We don't need the handle to the event anymore.\n          // This will also nudge both schedulers!\n          clReleaseEvent(reprogram_event);\n        }\n      }\n    }\n  }\n}\n\n// Copy memory definition from src_dev to dest_dev.\nstatic void\nl_device_memory_definition_copy(acl_device_def_autodiscovery_t *dest_dev,\n                                acl_device_def_autodiscovery_t *src_dev) {\n  unsigned int idef;\n  dest_dev->num_global_mem_systems = src_dev->num_global_mem_systems;\n  for (idef = 0; idef < src_dev->num_global_mem_systems; ++idef) {\n    dest_dev->global_mem_defs[idef].range =\n        src_dev->global_mem_defs[idef].range;\n    dest_dev->global_mem_defs[idef].type = src_dev->global_mem_defs[idef].type;\n    dest_dev->global_mem_defs[idef].burst_interleaved =\n        src_dev->global_mem_defs[idef].burst_interleaved;\n    dest_dev->global_mem_defs[idef].config_addr =\n        src_dev->global_mem_defs[idef].config_addr;\n    dest_dev->global_mem_defs[idef].num_global_banks =\n        src_dev->global_mem_defs[idef].num_global_banks;\n    dest_dev->global_mem_defs[idef].name = src_dev->global_mem_defs[idef].name;\n    dest_dev->global_mem_defs[idef].allocation_type =\n        src_dev->global_mem_defs[idef].allocation_type;\n    dest_dev->global_mem_defs[idef].primary_interface =\n        src_dev->global_mem_defs[idef].primary_interface;\n    dest_dev->global_mem_defs[idef].can_access_list =\n        src_dev->global_mem_defs[idef].can_access_list;\n    dest_dev->global_mem_defs[idef].id = src_dev->global_mem_defs[idef].id;\n  }\n}\n\n// Schedule an eager programming of the device onto the device op queue.\n// Return a positive number if we succeeded, 0 otherwise.\nint acl_submit_program_device_op(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n\n  // Only schedule if it is a valid event, and it isn't already scheduled.\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  if (!acl_command_queue_is_valid(event->command_queue)) {\n    return result;\n  }\n  if (!event->last_device_op) {\n    acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n    acl_device_op_t *last_op = 0;\n\n    // Precautionary, but it also nudges the device scheduler to try\n    // to free up old operation slots.\n    acl_forget_proposed_device_ops(doq);\n    // Try scheduling it.\n    last_op = acl_propose_device_op(doq, ACL_DEVICE_OP_REPROGRAM, event);\n    if (last_op) {\n      // We managed to add this to the device op queue.\n      // Record this program as the last one to be on the device\n      // after all device ops up to this point are finished.\n      auto *dev_bin = event->cmd.info.eager_program;\n      dev_bin->get_dev_prog()->device->last_bin = dev_bin;\n\n      // Mark this event not requiring another submit.\n      event->last_device_op = last_op;\n\n      // Commit and nudge the device op scheduler.\n      acl_commit_proposed_device_ops(doq);\n      result = 1;\n    }\n  }\n  return result;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_command_queue.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <string.h>\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl_command.h>\n#include <acl_command_queue.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_icd_dispatch.h>\n#include <acl_kernel.h>\n#include <acl_platform.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n// Command Queues\n// ==============\n//\n// A command is represented by its associated event(!).\n// (We create an event for a command even if the user didn't ask for it.)\n//\n// A command is in one of the following execution states:\n//    q = queued\n//          It's on the command queue, waiting until:\n//             prior events have completed (including queue barriers),\n//             prior commands on this queue have completed.\n//\n//    s = submitted\n//          The runtime has sent the \"start the command\" message\n//          to the HAL.\n//\n//    r = running\n//          The HAL has responded, indicating that the command is being\n//          executed by the hardware.\n//\n//    c = complete\n//          The HAL has responded, indicating that the command execution\n//          has finished.\n//\n//    An error state (value < 0)\n//\n// Notes:\n//    Only events in \"queued\" state may depend on other (earlier) events.\n//\n//    The normal progression is: q -> s -> r -> c\n//\n//    The transitions s->r and r->c are made asynchronously, i.e. possibly\n//    from outside main thread of control.  They occur in response to messages\n//    being received from the hardware.\n//\n//\n// Data structures\n// ===============\n//\n// We maintain a circular buffer of commands, ordered by the time at which\n// they were queud.\n//\n// The status of events in the queue is always in the following pattern,\n// even if out-of-order execution is (would be) allowed:\n//\n//    c* ( c | r | s )* q*\n//\n// The first segment of \"complete\" events can be removed if they are not\n// \"live\".  (An event is \"live\" if it is retained, or the bookkeeping still\n// says other events depend on it, or if its execution status is neither\n// complete nor error.)  See acl_event_is_live. Such events are redundant, and\n// should be removed to free up bookkeeping resources.\n//\n// The next segment of \"complete/running/submitted\" are being updated\n// asynchronously when (another thread) receives messages from acclerator\n// devices.\n//\n//\n// Algorithms\n// ==========\n//\n// Users enqueue commands and check their status via queries on the\n// associated events.  See:  clEnqueue*  and  clGetEventInfo*\n//\n// The system asynchronously updates execution status of submitted commands.\n//\n// In between the two, the runtime must update the queue based, responding\n// to the asynchronous status updates.  See: acl_update_queue\n\nACL_DEFINE_CL_OBJECT_ALLOC_FUNCTIONS(cl_command_queue);\n\nstatic int l_init_queue(cl_command_queue cq,\n                        cl_command_queue_properties properties,\n                        cl_context context, cl_device_id device);\n\n//////////////////////////////\n// OpenCL API\n\n// Create a command queue\nACL_EXPORT\nCL_API_ENTRY cl_command_queue CL_API_CALL\nclCreateCommandQueueWithPropertiesIntelFPGA(\n    cl_context context, cl_device_id device,\n    const cl_queue_properties *properties, cl_int *errcode_ret) {\n  cl_command_queue result = 0;\n  cl_command_queue_properties cq_properties = 0;\n  cl_uint q_size_properties = 0, idx = 0;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    BAIL(CL_INVALID_CONTEXT);\n  }\n  if (!acl_device_is_valid(device)) {\n    BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid device\");\n  }\n  if (!acl_context_uses_device(context, device)) {\n    BAIL_INFO(CL_INVALID_DEVICE, context,\n              \"Device is not associated with the context\");\n  }\n\n  // Get the properties. Only two possible properties: CL_QUEUE_PROPERTIES and\n  // CL_QUEUE_SIZE. Combining if any repetitive CL_QUEUE_PROPERTIES, but bailing\n  // on repetitive CL_QUEUE_SIZE The end of the properties list is specified\n  // with a zero.\n  while (properties != NULL && properties[idx] != 0) {\n    if (properties[idx] == CL_QUEUE_PROPERTIES) {\n      cq_properties |= properties[idx + 1];\n    } else if (properties[idx] == CL_QUEUE_SIZE) {\n      if (q_size_properties == 0)\n        q_size_properties = (cl_uint)properties[idx + 1];\n      else // This property was already given.\n        BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid queue properties\");\n    } else {\n      BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid queue properties\");\n    }\n    idx += 2;\n  }\n\n  // Check property bits.\n  {\n    // What is valid to say in the API?\n    const cl_command_queue_properties valid_properties =\n        CL_QUEUE_PROFILING_ENABLE | CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE |\n        CL_QUEUE_ON_DEVICE | CL_QUEUE_ON_DEVICE_DEFAULT;\n    if (cq_properties & ~(valid_properties)) {\n      BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid queue properties\");\n    }\n    // Also check the dependency of options:\n    if (((cq_properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) == 0 &&\n         (cq_properties & CL_QUEUE_ON_DEVICE)) ||\n        ((cq_properties & CL_QUEUE_ON_DEVICE) == 0 &&\n         (cq_properties & CL_QUEUE_ON_DEVICE_DEFAULT))) {\n      BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid queue properties\");\n    }\n  }\n  {\n    cl_command_queue_properties device_props = 0;\n    // We currently don't support CL_QUEUE_ON_DEVICE &\n    // CL_QUEUE_ON_DEVICE_DEFAULT. The availability of these features cannot be\n    // queried from current version of clGetDeviceInfo. So manually failing on\n    // those properties for now.\n    if (cq_properties & (CL_QUEUE_ON_DEVICE | CL_QUEUE_ON_DEVICE_DEFAULT))\n      BAIL_INFO(CL_INVALID_QUEUE_PROPERTIES, context,\n                \"Device does not support the specified queue properties\");\n    if (q_size_properties != 0) { // not supported yet.\n      BAIL_INFO(CL_INVALID_QUEUE_PROPERTIES, context,\n                \"Device does not support the specified queue properties\");\n    }\n\n    // Internal user may want to turn off support for OOO Queues\n    const char *disable_oooq =\n        acl_getenv(\"CL_CONTEXT_DISABLE_OOO_QUEUES_INTELFPGA\");\n    if (disable_oooq &&\n        (cq_properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE)) {\n      BAIL_INFO(CL_INVALID_QUEUE_PROPERTIES, context,\n                \"Device does not support the specified queue properties\");\n    }\n\n    // What does the device support?\n    // No, we're not checking the return result. It would be an internal\n    // error for this to fail.  But initializing device_props to 0 is a\n    // fail-safe.\n    clGetDeviceInfo(device, CL_DEVICE_QUEUE_PROPERTIES, sizeof(device_props),\n                    &device_props, 0);\n    if (cq_properties & ~(device_props)) {\n      BAIL_INFO(CL_INVALID_QUEUE_PROPERTIES, context,\n                \"Device does not support the specified queue properties\");\n    }\n  }\n\n  // Now actually allocate the command queue.\n  result = acl_alloc_cl_command_queue();\n  if (result == 0) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a command queue\");\n  }\n\n  // Fail to double the capacity of the pointer array\n  if (!l_init_queue(result, cq_properties, context, device)) {\n    acl_free_cl_command_queue(result);\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a command queue\");\n  }\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_COMMAND_QUEUE, result);\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_command_queue CL_API_CALL clCreateCommandQueueWithProperties(\n    cl_context context, cl_device_id device,\n    const cl_queue_properties *properties, cl_int *errcode_ret) {\n  return clCreateCommandQueueWithPropertiesIntelFPGA(context, device,\n                                                     properties, errcode_ret);\n}\n\n// Create a command queue. (This is opencl 1.0 API and is deprecated in 2.0)\nACL_EXPORT\nCL_API_ENTRY cl_command_queue CL_API_CALL clCreateCommandQueueIntelFPGA(\n    cl_context context, cl_device_id device,\n    cl_command_queue_properties properties, cl_int *errcode_ret) {\n  cl_queue_properties q_properties[] = {CL_QUEUE_PROPERTIES, 0, 0};\n  q_properties[1] = (cl_command_queue_properties)\n      properties; // Couldn't initialize this element in array, due to MSVC\n                  // compiler warning: non-constant aggregate initializer\n  return clCreateCommandQueueWithPropertiesIntelFPGA(context, device,\n                                                     q_properties, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_command_queue CL_API_CALL clCreateCommandQueue(\n    cl_context context, cl_device_id device,\n    cl_command_queue_properties properties, cl_int *errcode_ret) {\n  return clCreateCommandQueueIntelFPGA(context, device, properties,\n                                       errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclRetainCommandQueueIntelFPGA(cl_command_queue command_queue) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  acl_retain(command_queue);\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclRetainCommandQueue(cl_command_queue command_queue) {\n  return clRetainCommandQueueIntelFPGA(command_queue);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclReleaseCommandQueueIntelFPGA(cl_command_queue command_queue) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  acl_release(command_queue);\n  acl_print_debug_msg(\"Release command queue %d %p\\n\", command_queue->id,\n                      command_queue);\n\n  // delete the command queue if the queue is no longer retained\n  if (!acl_is_retained(command_queue)) {\n    acl_delete_command_queue(command_queue);\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclReleaseCommandQueue(cl_command_queue command_queue) {\n  return clReleaseCommandQueueIntelFPGA(command_queue);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetCommandQueueInfoIntelFPGA(\n    cl_command_queue command_queue, cl_command_queue_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_QUEUE_CONTEXT:\n    RESULT_PTR(command_queue->context);\n    break;\n  case CL_QUEUE_DEVICE:\n    RESULT_PTR(command_queue->device);\n    break;\n  case CL_QUEUE_REFERENCE_COUNT:\n    RESULT_UINT(acl_ref_count(command_queue));\n    break;\n  case CL_QUEUE_PROPERTIES:\n    RESULT_BITFIELD(command_queue->properties);\n    break;\n  default:\n    break;\n  }\n\n  if (result.size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Invalid or unsupported command queue property\");\n  }\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetCommandQueueInfo(\n    cl_command_queue command_queue, cl_command_queue_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  return clGetCommandQueueInfoIntelFPGA(command_queue, param_name,\n                                        param_value_size, param_value,\n                                        param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetCommandQueuePropertyIntelFPGA(\n    cl_command_queue command_queue, cl_command_queue_properties properties,\n    cl_bool enable, cl_command_queue_properties *old_properties) {\n  cl_command_queue_properties bad_properties;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  bad_properties =\n      ~((cl_command_queue_properties)CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE |\n        (cl_command_queue_properties)CL_QUEUE_PROFILING_ENABLE);\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  // Internal user may want to turn off support for OOO Queues\n  const char *disable_oooq =\n      acl_getenv(\"CL_CONTEXT_DISABLE_OOO_QUEUES_INTELFPGA\");\n  if (disable_oooq && (properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE)) {\n    ERR_RET(CL_INVALID_QUEUE_PROPERTIES, command_queue->context,\n            \"Can't set CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE property, \"\n            \"unsupported\");\n  }\n\n  if (old_properties) {\n    *old_properties = command_queue->properties;\n  }\n\n  if (properties & bad_properties) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Invalid or unsupported command queue property\");\n  }\n\n  if (enable) {\n    command_queue->properties |= properties;\n  } else {\n    command_queue->properties &= ~properties;\n  }\n\n  // No queue synchronization is required because we don't support\n  // out-of-order execution.\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetCommandQueueProperty(\n    cl_command_queue command_queue, cl_command_queue_properties properties,\n    cl_bool enable, cl_command_queue_properties *old_properties) {\n  return clSetCommandQueuePropertyIntelFPGA(command_queue, properties, enable,\n                                            old_properties);\n}\n\n// Wait until all previous commands have been issued to the device.\n// We don't wait until they are complete.\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclFlushIntelFPGA(cl_command_queue command_queue) {\n  bool any_queued = false;\n  const acl_hal_t *hal = acl_get_hal();\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  // Context is valid too.  Force a schedule update.\n  cl_context context = command_queue->context;\n\n  do {\n    any_queued = 0;\n    acl_idle_update(context);\n    if (command_queue->num_commands == 0) {\n      return CL_SUCCESS;\n    }\n\n    // Find if at least one event is not SUBMITTED\n    if (command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n      for (auto it = command_queue->commands.begin();\n           it != command_queue->commands.end() && !any_queued; ++it) {\n        any_queued = ((*it)->execution_status == CL_QUEUED);\n      }\n    } else {\n      // Events added to end of in-order queue, just check the last event\n      cl_event event = command_queue->inorder_commands.back();\n      any_queued = (event->execution_status == CL_QUEUED);\n    }\n\n    if (any_queued) {\n      // Wait until signaled, without burning CPU.\n      if (!hal->yield) {\n        acl_wait_for_device_update(context);\n      } else {\n        hal->yield(context->num_devices, context->device);\n      }\n    }\n\n  } while (any_queued);\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clFlush(cl_command_queue command_queue) {\n  return clFlushIntelFPGA(command_queue);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclFinishIntelFPGA(cl_command_queue command_queue) {\n  cl_event event = 0;\n  cl_int result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  // Spec says:\n  // \"enqueues a marker command which waits for either a list of events to\n  // complete, or if the list is empty it waits for all commands previously\n  // enqueued in command_queue to complete before it completes\"\n  result = clEnqueueMarkerWithWaitList(command_queue, 0, 0, &event);\n\n  if (result == CL_SUCCESS) {\n    result = clWaitForEvents(1, &event);\n    clReleaseEvent(event);\n  }\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clFinish(cl_command_queue command_queue) {\n  return clFinishIntelFPGA(command_queue);\n}\n\n//////////////////////////////\n// Internals\n\n// This function does 2 things:\n// 1) Initialize the various fields inside \"cq\"\n// 2) If cq->context->command_queue[] (i.e. array of queue pointers in the\n// context) reaches capacity, double that capacity\nstatic int l_init_queue(cl_command_queue cq,\n                        cl_command_queue_properties properties,\n                        cl_context context, cl_device_id device) {\n  acl_assert_locked();\n\n  // If we've used up all available command queue pointers, time to allocate for\n  // more\n  if (context->num_command_queues == context->num_command_queues_allocated) {\n    int old_capacity = context->num_command_queues_allocated;\n    int new_capacity = 2 * old_capacity;\n    if (new_capacity < old_capacity) {\n      return 0;\n    } // overflow.\n\n    cl_command_queue *command_queue = (cl_command_queue *)acl_realloc(\n        context->command_queue, new_capacity * sizeof(cl_command_queue));\n    if (!command_queue) {\n      return 0;\n    }\n\n    context->num_command_queues_allocated = new_capacity;\n    context->command_queue = command_queue;\n  }\n  cq->magic = get_magic_val<cl_command_queue>();\n  cq->id = (unsigned)(context->num_command_queues);\n  context->command_queue[context->num_command_queues++] = cq;\n\n  /* Initialize the queue */\n  acl_reset_ref_count(cq);\n  acl_retain(cq);\n  acl_retain(device);\n  cq->device = device;\n  acl_retain(context);\n  cq->context = context;\n\n  cq->dispatch = &acl_icd_dispatch;\n  cq->properties = properties;\n\n  // All user command queues submit commands.\n  cq->submits_commands = 1;\n\n  // Empty out the queue\n  cq->num_commands = 0;\n  cq->num_commands_submitted = 0;\n  cq->last_barrier = NULL;\n\n  return 1;\n}\n\nint acl_command_queue_is_valid(cl_command_queue command_queue) {\n  acl_assert_locked();\n\n  if (!acl_is_valid_ptr(command_queue)) {\n    return 0;\n  }\n  if (!acl_is_retained(command_queue)) {\n    return 0;\n  }\n  if (!acl_context_is_valid(command_queue->context)) {\n    return 0;\n  }\n  // Check that the id field is set correctly\n  if (command_queue !=\n      command_queue->context->command_queue[command_queue->id]) {\n    return 0;\n  }\n  return 1;\n}\n\nvoid acl_command_queue_add_event(cl_command_queue command_queue,\n                                 cl_event event) {\n  acl_assert_locked();\n  event->command_queue = command_queue;\n  acl_set_execution_status(event, CL_QUEUED);\n\n  if (command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n    command_queue->commands.insert(event);\n    try {\n      command_queue->new_commands.push_back(event);\n    } catch (const std::bad_alloc &) {\n      // Remove inserted command before allowing error to be thrown\n      command_queue->commands.erase(event);\n      throw;\n    }\n  } else {\n    command_queue->inorder_commands.push_back(event);\n  }\n  command_queue->num_commands++;\n}\n\n/**\n * Compares if the two events are pointing to the same kernel and accelerator\n * @param kernel_event is an already established kernel event, this is our\n * target to match\n * @param event this is the event in question. Is it a kernel? If so does it\n * point to same accelearator as kernel_event\n * @return true(1) iff event is a kernel and has the same acceleartor as the\n * kernel_event, else false(0)\n */\nint l_is_same_kernel_event(const cl_event kernel_event,\n                           const cl_event other_event) {\n  assert(kernel_event->cmd.type == CL_COMMAND_TASK ||\n         kernel_event->cmd.type == CL_COMMAND_NDRANGE_KERNEL);\n\n  if (other_event->cmd.type == CL_COMMAND_TASK ||\n      other_event->cmd.type == CL_COMMAND_NDRANGE_KERNEL) {\n    // it's a kernel\n\n    if (other_event->cmd.info.ndrange_kernel.device->id ==\n            kernel_event->cmd.info.ndrange_kernel.device->id &&\n        other_event->cmd.info.ndrange_kernel.kernel->accel_def->id ==\n            kernel_event->cmd.info.ndrange_kernel.kernel->accel_def->id) {\n      // same acceleartor\n      return 1;\n    }\n  }\n  return 0;\n}\n\nint acl_update_queue(cl_command_queue command_queue) {\n  acl_assert_locked();\n  // If command_queue is empty return 0 right away\n  if (!(command_queue->num_commands)) {\n    return 0;\n  }\n\n  // First nudge the device operation scheduler.\n  acl_update_device_op_queue(&(acl_platform.device_op_queue));\n\n  if (command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n    return acl_update_ooo_queue(command_queue);\n  } else {\n    return acl_update_inorder_queue(command_queue);\n  }\n}\n\n/**\n * Tries to fast relaunch a kernel event\n * @param event is the kernel event to be launched, where its only parent\n * event is a kernel event that tracks previous launch of the same kernel\n * and is submitted, running, or completed\n * @return true if the fast-kernel-launch succeeded, else false\n */\nbool acl_fast_relaunch_kernel(cl_event event) {\n  if (!(event->command_queue->properties &\n        CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE))\n    return false;\n\n  if (event->depend_on.size() != 1)\n    return false;\n\n  cl_event parent = *(event->depend_on.begin());\n\n  if (!(parent->command_queue->properties &\n        CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE))\n    return false;\n\n  if (parent->cmd.type != CL_COMMAND_TASK &&\n      parent->cmd.type != CL_COMMAND_NDRANGE_KERNEL)\n    return false;\n\n  if (parent->execution_status > CL_SUBMITTED ||\n      parent->last_device_op->status > CL_SUBMITTED)\n    return false;\n\n  if (!l_is_same_kernel_event(parent, event)) {\n    // dependent on a different kernel than parent,\n    // must wait for dependency to be resolved\n    // OR the dependent is not on the same device,\n    // not safe to preemptively push dependent to device_op_queue\n    return false;\n  }\n\n  // Special case: if subbuffers are present they may(!) cause a\n  // migration while another kernel is using that data.\n  if (acl_kernel_has_unmapped_subbuffers(\n          &(event->cmd.info.ndrange_kernel.memory_migration)))\n    return false;\n\n  // Fast Kernel Relaunch: submitting is safe even though has dependency\n  // If submission succeeds, remove dependency\n  bool success = acl_submit_command(event);\n  if (!success)\n    return false;\n  event->depend_on.erase(parent);\n  parent->depend_on_me.remove(event);\n  return true;\n}\n\nint acl_update_ooo_queue(cl_command_queue command_queue) {\n  int num_updates = 0;\n\n  // First, remove dependencies on completed events,\n  // as this may unblock other evevnts\n  // Completed events should be returned to the free pool\n  while (!command_queue->completed_commands.empty()) {\n    cl_event event = command_queue->completed_commands.front();\n\n    while (!event->depend_on_me.empty()) {\n      cl_event dependent = acl_remove_first_event_dependency(event);\n\n      if (event->cmd.type == CL_COMMAND_USER && event->execution_status < 0) {\n        // According to the OpenCL spec for clSetUserEventStatus,\n        // when a user event's execution status is set to be negative it\n        // causes all enqueued commands that wait on the user event to be\n        // terminated.\n        acl_set_execution_status(dependent, event->execution_status);\n        if (dependent->command_queue->properties &\n            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n          dependent->command_queue->completed_commands.push_back(\n              dependent); // dependent might be on another queue\n        }\n      }\n    }\n\n    // Return completed event to free pool\n    if (debug_mode > 0) {\n      assert(!command_queue->completed_commands\n                  .empty()); // assert the list is not empty in the debug mode\n    }\n    command_queue->completed_commands.pop_front();\n    acl_event_callback(event, event->execution_status); // notify user\n    if (event->completion_callback) {\n      event->completion_callback(event);\n    } // clean up resources\n    event->completion_callback =\n        0; // Ensure the completion callback is only called once.\n    if (event == command_queue->last_barrier) {\n      command_queue->last_barrier = NULL;\n    }\n    acl_maybe_delete_event(event);\n    if (command_queue->waiting_for_events) {\n      // We are in the middle of traversing command_queue->commands, defer\n      // the removal till later to avoid corruption\n      event->defer_removal = true;\n    } else {\n      command_queue->commands.erase(event);\n    }\n    event->not_popped = false;\n    num_updates++;\n    command_queue->num_commands--;\n    acl_release(command_queue);\n  }\n\n  // Next try to submit any events with no dependencies\n  // or whose only dependences can be handled by fast kernel relaunch\n  // unless they are on a user_event queue which never submits events\n  for (auto event_iter = command_queue->new_commands.begin();\n       event_iter != command_queue->new_commands.end();) {\n    cl_event event = *event_iter;\n    int success = 0;\n    if (!command_queue->submits_commands)\n      success = 1;\n    else {\n      if (event->depend_on.empty()) {\n        success = acl_submit_command(event);\n      } else {\n        success = acl_fast_relaunch_kernel(event);\n      }\n    }\n\n    // Increment before removal so we don't invalidate the iterator\n    event_iter++;\n    if (success) {\n      // num_commands_submitted isn't used for ooo queues today\n      // but keep it up-to-date in case someone wants to use it in the future\n      command_queue->num_commands_submitted++;\n      command_queue->new_commands.remove(event);\n      num_updates++;\n    }\n  }\n\n  return num_updates;\n}\n\n// Process updates on the queue.\n// This is part of the main event loop for the runtime.\n//    Respond to status updates from devices\n//    Process event completion\n//\n// Returns number of things updated.\n// If it's 0 then we're idle and can \"wait\" a bit longer perhaps.\n// If it's greater than zero, then we should continue updating the\n// context's queues because perhaps we have finished a command and\n// removed some event dependencies.\n// That might allow dependent events to become\n// ready and hence we can launch their associated commands.\nint acl_update_inorder_queue(cl_command_queue command_queue) {\n  int num_updates = 0;\n  acl_assert_locked();\n\n  // Process completed events in the queue.\n  // When we break out of the loop, the loop counter will equal the index of the\n  // first non-completed event in the queue.\n  unsigned queue_idx = 0;\n  while (queue_idx < command_queue->inorder_commands.size()) {\n    cl_event event = command_queue->inorder_commands[queue_idx];\n\n    if (acl_event_is_done(event)) {\n      // Remove this event from its dependencies.\n      num_updates += acl_notify_dependent_events(event);\n\n      // Call all its user-based callbacks.\n      acl_event_callback(event, event->execution_status);\n\n      // Process event completion.\n      if (acl_event_is_valid(event) && event->completion_callback) {\n        event->completion_callback(event);\n        // Ensure the completion callback is only called once.\n        event->completion_callback = 0;\n      }\n\n      // We can only delete this completed event if there's no more\n      // dependencies.\n      if (acl_maybe_delete_event(event) && event->not_popped) {\n        // Note: if event is marked as CL_COMPLETE inside of a callback this\n        // will trigger us to update all the queues and subsequently pop this\n        // event, while processing the callback. Hence when we come back to the\n        // original call the event might already be popped, and we need to avoid\n        // popping it the second time.\n\n        // Deleted event. Remove it from our queue.\n        if (command_queue->waiting_for_events) {\n          // We are in the middle of traversing command_queue->inorder_commands,\n          // defer the removal till later to avoid corruption\n          event->defer_removal = true;\n        } else {\n          command_queue->inorder_commands.erase(\n              command_queue->inorder_commands.begin() + queue_idx);\n        }\n        command_queue->num_commands--;\n        num_updates++;\n        event->not_popped = false;\n        // since the event is popped out (deleted), it is safe to\n        // decrease the refcount of the command queue by one\n        acl_release(command_queue);\n        acl_print_debug_msg(\"  Deleted event.  num_updates is now %d\\n\",\n                            num_updates);\n      } else {\n        // If an event is deleted from the queue, the same value of the pointer\n        // would now point to the next event in the queue. Hence we only\n        // increment when no deletion took place.\n        queue_idx++;\n      }\n    } else {\n      break;\n    }\n  }\n\n  // See if we should submit a command to the accelerator\n  if (command_queue->submits_commands) {\n    // Skip the completed events. Check the front of the queue.\n    for (; queue_idx < command_queue->inorder_commands.size(); queue_idx++) {\n      cl_event event = command_queue->inorder_commands[queue_idx];\n\n      if (event->execution_status != CL_QUEUED ||\n          event->is_on_device_op_queue) {\n        continue;\n      }\n\n      if (event->cmd.type == CL_COMMAND_TASK ||\n          event->cmd.type == CL_COMMAND_NDRANGE_KERNEL) {\n\n        unsigned int safe_to_submit = 1;\n\n        // iterate over dependencies, make sure only dependent\n        // on events with the same accelerator as itself\n        for (cl_event i_depend_on : event->depend_on) {\n          if (!l_is_same_kernel_event(event, i_depend_on) ||\n              i_depend_on->last_device_op == NULL ||\n              i_depend_on->last_device_op->status > CL_SUBMITTED) {\n            // Dependent on a different kernel than itself,\n            // must wait for dependency to be resolved\n            // OR the kernel dependency is not on the device,\n            // not safe to preemptively push event to device_op_queue\n            // Assumption: if depend_on is a kernel then last device_op is a\n            // kernel\n            safe_to_submit = 0;\n            break;\n          }\n        }\n\n        // no need to go on to the next check, unsafe to submit\n        if (!safe_to_submit) {\n          break;\n        }\n\n        // Special case: if subbuffers are present they may(!) cause a\n        // migration while another kernel is using that data.\n        if (acl_kernel_has_unmapped_subbuffers(\n                &(event->cmd.info.ndrange_kernel.memory_migration)) &&\n            (!event->depend_on.empty() ||\n             command_queue->num_commands_submitted != 0)) {\n          break;\n        }\n\n        if (command_queue->num_commands_submitted == 0) {\n          int local_updates = acl_submit_command(event);\n          command_queue->num_commands_submitted += local_updates;\n          num_updates += local_updates;\n          continue; // there might be another kernel behind us that can be\n                    // submitted aswell\n        } else {\n          // Oh-oh, there is something else already submitted. Need to check\n          // that it is safe to submit this as well 1) submitted is a kernel\n          // event + it has the same accelerator as itself 2) submitted\n          // kernel-op is on the device Note: we can cheat for this in-order\n          // queue and only check the last submitted event, because if there are\n          // more, they will be the same\n\n          // If there is a command that is submitted, then it must still be in\n          // our queue\n          cl_event submitted_event =\n              command_queue->inorder_commands[queue_idx - 1];\n\n          if (l_is_same_kernel_event(event, submitted_event)) {\n            if (submitted_event->last_device_op->status <= CL_SUBMITTED) {\n              // Assumption: last device_op of the submitted kernel event is a\n              // kernel_op\n              int local_updates = acl_submit_command(event);\n              command_queue->num_commands_submitted += local_updates;\n              num_updates += local_updates;\n              continue; // there might be another kernel behind us that can be\n                        // submitted aswell\n            }\n          }\n        }\n        // Can't be submitted, we shouldn't check things behind it: in-order\n        // queue limitation\n        break;\n      } else { // Not a kernel event\n        if (command_queue->num_commands_submitted == 0 &&\n            event->depend_on.empty()) {\n          // it is safe to submit: nothing else submitted AND all dependencies\n          // are resolved\n          int local_updates = acl_submit_command(event);\n          command_queue->num_commands_submitted += local_updates;\n          num_updates += local_updates;\n        }\n        break; // no more events can be submitted\n      }\n    }\n  }\n  return num_updates;\n}\n\n// Update internal data structures in response to external messages.\n// Do this as part of a waiting loop, along with acl_hal_yield();\nvoid acl_idle_update_queue(cl_command_queue command_queue) {\n  int num_updates;\n  unsigned iters = 0;\n  const unsigned max_iters = 10000; // This is unreasonably high.\n  const unsigned id = command_queue->id;\n  acl_assert_locked();\n\n  do {\n    num_updates = acl_update_queue(command_queue);\n    acl_print_debug_msg(\" cq[%d] had %d updates\\n\", id, num_updates);\n  } while ((num_updates > 0) && (++iters < max_iters) &&\n           acl_is_retained(command_queue));\n\n  if (num_updates) {\n    acl_print_debug_msg(\" cq[%d] idle update: still had %d 'updates' after %u \"\n                        \"iters. Breaking out\\n\",\n                        id, num_updates, iters);\n  }\n}\n\n// Delete the command queue if the command queue is no longer retained.\n// This function will be called either in (1) clReleaseCommandQueue; or (2)\n// acl_idle_update. On the one hand, for most cases, after the host is done,\n// there will be no alive event/command left inside the queue, a user calls (1)\n// to release and delete the command queue; On the other hand, in some special\n// cases, the user would call (1) before the last alive event/command is done,\n// and the queue will be retained by that event until it is completed. Runtime\n// will keep calling (2) to update all the command queues and deleting those not\n// being retained any more (no live event left and having been called to (1) by\n// the user already).\nvoid acl_delete_command_queue(cl_command_queue command_queue) {\n  acl_assert_locked();\n\n  if (!acl_is_retained(command_queue)) {\n\n    acl_print_debug_msg(\"Delete command queue %d %p\\n\", command_queue->id,\n                        command_queue);\n\n    cl_device_id device = command_queue->device;\n\n    acl_release(device);\n\n    // Remove queue from context's command queue pointer list\n    //  (*) At all time, we want all valid command queue pointers to be at the\n    //  start of context->command_queue[],\n    //       and the number of valid command queue pointers is always\n    //       context->num_command_queues\n    //  For example, say initially num_command_queues = 5, and we're releasing\n    //  the queue pointed to by command_queue[2], Satisfying (*) means after the\n    //  release, num_command_queues = 4, and the first 4 pointers point to valid\n    //  queues. To do this, first set command_queue[2] to point to the same\n    //  queue as the last pointer i.e. command_queue[4] Then we can safely\n    //  disregard command_queue[4] by doing \"num_command_queues--\". Now\n    //  num_command_queues = 4\n    cl_context context = command_queue->context;\n    unsigned id = command_queue->id;\n    context->command_queue[id] =\n        context->command_queue[--context->num_command_queues];\n    // Reset the id field accordingly since this queue is now pointed to by a\n    // different pointer\n    context->command_queue[id]->id = id;\n    context->last_command_queue_idx_with_update = -1;\n    clReleaseContext(context);\n\n    // No need to clean up anything, destructors will take care of it.\n    assert(command_queue->new_commands.empty());\n    assert(command_queue->completed_commands.empty());\n    assert(command_queue->commands.empty());\n    assert(command_queue->inorder_commands.empty());\n\n    acl_untrack_object(command_queue);\n    acl_free_cl_command_queue(command_queue);\n  }\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_event.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <new>\n#include <string.h>\n#include <vector>\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_command.h>\n#include <acl_command_queue.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_icd_dispatch.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_types.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n// Events\n// ======\n// Events are the fundamental dependency and synchronization primitive in\n// OpenCL.\n//\n// Events track the lifecycle and completion of:\n//    Buffer transfers (and buffer mapping)\n//    NDRange/Task execution\n//    Completion of previously created events (synchronization \"join\" point)\n//       An explicitly provided list\n//       Or all previously scheduled commands on a command queue\n//\n// Some events (those that track buffer transfers and task execution by\n// devices) are the host-side interface for a system-wide process.\n// The system therefore needs a host-independent way to refer to a unique event,\n// so we give each event is own integer ID.\n// (we could have passed pointers, but that's less friendly for debugging)\n//\n// The lifecycle of an Event is:\n//\n//       Create( command_type, depend on a set of existing events )\n//\n//       Add dependency from a later event to myself.\n//\n//       Update( status update )\n//          In case of successful completion:\n//             trigger notify dependent commands,\n//             downstream dependent events.\n//\n//       Retain:  increase reference count\n//       Release:  decrease reference count.\n//          Whe reference count is 0, associated command is completed, and\n//          no downstream dependent events are waiting for this event\n//          (the dependency has been broken)\n//\n\nACL_DEFINE_CL_OBJECT_ALLOC_FUNCTIONS(cl_event);\n\n//////////////////////////////\n// Local functions\n// Release all implicitly retained cl_objects used by this cmd.\nstatic void l_release_command_resources(acl_command_info_t &cmd);\n// Returns an unused event_object, or 0 if we ran out of memory.\nstatic cl_event l_get_unused_event(cl_context context);\n// Return the event back into a free pool\nstatic void l_return_event_to_free_pool(cl_event event);\n// Edits the event timing info to record a state change\nstatic void l_record_milestone(cl_event event, cl_profiling_info milestone);\n//////////////////////////////\n// OpenCL API\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainEventIntelFPGA(cl_event event) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_event_is_valid(event)) {\n    return CL_INVALID_EVENT;\n  }\n  acl_retain(event);\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainEvent(cl_event event) {\n  return clRetainEventIntelFPGA(event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseEventIntelFPGA(cl_event event) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_event_is_valid(event)) {\n    return CL_INVALID_EVENT;\n  }\n  if (!acl_is_retained(event)) {\n    ERR_RET(CL_INVALID_EVENT, event->context,\n            \"Trying to release an event that is not retained\");\n  }\n  acl_release(event);\n\n  if (!acl_is_retained(event)) {\n    // We can't delete the event here.\n    // It will be deleted by the command queue that owns it.\n\n    // But prod that command queue.\n    // It's a valid command queue because the ACL_VALIDATE_EVENT check.\n    //\n    // Don't do full acl_idle_update because it will take a very long time.\n    // But that also means the user may have to run clFinish more often\n    // than you'd like.\n    //\n    // Note that if an event is not alive and hence being popped (deleted) from\n    // the queue the refcount of the command queue should be decremented by one\n    if (event->not_popped) {\n      acl_update_queue(event->command_queue);\n    } else {\n\n      // For events that were previously popped, they wont be deleted through\n      // acl_update_queue since they are no longer in the queue.\n      //\n      // Manually call acl_maybe_delete_event on popped events\n      // Note that since the event is no longer in the queue (had been popped\n      // out before), no need to release the queue this time\n      acl_maybe_delete_event(event);\n    }\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseEvent(cl_event event) {\n  return clReleaseEventIntelFPGA(event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetEventInfoIntelFPGA(\n    cl_event event, cl_event_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_event_is_valid(event)) {\n    return CL_INVALID_EVENT;\n  }\n\n  // Give the scheduler a nudge.\n  if (param_name == CL_EVENT_COMMAND_EXECUTION_STATUS &&\n      event->execution_status > CL_COMPLETE) {\n    acl_idle_update(event->context);\n  }\n\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          event->context);\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_EVENT_COMMAND_QUEUE:\n    if (event->cmd.type == CL_COMMAND_USER) {\n      // The OpenCL 1.2 spec for clGetEventInfo states that queries to\n      // CL_EVENT_COMMAND_QUEUE for user events should always return NULL. We\n      // place user events on a special user_event_queue internally but we\n      // should not expose that to the user.\n      RESULT_PTR(NULL);\n    } else {\n      RESULT_PTR(event->command_queue);\n    }\n    break;\n  case CL_EVENT_CONTEXT:\n    RESULT_PTR(event->context);\n    break;\n  case CL_EVENT_COMMAND_TYPE:\n    RESULT_ENUM(event->cmd.type);\n    break;\n  case CL_EVENT_COMMAND_EXECUTION_STATUS:\n    RESULT_ENUM(event->execution_status);\n    break;\n  case CL_EVENT_REFERENCE_COUNT:\n    RESULT_UINT(acl_ref_count(event));\n    break;\n  default:\n    break;\n  }\n\n  if (result.size == 0) {\n    ERR_RET(CL_INVALID_VALUE, event->context,\n            \"Invalid or unsupported event query\");\n  }\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, event->context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetEventInfo(cl_event event,\n                                               cl_event_info param_name,\n                                               size_t param_value_size,\n                                               void *param_value,\n                                               size_t *param_value_size_ret) {\n  return clGetEventInfoIntelFPGA(event, param_name, param_value_size,\n                                 param_value, param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetEventProfilingInfoIntelFPGA(\n    cl_event event, cl_profiling_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_event_is_valid(event)) {\n    return CL_INVALID_EVENT;\n  }\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          event->context);\n\n  // check if the event supports the profiling and error out accordingly\n  if (event->cmd.type == CL_COMMAND_USER) {\n    ERR_RET(CL_PROFILING_INFO_NOT_AVAILABLE, event->context,\n            \"Profiling information is not available for user events\");\n  } else if (!event->support_profiling) {\n    // since user event will not have command_queue set, no need to check again\n    ERR_RET(\n        CL_PROFILING_INFO_NOT_AVAILABLE, event->context,\n        \"Profiling information is not available because \"\n        \"CL_QUEUE_PROFILING_ENABLE was not set on the event's command queue\");\n  }\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_PROFILING_COMMAND_QUEUED:\n    RESULT_ULONG(event->timestamp[3]);\n    break;\n  case CL_PROFILING_COMMAND_SUBMIT:\n    RESULT_ULONG(event->timestamp[2]);\n    break;\n  case CL_PROFILING_COMMAND_START:\n    RESULT_ULONG(event->timestamp[1]);\n    break;\n  case CL_PROFILING_COMMAND_END:\n    RESULT_ULONG(event->timestamp[0]);\n    break;\n  default:\n    break;\n  }\n\n  if (result.size == 0) {\n    ERR_RET(CL_INVALID_VALUE, event->context, \"Invalid event profiling query\");\n  }\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, event->context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetEventProfilingInfo(\n    cl_event event, cl_profiling_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  return clGetEventProfilingInfoIntelFPGA(event, param_name, param_value_size,\n                                          param_value, param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_event CL_API_CALL\nclCreateUserEventIntelFPGA(cl_context context, cl_int *errcode_ret) {\n  cl_event result = 0;\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context))\n    BAIL(CL_INVALID_CONTEXT);\n\n  // Create the user event on the user_event_queue.\n  // In our model, every event is attached to some command queue.\n  // But user events should never block scheduler progress.\n  // So they must go on their own out-of-order command queue.\n  status = acl_create_event(context->user_event_queue, 0,\n                            0, // depends on nothing else.\n                            CL_COMMAND_USER, &result);\n  if (status != CL_SUCCESS)\n    BAIL(status); // already signaled error\n\n  // As per spec.\n  acl_set_execution_status(result, CL_SUBMITTED);\n\n  if (errcode_ret)\n    *errcode_ret = CL_SUCCESS;\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_event CL_API_CALL clCreateUserEvent(cl_context context,\n                                                    cl_int *errcode_ret) {\n  return clCreateUserEventIntelFPGA(context, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclSetUserEventStatusIntelFPGA(cl_event event, cl_int execution_status) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_event_is_valid(event)) {\n    return CL_INVALID_EVENT;\n  }\n\n  // Either negative, or CL_COMPLETE (which itself is 0)\n  if (execution_status <= CL_COMPLETE) {\n    if (event->execution_status <= CL_COMPLETE) {\n      ERR_RET(\n          CL_INVALID_OPERATION, event->context,\n          \"User event has already been completed or terminated with an error\");\n    }\n\n    acl_set_execution_status(event, execution_status);\n\n    // Nudge the scheduler.\n    acl_idle_update(event->context);\n\n    return CL_SUCCESS;\n  } else {\n    ERR_RET(CL_INVALID_VALUE, event->context, \"Invalid execution status\");\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clSetUserEventStatus(cl_event event,\n                                                     cl_int execution_status) {\n  return clSetUserEventStatusIntelFPGA(event, execution_status);\n}\n\n// registers a user callback function for a specific command execution status.\nACL_EXPORT\nCL_API_ENTRY cl_int clSetEventCallbackIntelFPGA(\n    cl_event event, cl_int command_exec_callback_type,\n    void(CL_CALLBACK *pfn_event_notify)(cl_event event,\n                                        cl_int event_command_exec_status,\n                                        void *user_data),\n    void *user_data) {\n  acl_event_user_callback *cb;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_event_is_valid(event)) {\n    return CL_INVALID_EVENT;\n  }\n\n  if (pfn_event_notify == NULL) {\n    return CL_INVALID_VALUE;\n  }\n  if (command_exec_callback_type != CL_SUBMITTED &&\n      command_exec_callback_type != CL_RUNNING &&\n      command_exec_callback_type != CL_COMPLETE) {\n    return CL_INVALID_VALUE;\n  }\n\n  cb = (acl_event_user_callback *)acl_malloc(sizeof(acl_event_user_callback));\n  if (!cb)\n    return CL_OUT_OF_HOST_MEMORY;\n\n  cb->notify_user_data = user_data;\n  cb->event_notify_fn = pfn_event_notify;\n  cb->registered_exec_status = command_exec_callback_type;\n  cb->next = event->callback_list;\n  event->callback_list = cb;\n  clRetainEvent(event); // Retain the event once for each registered callback.\n                        // Will release once after each callback.\n\n  // If the function is registered for a state that is already completed we\n  // should do the callback now. calling the callbacks to make sure no excess\n  // status is already passed.\n  acl_event_callback(event, event->execution_status);\n\n  return CL_SUCCESS;\n}\n\n// registers a user callback function for a specific command execution status.\nACL_EXPORT\nCL_API_ENTRY cl_int clSetEventCallback(\n    cl_event event, cl_int command_exec_callback_type,\n    void(CL_CALLBACK *pfn_event_notify)(cl_event event,\n                                        cl_int event_command_exec_status,\n                                        void *user_data),\n    void *user_data) {\n  return clSetEventCallbackIntelFPGA(event, command_exec_callback_type,\n                                     pfn_event_notify, user_data);\n}\n//////////////////////////////\n// Internals\n\nvoid acl_event_callback(cl_event event, cl_int event_command_exec_status) {\n  // This function should not be called from a signal handler, but from within a\n  // lock, as it calls user defined callback functions. So a lock is required.\n  acl_assert_locked();\n\n  // Call the notifcation function registered via clSetEventCallback calls.\n  // if event is valid all the list will be searched and all functions with\n  // execution status equal to or past the status specified by\n  // command_exec_status will be called. However, each function should be called\n  // one and only once. So it will be removed afterwards.\n  if (acl_event_is_valid(event)) {\n    acl_event_user_callback *cb_head = event->callback_list;\n    acl_event_user_callback *pre = NULL, *temp;\n    int release = 0;\n    while (cb_head) {\n      cl_bool cond_1 = (cl_bool)(event_command_exec_status <\n                                 0); // Abnormal termination should be Treated\n                                     // as CL_COMPLETE, while passing the error\n                                     // code. All functions must be called!\n      cl_bool cond_2 = (cl_bool)(event_command_exec_status >= 0 &&\n                                 cb_head->registered_exec_status >=\n                                     event_command_exec_status);\n      if (cond_1 || cond_2) {\n        acl_event_notify_fn_t event_notify_fn = cb_head->event_notify_fn;\n        void *notify_user_data = cb_head->notify_user_data;\n        // removing that callback from the list and calling it.\n        if (pre == NULL)\n          event->callback_list = cb_head->next;\n        else\n          pre->next = cb_head->next;\n        temp = cb_head;\n        cb_head = cb_head->next;\n        acl_free(temp);\n        {\n          acl_suspend_lock_guard lock{acl_mutex_wrapper};\n          event_notify_fn(event, event_command_exec_status, notify_user_data);\n        }\n        release++;\n      } else {\n        pre = cb_head;\n        cb_head = cb_head->next;\n      }\n    }\n    // Decrement the ref_count afte the callbacks are called.\n    while (release--) {\n      assert(acl_is_retained(event));\n      acl_release(event);\n    }\n  }\n}\n\n// Is this event in use by the system?\n// (If it's in use by the user, i.e. has refcount > 0, then it is\n// considered in use by the system.)\nint acl_event_is_valid(cl_event event) {\n  acl_assert_locked();\n\n#ifdef REMOVE_VALID_CHECKS\n  return 1;\n#else\n  if (!acl_is_valid_ptr(event)) {\n    return 0;\n  }\n  if (!acl_is_valid_ptr(event->context)) {\n    return 0;\n  }\n  cl_context context = event->context;\n  if (event->id > context->num_events) {\n    return 0;\n  }\n  if (context->used_events.find(event) == context->used_events.end()) {\n    return 0;\n  }\n  return 1;\n#endif\n}\n\nvoid acl_reset_event(cl_event event) {\n  int i;\n  acl_assert_locked();\n\n  acl_reset_ref_count(event);\n  event->context = 0;\n  event->command_queue = 0;\n  event->cmd = {0};\n  event->cmd.type = CL_COMMAND_MARKER; // should be innocuous\n  event->execution_status = CL_QUEUED; // Initial state is queued\n  event->ptr_hashtable.clear();\n\n  for (i = 0; i < ACL_NUM_PROFILE_TIMESTAMPS; i++) {\n    event->timestamp[i] = 0;\n  }\n\n  // Reset the event dependency lists.\n  event->depend_on.clear();\n  event->depend_on_me.clear();\n\n  event->is_on_device_op_queue = 0;\n\n  // All callbacks are expected to be called before here. So if callback_list is\n  // not empty, either it is not initialized correctly, or some callbacks still\n  // remain uncalled.\n  assert(event->callback_list == NULL);\n}\n\nint acl_event_is_live(cl_event event) {\n  acl_assert_locked();\n\n  // Assume the pointer is valid.\n  if (!event->depend_on_me.empty()) {\n    return 1;\n  } // Some other event depends on me\n  if (!event->depend_on.empty()) {\n    return 1;\n  } // I depend on some other event.\n  if (!acl_event_is_done(event)) {\n    return 1;\n  }\n  return 0;\n}\n\n// Return non-zero if the event was removed.\nint acl_maybe_delete_event(cl_event event) {\n  acl_assert_locked();\n\n  if (acl_event_is_valid(event) && !acl_event_is_live(event)) {\n    // Release the implicitly retained cl_objects that were used by this event\n    // as soon as the event is complete.\n    // Even if the event itself is retained by the user we shouldn't retain any\n    // other cl_objects.\n    l_release_command_resources(event->cmd);\n\n    // this will be called only when the event is popped from the queue, set the\n    // command queue to NULL\n    event->command_queue = NULL;\n\n    // check to see if the event is retained to see if we should only pop it\n    // from queue or delete it completely popped events will do this step later\n    // when they are being deleted, events being deleted will do it now\n    if (!acl_is_retained(event)) {\n      cl_context context;\n\n      acl_print_debug_msg(\n          \"    Deleting event [%d], so releasing command resources\\n\",\n          event->id);\n\n      acl_untrack_object(event);\n\n      context = event->context;\n      // The event is already removed from all dependency lists.\n      //\n      // Just go back onto the free list for the context.\n\n      acl_print_debug_msg(\" adding event %d to free event pool\\n\", event->id);\n      context->used_events.erase(event);\n      context->free_events.push_back(event);\n    }\n    return 1;\n  }\n  return 0;\n}\n\nstatic void l_release_command_resources(acl_command_info_t &cmd) {\n  acl_assert_locked();\n\n  switch (cmd.type) {\n  case CL_COMMAND_READ_BUFFER:\n  case CL_COMMAND_WRITE_BUFFER:\n  case CL_COMMAND_COPY_BUFFER:\n    if (cmd.info.mem_xfer.src_mem) {\n      clReleaseMemObject(cmd.info.mem_xfer.src_mem);\n      cmd.info.mem_xfer.src_mem = nullptr;\n    }\n    if (cmd.info.mem_xfer.dst_mem) {\n      clReleaseMemObject(cmd.info.mem_xfer.dst_mem);\n      cmd.info.mem_xfer.dst_mem = nullptr;\n    }\n    break;\n\n  case CL_COMMAND_MAP_BUFFER:\n  case CL_COMMAND_UNMAP_MEM_OBJECT:\n    if (cmd.trivial && cmd.info.trivial_mem_mapping.mem) {\n      clReleaseMemObject(cmd.info.trivial_mem_mapping.mem);\n      cmd.info.trivial_mem_mapping.mem = nullptr;\n    } else if (!cmd.trivial) {\n      // This goes through the regular mem copy flow.\n      if (cmd.info.mem_xfer.src_mem) {\n        clReleaseMemObject(cmd.info.mem_xfer.src_mem);\n        cmd.info.mem_xfer.src_mem = nullptr;\n      }\n      if (cmd.info.mem_xfer.dst_mem) {\n        clReleaseMemObject(cmd.info.mem_xfer.dst_mem);\n        cmd.info.mem_xfer.dst_mem = nullptr;\n      }\n    }\n    break;\n\n  case CL_COMMAND_TASK:\n  case CL_COMMAND_NDRANGE_KERNEL:\n    if (cmd.info.ndrange_kernel.memory_migration.num_mem_objects != 0 &&\n        cmd.info.ndrange_kernel.memory_migration.src_mem_list) {\n      // src_mem should be user-provided buffers, users are responsible for\n      // releasing them Just free the src memory list here\n      acl_free(cmd.info.ndrange_kernel.memory_migration.src_mem_list);\n      cmd.info.ndrange_kernel.memory_migration.src_mem_list = nullptr;\n    }\n    // Cleanup is handled via the completion callback.\n    break;\n\n  case CL_COMMAND_PROGRAM_DEVICE_INTELFPGA:\n    // Balance out the retain when we scheduled the command.\n    if (cmd.info.eager_program) {\n      clReleaseProgram(cmd.info.eager_program->get_dev_prog()->program);\n      cmd.info.eager_program = nullptr;\n    }\n    break;\n\n  case CL_COMMAND_MIGRATE_MEM_OBJECTS:\n    for (size_t i = 0; i < cmd.info.memory_migration.num_mem_objects; ++i) {\n      clReleaseMemObject(cmd.info.memory_migration.src_mem_list[i].src_mem);\n      cmd.info.memory_migration.src_mem_list[i].src_mem = nullptr;\n    }\n\n    if (cmd.info.memory_migration.src_mem_list) {\n      acl_free(cmd.info.memory_migration.src_mem_list);\n      cmd.info.memory_migration.src_mem_list = nullptr;\n    }\n\n    cmd.info.memory_migration.num_mem_objects = 0;\n    cmd.info.memory_migration.num_alloc = 0;\n    break;\n\n  case CL_COMMAND_READ_HOST_PIPE_INTEL:\n  case CL_COMMAND_WRITE_HOST_PIPE_INTEL:\n    // Nothing to cleanup\n    break;\n  case CL_COMMAND_READ_GLOBAL_VARIABLE_INTEL:\n  case CL_COMMAND_WRITE_GLOBAL_VARIABLE_INTEL:\n    // Cleanup is handled by the host free.\n    break;\n  default:\n    break;\n  }\n}\n\n// Update event status.\n// Can be called from within a HAL interrupt OR user thread.\n// We can't do much: just set a flag in the right spot.\n// A later pass of the cooperative scheduler will take action.\nvoid acl_set_execution_status(cl_event event, int new_status) {\n  // This function can potentially be called by a HAL that does not use the\n  // ACL global lock, so we need to use acl_lock() instead of\n  // acl_assert_locked(). However, the MMD HAL calls this function from a unix\n  // signal handler, which can't lock mutexes, so we don't lock in that case.\n  // All functions called from this one therefore have to use\n  // acl_assert_locked_or_sig() instead of just acl_assert_locked().\n  std::unique_lock lock{acl_mutex_wrapper, std::defer_lock};\n  if (!acl_is_inside_sig()) {\n    lock.lock();\n  }\n\n  if (event) { // just being defensive\n    if (event->current_device_op) {\n      // Update the device operation, instead of this event directly.\n      acl_set_device_op_execution_status(event->current_device_op, new_status);\n    } else {\n      // Update this device operation directly.\n      cl_int effective_status = new_status;\n      switch (new_status) {\n      case CL_QUEUED:\n      case CL_SUBMITTED:\n      case CL_RUNNING:\n      case CL_COMPLETE:\n        l_record_milestone(event, (cl_profiling_info)new_status);\n        if (!acl_is_inside_sig())\n          acl_event_callback(event, new_status);\n        break;\n      default:\n        if (new_status >= 0) {\n          // Not given a valid status.  Internal error?\n          // Change to a negative status so command queue processing works.\n\n          // we can't call the user context callback from a signal handler\n          if (!acl_is_inside_sig()) {\n            if (event->command_queue) {\n              acl_context_callback(\n                  event->context,\n                  \"Internal error: Setting invalid event status \"\n                  \"with positive value\");\n            }\n          }\n          effective_status = ACL_INVALID_EXECUTION_STATUS; // this is negative\n        }\n        if (effective_status < 0) {\n          // An error condition.  Record as complete.\n          l_record_milestone(event, CL_COMPLETE);\n          if (!acl_is_inside_sig())\n            acl_event_callback(\n                event, effective_status); // abnormal termination; making all\n                                          // the uncalled callbacks till\n                                          // CL_COMPLETE but sending error code\n                                          // instead of command_exec_status\n        }\n        break;\n      }\n\n      acl_print_debug_msg(\"       Event [%d] %p status <- %d (%d)\\n\", event->id,\n                          event, effective_status, new_status);\n\n      event->execution_status = effective_status;\n      // Update queues\n      if (effective_status <= CL_COMPLETE) {\n        cl_command_queue command_queue = event->command_queue;\n        assert(command_queue != NULL);\n\n        if (command_queue->properties &\n            CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n          command_queue->completed_commands.push_back(event);\n        } else {\n          if (effective_status == CL_COMPLETE ||\n              effective_status == ACL_INVALID_EXECUTION_STATUS) {\n            command_queue->num_commands_submitted--;\n          }\n        }\n      }\n    }\n\n    // Signal all waiters.\n    acl_signal_device_update();\n  }\n}\n\nstatic void l_record_milestone(cl_event event, cl_profiling_info milestone) {\n  acl_assert_locked_or_sig();\n\n  if (acl_event_is_valid(event) &&\n      acl_command_queue_is_valid(event->command_queue) &&\n      (event->command_queue->properties & CL_QUEUE_PROFILING_ENABLE)) {\n    cl_ulong ts = acl_get_hal()->get_timestamp();\n\n    acl_print_debug_msg(\"         Event [%d] status <- %d @ TS %lu\\n\",\n                        event->id, milestone, ts);\n\n    switch (milestone) {\n    case CL_QUEUED:\n    case CL_SUBMITTED:\n    case CL_RUNNING:\n    case CL_COMPLETE:\n      event->timestamp[milestone] = ts;\n      break;\n    default:\n      break;\n    }\n  }\n}\n\n// Create an event that depends on the given events.\n// It's the caller's job to fill in the rest, e.g. execution status.\n// Consider this a step in what would be an Event constructor.\n//\n// The caller must pass a valid return event pointer.\n// A caller should be responsible for the event he/she created.\n//\n// Return 0 if we ran out of memory.\n//\n// The result is undefined (or even crash!) if the passed-in events are\n// invalid in any way, e.g. not actually live.\n//\n// If successful, the returned event is already implicitly retained.\ncl_int acl_create_event(cl_command_queue command_queue, cl_uint num_events,\n                        const cl_event *events, cl_command_type command_type,\n                        cl_event *new_event_ret) {\n  cl_event event;\n  cl_int result;\n  cl_context context;\n  acl_assert_locked();\n\n  // defensively guarding that the reference of return event passed by the\n  // caller should be valid\n  assert(new_event_ret &&\n         \"The reference of the return event is invalid. A caller must pass a \"\n         \"valid event pointer when creates a new event\");\n\n#ifndef REMOVE_VALID_CHECKS\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  result =\n      acl_check_events_in_context(command_queue->context, num_events, events);\n  switch (result) {\n  case CL_SUCCESS:\n    break;\n  case CL_INVALID_CONTEXT:\n    return result;\n  default:\n    return CL_INVALID_EVENT_WAIT_LIST;\n  }\n#else\n  result = CL_SUCCESS;\n#endif\n\n  context = command_queue->context;\n\n  event = l_get_unused_event(context);\n  if (event == 0) {\n    acl_context_callback(context, \"Could not allocate an event or command\");\n    return CL_OUT_OF_HOST_MEMORY;\n  }\n\n  event->dispatch = &acl_icd_dispatch;\n\n  // No callback by default.\n  event->completion_callback = 0;\n\n  // No device operation, by default.\n  event->last_device_op = 0;\n\n  // The currently executing device op servicing this event/command.\n  event->current_device_op = 0;\n\n  if (events && num_events) {\n    try {\n      // The event dependency logic requires unique events.\n      std::set<cl_event> active_dependency;\n      // Special case: add barrier dependency if present.\n      if (command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE &&\n          command_queue->last_barrier) {\n        active_dependency.insert(command_queue->last_barrier);\n      }\n\n      // Fill active_dependency[] by removing duplicates from events[].\n      for (cl_uint i = 0; i < num_events; i++) {\n        if (active_dependency.find(events[i]) == active_dependency.end()) {\n          // If an event is already finished, no need to add a dependency on it\n          if (!acl_event_is_done(events[i])) {\n            active_dependency.insert(events[i]);\n          }\n        }\n      }\n      // Now add dependency arcs.\n      for (cl_event dependency : active_dependency) {\n        event->depend_on.insert(dependency);\n        dependency->depend_on_me.push_back(event);\n      }\n    } catch (const std::bad_alloc &) {\n      // Remove added dependencies\n      for (int i = static_cast<int>(num_events) - 1; i >= 0; i--) {\n        if (events[i]->depend_on_me.back()->id == event->id) {\n          events[i]->depend_on_me.pop_back();\n        }\n      }\n\n      // Return event back to free pool\n      event->depend_on.clear();\n      l_return_event_to_free_pool(event);\n      event = 0; // Signal that we're going to fail.\n    }\n  }\n\n  if (event) {\n    // Success!\n\n    // Normally, the command info union usage is determined only\n    // by the command type.  But in the case of CL_COMMAND_MAP_BUFFER\n    // and CL_COMMAND_UNMAP_MEM_OBJECT, we need this extra bit to know\n    // whether to use the mem_xfer or trivial_mem_mapping subfields.\n    // By default, consider it \"non-trivial\".\n    event->cmd.trivial = 0;\n\n    event->not_popped = true;\n\n    // Set top-level command info.\n    event->cmd.type = command_type; // trust that it's a valid value.\n    // check if this event can be profiled based on the command_queue info\n    // note that if it is a user event, the user_event_queue doesn't support\n    // profiler\n    event->support_profiling =\n        (command_queue->properties & CL_QUEUE_PROFILING_ENABLE) != 0;\n\n    try {\n      acl_command_queue_add_event(command_queue, event);\n    } catch (const std::bad_alloc &) {\n      if (events && num_events) {\n        // Remove added dependencies\n        for (int i = static_cast<int>(num_events) - 1; i >= 0; i--) {\n          if (events[i]->depend_on_me.back()->id == event->id) {\n            events[i]->depend_on_me.pop_back();\n          }\n        }\n      }\n\n      // Return event back to free pool\n      event->depend_on.clear();\n      l_return_event_to_free_pool(event);\n\n      acl_context_callback(context, \"Could not add event to command queue\");\n      return CL_OUT_OF_HOST_MEMORY;\n    }\n\n    // Retention\n    acl_retain(event);\n    acl_retain(command_queue); // retain the referred command queue as well when\n                               // an event is created attached to the queue\n    *new_event_ret = event;\n\n    if (debug_mode > 0) {\n      printf(\"  Notify: Event [%d] has been created:\\n\", event->id);\n      acl_dump_event(event);\n    }\n\n    acl_track_object(ACL_OBJ_EVENT, event);\n  } else {\n    acl_context_callback(context, \"Could not allocate an event\");\n    return CL_OUT_OF_HOST_MEMORY;\n  }\n\n  return result;\n}\n\n// Returns an unused event_object, or NULL if we ran out of memory.\n// We allocate the *pointers* to the event structs in batches that roughly\n// double in size each time we need more.\n// We allocate individual structures one-by-one.\nstatic cl_event l_get_unused_event(cl_context context) {\n  acl_assert_locked();\n  cl_event result = 0;\n  bool reused_event = false;\n  if (context->free_events.empty()) {\n    // create event and directly push to used_events\n    result = acl_alloc_cl_event();\n    if (!result)\n      return 0;\n    result->id = context->num_events;\n    context->num_events++;\n    result->callback_list = NULL; // initializing the callback list.\n  } else {\n    // remove event from the pool of avaliable ones\n    result = context->free_events.front();\n    context->free_events.pop_front();\n    reused_event = true;\n  }\n\n  try {\n    acl_reset_event(result);\n    result->context = context;\n    context->used_events.insert(result);\n  } catch (const std::bad_alloc &) {\n    if (reused_event) {\n      context->free_events.push_back(result);\n    } else {\n      acl_free_cl_event(result);\n      context->num_events--;\n    }\n    result = 0;\n  }\n  return result;\n}\n\n// Returns an event object to the unused pool\nstatic void l_return_event_to_free_pool(cl_event event) {\n  acl_assert_locked();\n  cl_context context = event->context;\n  context->used_events.erase(event);\n  context->free_events.push_back(event);\n}\n\ncl_int acl_check_events_in_context(cl_context context, cl_uint num_events,\n                                   const cl_event *events) {\n  cl_uint i;\n  acl_assert_locked();\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n  if (num_events > 0 && events == 0) {\n    acl_context_callback(\n        context, \"Event count is positive but event array is not specified\");\n    return CL_INVALID_VALUE;\n  }\n  if (num_events == 0 && events != 0) {\n    acl_context_callback(context,\n                         \"Event count is zero but event array is specified\");\n    return CL_INVALID_VALUE;\n  }\n\n  for (i = 0; i < num_events; i++) {\n    cl_event event = events[i];\n\n    // check the event and context based on the event status\n    if (!acl_event_is_valid(event)) {\n      acl_context_callback(context, \"Invalid event specified\");\n      return CL_INVALID_EVENT;\n    }\n\n    // Check for consistent context\n    if (event->context != context) {\n      acl_context_callback(context, \"Event from incorrect context\");\n      return CL_INVALID_CONTEXT;\n    }\n  }\n  return CL_SUCCESS;\n}\n\n// Return CL_SUCCESS if the list of events is non-empty and valid\n// and have consistent context.\n// Otherwise return CL_INVALID_VALUE or other error status.\ncl_int acl_check_events(cl_uint num_events, const cl_event *events) {\n  acl_assert_locked();\n\n  if (num_events == 0) {\n    return CL_INVALID_VALUE;\n  }\n  if (events == 0) {\n    return CL_INVALID_VALUE;\n  }\n\n  // Get the context of the first event to send into\n  // acl_check_events_with_context.\n  // Don't check the callback because we can't trust the context just yet.\n  if (!acl_event_is_valid(events[0])) {\n    return CL_INVALID_EVENT;\n  }\n\n  // This will call the error callback as necessary.\n  return acl_check_events_in_context(events[0]->context, num_events, events);\n}\n\n//////////////////////////////\n// Internals - Event dependencies\n\n// Remove this event from all the dependency lists of downstream events.\n// Helper function only used by acl_update_inorder_queue()\n// Returns number of notifications.\nint acl_notify_dependent_events(cl_event event) {\n  acl_assert_locked();\n  if (!acl_event_is_valid(event)) {\n    return 0;\n  }\n\n  if (debug_mode > 0) {\n    if (!event->depend_on.empty()) {\n      printf(\"  Notify: Event [%d] does not have dependent events\\n\",\n             event->id);\n    } else {\n      printf(\"  Notify: Event [%d] has dependent events. Before:\\n\", event->id);\n      acl_dump_event(event);\n    }\n  }\n\n  for (cl_event dependent : event->depend_on_me) {\n    dependent->depend_on.erase(event);\n\n    // According to the OpenCL spec for clSetUserEventStatus,\n    // when a user event's execution status is set to be negative it\n    // causes all enqueued commands that wait on the user event to be\n    // terminated.\n    if (event->cmd.type == CL_COMMAND_USER && event->execution_status < 0) {\n      acl_set_execution_status(dependent, event->execution_status);\n    }\n  }\n\n  int num_updates = static_cast<int>(event->depend_on_me.size());\n  event->depend_on_me.clear();\n\n  if ((debug_mode > 0) && num_updates) {\n    printf(\"  Notify: Event [%d] updated %d dependent events. After:\\n\",\n           event->id, num_updates);\n    acl_dump_event(event);\n  }\n\n  return num_updates;\n}\n\n// Removes the dependency link between the given event\n// and the first event that depends on it\ncl_event acl_remove_first_event_dependency(cl_event event) {\n  acl_assert_locked();\n\n  assert(acl_event_is_valid(event));\n  assert(event->depend_on.empty());\n\n  cl_event dependent = event->depend_on_me.front();\n  dependent->depend_on.erase(event);\n  event->depend_on_me.pop_front();\n  return dependent;\n}\n\n// Are the resources required to run this event available?\n// Used to avoid contention on kernel hardware blocks.\nint acl_event_resources_are_available(cl_event event) {\n  int result = 1;\n  acl_assert_locked();\n\n  switch (event->cmd.type) {\n  case CL_COMMAND_TASK:\n  case CL_COMMAND_NDRANGE_KERNEL: {\n    // Is the accelerator block currently occupied?\n    const auto kernel = event->cmd.info.ndrange_kernel.kernel;\n    auto *dev_prog = event->cmd.info.ndrange_kernel.dev_bin->get_dev_prog();\n    result = (nullptr == dev_prog->current_event[kernel->accel_def]);\n  } break;\n  default:\n    break;\n  }\n  return result;\n}\n\n#ifdef ACL_DEBUG\nvoid acl_dump_event(cl_event event) {\n  const char *type_name = \"\";\n  acl_assert_locked();\n\n  cl_command_queue cq = event->command_queue;\n  cl_context context = event->context;\n#define NNN(T)                                                                 \\\n  case T:                                                                      \\\n    type_name = #T;                                                            \\\n    break;\n  switch (event->cmd.type) {\n    NNN(CL_COMMAND_TASK)\n    NNN(CL_COMMAND_NDRANGE_KERNEL)\n    NNN(CL_COMMAND_MARKER)\n    NNN(CL_COMMAND_READ_BUFFER)\n    NNN(CL_COMMAND_WRITE_BUFFER)\n    NNN(CL_COMMAND_COPY_BUFFER)\n    NNN(CL_COMMAND_USER)\n    NNN(CL_COMMAND_MAP_BUFFER)\n    NNN(CL_COMMAND_WAIT_FOR_EVENTS_INTELFPGA)\n    NNN(CL_COMMAND_PROGRAM_DEVICE_INTELFPGA)\n    NNN(CL_COMMAND_READ_HOST_PIPE_INTEL)\n    NNN(CL_COMMAND_WRITE_HOST_PIPE_INTEL)\n  default:\n    break;\n  }\n  acl_print_debug_msg(\"       Event [%d] %p = {\\n\", event->id, event);\n  acl_print_debug_msg(\"          .refcnt            %u\\n\",\n                      acl_ref_count(event));\n  // the event can be popped, therefore the queue can be invalid\n  if (cq)\n    acl_print_debug_msg(\n        \"          .cq                %d %s\\n\", event->command_queue->id,\n        (cq == context->user_event_queue\n             ? \"user_event_queue\"\n             : (cq == context->auto_queue ? \"auto_queue\" : \"\")));\n  else\n    acl_print_debug_msg(\"the event doesn't belong to any command queue\");\n  acl_print_debug_msg(\"          .execution_status  %d\\n\",\n                      event->execution_status);\n  acl_print_debug_msg(\"          .cmd_type          0x%4X %s\\n\",\n                      event->cmd.type, type_name);\n  acl_print_debug_msg(\"          .depend_on_me = {\");\n  for (cl_event dependent : event->depend_on_me) {\n    if (dependent) {\n      acl_print_debug_msg(\" %d,\", dependent->id);\n    } else {\n      acl_print_debug_msg(\" null,\");\n    }\n  }\n  acl_print_debug_msg(\" }\\n\");\n  acl_print_debug_msg(\"          .i_depend_on = {\");\n  for (cl_event depender : event->depend_on) {\n    if (depender) {\n      acl_print_debug_msg(\" %d,\", depender->id);\n    } else {\n      acl_print_debug_msg(\" null,\");\n    }\n  }\n  acl_print_debug_msg(\" }\\n\");\n  acl_print_debug_msg(\"          .times { %d %d %d %d } \\n\",\n                      event->timestamp[0], event->timestamp[1],\n                      event->timestamp[2], event->timestamp[3]);\n  acl_print_debug_msg(\"       }\\n\");\n}\n#endif\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_platform.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <assert.h>\n#include <optional>\n#include <stddef.h>\n#include <stdio.h>\n#include <string.h>\n#include <vector>\n\n#ifdef _WIN32\n#include <windows.h>\n#endif\n\n#ifdef __linux__\n#include <sys/sysinfo.h>\n#endif\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_auto.h>\n#include <acl_auto_configure.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_icd_dispatch.h>\n#include <acl_kernel.h>\n#include <acl_platform.h>\n#include <acl_printf.h>\n#include <acl_shipped_board_cfgs.h> // for acl_shipped_board_cfgs[]\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_types.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n//////////////////////////////\n// Global variables\n// Platform is not initialized, and don't use absent devices\n// Need to \"export\" it since its address is taken in some unit tests.\nACL_EXPORT\nstruct _cl_platform_id acl_platform = {\n    nullptr // Dispatch is empty\n    ,\n    0 // acl_platform is not initialized\n    ,\n    0 // default value for device_exception_platform_counter\n    ,\n    \"\" // No offline device specified by an environment variable, as far as we\n       // know right now.\n};\n\n// Used to detect if user is creating contexts/getting platform ids in multiple\n// processes.\nint platform_owner_pid;\n// ID of the thread that created platform.\n// Used to detect if user is doing multithreading.\nint platform_owner_tid = -1;\n\ncl_platform_id acl_get_platform() { return &acl_platform; }\n\n// Static global copies of shipped board definitions.\n// Need either this storage, or a way to deep copy the accelerator\n// definitions.\nstatic std::vector<std::optional<acl_system_def_t>> shipped_board_defs;\n\n//////////////////////////////\n// Local functions.\nstatic void l_initialize_offline_devices(int offline_mode);\nstatic void l_initialize_devices(const acl_system_def_t *present_board_def,\n                                 int offline_mode, unsigned int num_devices,\n                                 const cl_device_id *devices);\nstatic void l_add_device(int idx);\n\n//////////////////////////////\n// OpenCL API\n\n// Return info about available platforms.\n// Can be used to query the number of available platforms, or to get their\n// ids, or both.\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclGetPlatformIDsIntelFPGA(cl_uint num_entries, cl_platform_id *platforms,\n                          cl_uint *num_platforms_ret) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Set this in case of early return due to error in other arguments.\n  if (num_platforms_ret) {\n    *num_platforms_ret = 1;\n  }\n\n  if (platforms && num_entries <= 0) {\n    return CL_INVALID_VALUE;\n  }\n  if (num_platforms_ret == 0 && platforms == 0) {\n    return CL_INVALID_VALUE;\n  }\n\n  // We want to support two kinds of flows:\n  //\n  //    1. Where there is a valid board available for the host to use,\n  //    e.g. in the PCIe slot.  It will have at least a blank OpenCL SOF\n  //    programmed into it which responds to auto-discovery with a\n  //    configuration string.\n  //\n  //    2. Offline compilation mode.  Where the user can use a context\n  //    property (or environment variable) to forcibly select a device\n  //    from the list of boards we support.\n  //\n  // Advertise all supported boards via device ids.\n  // But normally only the discovered one is marked as available.\n  // If in offline compilation mode, only the\n  //\n\n  // If we don't have a valid system here, check if the autodiscovery can find\n  // it. We also need to get the HAL definition.\n  if (!acl_platform.initialized) {\n    cl_bool result;\n    // Load definitions of devices we can target:\n    //    - Probed devices we find actually attached to the host (probed on\n    //    PCIe), if any,\n    //    - Followed by followed by the list of devices we ship.\n    // Only probed devices will be marked with .present == 1.\n    //\n    // In the end this calls back into acl_init_platform which also sets\n    // acl_platform.initialized = 1.\n    result = acl_init_from_hal_discovery();\n    if (!result) {\n      return CL_PLATFORM_NOT_FOUND_KHR;\n    }\n  }\n  if (!acl_get_hal()) {\n    return CL_PLATFORM_NOT_FOUND_KHR;\n  }\n  if (!acl_platform.initialized) {\n    return CL_PLATFORM_NOT_FOUND_KHR;\n  }\n\n  // Return some data\n  if (platforms) {\n    platforms[0] = &acl_platform;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetPlatformIDs(cl_uint num_entries,\n                                                 cl_platform_id *platforms,\n                                                 cl_uint *num_platforms_ret) {\n  return clGetPlatformIDsIntelFPGA(num_entries, platforms, num_platforms_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclIcdGetPlatformIDsKHR(cl_uint num_entries, cl_platform_id *platforms,\n                       cl_uint *num_platforms_ret) {\n  return clGetPlatformIDsIntelFPGA(num_entries, platforms, num_platforms_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetPlatformInfoIntelFPGA(\n    cl_platform_id platform, cl_platform_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  const char *str = 0;\n  size_t result_len;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_platform_is_valid(platform)) {\n    return CL_INVALID_PLATFORM;\n  }\n\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          0);\n\n  switch (param_name) {\n  // We don't offer an online compiler.\n  case CL_PLATFORM_PROFILE:\n    str = acl_platform.profile;\n    break;\n  case CL_PLATFORM_VERSION:\n    str = acl_platform.version;\n    break;\n  case CL_PLATFORM_NAME:\n    str = acl_platform.name;\n    break;\n  case CL_PLATFORM_VENDOR:\n    str = acl_platform.vendor;\n    break;\n  case CL_PLATFORM_EXTENSIONS:\n    str = acl_platform.extensions;\n    break;\n  case CL_PLATFORM_ICD_SUFFIX_KHR:\n    str = acl_platform.suffix;\n    break;\n  default:\n    return CL_INVALID_VALUE;\n    break;\n  }\n  assert(str);\n  result_len = strnlen(str, MAX_NAME_SIZE) + 1; // Remember the terminating NUL\n\n  if (param_value) {\n    // Actually try to return the string.\n    if (param_value_size < result_len) {\n      // Buffer is too small to hold the return value.\n      return CL_INVALID_VALUE;\n    }\n    strncpy((char *)param_value, str, result_len);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result_len;\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetPlatformInfo(\n    cl_platform_id platform, cl_platform_info param_name,\n    size_t param_value_size, void *param_value, size_t *param_value_size_ret) {\n  return clGetPlatformInfoIntelFPGA(platform, param_name, param_value_size,\n                                    param_value, param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclUnloadPlatformCompilerIntelFPGA(cl_platform_id platform) {\n  // Not fully implemented yet.\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_platform_is_valid(platform)) {\n    return CL_INVALID_PLATFORM;\n  }\n  // For the sake of MSVC compiler warnings.\n  // We don't have any platform compilers, so unloading is successful!\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclUnloadPlatformCompiler(cl_platform_id platform) {\n  return clUnloadPlatformCompilerIntelFPGA(platform);\n}\n\n//////////////////////////////\n// Internals\n\nint acl_platform_is_valid(cl_platform_id platform) {\n  // We only support one value!\n  return platform == &acl_platform;\n}\n\nconst char *acl_platform_extensions() {\n  return \"cl_khr_byte_addressable_store\" // yes, we have correct access to\n                                         // individual bytes!\n         \" cles_khr_int64\" // yes, we support 64-bit ints in the embedded\n                           // profile\n\n         \" cl_khr_icd\"\n#if ACL_SUPPORT_IMAGES == 1\n         \" cl_khr_3d_image_writes\"\n#endif\n#if ACL_SUPPORT_DOUBLE == 1\n         \" cl_khr_fp64\"\n#endif\n#ifdef ACL_120\n         \" cl_khr_global_int32_base_atomics\"\n         \" cl_khr_global_int32_extended_atomics\"\n         \" cl_khr_local_int32_base_atomics\"\n         \" cl_khr_local_int32_extended_atomics\"\n#endif\n#ifndef __arm__\n         \" cl_intel_unified_shared_memory\"\n#endif\n         \" cl_intel_create_buffer_with_properties\"\n         \" cl_intel_mem_channel_property\"\n         \" cl_intel_mem_alloc_buffer_location\"\n         \" cl_intel_program_scope_host_pipe\"\n         \" cl_intel_global_variable_access\";\n}\n\n// Initialize the internal bookkeeping based on the system definition\n// provided to us.\nvoid acl_init_platform(void) {\n  acl_assert_locked();\n\n  acl_platform.dispatch = &acl_icd_dispatch;\n  for (int i = 0; i < ACL_MAX_DEVICE; i++) {\n    acl_platform.device[i].dispatch = &acl_icd_dispatch;\n  }\n\n  acl_platform.initialized = 1;\n\n  // Debug mode to find leaks.\n  acl_platform.track_leaked_objects = (0 != acl_getenv(\"ACL_TRACK_LEAKS\"));\n  acl_platform.cl_obj_head = 0;\n\n  // Set offline_device property\n  const char *offline_device =\n      acl_get_offline_device_user_setting(&acl_platform.offline_mode);\n  if (offline_device) {\n    acl_platform.offline_device = offline_device;\n  }\n\n  acl_platform.name = \"Intel(R) FPGA SDK for OpenCL(TM)\";\n  acl_platform.vendor = \"Intel(R) Corporation\";\n  acl_platform.suffix = \"IntelFPGA\";\n#ifdef ACL_120\n  acl_platform.version =\n      \"OpenCL 1.2 Intel(R) FPGA SDK for OpenCL(TM), Version \" ACL_VERSION_PLAIN;\n#else\n  acl_platform.version =\n      \"OpenCL 1.0 Intel(R) FPGA SDK for OpenCL(TM), Version \" ACL_VERSION_PLAIN;\n#endif\n  // The extensions string specifies the extensions supported by our framework.\n  // To add an extension, append a flag name and separate it from others using a\n  // space.\n  acl_platform.extensions = acl_platform_extensions();\n\n  acl_platform.profile = \"EMBEDDED_PROFILE\";\n  acl_platform.hal = acl_get_hal();\n\n  if (acl_platform.hal == nullptr) {\n    return;\n  }\n\n  // User is supposed to interact with runtime only on a single process.\n  // Keeping the pid of that process.\n  if (platform_owner_pid == 0) {\n    platform_owner_pid = acl_get_pid();\n  }\n\n  if (platform_owner_tid == 0) {\n    platform_owner_tid = acl_get_thread_id();\n  }\n\n  // Register the callbacks now that we have a HAL\n  acl_platform.hal->register_callbacks(\n      acl_set_execution_status, acl_receive_kernel_update, acl_profile_update,\n      acl_receive_device_exception, acl_schedule_printf_buffer_pickup);\n\n  // These are the OpenCL runtime objects that can be allocated.\n  // See the UML object diagram for the OpenCL 1.1 runtime for ownership\n  // rules.  (Figure 2.1, page 20 in the OpenCL 1.1 spec.)\n  //\n  // Ownership is as follows:\n  //    The Context has a composition relation with:\n  //       CommandQueue\n  //       Event\n  //       MemObject\n  //       Sampler\n  //       Program\n  //    That means the Context object is responsible for teardown of those\n  //    other objects with which is is associated.\n  //\n  //    The CommandQueue has a composition relation with:\n  //       Event  (presumably different instances than the ones associated\n  //       with CommandQueue?)\n  //\n  //    The Device objects are not owned by any other object.\n\n  // We use reference counts in two ways:\n  //\n  //    Transitive destruction from owner objects to the composition\n  //    children.\n  //\n  //    All objects: Tracking whether a statically allocated object is in use.\n\n  // This is NULL when there is no device attached to the host.  This\n  // occurs in \"offline capture\" mode when we have set\n  // CL_CONTEXT_OFFLINE_DEVICE_INTELFPGA to the name of the device to emulate\n  // having.\n  acl_platform.initial_board_def = acl_present_board_def();\n\n  switch (acl_platform.offline_mode) {\n  case ACL_CONTEXT_OFFLINE_AND_AUTODISCOVERY:\n    acl_platform.num_devices =\n        acl_platform.initial_board_def->num_devices +\n        (offline_device ? 1 : 0); // the devices in the board def + 1 for the\n                                  // offline device, if it exists\n    break;\n  case ACL_CONTEXT_MPSIM:\n    acl_platform.num_devices =\n        acl_platform.initial_board_def\n            ->num_devices; // Simulator has its mmd, so it is loaded like an\n                           // actual online device\n    break;\n  case ACL_CONTEXT_OFFLINE_ONLY:\n    acl_platform.num_devices = 1; // Only the offline device\n    break;\n  }\n\n  for (unsigned int i = 0; i < acl_platform.num_devices; i++) {\n    // initialize static information for these devices\n    l_add_device(static_cast<int>(i));\n  }\n\n  l_initialize_offline_devices(acl_platform.offline_mode);\n\n  // Device operation queue.\n  acl_init_device_op_queue(&acl_platform.device_op_queue);\n\n  // Initialize sampler allocator.\n  for (int i = 0; i < ACL_MAX_SAMPLER; i++) {\n    cl_sampler sampler = &(acl_platform.sampler[i]);\n    sampler->id = i;\n    acl_reset_ref_count(sampler);\n    // Hook up the free chain.\n    sampler->link = (i == ACL_MAX_SAMPLER - 1 ? ACL_OPEN : i + 1);\n  }\n  // Now initialize the regions.\n  // Note: The free cl_mem list will be shared between all regions.\n  acl_platform.free_sampler_head = 0;\n\n  // Cache various limits.\n\n  acl_platform.max_compute_units = 1; // minimum allowed\n  acl_platform.max_constant_args = 8; // minimum allowed\n  acl_platform.max_param_size = 256;  // minimum allowed for embedded profile.\n                                      // full profile is 1KB. Affects HW too.\n  acl_platform.max_work_item_dimensions = 3; // minimum allowed\n\n  // Want this positive, even if user puts it into a signed int\n  acl_platform.max_work_group_size = 0x7fffffff;\n  acl_platform.max_work_item_sizes = acl_platform.max_work_group_size;\n\n  acl_platform.mem_base_addr_align = ACL_MEM_ALIGN * 8;\n\n  // Maximum size of the internal buffer that\n  // holds the output of printf calls from a kernel\n  acl_platform.printf_buffer_size = ACL_PRINTF_BUFFER_TOTAL_SIZE;\n  // printf buffer size override\n  {\n    const char *override = 0;\n    override = acl_getenv(\"ACL_PRINTF_BUFFER_SIZE\");\n    if (override) {\n      // There was a string.\n      char *endptr = 0;\n      long value = strtol(override, &endptr, 10);\n      if (endptr == override // no valid characters\n          || *endptr         // an invalid character\n          || (value <= 0 || value > (long)ACL_PRINTF_BUFFER_TOTAL_SIZE)) {\n        fprintf(stderr,\n                \"Warning: Invalid value in enviornment variable \"\n                \"ACL_PRINTF_BUFFER_SIZE. Falling back to default size\\n\");\n        fprintf(stderr,\n                \"         ACL_PRINTF_BUFFER_SIZE must be between 0 and %u\\n\",\n                ACL_PRINTF_BUFFER_TOTAL_SIZE);\n      } else {\n        // Was ok.\n        acl_platform.printf_buffer_size = (unsigned int)value;\n      }\n    }\n  }\n\n  acl_platform.min_data_type_align_size = ACL_MEM_ALIGN;\n\n  acl_platform.single_fp_config =\n      CL_FP_ROUND_TO_NEAREST | CL_FP_INF_NAN; // minimum allowed\n  acl_platform.double_fp_config = // Minimum allowed in OpenCL 1.0 - 1.2,\n                                  // provided doubles are supported\n      CL_FP_FMA | CL_FP_ROUND_TO_NEAREST | CL_FP_ROUND_TO_ZERO |\n      CL_FP_ROUND_TO_INF | CL_FP_INF_NAN | CL_FP_DENORM;\n  acl_platform.queue_properties =\n      CL_QUEUE_PROFILING_ENABLE | CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE;\n\n  // Set up capturing of kernel sources.\n  {\n    const char *capture_base_path = acl_getenv(\"ACL_CAPTURE_BASE\");\n    if (capture_base_path &&\n        strnlen(capture_base_path, MAX_NAME_SIZE) < ACL_MAX_PATH) {\n      strncpy(&(acl_platform.capture_base_path[0]), capture_base_path,\n              ACL_MAX_PATH - 1);\n      acl_platform.capture_base_path[ACL_MAX_PATH - 1] = '\\0';\n      acl_platform.next_capture_id = 0;\n    } else {\n      acl_platform.capture_base_path[0] = '\\0';\n      acl_platform.next_capture_id = ACL_OPEN;\n    }\n  }\n\n  acl_platform.max_pipe_args = 16;\n  acl_platform.pipe_max_active_reservations = 1;\n  acl_platform.pipe_max_packet_size = 1024;\n\n  acl_platform.host_auto_mem.first_block = NULL;\n  acl_platform.host_auto_mem.range.begin = 0;\n  acl_platform.host_auto_mem.range.next = 0;\n\n  acl_platform.global_mem.first_block = NULL;\n  // The range will be determined from the memory on each device\n  acl_platform.global_mem.range.begin = 0;\n  acl_platform.global_mem.range.next = 0;\n\n  acl_platform.host_user_mem.first_block = NULL;\n  acl_platform.host_user_mem.range.begin = 0; // fake\n  acl_platform.host_user_mem.range.next =\n      (void *)(0xffffffffffffffffULL); // fake\n\n  acl_platform.num_global_banks = 0;\n}\n\nvoid acl_finalize_init_platform(unsigned int num_devices,\n                                const cl_device_id *devices) {\n  int have_single_bank_with_shared_memory;\n  acl_assert_locked();\n  assert(num_devices > 0);\n\n  l_initialize_devices(acl_present_board_def(), acl_platform.offline_mode,\n                       num_devices, devices);\n\n  if (is_SOC_device()) {\n    size_t cur_num_banks =\n        devices[0]->def.autodiscovery_def.global_mem_defs[0].num_global_banks;\n    // In shared memory system, host memory is device accessible if have only\n    // one bank (which can only be shared memory).\n    have_single_bank_with_shared_memory = (cur_num_banks == 1);\n  } else {\n    have_single_bank_with_shared_memory = 0;\n  }\n\n  acl_platform.host_auto_mem.is_user_provided = 0;\n  acl_platform.host_auto_mem.is_host_accessible = 1;\n  acl_platform.host_auto_mem.is_device_accessible =\n      have_single_bank_with_shared_memory; // assume separate memory \"segment\"\n  acl_platform.host_auto_mem.uses_host_system_malloc = 1;\n\n  acl_platform.global_mem.is_user_provided = 0;\n  acl_platform.global_mem.is_host_accessible =\n      have_single_bank_with_shared_memory; // assume separate memory \"segment\"\n  acl_platform.global_mem.is_device_accessible = 1;\n  acl_platform.global_mem.uses_host_system_malloc =\n      have_single_bank_with_shared_memory;\n\n  acl_platform.host_user_mem.is_user_provided = 1;\n  acl_platform.host_user_mem.is_host_accessible = 1;\n  acl_platform.host_user_mem.is_device_accessible = (int)is_SOC_device();\n  acl_platform.host_user_mem.uses_host_system_malloc =\n      have_single_bank_with_shared_memory;\n}\n\nstatic void l_show_devs(const char *prefix) {\n  unsigned int i;\n  acl_assert_locked();\n\n  for (i = 0; i < acl_platform.num_devices; i++) {\n    acl_print_debug_msg(\n        \" %s Device[%d] = %p %s present %d \\n\", (prefix ? prefix : \"\"), i,\n        acl_platform.device[i].def.autodiscovery_def.name.c_str(),\n        acl_platform.device[i].def.autodiscovery_def.name.c_str(),\n        acl_platform.device[i].present);\n  }\n}\n\nstatic void l_initialize_offline_devices(int offline_mode) {\n  acl_platform.global_mem.range.begin = 0;\n  acl_platform.global_mem.range.next = 0;\n\n  // Parse the definitions of shipped boards, but just once.\n  // Don't necessarily load them into the device list.\n  //\n  // See acl_shipped_board_cfgs.h which is generated via the\n  // board_cfgs makefile target.\n  // It's an array of shipped_board_cfg struct where name is board name, and\n  // board_cfgs is the auto configuration string.\n  shipped_board_defs.clear();\n  for (const auto &board_cfg : acl_shipped_board_cfgs) {\n    // This is always different storage.\n    // We need this because l_add_device will just pointer-copy the\n    // acl_device_def_t.\n    auto &board_def = shipped_board_defs.emplace_back();\n    board_def.emplace();\n    std::string err_msg;\n    if (!acl_load_device_def_from_str(\n            board_cfg.cfg, board_def->device[0].autodiscovery_def, err_msg)) {\n      board_def.reset();\n      continue;\n    }\n    board_def->num_devices = 1;\n  }\n\n  if (!acl_platform.offline_device.empty()) {\n    unsigned int board_count = 1;\n    int device_index = 0;\n    if (offline_mode == ACL_CONTEXT_OFFLINE_AND_AUTODISCOVERY) {\n      // In this case, we place the offline device at the end of the device list\n      // (after the autodiscovered devices).\n      device_index = (int)(acl_platform.num_devices - board_count);\n    }\n\n    // If the user specified an offline device, then load it.\n    // Search the shipped board defs for the device:\n    for (const auto &board_def : shipped_board_defs) {\n      if (!board_def.has_value())\n        continue;\n      if (acl_platform.offline_device !=\n          board_def->device[0].autodiscovery_def.name)\n        continue;\n\n      for (unsigned j = 0; j < board_count; j++) {\n        // Bail if we haven't been told to use absent devices\n        if (acl_platform.offline_device !=\n            board_def->device[0].autodiscovery_def.name)\n          continue;\n        // Add HW specific device definition\n        acl_platform.device[device_index].def =\n            board_def->device[0]; // Struct Copy\n        acl_platform.device[device_index].present = false;\n        device_index++;\n      }\n    }\n  }\n  l_show_devs(\"offline\");\n}\n\n// Initialize acl_platform with device information.\n// Also determine global mem address range.\nstatic void l_initialize_devices(const acl_system_def_t *present_board_def,\n                                 int offline_mode, unsigned int num_devices,\n                                 const cl_device_id *devices) {\n  unsigned int i, j;\n  acl_assert_locked();\n\n  acl_print_debug_msg(\"\\n\\nReset device list:   %d\\n\", offline_mode);\n\n  if (present_board_def) {\n    acl_print_debug_msg(\"\\n\\nPresent board def:   %d\\n\",\n                        present_board_def->num_devices);\n  }\n\n  // shipped_board_def populated earlier in l_initialize_offline_devices\n\n  if (offline_mode == ACL_CONTEXT_OFFLINE_AND_AUTODISCOVERY ||\n      offline_mode == ACL_CONTEXT_MPSIM) {\n    unsigned int num_platform_devices = acl_platform.num_devices;\n    if (!acl_platform.offline_device.empty()) {\n      // In this case there's an extra offline devices at the end of the list.\n      // Do not check it.\n      num_platform_devices -= 1;\n    }\n\n    // Then add the present devices, if any.\n    if (present_board_def) { // Might be zero probed devices present\n      for (i = 0; i < num_platform_devices; i++) {\n        // Only add devices from the given device list\n        for (j = 0; j < num_devices; j++) {\n          if (&(acl_platform.device[i]) == devices[j]) {\n            // Add HW specific device definition.\n            // present_board_def->device[i] only gets updated in\n            // acl_hal_mmd_try_devices if opened_count is 0, after which it\n            // immediately increment opened_count. Therefore here we check if\n            // opened_count is 1 to update device def on first device open. If\n            // opened_count is greater than 1, device def would not be updated\n            // and retains information from last program. This way we can avoid\n            // reprogram when kernel is created with the same binary with the\n            // one that is currently loaded, but in a different cl_context.\n            if (acl_platform.device[i].opened_count == 1) {\n              acl_platform.device[i].def =\n                  present_board_def->device[i]; // Struct copy.\n              acl_platform.device[i].present = 1;\n            }\n            break;\n          }\n        }\n      }\n    }\n    l_show_devs(\"probed\");\n\n    // Initialize all the present devices via the HAL.\n    if (present_board_def) {\n      acl_platform.hal->init_device(present_board_def);\n    }\n  }\n}\n\n/* Intitializes the static information for the device. This is agnostic of\n   Simulator or HW.*/\nstatic void l_add_device(int idx) {\n  acl_assert_locked();\n  acl_print_debug_msg(\"adding device %d\\n\", idx);\n\n  // Add the device.\n  cl_device_id device = &(acl_platform.device[idx]);\n  device->id = idx;\n  device->platform = &acl_platform;\n  acl_reset_ref_count(device);\n  device->type = CL_DEVICE_TYPE_ACCELERATOR;\n  device->vendor_id = 0x1172; // Altera's PCIe vendor ID.\n  device->version =\n      acl_platform.version; // Just use the same as the platform version.\n\n  // Track ACDS version, must be in \\d+\\.\\d+ form, according to OpenCL spec.\n  device->driver_version = ACL_VERSION_PLAIN_FOR_DRIVER_QUERY;\n\n  device->min_local_mem_size = 16 * 1024; // Min value for OpenCL full profile.\n  device->address_bits = 64;              // Yes, our devices are 64-bit.\n}\n\n// These functions check to see if a given object is known to the system.\n// acl_*_is_valid( * );\n// This is simple because everything is statically allocated.\n\nint acl_device_is_valid_ptr(cl_device_id device) {\n  unsigned int i;\n  acl_assert_locked();\n\n  for (i = 0; i < acl_platform.num_devices; i++) {\n    if (device == &(acl_platform.device[i])) {\n      return 1;\n    }\n  }\n  return 0;\n}\n\nint acl_kernel_is_valid_ptr(cl_kernel kernel) {\n  acl_assert_locked();\n\n#ifdef REMOVE_VALID_CHECKS\n  return 1;\n#else\n  // This has to be fast because it's used in many fundamental APIs.\n  // We can assume the id field has been set.\n  if (acl_is_valid_ptr(kernel)) {\n    return 1;\n  }\n\n  return 0;\n#endif\n}\n\nint acl_sampler_is_valid_ptr(cl_sampler sampler) {\n  acl_assert_locked();\n#ifdef REMOVE_VALID_CHECKS\n  return 1;\n#else\n  // MemObjects belong to the Platform\n  if (sampler == 0) {\n    return 0;\n  }\n  {\n    // Check for valid access range, before trying to access the id field.\n    const cl_sampler first_sampler = &(acl_platform.sampler[0]);\n    const cl_sampler last_sampler =\n        &(acl_platform.sampler[ACL_MAX_SAMPLER - 1]);\n    int id;\n    if ((first_sampler - sampler) > 0)\n      return 0;\n    if ((sampler - last_sampler) > 0)\n      return 0;\n    id = sampler->id;\n    if (id < 0 || id >= ACL_MAX_SAMPLER) {\n      return 0;\n    }\n    if (sampler == &(acl_platform.sampler[id])) {\n      return 1;\n    }\n  }\n  return 0;\n#endif\n}\n\nint acl_pipe_is_valid_pointer(cl_mem mem_obj, cl_kernel kernel) {\n  acl_assert_locked();\n\n  if (mem_obj == 0) {\n    return 0;\n  }\n  {\n    for (const auto &pipe : kernel->program->context->pipe_vec) {\n      if (pipe == mem_obj) {\n        assert((mem_obj)->mem_object_type == CL_MEM_OBJECT_PIPE);\n        return 1;\n      }\n    }\n  }\n  return 0;\n}\n\nvoid acl_release_leaked_objects(void) {\n  acl_assert_locked();\n\n  if (acl_platform.track_leaked_objects) {\n    acl_cl_object_node_t *node = acl_platform.cl_obj_head;\n    while (node) {\n      acl_cl_object_node_t *next = node->next;\n\n      switch (node->type) {\n      case ACL_OBJ_CONTEXT:\n        clReleaseContext((cl_context)(node->object));\n        break;\n      case ACL_OBJ_MEM_OBJECT:\n        clReleaseMemObject((cl_mem)(node->object));\n        break;\n      case ACL_OBJ_PROGRAM:\n        clReleaseProgram((cl_program)(node->object));\n        break;\n      case ACL_OBJ_KERNEL:\n        clReleaseKernel((cl_kernel)(node->object));\n        break;\n      case ACL_OBJ_COMMAND_QUEUE:\n        clReleaseCommandQueue((cl_command_queue)(node->object));\n        break;\n      case ACL_OBJ_EVENT:\n        clReleaseEvent((cl_event)(node->object));\n        break;\n      }\n      acl_free(node);\n\n      node = next;\n    }\n    acl_platform.cl_obj_head = 0;\n  }\n}\n\nvoid acl_track_object(acl_cl_object_type_t type, void *object) {\n  acl_assert_locked();\n\n  if (acl_platform.track_leaked_objects) {\n    acl_cl_object_node_t *node =\n        (acl_cl_object_node_t *)acl_malloc(sizeof(acl_cl_object_node_t));\n    if (node) {\n      node->type = type;\n      node->object = object;\n      node->next = acl_platform.cl_obj_head;\n      acl_platform.cl_obj_head = node;\n    }\n  }\n}\n\nvoid acl_untrack_object(void *object) {\n  acl_assert_locked();\n\n  if (acl_platform.track_leaked_objects) {\n    acl_cl_object_node_t *node;\n    acl_cl_object_node_t **referrer = &(acl_platform.cl_obj_head);\n    while ((node = *referrer) != NULL) {\n      if (node->object == object) {\n        *referrer = node->next;\n        acl_free(node);\n        return;\n      }\n      referrer = &(node->next);\n    }\n  }\n}\n\nvoid acl_receive_device_exception(unsigned physical_device_id,\n                                  CL_EXCEPTION_TYPE_INTEL exception_type,\n                                  void *user_private_info, size_t user_cb) {\n  // This function can potentially be called by a HAL that does not use the\n  // ACL global lock, so we need to use acl_lock() instead of\n  // acl_assert_locked(). However, the MMD HAL calls this function from a unix\n  // signal handler, which can't lock mutexes, so we don't lock in that case.\n  // All functions called from this one therefore have to use\n  // acl_assert_locked_or_sig() instead of just acl_assert_locked().\n  CL_EXCEPTION_TYPE_INTEL current_exception, listen_mask;\n\n  std::unique_lock lock{acl_mutex_wrapper, std::defer_lock};\n  if (!acl_is_inside_sig()) {\n    lock.lock();\n  }\n  current_exception =\n      acl_platform.device[physical_device_id].device_exception_status;\n  listen_mask = acl_platform.device[physical_device_id].listen_mask;\n\n  acl_platform.device[physical_device_id].device_exception_status =\n      listen_mask & (current_exception | exception_type);\n\n  // Provide private_info and cb to a user\n  if (user_private_info && (listen_mask & exception_type)) {\n    int exception_number = 0;\n    CL_EXCEPTION_TYPE_INTEL exception_type_shifter = exception_type;\n    for (unsigned int i = 0; i < sizeof(CL_EXCEPTION_TYPE_INTEL) * 8; ++i) {\n      exception_type_shifter >>= 1;\n      if (exception_type_shifter == 0)\n        break;\n      exception_number += 1;\n    }\n    if (exception_number < 64) {\n      if (acl_platform.device[physical_device_id]\n              .exception_private_info[exception_number]) {\n        free(acl_platform.device[physical_device_id]\n                 .exception_private_info[exception_number]);\n        acl_platform.device[physical_device_id]\n            .exception_private_info[exception_number] = NULL;\n        acl_platform.device[physical_device_id].exception_cb[exception_number] =\n            0;\n      }\n\n      acl_platform.device[physical_device_id]\n          .exception_private_info[exception_number] = malloc(user_cb);\n      memcpy(acl_platform.device[physical_device_id]\n                 .exception_private_info[exception_number],\n             user_private_info, user_cb);\n      acl_platform.device[physical_device_id].exception_cb[exception_number] =\n          user_cb;\n    }\n  }\n\n  // Platform counter counts how many devices have at least one exception\n  // Increment only if a device didn't have any exceptions at first, and this\n  // exception wasn't filtered by listen_mask\n  if (!current_exception &&\n      acl_platform.device[physical_device_id].device_exception_status) {\n    acl_platform.device_exception_platform_counter += 1;\n  }\n\n  if (listen_mask & exception_type) {\n    // Signal all waiters so that acl_idle_update() calls user's exception\n    // callback\n    acl_signal_device_update();\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY void CL_API_CALL\nclTrackLiveObjectsIntelFPGA(cl_platform_id platform) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (platform == &acl_platform) {\n    acl_platform.track_leaked_objects = 1;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY void CL_API_CALL clReportLiveObjectsIntelFPGA(\n    cl_platform_id platform,\n    void(CL_CALLBACK *report_fn)(void *, void *, const char *, cl_uint),\n    void *user_data) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (platform == &acl_platform) {\n    acl_cl_object_node_t *node = acl_platform.cl_obj_head;\n    while (node) {\n      acl_cl_object_node_t *next = node->next;\n      const char *name = \"<none!>\";\n      cl_uint refcount = 0;\n\n      switch (node->type) {\n      case ACL_OBJ_CONTEXT:\n        name = \"cl_context\";\n        refcount = acl_ref_count((cl_context)node->object);\n        break;\n      case ACL_OBJ_MEM_OBJECT:\n        name = \"cl_mem\";\n        refcount = acl_ref_count((cl_mem)node->object);\n        break;\n      case ACL_OBJ_PROGRAM:\n        name = \"cl_program\";\n        refcount = acl_ref_count((cl_program)node->object);\n        break;\n      case ACL_OBJ_KERNEL:\n        name = \"cl_kernel\";\n        refcount = acl_ref_count((cl_kernel)node->object);\n        break;\n      case ACL_OBJ_COMMAND_QUEUE:\n        name = \"cl_command_queue\";\n        refcount = acl_ref_count((cl_command_queue)node->object);\n        break;\n      case ACL_OBJ_EVENT:\n        name = \"cl_event\";\n        refcount = acl_ref_count((cl_event)node->object);\n        break;\n      }\n      if (report_fn) {\n        void *object = node->object;\n        {\n          acl_suspend_lock_guard lock{acl_mutex_wrapper};\n          report_fn(user_data, object, name, refcount);\n        }\n      }\n\n      node = next;\n    }\n  }\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_mem.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <algorithm>\n#include <cassert>\n#include <cstdio>\n#include <sstream>\n#include <string>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_device_op.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_icd_dispatch.h>\n#include <acl_mem.h>\n#include <acl_platform.h>\n#include <acl_support.h>\n#include <acl_svm.h>\n#include <acl_util.h>\n#include <check_copy_overlap.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n// The OpenCL runtime must manage memory allocations for:\n//    - MemObjects pointing to user-provided buffers in host memory\n//    - MemObjects pointing to buffers in host memory provided by OpenCL\n//    - MemObjects pointing to device global memory\n//\n// In our implementation the memory allocator resides on the host\n// processor.  So the allocator bookkeeping must exist outside of the\n// memory it is managing.\n//\n// Our allocator is optimized for simplicity, and for the case where there\n// are only a few largish blocks in the system at a time.\n//\n// The scheme is as follows:\n//   A Region is either for user-provided memory or not.\n//\n//   In regions of system-provided memory span a contiguous set of addresses.\n//   We assume the beginning address has the most general or stringent\n//   alignment, e.g. 4 or 8 byte.\n//\n//   Each Region is associated with a linked list of allocated\n//   non-overlapping address ranges.\n//   The list is ordered by beginning address.\n//\n//   Allocation is first-fit.\n//   Deallocation just removes the allocated range from the list.\n//\n//   The allocator itself needs to dynamically provide link objects to the\n//   algorithm.  We keep them on a linked list, threaded via the link\n//   objects themselves.\n\nstatic void *l_get_address_of_writable_copy(cl_mem mem,\n                                            unsigned int physical_device_id,\n                                            int *on_host_ptr,\n                                            cl_bool is_dest_unmap);\n\nstatic cl_int l_enqueue_mem_transfer(cl_command_queue command_queue,\n                                     cl_bool blocking, cl_mem src_buffer,\n                                     size_t src_offset[3], size_t src_row_pitch,\n                                     size_t src_slice_pitch, cl_mem dst_buffer,\n                                     size_t dst_offset[3], size_t dst_row_pitch,\n                                     size_t dst_slice_pitch, size_t cb[3],\n                                     cl_uint num_events, const cl_event *events,\n                                     cl_event *event, cl_command_type type,\n                                     cl_map_flags map_flags);\n\nstatic void auto_unmap_mem(cl_context context, unsigned int physical_id,\n                           cl_mem src_mem, acl_device_op_t *op);\nstatic void l_mem_transfer_buffer_explicitly(cl_context context,\n                                             acl_device_op_t *op,\n                                             unsigned int physical_device_id,\n                                             const acl_command_info_t &cmd);\nstatic void\nacl_forcibly_release_all_memory_for_context_in_region(cl_context context,\n                                                      acl_mem_region_t *region);\nstatic void acl_dump_mem_internal(cl_mem mem);\n\nstatic void l_get_working_range(const acl_block_allocation_t *block_allocation,\n                                unsigned physical_device_id,\n                                unsigned target_mem_id, unsigned bank_id,\n                                acl_addr_range_t *working_range,\n                                void **initial_try);\nstatic int acl_allocate_block(acl_block_allocation_t *block_allocation,\n                              const cl_mem mem, unsigned physical_device_id,\n                              unsigned target_mem_id);\nstatic int copy_image_metadata(cl_mem mem);\nstatic void remove_mem_block_linked_list(acl_block_allocation_t *block);\nstatic cl_bool is_image(cl_mem mem);\nstatic void l_free_image_members(cl_mem mem);\n\ncl_int acl_convert_image_format(const void *input_element, void *output_element,\n                                cl_image_format format_from,\n                                cl_image_format format_to);\n\nstatic size_t get_offset_for_image_param(cl_context context,\n                                         cl_mem_object_type mem_object_type,\n                                         const char *name);\n\n// This callback is used to free the allocated host memory needed for memory\n// transfers, currently used in clEnqueueFillBuffer, clEnqueueFillImage and\n// clEnqueueMemsetINTEL\nvoid CL_CALLBACK acl_free_allocation_after_event_completion(\n    cl_event event, cl_int event_command_exec_status, void *callback_data) {\n  void **callback_ptrs = (void **)\n      callback_data; // callback_ptrs[0] is the allocated memory in host and\n                     // callback_ptrs[1] is either NULL or pointer to event.\n  event_command_exec_status =\n      event_command_exec_status; // Avoiding Windows warning.\n  event = event;\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (callback_ptrs[0]) {\n    acl_mem_aligned_free(event->context, (acl_aligned_ptr_t *)callback_ptrs[0]);\n    acl_free(callback_ptrs[0]);\n  }\n  if (callback_ptrs[1])\n    clReleaseEvent(((cl_event)callback_ptrs[1]));\n  acl_free(callback_data);\n}\n\nACL_DEFINE_CL_OBJECT_ALLOC_FUNCTIONS(cl_mem);\n\n//////////////////////////////\n// OpenCL API\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainMemObjectIntelFPGA(cl_mem mem) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_mem_is_valid(mem)) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  acl_retain(mem);\n\n  acl_print_debug_msg(\"Retain  mem[%p] now %u\\n\", mem, acl_ref_count(mem));\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainMemObject(cl_mem mem) {\n  return clRetainMemObjectIntelFPGA(mem);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseMemObjectIntelFPGA(cl_mem mem) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // In the double-free case, we'll error out here, for two reasons:\n  // 1) the reference count will be 0.\n  // 1) mem->region == 0\n  if (!acl_mem_is_valid(mem)) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  acl_release(mem);\n\n  acl_print_debug_msg(\"Release mem[%p] now %u\\n\", mem, acl_ref_count(mem));\n\n  if (!acl_ref_count(mem)) {\n    cl_context context = mem->context;\n    // Free the memory object.\n\n    // Calling the user registered destructor callbacks, before freeing the\n    // resources.\n    acl_mem_destructor_callback(mem);\n\n    // For sub-buffers, we don't free the host buffer. Instead, we free the\n    // device memory, remove the sub-buffer from the list of active memory\n    // objects and release the parent buffer\n    if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER &&\n        mem->fields.buffer_objs.is_subbuffer) {\n      if (mem->block_allocation != NULL &&\n          mem->block_allocation->region != NULL &&\n          !mem->block_allocation->region->is_user_provided) {\n        remove_mem_block_linked_list(mem->block_allocation);\n      }\n\n      if (mem->fields.buffer_objs.next_sub != NULL) {\n        mem->fields.buffer_objs.next_sub->fields.buffer_objs.prev_sub =\n            mem->fields.buffer_objs.prev_sub;\n      }\n      if (mem->fields.buffer_objs.prev_sub != NULL) {\n        mem->fields.buffer_objs.prev_sub->fields.buffer_objs.next_sub =\n            mem->fields.buffer_objs.next_sub;\n      }\n      --mem->fields.buffer_objs.parent->fields.buffer_objs.num_subbuffers;\n      clReleaseMemObject(mem->fields.buffer_objs.parent);\n    } else {\n      if (is_image(mem)) {\n        l_free_image_members(mem);\n      }\n      // The only case wehre mem->region->is_user_provided && mem->host_mem.raw\n      // != NULL is when user creates a buffer with CL_MEM_USE_HOST_PTR set and\n      // the pointer is allocated with clSVMAlloc.\n      if (mem->block_allocation != NULL &&\n          mem->block_allocation->region != NULL &&\n          !mem->block_allocation->region->is_user_provided &&\n          mem->host_mem.raw) {\n        // We've used system malloc somewhere along the way.\n        // Release that block of memory.\n        acl_mem_aligned_free(mem->context, &mem->host_mem);\n      }\n\n      // We don't currently allocate anything for pipes (except host pipes),\n      // so there is nothing to free\n      if (mem->host_pipe_info != NULL) {\n        if (mem->host_pipe_info->m_channel_handle > 0) {\n          acl_get_hal()->hostchannel_destroy(\n              mem->host_pipe_info->m_physical_device_id,\n              mem->host_pipe_info->m_channel_handle);\n          mem->host_pipe_info->m_channel_handle = -1;\n        }\n        for (auto &host_op : mem->host_pipe_info->m_host_op_queue) {\n          acl_context_callback(\n              mem->context,\n              \"Warning: The pipe being destroyed has pending operations!\");\n          if (host_op.m_host_buffer != NULL) {\n            free(host_op.m_host_buffer);\n          }\n        }\n        acl_mutex_destroy(&(mem->host_pipe_info->m_lock));\n        acl_delete(mem->host_pipe_info);\n        context->pipe_vec.erase(std::remove(context->pipe_vec.begin(),\n                                            context->pipe_vec.end(), mem));\n      }\n      if (mem->block_allocation != NULL &&\n          mem->block_allocation->region != NULL &&\n          !mem->block_allocation->region->is_user_provided) {\n        remove_mem_block_linked_list(mem->block_allocation);\n      }\n    }\n\n    acl_untrack_object(mem);\n    acl_delete(mem->block_allocation);\n    acl_free_cl_mem(mem);\n    clReleaseContext(context);\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseMemObject(cl_mem mem) {\n  return clReleaseMemObjectIntelFPGA(mem);\n}\n\nstatic int acl_do_physical_buffer_allocation(unsigned physical_device_id,\n                                             cl_mem mem) {\n#ifdef MEM_DEBUG_MSG\n  printf(\"acl_do_physical_buffer_allocation\\n\");\n#endif\n\n  acl_assert_locked();\n\n  // When mem_id == 0 and buffer location is not set, it indicates the mem_id\n  // is not finalized yet so need to set it to a real value here.\n  bool glob_mem = mem->block_allocation->region == &(acl_platform.global_mem);\n  if (glob_mem && mem->mem_id == 0 && !mem->buffer_location_set) {\n    // Memory migration between SVM and device global memory is not supported.\n    // When the device supports device global memory, if SVM is also supported,\n    // do physical buffer allocation on device global memory only; else if SVM\n    // is not supported, we can only allocate on device global memory.\n    if (acl_svm_device_supports_physical_memory(physical_device_id)) {\n      assert(\n          acl_platform.device[physical_device_id]\n                  .def.autodiscovery_def.num_global_mem_systems > 0 &&\n          \"Device is not configured to support SVM and device global memory.\");\n      int tmp_mem_id = acl_get_fit_device_global_memory(\n          acl_platform.device[physical_device_id].def, mem->size);\n      assert(tmp_mem_id >= 0 && \"Device does not have any device global memory \"\n                                \"for the allocation size.\");\n      mem->mem_id = (unsigned int)tmp_mem_id;\n    }\n  } else if (mem->buffer_location_set) {\n    // Sanity check if the specified buffer location is large enough\n    assert(\n        ACL_RANGE_SIZE(acl_platform.device[physical_device_id]\n                           .def.autodiscovery_def.global_mem_defs[mem->mem_id]\n                           .get_usable_range()) >= mem->size &&\n        \"Specified buffer location does not fit the requested allocation size\");\n  }\n\n  int result = 0;\n  if (mem->reserved_allocations[physical_device_id].size() == 0) {\n    acl_resize_reserved_allocations_for_device(\n        mem, acl_platform.device[physical_device_id].def);\n  }\n  if (glob_mem &&\n      mem->reserved_allocations[physical_device_id][mem->mem_id] != NULL) {\n    result = 1;\n    // We can only get here if the allocation was deferred,\n    // which means the original block_allocation isn't in the reserved list,\n    // which means we have to delete it.\n    acl_delete(mem->block_allocation);\n    mem->block_allocation =\n        mem->reserved_allocations[physical_device_id][mem->mem_id];\n  } else {\n    result = acl_allocate_block(mem->block_allocation, mem, physical_device_id,\n                                mem->mem_id);\n  }\n\n  if (glob_mem && result) {\n    mem->reserved_allocations[physical_device_id][mem->mem_id] =\n        mem->block_allocation;\n  }\n\n  // For images, copy the additional meta data (width, height, etc) after\n  // allocating\n  if (result && is_image(mem)) {\n    result = copy_image_metadata(mem);\n  }\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int clSetMemObjectDestructorCallbackIntelFPGA(\n    cl_mem memobj,\n    void(CL_CALLBACK *pfn_notify)(cl_mem memobj, void *user_data),\n    void *user_data) {\n  acl_mem_destructor_user_callback *cb;\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_mem_is_valid(memobj)) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  if (pfn_notify == NULL) {\n    return CL_INVALID_VALUE;\n  }\n\n  cb = (acl_mem_destructor_user_callback *)acl_malloc(\n      sizeof(acl_mem_destructor_user_callback));\n  if (!cb)\n    return CL_OUT_OF_HOST_MEMORY;\n\n  // Push to the front of the list.\n  cb->notify_user_data = user_data;\n  cb->mem_destructor_notify_fn = pfn_notify;\n  cb->next = memobj->destructor_callback_list;\n  memobj->destructor_callback_list = cb;\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int clSetMemObjectDestructorCallback(\n    cl_mem memobj,\n    void(CL_CALLBACK *pfn_notify)(cl_mem memobj, void *user_data),\n    void *user_data) {\n  return clSetMemObjectDestructorCallbackIntelFPGA(memobj, pfn_notify,\n                                                   user_data);\n}\n\n// Given a device, finalize the buffer allocation\nint acl_bind_buffer_to_device(cl_device_id device, cl_mem mem) {\n  acl_assert_locked();\n  assert(mem);\n  assert(device);\n  if (mem->allocation_deferred) {\n    unsigned int physical_device_id = device->def.physical_device_id;\n    if (acl_do_physical_buffer_allocation(physical_device_id, mem)) {\n      mem->allocation_deferred = 0;\n      // If we need to copy from the host pointer, copy now.\n      // If this is a sub buffer or a buffer with sub buffers, mark it as\n      // auto-mapped, keep the data on the host for now and copy when it is\n      // actually used. This prevents us from copying over data that may be\n      // modified by an overlapping sub-buffer before this location is used.\n      if ((mem->mem_object_type == CL_MEM_OBJECT_BUFFER &&\n           acl_is_sub_or_parent_buffer(mem))) {\n        mem->auto_mapped = 1;\n        mem->writable_copy_on_host = 1;\n      } else if ((mem->flags & CL_MEM_COPY_HOST_PTR) &&\n                 mem->writable_copy_on_host == 1) {\n        // We've allocated the memory -- now do a blocking copy of the host mem\n        // buffer. This code should only trigger if the buffer is accessed\n        // without enqueuing a kernel that uses it. This is generally not a\n        // useful sequence of operations in the host program since the contents\n        // of the buffer hasn't changed since it was created if no kernel\n        // operates on it. However, this is still valid OpenCL code so we have\n        // to ensure it works.\n\n        cl_event unmap_event;\n        cl_int status = clEnqueueUnmapMemObject(mem->context->auto_queue, mem,\n                                                mem->host_mem.aligned_ptr, 0,\n                                                NULL, // wait list\n                                                &unmap_event);\n        if (status != CL_SUCCESS) {\n          return 0;\n        }\n\n        status = clWaitForEvents(1, &unmap_event);\n        status |= clReleaseEvent(unmap_event);\n        if (status != CL_SUCCESS) {\n          return 0;\n        }\n\n        mem->mem_cpy_host_ptr_pending = 0;\n      }\n    } else {\n      return 0;\n    }\n  }\n  return 1;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem clCreateBufferWithPropertiesINTEL(\n    cl_context context, const cl_mem_properties_intel *properties,\n    cl_mem_flags flags, size_t size, void *host_ptr, cl_int *errcode_ret) {\n\n  cl_mem result = 0;\n  cl_mem mem;\n  cl_bool context_has_device_with_only_svm;\n  cl_bool context_has_device_with_physical_mem;\n  unsigned int idevice;\n  cl_uint bank_id = 0;\n  cl_uint tmp_mem_id = 0;\n  bool buffer_location_set = false;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"CreateBuffer\\n\");\n#endif\n\n  while (properties != NULL && *properties != 0) {\n    switch (*properties) {\n    case CL_MEM_CHANNEL_INTEL: {\n      if (flags & CL_CHANNEL_7_INTELFPGA) {\n        BAIL_INFO(CL_INVALID_DEVICE, context,\n                  \"Both channel flag and channel property are set\");\n      }\n      bank_id = (cl_uint) * (properties + 1);\n    } break;\n    case CL_MEM_ALLOC_BUFFER_LOCATION_INTEL: {\n      buffer_location_set = true;\n      tmp_mem_id = (cl_uint) * (properties + 1);\n\n      // In FullSystem flow, buffer location is always the index of the global\n      // memories. Therefore, there is no additional handling needed for FS.\n\n      // However, in SYCL_HLS(IPA) flow, the user passed buffer_location<id>\n      // maps to the global memory with the same id. Runtime needs to find the\n      // correct index of the global memory with that id. The id filed is\n      // introduced in 2024.2 in auto-discovery string. This change is for\n      // accessor only and the USM buffer location change is done in the\n      // simulator.\n\n      // Runtime needs to determine whether it's FS or SYCL_HLS first by\n      // checking if the global memory id exist or not. If exists, then it's\n      // SYCL_HLS flow.\n      bool is_SYCL_HLS = false;\n\n      // We document the limitation here:\n      // https://www.intel.com/content/www/us/en/docs/oneapi-fpga-add-on/\n      // developer-guide/2024-0/targeting-multiple-homogeneous-fpga-devices.html\n      // All FPGA devices used must be of the same FPGA card (same -Xstarget\n      // target). There might be some workaround supporting multi-device with\n      // different boards but they are for FS, not for SYCL_HLS.\n\n      // Therefore, we can safely assume all devices have the same\n      // global_mem_defs in SYCL_HLS flow as of 2024.2. So we can just check\n      // acl_platform.device[0].\n\n      auto global_mem_defs =\n          acl_platform.device[0].def.autodiscovery_def.global_mem_defs;\n\n      for (const auto &global_mem_def : global_mem_defs) {\n        if (global_mem_def.id != \"-\") {\n          is_SYCL_HLS = true;\n          break;\n        }\n      }\n\n      if (is_SYCL_HLS) {\n        // find the correct index in the global memory\n        long index = -1;\n        for (auto it = begin(global_mem_defs); it != end(global_mem_defs);\n             ++it) {\n          if (stoul(it->id) == tmp_mem_id) {\n            index = it - global_mem_defs.begin();\n            break;\n          }\n        }\n\n        if (index == -1) {\n          BAIL_INFO(CL_INVALID_VALUE, context,\n                    \"Invalid Buffer Location id provided\");\n        }\n\n        // Update the tmp_mem_id to the corect index in the global memories\n        // vector.\n        tmp_mem_id = static_cast<cl_uint>(index);\n      }\n\n    } break;\n    default: {\n      BAIL_INFO(CL_INVALID_DEVICE, context, \"Invalid properties\");\n    }\n    }\n    properties += 2;\n  }\n\n#ifndef REMOVE_VALID_CHECKS\n  if (!acl_context_is_valid(context))\n    BAIL(CL_INVALID_CONTEXT);\n\n  if (bank_id > 7) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid channel property value\");\n  }\n\n  // Check flags\n  {\n    // Check for invalid enum bits\n    if (flags & ~(CL_MEM_READ_WRITE | CL_MEM_READ_ONLY | CL_MEM_WRITE_ONLY |\n                  CL_MEM_USE_HOST_PTR | CL_MEM_ALLOC_HOST_PTR |\n                  CL_MEM_COPY_HOST_PTR | CL_MEM_HOST_WRITE_ONLY |\n                  CL_MEM_HOST_READ_ONLY | CL_MEM_HOST_NO_ACCESS |\n                  CL_CHANNEL_7_INTELFPGA | CL_MEM_HETEROGENEOUS_INTELFPGA)) {\n      BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid or unsupported flags\");\n    }\n\n    {\n      // Check for exactly one read/write spec\n      int num_rw_specs = 0;\n      if (flags & CL_MEM_READ_WRITE)\n        num_rw_specs++;\n      if (flags & CL_MEM_READ_ONLY)\n        num_rw_specs++;\n      if (flags & CL_MEM_WRITE_ONLY)\n        num_rw_specs++;\n      // Default to CL_MEM_READ_WRITE.\n      if (num_rw_specs > 1) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"More than one read/write flag is specified\");\n      }\n      if (num_rw_specs == 0)\n        flags |= CL_MEM_READ_WRITE;\n\n      // Check for exactly one host read/write/no_access spec\n      num_rw_specs = 0;\n      if (flags & CL_MEM_HOST_READ_ONLY)\n        num_rw_specs++;\n      if (flags & CL_MEM_HOST_WRITE_ONLY)\n        num_rw_specs++;\n      if (flags & CL_MEM_HOST_NO_ACCESS)\n        num_rw_specs++;\n      if (num_rw_specs > 1) {\n        BAIL_INFO(\n            CL_INVALID_VALUE, context,\n            \"More than one host read/write/no_access flags are specified\");\n      }\n    }\n\n    // Check exclusion between use-host-ptr and others\n    if ((flags & CL_MEM_USE_HOST_PTR) && (flags & CL_MEM_ALLOC_HOST_PTR)) {\n      BAIL_INFO(CL_INVALID_VALUE, context,\n                \"Flags CL_MEM_USE_HOST_PTR and CL_MEM_ALLOC_HOST_PTR are both \"\n                \"specified but are mutually exclusive\");\n    }\n    if ((flags & CL_MEM_USE_HOST_PTR) && (flags & CL_MEM_COPY_HOST_PTR)) {\n      BAIL_INFO(CL_INVALID_VALUE, context,\n                \"Flags CL_MEM_USE_HOST_PTR and CL_MEM_COPY_HOST_PTR are both \"\n                \"specified but are mutually exclusive\");\n    }\n  }\n\n  // Check host_ptr\n  if (host_ptr == 0 && (flags & CL_MEM_USE_HOST_PTR)) {\n    BAIL_INFO(CL_INVALID_HOST_PTR, context,\n              \"Flag CL_MEM_USE_HOST_PTR is specified, but no host pointer is \"\n              \"provided\");\n  }\n  if (host_ptr == 0 && (flags & CL_MEM_COPY_HOST_PTR)) {\n    BAIL_INFO(CL_INVALID_HOST_PTR, context,\n              \"Flag CL_MEM_COPY_HOST_PTR is specified, but no host pointer is \"\n              \"provided\");\n  }\n  if (host_ptr != 0 &&\n      !(flags & (CL_MEM_USE_HOST_PTR | CL_MEM_COPY_HOST_PTR))) {\n    BAIL_INFO(CL_INVALID_HOST_PTR, context,\n              \"A host pointer is provided without also specifying one of \"\n              \"CL_MEM_USE_HOST_PTR or CL_MEM_COPY_HOST_PTR\");\n  }\n\n  // Check size\n  if (size == 0) {\n    BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n              \"Memory buffer cannot be of size zero\");\n  }\n  // If using host memory, then just accept any size.\n  if (!(flags & CL_MEM_USE_HOST_PTR) && (size > context->max_mem_alloc_size)) {\n    BAIL_INFO(CL_INVALID_BUFFER_SIZE, context,\n              \"Requested memory object size exceeds device limits\");\n  }\n\n#endif\n\n  auto *new_block = acl_new<acl_block_allocation_t>();\n  if (!new_block) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a cl_mem object\");\n  }\n\n  // Now actually allocate the mem object.\n  mem = acl_alloc_cl_mem();\n  if (!mem) {\n    acl_delete(new_block);\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a cl_mem object\");\n  }\n  mem->mem_id = tmp_mem_id;\n  mem->buffer_location_set = buffer_location_set;\n\n  mem->block_allocation = new_block;\n  mem->block_allocation->mem_obj = mem;\n\n  acl_reset_ref_count(mem);\n\n  mem->mem_object_type = CL_MEM_OBJECT_BUFFER;\n  mem->dispatch = &acl_icd_dispatch;\n  mem->allocation_deferred = 0;\n  mem->host_mem.aligned_ptr = NULL;\n  mem->host_mem.raw = NULL;\n  mem->mem_cpy_host_ptr_pending = 0;\n  mem->destructor_callback_list = NULL;\n  mem->fields.buffer_objs.is_subbuffer = CL_FALSE;\n  mem->fields.buffer_objs.parent = NULL;\n  mem->fields.buffer_objs.next_sub = NULL;\n  mem->fields.buffer_objs.sub_origin = 0;\n  mem->fields.buffer_objs.parent = 0;\n  mem->fields.buffer_objs.num_subbuffers = 0;\n  // Determine what region it goes into.\n  if (flags & CL_MEM_USE_HOST_PTR) {\n    mem->block_allocation->region = &(acl_platform.host_user_mem);\n    mem->block_allocation->range.begin = host_ptr;\n    mem->block_allocation->range.next = (void *)((char *)host_ptr + size);\n#ifdef ACL_HOST_MEMORY_SHARED\n  } else if (flags & CL_MEM_ALLOC_HOST_PTR) {\n    mem->block_allocation->region = &(acl_platform.host_auto_mem);\n#endif\n  } else {\n    // CL_MEM_ALLOC_HOST_PTR is handled by this section.\n    // The caller wants device global memory.\n    // Special case: If CL_MEM_COPY_HOST_PTR then the buffer is in *global*\n    // memory and we have to schedule a copy now.\n\n    mem->block_allocation->region = context->global_mem;\n    if (mem->block_allocation->region == &(context->emulated_global_mem)) {\n      // Will use host malloc, so no working_range or banking required.\n      acl_print_debug_msg(\"emulating memory %p %p\\n\",\n                          mem->block_allocation->region,\n                          &(context->emulated_global_mem));\n    }\n  }\n\n  // Device buffers that are not host accessible always start\n  // as not being mapped to the host.\n  // Set the home location for this buffer.\n  mem->writable_copy_on_host =\n      mem->block_allocation->region->is_host_accessible;\n\n  // Determine if this is SVM memory and, if so, allocate the memory for it\n  context_has_device_with_physical_mem = CL_FALSE;\n  context_has_device_with_only_svm = CL_FALSE;\n  for (idevice = 0; idevice < context->num_devices; ++idevice) {\n    if (acl_svm_device_supports_physical_memory(\n            context->device[idevice]->def.physical_device_id)) {\n      context_has_device_with_physical_mem = CL_TRUE;\n    } else if (acl_svm_device_supports_any_svm(\n                   context->device[idevice]->def.physical_device_id)) {\n      context_has_device_with_only_svm = CL_TRUE;\n    }\n  }\n\n  if ((flags & CL_MEM_USE_HOST_PTR &&\n       acl_ptr_is_contained_in_context_svm(context, host_ptr)) ||\n      (context_has_device_with_only_svm &&\n       !context_has_device_with_physical_mem)) {\n    mem->is_svm = CL_TRUE;\n  } else if (context_has_device_with_only_svm &&\n             context_has_device_with_physical_mem) {\n    acl_delete(mem->block_allocation);\n    acl_free_cl_mem(mem);\n    BAIL_INFO(CL_MEM_OBJECT_ALLOCATION_FAILURE, context,\n              \"Detected devices with only SVM and on-board memory in the same \"\n              \"context. Altera does not currently support this combination and \"\n              \"cannot allocate requested memory object.\");\n  } else {\n    mem->is_svm = CL_FALSE;\n  }\n\n  if (mem->block_allocation->region->is_user_provided) {\n    if (mem->is_svm &&\n        host_ptr != (void *)ACL_MEM_ALIGN) { // Special case: if host_ptr =\n                                             // ACL_MEM_ALIGN, then this is the\n                                             // context->unwrapped_host_mem,\n                                             // which can never be svmAlloc-ed.\n      if (acl_ptr_is_contained_in_context_svm(context, host_ptr)) {\n        acl_aligned_ptr_t ptr;\n        ptr.device_addr = 0;\n        ptr.raw = host_ptr;\n        ptr.aligned_ptr = (void *)(((size_t)ptr.raw + (ACL_MEM_ALIGN - 1)) &\n                                   ~((size_t)(ACL_MEM_ALIGN - 1)));\n        ptr.alignment =\n            ACL_MEM_ALIGN; // data will be aligned to at least one page.\n        ptr.size = size;\n        mem->host_mem = ptr;\n      } else {\n        BAIL_INFO(\n            CL_INVALID_HOST_PTR, context,\n            \"On a system that only supports SVM and does not support \"\n            \"fine-grained system SVM, \"\n            \" provided host pointers must be allocated using clSVMAlloc.\");\n      }\n    }\n    result = mem;\n  } else if (mem->block_allocation->region->uses_host_system_malloc ||\n             mem->is_svm) {\n// ARM SoC with 1 bank or BSP with only SVM\n// Allocate something slightly larger to ensure alignment.\n#ifdef ACL_HOST_MEMORY_SHARED\n    if (acl_get_hal() == NULL || acl_get_hal()->legacy_shared_alloc == NULL) {\n      // No shared_alloc function in kernel capture mode for conformance tests\n      // (CL_CONTEXT_COMPILER_MODE_INTELFPGA=1 flow)\n      // Just alloc the memory normally.\n      mem->host_mem = acl_mem_aligned_malloc(size);\n    } else {\n\n      // Ask the driver for physically-contiguous memory.\n      acl_aligned_ptr_t ptr;\n      ptr.raw =\n          acl_get_hal()->legacy_shared_alloc(context, size, &ptr.device_addr);\n\n      // using size_t. Will break when have 64-bit pointers with 32-bit ARM cpu.\n      // assuming shared memory for CV SoC is bank #1 if there are\n      // two banks and bank #0 if there is only one bank.\n\n      // Assume ARM is always going to have one device !\n      acl_addr_range_t cur_working_range =\n          acl_platform.device[0].def.autodiscovery_def.global_mem_defs[0].range;\n      size_t cur_num_banks = acl_platform.device[0]\n                                 .def.autodiscovery_def.global_mem_defs[0]\n                                 .num_global_banks;\n      size_t cur_bank_size;\n      size_t start_of_bank_one = 0;\n      if (cur_num_banks == 1) {\n        // shared memory is bank #0\n        start_of_bank_one = (size_t)cur_working_range.begin;\n      } else if (cur_num_banks == 2) {\n        // shared memory is bank #1\n        cur_bank_size =\n            ((size_t)cur_working_range.next - (size_t)cur_working_range.begin) /\n            cur_num_banks;\n        start_of_bank_one = (size_t)cur_working_range.begin + cur_bank_size;\n      }\n\n      ptr.device_addr += start_of_bank_one;\n      ptr.aligned_ptr = ptr.raw;\n      ptr.alignment =\n          ACL_MEM_ALIGN; // data will be aligned to at least one page.\n      ptr.size = size;\n      mem->host_mem = ptr;\n    }\n#else\n#ifdef SYSTEM_SVM\n    // can pass in any pointer to FPGA, alloc using normal systemn call\n    mem->host_mem = acl_mem_aligned_malloc(size);\n#else\n    // do the same thing as ARM (without special cases for non-shared bank)\n    // alloc with legacy_shared_alloc if available and device only has SVM\n    if (!context_has_device_with_only_svm || acl_get_hal() == NULL ||\n        acl_get_hal()->legacy_shared_alloc == NULL) {\n      // No shared_alloc function in kernel capture mode for conformance tests\n      // (CL_CONTEXT_COMPILER_MODE_INTELFPGA=1 flow)\n      // Just alloc the memory normally.\n      mem->host_mem = acl_mem_aligned_malloc(size);\n    } else {\n      // Ask the mmd for memory.\n      acl_aligned_ptr_t ptr;\n      ptr.raw =\n          acl_get_hal()->legacy_shared_alloc(context, size, &ptr.device_addr);\n      ptr.aligned_ptr = ptr.raw;\n      ptr.alignment =\n          ACL_MEM_ALIGN; // data will be aligned to at least one page.\n      ptr.size = size;\n      mem->host_mem = ptr;\n    }\n#endif // SYSTEM_SVM\n#endif // ACL_HOST_MEMORY_SHARED\n\n    if (mem->host_mem.raw == 0) {\n      acl_free_cl_mem(mem);\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not allocate a buffer in host memory\");\n    }\n    mem->block_allocation->range.begin = mem->host_mem.aligned_ptr;\n    mem->block_allocation->range.next =\n        (void *)(((char *)mem->host_mem.aligned_ptr) + size);\n\n    // Put it on the allocation list\n    mem->block_allocation->next_block_in_region =\n        mem->block_allocation->region->first_block;\n    mem->block_allocation->region->first_block = mem->block_allocation;\n\n    // Use the first device available in the context\n    int device_id = context->device[0]->id;\n    unsigned int physical_id =\n        acl_platform.device[device_id].def.physical_device_id;\n    if (mem->reserved_allocations[physical_id].size() == 0) {\n      acl_resize_reserved_allocations_for_device(\n          mem, acl_platform.device[device_id].def);\n    }\n    mem->reserved_allocations[physical_id][mem->mem_id] = mem->block_allocation;\n\n    result = mem;\n  } else {\n    // Perform our own first-fit allocation algorithm in the region.\n    // Assign to result only if we succeeded.\n\n    // Try getting host backing store first, if required.\n    // Do this first before modifying any of our other bookkeeping.\n\n    if ((context->device_buffers_have_backing_store ||\n         (((context->num_devices > 1) ||\n           (flags & CL_MEM_HETEROGENEOUS_INTELFPGA)) &&\n          (flags & CL_MEM_COPY_HOST_PTR))) &&\n        !mem->block_allocation->region\n             ->is_host_accessible // needs backing store\n    ) {\n      mem->host_mem = acl_mem_aligned_malloc(size);\n      if (mem->host_mem.raw == 0) {\n        acl_delete(mem->block_allocation);\n        acl_free_cl_mem(mem);\n        BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                  \"Could not allocate backing store for a device buffer\");\n      }\n    }\n\n    mem->context = context;\n    mem->flags = flags;\n    mem->size = size;\n\n    mem->bank_id = 0;\n    if (is_SOC_device()) {\n      // HPS DDR is system managed for SoC.\n      // Only allocate on device DDR.\n      acl_print_debug_msg(\"Ignoring CHANNEL setting on ARM\\n\");\n    } else {\n      // Using channel flag for setting bank id\n      if ((flags & CL_CHANNEL_7_INTELFPGA) != CL_CHANNEL_AUTO_INTELFPGA) {\n        bank_id =\n            ((cl_uint)flags & CL_CHANNEL_7_INTELFPGA) / CL_CHANNEL_1_INTELFPGA;\n      }\n      mem->bank_id = bank_id;\n    }\n\n    // For device global memory, we will defer the allocation until we know\n    // where it should go\n    if (mem->block_allocation->region == &acl_platform.global_mem) {\n      mem->allocation_deferred = 1;\n      result = mem;\n    } else {\n      // physical_device_id is set to 0 because it's not used\n      // when mem->block_allocation->region != &acl_platform.global_mem\n      if (acl_do_physical_buffer_allocation(0, mem))\n        result = mem;\n      else\n        result = 0;\n    }\n\n    // Out of memory, or fragmentation is too bad.\n    if (result == 0) {\n      cl_int code = mem->block_allocation->region->is_host_accessible\n                        ? CL_OUT_OF_HOST_MEMORY\n                        : CL_OUT_OF_RESOURCES;\n      acl_delete(mem->block_allocation);\n      acl_free_cl_mem(mem);\n      BAIL_INFO(code, context,\n                \"Could not allocate a buffer of the specified size due to \"\n                \"fragmentation or exhaustion\");\n    }\n  }\n\n  result->context = context;\n  result->flags = flags;\n  result->size = size;\n  result->fields.buffer_objs.host_ptr = host_ptr;\n  result->mapping_count = 0;\n\n  result->bank_id = 0;\n  if (is_SOC_device()) {\n    // HPS DDR is system managed for SoC.\n    // Only allocate on device DDR.\n    acl_print_debug_msg(\"Ignoring CHANNEL setting on ARM\\n\");\n  } else {\n    // Using channel flag for setting bank id\n    if ((flags & CL_CHANNEL_7_INTELFPGA) != CL_CHANNEL_AUTO_INTELFPGA) {\n      bank_id =\n          ((cl_uint)flags & CL_CHANNEL_7_INTELFPGA) / CL_CHANNEL_1_INTELFPGA;\n    }\n    result->bank_id = bank_id;\n  }\n\n  acl_retain(result);\n  acl_retain(context);\n\n  // Handling the special case when clCreateBuffer is called with the\n  // 'CL_MEM_COPY_HOST_PTR' flag. What do we have to do in this case? The user\n  // is telling us to make a copy of this pointer NOW (i.e. before returning).\n  // So, we have two options right now:\n  //      1) Defer the copy to the device(s) to a later time by making a local\n  //         copy of the data in host memory.\n  //      2) Pre-emptively write the data to ALL devices in the context.\n  //\n  // (1) is the safer option but, for the trivial case of a single-device\n  // context, it results in a performance hit by doing an extra\n  // host->host memcpy.\n  // (2) could save us the extra memcpy from (1) for the common single-device\n  // context case, but this preemptive broadcast write to all devices could\n  // result in a TON of extra host->device copies in multi-device contexts\n  // and oversubscription of memory!\n  //\n  // A middle ground between (1) and (2) would be to do (2) if there is a\n  // single device in the context, and do (1) if there are multiple devices\n  // in the context (we only know the context of the buffer, we don't know the\n  // specific device, yet).\n  // However, this middle ground could confuse users; if their OpenCL program\n  // only uses one device (e.g. they always use device[0]), then their\n  // performance will change depending on the number of devices in their\n  // context!\n  if (flags & CL_MEM_COPY_HOST_PTR) {\n    if (mem->allocation_deferred) {\n// The case where we will defer the copy to when we know which device\n// the buffer is bound to. Based on the earlier code in this function\n// that sets 'mem->allocation_deferred', we will enter this block\n// whenever the target memory on the device is global memory. We defer\n// to a later time, but we'd better make a copy somewhere. We will\n// make a copy in host memory for now with a blocking memcpy.\n// This is an 'extra' memcpy and can hurt performance.\n#ifdef MEM_DEBUG_MSG\n      printf(\"clCreateBuffer called with CL_MEM_COPY_HOST_PTR, making a \"\n             \"temporary copy before returning!\\n\");\n#endif\n      safe_memcpy(mem->host_mem.aligned_ptr, host_ptr, size, size, size);\n\n      // This flag indicates that we haven't actually performed the copy of\n      // the memory; it is just in the host_mem storage.\n      mem->mem_cpy_host_ptr_pending = 1;\n      mem->writable_copy_on_host = 1;\n\n      // Don't need to skip the check since this\n      // code branch doesn't internally call clEnqueueWriteBuffer.\n      mem->copy_host_ptr_skip_check = CL_FALSE;\n    } else {\n      // The case where we will directly copy the data to device. Based on\n      // the code earlier in this function, which sets\n      // 'mem->allocation_deferred', the only time we will enter this\n      // else-block is if the target memory on the device is NOT global\n      // memory. I think the rational here is device memory that is\n      // host-accessible but NOT global memory will be \"specific\". For us,\n      // maybe some specific on-chip FPGA memory?\n\n      // We will do a direct blocking copy now, so we will do our own error\n      // checking once the copy is done.\n      mem->copy_host_ptr_skip_check = CL_TRUE;\n\n      // Enqueue the buffer write.\n      // Must be blocking since the call has to finish before returning\n      // from this function. We choose 'auto_queue' queue here which is tied\n      // to the default device (device[0], see acl_context.cpp).\n      cl_int status = clEnqueueWriteBuffer(\n          context->auto_queue, result,\n          1, // blocking write\n          0, // offset\n          result->size, result->fields.buffer_objs.host_ptr, 0, 0, // wait list\n          0);\n\n      // Check the status of the write once it returns\n      if (status != CL_SUCCESS) {\n        acl_release(result);\n        acl_release(context);\n        acl_delete(mem->block_allocation);\n        acl_free_cl_mem(mem);\n        // Need an error status valid to return from this function\n        BAIL_INFO(CL_OUT_OF_RESOURCES, context,\n                  \"Could not copy data into the allocated buffer\");\n      }\n    }\n  }\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_MEM_OBJECT, result);\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"CreateBuffer Finished:  %zx\\n\", (size_t)result);\n#endif\n\n  return result;\n}\n\n// Create a buffer.\n// In some cases, we copy memory from host to global memory.\n//\n// If the user specified USE_HOST_MEMORY, then when a kernel is invoked\n// with that mem object, we will have to schedule two copy operations:\n// Before the kernel starts, copy the buffer from host to global mem;\n// after the kernel ends, copy the buffer from global mem to host mem.\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateBufferIntelFPGA(cl_context context,\n                                                        cl_mem_flags flags,\n                                                        size_t size,\n                                                        void *host_ptr,\n                                                        cl_int *errcode_ret) {\n  return clCreateBufferWithPropertiesINTEL(context, NULL, flags, size, host_ptr,\n                                           errcode_ret);\n}\n\n// Create a buffer with additional properties\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateBufferWithProperties(\n    cl_context context, const cl_mem_properties *properties, cl_mem_flags flags,\n    size_t size, void *host_ptr, cl_int *errcode_ret) {\n  return clCreateBufferWithPropertiesINTEL(context, properties, flags, size,\n                                           host_ptr, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateBuffer(cl_context context,\n                                               cl_mem_flags flags, size_t size,\n                                               void *host_ptr,\n                                               cl_int *errcode_ret) {\n  return clCreateBufferIntelFPGA(context, flags, size, host_ptr, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateSubBufferIntelFPGA(\n    cl_mem buffer, cl_mem_flags flags, cl_buffer_create_type buffer_create_type,\n    const void *buffer_create_info, cl_int *errcode_ret) {\n  cl_mem result = 0;\n  cl_bool aligns_with_any_device = CL_FALSE;\n  cl_mem_flags sub_flags;\n  unsigned int idevice;\n  cl_context context;\n\n  cl_mem mem;\n  int num_rw_specs = 0;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"CreateSubBuffer\");\n#endif\n\n  if (!acl_mem_is_valid(buffer)) {\n    BAIL(CL_INVALID_MEM_OBJECT);\n  }\n  if (buffer->mem_object_type != CL_MEM_OBJECT_BUFFER ||\n      buffer->fields.buffer_objs.is_subbuffer) {\n    BAIL(CL_INVALID_MEM_OBJECT);\n  }\n  context = buffer->context;\n\n  // Check for invalid enum bits\n  // CL_MEM_USE_HOST_PTR, CL_MEM_ALLOC_HOST_PTR and CL_MEM_COPY_HOST_PTR are\n  // not valid flags for sub-buffers so are removed from this list\n  if (flags &\n      ~(CL_MEM_READ_WRITE | CL_MEM_READ_ONLY | CL_MEM_WRITE_ONLY |\n        CL_MEM_HOST_WRITE_ONLY | CL_MEM_HOST_READ_ONLY | CL_MEM_HOST_NO_ACCESS |\n        CL_CHANNEL_7_INTELFPGA | CL_MEM_HETEROGENEOUS_INTELFPGA)) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid or unsupported flags\");\n  }\n\n  // Check for exactly one read/write spec\n  if (flags & CL_MEM_READ_WRITE)\n    num_rw_specs++;\n  if (flags & CL_MEM_READ_ONLY)\n    num_rw_specs++;\n  if (flags & CL_MEM_WRITE_ONLY)\n    num_rw_specs++;\n  if (num_rw_specs > 1) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"More than one read/write flag is specified\");\n  }\n\n  // Check for exactly one host read/write/no_access spec\n  num_rw_specs = 0;\n  if (flags & CL_MEM_HOST_READ_ONLY)\n    num_rw_specs++;\n  if (flags & CL_MEM_HOST_WRITE_ONLY)\n    num_rw_specs++;\n  if (flags & CL_MEM_HOST_NO_ACCESS)\n    num_rw_specs++;\n  if (num_rw_specs > 1) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"More than one host read/write/no_access flags are specified\");\n  }\n\n  // If the parent buffer is write only then the sub-buffer cannot read.\n  // If the parent buffer is read only then the sub-buffer cannot write.\n  if (((buffer->flags & CL_MEM_WRITE_ONLY) &&\n       ((flags & CL_MEM_READ_WRITE) || (flags & CL_MEM_READ_ONLY))) ||\n      ((buffer->flags & CL_MEM_READ_ONLY) &&\n       ((flags & CL_MEM_READ_WRITE) || (flags & CL_MEM_WRITE_ONLY))) ||\n      ((buffer->flags & CL_MEM_HOST_WRITE_ONLY) &&\n       (flags & CL_MEM_HOST_READ_ONLY)) ||\n      ((buffer->flags & CL_MEM_HOST_READ_ONLY) &&\n       (flags & CL_MEM_HOST_WRITE_ONLY)) ||\n      ((buffer->flags & CL_MEM_HOST_NO_ACCESS) &&\n       ((flags & CL_MEM_HOST_READ_ONLY) || (flags & CL_MEM_HOST_WRITE_ONLY)))) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"Read/write flags are incompatible with the parent buffer\");\n  }\n\n  if (buffer_create_type != CL_BUFFER_CREATE_TYPE_REGION) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid buffer_create_type value\");\n  }\n\n  if (buffer_create_info == NULL) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Empty buffer_create_info\");\n  }\n\n  if (((cl_buffer_region *)buffer_create_info)->origin +\n          ((cl_buffer_region *)buffer_create_info)->size >\n      buffer->size) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"Origin plus size is out of bounds of parent buffer\");\n  }\n\n  if (((cl_buffer_region *)buffer_create_info)->size == 0) {\n    BAIL_INFO(CL_INVALID_BUFFER_SIZE, context, \"Sub-buffer size is zero\");\n  }\n\n  for (idevice = 0; idevice < context->num_devices; ++idevice) {\n    int device_mem_base_addr_align = 0;\n    int status_code;\n\n    status_code = clGetDeviceInfoIntelFPGA(\n        context->device[idevice], CL_DEVICE_MEM_BASE_ADDR_ALIGN, sizeof(int),\n        &device_mem_base_addr_align, NULL);\n    if (status_code != CL_SUCCESS) {\n      BAIL(CL_OUT_OF_HOST_MEMORY);\n    }\n\n    if (!((((cl_buffer_region *)buffer_create_info)->origin * 8) &\n          (device_mem_base_addr_align - 1))) {\n      aligns_with_any_device = CL_TRUE;\n      break;\n    }\n  }\n\n  if (!aligns_with_any_device) {\n    BAIL_INFO(CL_MISALIGNED_SUB_BUFFER_OFFSET, context,\n              \"Sub-buffer offset does not align with any device in context\");\n  }\n\n  acl_block_allocation_t *new_block = acl_new<acl_block_allocation_t>();\n  if (!new_block) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a cl_mem object\");\n  }\n  // Now actually allocate the mem object.\n  mem = acl_alloc_cl_mem();\n  if (!mem) {\n    acl_delete(new_block);\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a cl_mem object\");\n  }\n\n  mem->block_allocation = new_block;\n  mem->block_allocation->mem_obj = mem;\n\n  acl_reset_ref_count(mem);\n\n  sub_flags = flags;\n\n  // If read/write flags are not specified, they are inherited from the parent\n  // buffer\n  if (!(flags & (CL_MEM_READ_WRITE | CL_MEM_READ_ONLY | CL_MEM_WRITE_ONLY))) {\n    sub_flags |= (buffer->flags &\n                  (CL_MEM_READ_WRITE | CL_MEM_READ_ONLY | CL_MEM_WRITE_ONLY));\n  }\n  // If host read/write flags are not specified, they are inherited from the\n  // parent buffer\n  if (!(flags & (CL_MEM_HOST_WRITE_ONLY | CL_MEM_HOST_READ_ONLY |\n                 CL_MEM_HOST_NO_ACCESS))) {\n    sub_flags |=\n        (buffer->flags & (CL_MEM_HOST_WRITE_ONLY | CL_MEM_HOST_READ_ONLY |\n                          CL_MEM_HOST_NO_ACCESS));\n  }\n\n  sub_flags |= (buffer->flags & (CL_MEM_USE_HOST_PTR | CL_MEM_ALLOC_HOST_PTR |\n                                 CL_MEM_COPY_HOST_PTR));\n\n  mem->mem_object_type = CL_MEM_OBJECT_BUFFER;\n  mem->dispatch = &acl_icd_dispatch;\n  mem->allocation_deferred = 0;\n\n  // If we have sub buffers, we need to be able to mirror the data to the host\n  if (!buffer->host_mem.aligned_ptr) {\n    // Initialize the host_mem entries for the parent buffer\n    if (buffer->block_allocation->region\n            ->is_user_provided) { // CL_MEM_USE_HOST_PTR\n      // When a buffer is created using CL_MEM_USE_HOST_PTR, there is no need to\n      // allocate additional memory on the host for this buffer. Simply\n      // initialize host_ptr_mem to point into the inputted host memory region.\n      acl_aligned_ptr_t host_ptr_mem;\n      host_ptr_mem.aligned_ptr = buffer->block_allocation->range.begin;\n      host_ptr_mem.raw = buffer->block_allocation->range.begin;\n      host_ptr_mem.alignment = 0;\n      host_ptr_mem.size = ((cl_buffer_region *)buffer_create_info)->size;\n      buffer->host_mem = host_ptr_mem;\n    } else {\n      buffer->host_mem = acl_mem_aligned_malloc(buffer->size);\n      if (!buffer->host_mem.raw) {\n        acl_free_cl_mem(mem);\n        BAIL_INFO(\n            CL_OUT_OF_HOST_MEMORY, context,\n            \"Could not allocate backing store for a device buffer with sub \"\n            \"buffers\");\n      }\n    }\n  }\n  // Uses the same host location as the main buffer, but offsets\n  // into it\n  mem->host_mem.aligned_ptr = (char *)buffer->host_mem.aligned_ptr +\n                              ((cl_buffer_region *)buffer_create_info)->origin;\n\n  if (buffer->host_mem.device_addr != 0) {\n    mem->host_mem.device_addr =\n        buffer->host_mem.device_addr +\n        ((cl_buffer_region *)buffer_create_info)->origin;\n  } else {\n    mem->host_mem.device_addr = 0;\n  }\n  // Not used for sub buffers\n  mem->host_mem.raw = NULL;\n  mem->host_mem.size = 0;\n  mem->host_mem.alignment = 0;\n\n  mem->mem_cpy_host_ptr_pending = 0;\n  mem->destructor_callback_list = NULL;\n  mem->fields.buffer_objs.is_subbuffer = CL_TRUE;\n  mem->fields.buffer_objs.parent = buffer;\n  mem->fields.buffer_objs.num_subbuffers = 0;\n  // Insert at the head of the list of sub buffers for the parent object\n  mem->fields.buffer_objs.next_sub = buffer->fields.buffer_objs.next_sub;\n  buffer->fields.buffer_objs.next_sub = mem;\n  if (mem->fields.buffer_objs.next_sub != NULL) {\n    mem->fields.buffer_objs.next_sub->fields.buffer_objs.prev_sub = mem;\n  }\n  mem->fields.buffer_objs.prev_sub = buffer;\n  ++buffer->fields.buffer_objs.num_subbuffers;\n\n  mem->fields.buffer_objs.sub_origin =\n      ((cl_buffer_region *)buffer_create_info)->origin;\n  mem->size = ((cl_buffer_region *)buffer_create_info)->size;\n\n  mem->block_allocation->region = buffer->block_allocation->region;\n  // Always start on the host.\n  // If parent buffer or overlapping subbuffer data is on a device,\n  // it will be copied back to the host before the sub buffer is used.\n  mem->writable_copy_on_host = 1;\n  mem->auto_mapped = 1;\n\n  mem->is_svm = buffer->is_svm;\n\n  if (mem->block_allocation->region->is_user_provided) { // CL_MEM_USE_HOST_PTR\n    // We'll set the block allocation to point into the block allocation of the\n    // parent buffer. We won't put it in the linked list. This is probably\n    // \"bad\", but you can't release the parent buffer before the child buffer so\n    // it shouldn't be possible for anyone else to attempt to allocate the same\n    // region, and when memory is released in\n    // acl_forcibly_release_all_memory_for_context, the release of the parent\n    // buffer should release this memory as well.\n\n    mem->block_allocation->range.begin = mem->host_mem.aligned_ptr;\n    mem->block_allocation->range.next =\n        (void *)(((char *)mem->host_mem.aligned_ptr) + mem->size);\n    // Don't put it in the allocation list\n    mem->block_allocation->next_block_in_region = NULL;\n    mem->block_allocation->region->first_block = NULL;\n\n    result = mem;\n  } else if (mem->block_allocation->region->uses_host_system_malloc ||\n             mem->is_svm) {\n    mem->block_allocation->range.begin = mem->host_mem.aligned_ptr;\n    mem->block_allocation->range.next =\n        (void *)(((char *)mem->host_mem.aligned_ptr) + mem->size);\n\n    // Put it on the allocation list\n    mem->block_allocation->next_block_in_region =\n        mem->block_allocation->region->first_block;\n    mem->block_allocation->region->first_block = mem->block_allocation;\n\n    result = mem;\n  } else {\n    // Perform our own first-fit allocation algorithm in the region.\n    // Assign to result only if we succeeded.\n\n    mem->context = context;\n    mem->flags = sub_flags;\n    mem->mem_id = buffer->mem_id;\n\n    if (is_SOC_device()) {\n      // HPS DDR is system managed for SoC.\n      // Only allocate on device DDR.\n      acl_print_debug_msg(\"Ignoring CHANNEL setting on ARM\\n\");\n    } else {\n      // Ensure that if a bank id is specified, it is the same as the parent\n      // buffer\n      if ((sub_flags & CL_CHANNEL_7_INTELFPGA) != CL_CHANNEL_AUTO_INTELFPGA) {\n        cl_uint sub_bank_id = ((cl_uint)sub_flags & CL_CHANNEL_7_INTELFPGA) /\n                              CL_CHANNEL_1_INTELFPGA;\n        if (sub_bank_id != buffer->bank_id) {\n          BAIL_INFO(CL_INVALID_VALUE, context,\n                    \"Sub-buffer bank id does not match parent buffer bank id\");\n        }\n      }\n    }\n    mem->bank_id = buffer->bank_id;\n\n    // For device global memory, we will defer the allocation until we know\n    // where it should go\n    if (mem->block_allocation->region == &acl_platform.global_mem) {\n      mem->allocation_deferred = 1;\n      result = mem;\n    } else {\n      // physical_device_id is set to 0 because it's not used\n      // when mem->block_allocation->region != &acl_platform.global_mem\n      if (acl_do_physical_buffer_allocation(0, mem))\n        result = mem;\n      else\n        result = 0;\n    }\n\n    // Out of memory, or fragmentation is too bad.\n    // It's ok to bail here because we haven't damaged the .link on the\n    // candidate cl_mem object, nor updated the free list link.\n    if (result == 0) {\n      cl_int code = mem->block_allocation->region->is_host_accessible\n                        ? CL_OUT_OF_HOST_MEMORY\n                        : CL_OUT_OF_RESOURCES;\n      acl_free_cl_mem(mem);\n      BAIL_INFO(code, context,\n                \"Could not allocate a buffer of the specified size due to \"\n                \"fragmentation or exhaustion\");\n    }\n  }\n\n  result->context = context;\n  result->flags = sub_flags;\n  result->fields.buffer_objs.host_ptr =\n      (char *)buffer->fields.buffer_objs.host_ptr +\n      ((cl_buffer_region *)buffer_create_info)->origin;\n  result->mapping_count = 0;\n\n  if (is_SOC_device()) {\n    // HPS DDR is system managed for SoC.\n    // Only allocate on device DDR.\n    acl_print_debug_msg(\"Ignoring CHANNEL setting on ARM\\n\");\n  } else {\n    // Ensure that if a bank id is specified, it is the same as the parent\n    // buffer\n    if ((sub_flags & CL_CHANNEL_7_INTELFPGA) != CL_CHANNEL_AUTO_INTELFPGA) {\n      cl_uint sub_bank_id = ((cl_uint)sub_flags & CL_CHANNEL_7_INTELFPGA) /\n                            CL_CHANNEL_1_INTELFPGA;\n      if (sub_bank_id != buffer->bank_id) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"Sub-buffer bank id does not match parent buffer bank id\");\n      }\n    }\n  }\n  result->bank_id = buffer->bank_id;\n\n  acl_retain(result);\n  acl_retain(result->fields.buffer_objs.parent);\n  acl_retain(result->context);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_MEM_OBJECT, result);\n\n#ifdef MEM_DEBUG_MSG\n  printf(\" %zx\\n\", (size_t)result);\n#endif\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateSubBuffer(\n    cl_mem buffer, cl_mem_flags flags, cl_buffer_create_type buffer_create_type,\n    const void *buffer_create_info, cl_int *errcode_ret) {\n  return clCreateSubBufferIntelFPGA(buffer, flags, buffer_create_type,\n                                    buffer_create_info, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetMemObjectInfoIntelFPGA(\n    cl_mem mem, cl_mem_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  cl_context context;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  RESULT_INIT;\n\n  if (!acl_mem_is_valid(mem)) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  context = mem->context;\n\n  switch (param_name) {\n  case CL_MEM_ALLOC_BUFFER_LOCATION_INTEL:\n    RESULT_UINT(mem->mem_id);\n    break;\n  case CL_MEM_TYPE:\n    RESULT_ENUM(mem->mem_object_type);\n    break;\n  case CL_MEM_FLAGS:\n    RESULT_BITFIELD(mem->flags);\n    break;\n  case CL_MEM_SIZE:\n    RESULT_SIZE_T(mem->size);\n    break; // ignores alignment adjustment\n  case CL_MEM_HOST_PTR:\n    if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER)\n      RESULT_PTR(mem->fields.buffer_objs.host_ptr);\n    if (is_image(mem))\n      RESULT_PTR(mem->fields.image_objs.host_ptr);\n    if (mem->mem_object_type == CL_MEM_OBJECT_PIPE)\n      RESULT_PTR(NULL);\n    break;\n  case CL_MEM_MAP_COUNT:\n    RESULT_UINT(mem->mapping_count);\n    break;\n  case CL_MEM_REFERENCE_COUNT:\n    if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER) {\n      RESULT_UINT(acl_ref_count(mem) -\n                  (cl_uint)mem->fields.buffer_objs.num_subbuffers);\n    } else {\n      RESULT_UINT(acl_ref_count(mem));\n    }\n    break;\n  case CL_MEM_CONTEXT:\n    RESULT_PTR(mem->context);\n    break;\n  case CL_MEM_USES_SVM_POINTER:\n    if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER) {\n      RESULT_BOOL(\n          (cl_bool)(mem->fields.buffer_objs.host_ptr != NULL &&\n                    acl_ptr_is_contained_in_context_svm(\n                        mem->context, mem->fields.buffer_objs.host_ptr)));\n    } else if (is_image(mem)) {\n      RESULT_BOOL(\n          (cl_bool)(mem->fields.image_objs.host_ptr != NULL &&\n                    acl_ptr_is_contained_in_context_svm(\n                        mem->context, mem->fields.image_objs.host_ptr)));\n    } else {\n      RESULT_BOOL(CL_FALSE);\n    }\n    break;\n  case CL_MEM_ASSOCIATED_MEMOBJECT:\n    if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER &&\n        mem->fields.buffer_objs.is_subbuffer) {\n      RESULT_PTR(mem->fields.buffer_objs.parent);\n      break;\n    } else if (is_image(mem)) {\n      RESULT_PTR(mem->fields.image_objs.image_desc->mem_object);\n      break;\n    } else {\n      RESULT_PTR(NULL);\n      break;\n    }\n  case CL_MEM_OFFSET:\n    if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER &&\n        mem->fields.buffer_objs.is_subbuffer) {\n      RESULT_SIZE_T(mem->fields.buffer_objs.sub_origin);\n      break;\n    } else {\n      RESULT_SIZE_T(0);\n    }\n  default:\n    break;\n  }\n\n  if (result.size == 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid or unsupported memory object query\");\n  }\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclGetMemObjectInfo(cl_mem mem, cl_mem_info param_name, size_t param_value_size,\n                   void *param_value, size_t *param_value_size_ret) {\n  return clGetMemObjectInfoIntelFPGA(mem, param_name, param_value_size,\n                                     param_value, param_value_size_ret);\n}\n\nACL_EXPORT CL_API_ENTRY cl_mem CL_API_CALL clCreateImageIntelFPGA(\n    cl_context context, cl_mem_flags flags, const cl_image_format *image_format,\n    const cl_image_desc *image_desc, void *host_ptr, cl_int *errcode_ret) {\n  size_t image_size;\n  size_t element_size;\n  cl_mem return_buffer;\n  cl_int local_errcode_ret = CL_SUCCESS;\n  size_t max_2d_image_width = 0;\n  size_t max_3d_image_width = 0;\n  size_t max_2d_image_height = 0;\n  size_t max_3d_image_height = 0;\n  size_t max_3d_image_depth = 0;\n  cl_uint num_image_formats;\n  cl_image_format *supported_image_formats;\n  unsigned iformat;\n  cl_bool found_image_format;\n  unsigned int idevice;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    BAIL(CL_INVALID_CONTEXT);\n  }\n\n  // Check the maximum image sizes for all available devices in the context\n  // The image cannot be larger than the smallest image size within this context\n  for (idevice = 0; idevice < context->num_devices; ++idevice) {\n    size_t size;\n    clGetDeviceInfo(context->device[idevice], CL_DEVICE_IMAGE2D_MAX_WIDTH,\n                    sizeof(size_t), &size, NULL);\n    if (size > max_2d_image_width)\n      max_2d_image_width = size;\n    clGetDeviceInfo(context->device[idevice], CL_DEVICE_IMAGE3D_MAX_WIDTH,\n                    sizeof(size_t), &size, NULL);\n    if (size > max_3d_image_width)\n      max_3d_image_width = size;\n    clGetDeviceInfo(context->device[idevice], CL_DEVICE_IMAGE2D_MAX_HEIGHT,\n                    sizeof(size_t), &size, NULL);\n    if (size > max_2d_image_height)\n      max_2d_image_height = size;\n    clGetDeviceInfo(context->device[idevice], CL_DEVICE_IMAGE3D_MAX_HEIGHT,\n                    sizeof(size_t), &size, NULL);\n    if (size > max_3d_image_height)\n      max_3d_image_height = size;\n    clGetDeviceInfo(context->device[idevice], CL_DEVICE_IMAGE3D_MAX_DEPTH,\n                    sizeof(size_t), &size, NULL);\n    if (size > max_3d_image_depth)\n      max_3d_image_depth = size;\n  }\n\n  if (image_format == NULL) {\n    BAIL_INFO(CL_INVALID_IMAGE_FORMAT_DESCRIPTOR, context,\n              \"image_format is NULL\");\n  }\n\n  element_size =\n      acl_get_image_element_size(context, image_format, &local_errcode_ret);\n  if (local_errcode_ret != CL_SUCCESS) {\n    BAIL(local_errcode_ret);\n  }\n\n  if (image_desc == NULL) {\n    BAIL_INFO(CL_INVALID_IMAGE_DESCRIPTOR, context, \"image_desc is NULL\");\n  }\n\n  local_errcode_ret = clGetSupportedImageFormats(\n      context, flags, image_desc->image_type, 0, NULL, &num_image_formats);\n  if (local_errcode_ret != CL_SUCCESS) {\n    BAIL(local_errcode_ret);\n  }\n  supported_image_formats = (cl_image_format *)acl_malloc(\n      sizeof(cl_image_format) * num_image_formats);\n  local_errcode_ret = clGetSupportedImageFormats(\n      context, flags, image_desc->image_type, num_image_formats,\n      supported_image_formats, NULL);\n  found_image_format = CL_FALSE;\n  if (local_errcode_ret == CL_SUCCESS) {\n    for (iformat = 0; iformat < num_image_formats; ++iformat) {\n      if (supported_image_formats[iformat].image_channel_order ==\n              image_format->image_channel_order &&\n          supported_image_formats[iformat].image_channel_data_type ==\n              image_format->image_channel_data_type) {\n        found_image_format = CL_TRUE;\n        break;\n      }\n    }\n  }\n  acl_free(supported_image_formats);\n\n  if (local_errcode_ret != CL_SUCCESS) {\n    BAIL(local_errcode_ret);\n  }\n  if (!found_image_format) {\n    BAIL_INFO(CL_IMAGE_FORMAT_NOT_SUPPORTED, context,\n              \"Unsupported image format\");\n  }\n\n  // Allocate the memory for the image. This size (and sometimes the method)\n  // is different for each image type.\n  switch (image_desc->image_type) {\n  case CL_MEM_OBJECT_IMAGE1D:\n    image_size =\n        element_size * image_desc->image_width +\n        get_offset_for_image_param(context, image_desc->image_type, \"data\");\n    if (image_desc->image_width <= 0) {\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image width cannot be zero for a 1D object\");\n    }\n    if (image_size > context->max_mem_alloc_size) {\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image size exceeds maximum alloc size\");\n    }\n    return_buffer =\n        clCreateBuffer(context, flags, image_size, host_ptr, errcode_ret);\n    if (return_buffer == NULL)\n      return NULL;\n    return_buffer->fields.image_objs.image_format =\n        (cl_image_format *)acl_malloc(sizeof(cl_image_format));\n    return_buffer->fields.image_objs.image_desc =\n        (cl_image_desc *)acl_malloc(sizeof(cl_image_desc));\n    break;\n  case CL_MEM_OBJECT_IMAGE1D_BUFFER:\n    // Need to actually allocate/assign the buffer data here\n    BAIL_INFO(CL_IMAGE_FORMAT_NOT_SUPPORTED, context,\n              \"Do not support images created from buffers\");\n    // Need to actually allocate/assign the buffer data here\n    break;\n  case CL_MEM_OBJECT_IMAGE1D_ARRAY:\n    image_size =\n        element_size * image_desc->image_width * image_desc->image_array_size +\n        get_offset_for_image_param(context, image_desc->image_type, \"data\");\n    if (image_desc->image_width <= 0) {\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image width cannot be zero for a 1D object\");\n    }\n    if (image_size > context->max_mem_alloc_size) {\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image size exceeds maximum alloc size\");\n    }\n    return_buffer =\n        clCreateBuffer(context, flags, image_size, host_ptr, errcode_ret);\n    if (return_buffer == NULL)\n      return NULL;\n    return_buffer->fields.image_objs.image_format =\n        (cl_image_format *)acl_malloc(sizeof(cl_image_format));\n    return_buffer->fields.image_objs.image_desc =\n        (cl_image_desc *)acl_malloc(sizeof(cl_image_desc));\n    break;\n  case CL_MEM_OBJECT_IMAGE2D:\n    // If we change this, need to actually allocate/assign the buffer data here\n    if (image_desc->mem_object != NULL &&\n        image_desc->mem_object->mem_object_type == CL_MEM_OBJECT_BUFFER) {\n      BAIL_INFO(CL_IMAGE_FORMAT_NOT_SUPPORTED, context,\n                \"Do not support images created from buffers\");\n      // Copy information from the other image object\n    } else if (image_desc->mem_object != NULL &&\n               image_desc->mem_object->mem_object_type ==\n                   CL_MEM_OBJECT_BUFFER) {\n      BAIL_INFO(CL_IMAGE_FORMAT_NOT_SUPPORTED, context,\n                \"Do not support images created from other images\");\n      // Allocate a new image object\n    } else {\n      image_size =\n          element_size * image_desc->image_width * image_desc->image_height +\n          get_offset_for_image_param(context, image_desc->image_type, \"data\");\n      if (image_desc->image_width <= 0) {\n        BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                  \"image width cannot be zero for a 2D object\");\n      }\n      if (image_desc->image_width > max_2d_image_width) {\n        BAIL_INFO(\n            CL_INVALID_IMAGE_SIZE, context,\n            \"image width exceeds maximum width for all devices in context\");\n      }\n      if (image_desc->image_height <= 0) {\n        BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                  \"image height cannot be zero for a 2D object\");\n      }\n      if (image_desc->image_height > max_2d_image_height) {\n        BAIL_INFO(\n            CL_INVALID_IMAGE_SIZE, context,\n            \"1 image height exceeds maximum height for all devices in context\");\n      }\n      if (image_size > context->max_mem_alloc_size) {\n        BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                  \"image size exceeds maximum alloc size\");\n      }\n      return_buffer =\n          clCreateBuffer(context, flags, image_size, host_ptr, errcode_ret);\n      if (return_buffer == NULL)\n        return NULL;\n      return_buffer->fields.image_objs.image_format =\n          (cl_image_format *)acl_malloc(sizeof(cl_image_format));\n      return_buffer->fields.image_objs.image_desc =\n          (cl_image_desc *)acl_malloc(sizeof(cl_image_desc));\n    }\n    break;\n  case CL_MEM_OBJECT_IMAGE2D_ARRAY:\n    image_size =\n        element_size * image_desc->image_width * image_desc->image_height *\n            image_desc->image_array_size +\n        get_offset_for_image_param(context, image_desc->image_type, \"data\");\n    if (image_desc->image_width <= 0)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image width cannot be zero for a 2D object\");\n    if (image_desc->image_width > max_2d_image_width)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image width exceeds maximum width for all devices in context\");\n    if (image_desc->image_height <= 0)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image height cannot be zero for a 2D object\");\n    if (image_desc->image_height > max_2d_image_height)\n      BAIL_INFO(\n          CL_INVALID_IMAGE_SIZE, context,\n          \"2 image height exceeds maximum height for all devices in context\");\n    if (image_size > context->max_mem_alloc_size)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image size exceeds maximum alloc size\");\n    return_buffer =\n        clCreateBuffer(context, flags, image_size, host_ptr, errcode_ret);\n    if (return_buffer == NULL)\n      return NULL;\n    return_buffer->fields.image_objs.image_format =\n        (cl_image_format *)acl_malloc(sizeof(cl_image_format));\n    return_buffer->fields.image_objs.image_desc =\n        (cl_image_desc *)acl_malloc(sizeof(cl_image_desc));\n    break;\n  case CL_MEM_OBJECT_IMAGE3D:\n    image_size =\n        element_size * image_desc->image_width * image_desc->image_height *\n            image_desc->image_depth +\n        get_offset_for_image_param(context, image_desc->image_type, \"data\");\n    if (image_desc->image_width <= 0)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image width cannot be zero for a 3D object\");\n    if (image_desc->image_width > max_3d_image_width)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image width exceeds maximum width for all devices in context\");\n    if (image_desc->image_height <= 0)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image height cannot be zero for a 3D object\");\n    if (image_desc->image_height > max_3d_image_height)\n      BAIL_INFO(\n          CL_INVALID_IMAGE_SIZE, context,\n          \"image height exceeds maximum height for all devices in context\");\n    if (image_desc->image_depth <= 0)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image depth cannot be zero for a 3D object\");\n    if (image_desc->image_depth > max_3d_image_depth)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image depth exceeds maximum depth for all devices in context\");\n    if (image_size > context->max_mem_alloc_size)\n      BAIL_INFO(CL_INVALID_IMAGE_SIZE, context,\n                \"image size exceeds maximum alloc size\");\n    return_buffer =\n        clCreateBuffer(context, flags, image_size, host_ptr, errcode_ret);\n    if (return_buffer == NULL)\n      return NULL;\n    return_buffer->fields.image_objs.image_format =\n        (cl_image_format *)acl_malloc(sizeof(cl_image_format));\n    return_buffer->fields.image_objs.image_desc =\n        (cl_image_desc *)acl_malloc(sizeof(cl_image_desc));\n    break;\n  default:\n    BAIL_INFO(CL_INVALID_IMAGE_DESCRIPTOR, context, \"invalid image type\");\n    break;\n  }\n\n  // Set up image specific fields in cl_mem object\n  return_buffer->mem_object_type = image_desc->image_type;\n  return_buffer->fields.image_objs.host_ptr = host_ptr;\n  safe_memcpy(return_buffer->fields.image_objs.image_format, image_format,\n              sizeof(cl_image_format), sizeof(cl_image_format),\n              sizeof(cl_image_format));\n  safe_memcpy(return_buffer->fields.image_objs.image_desc, image_desc,\n              sizeof(cl_image_desc), sizeof(cl_image_desc),\n              sizeof(cl_image_desc));\n\n  // Images always need a backing store for map/unmap\n  if (!return_buffer->host_mem.aligned_ptr) {\n    return_buffer->host_mem = acl_mem_aligned_malloc(image_size);\n    if (return_buffer->host_mem.raw == 0) {\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not allocate backing store for a device image\");\n    }\n  }\n\n  // If the memory was allocated, copy over the image meta data (width, height,\n  // etc)\n  if (!return_buffer->allocation_deferred &&\n      !(return_buffer->block_allocation->region ==\n        &(acl_platform.host_user_mem))) {\n    copy_image_metadata(return_buffer);\n  }\n  return return_buffer;\n}\n\nACL_EXPORT CL_API_ENTRY cl_mem CL_API_CALL clCreateImage(\n    cl_context context, cl_mem_flags flags, const cl_image_format *image_format,\n    const cl_image_desc *image_desc, void *host_ptr, cl_int *errcode_ret) {\n  return clCreateImageIntelFPGA(context, flags, image_format, image_desc,\n                                host_ptr, errcode_ret);\n}\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4100)\n#endif\n// Image non-support\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateImage2DIntelFPGA(\n    cl_context context, cl_mem_flags flags, const cl_image_format *image_format,\n    size_t image_width, size_t image_height, size_t image_row_pitch,\n    void *host_ptr, cl_int *errcode_ret) {\n  cl_image_desc image_desc;\n  image_desc.image_type = CL_MEM_OBJECT_IMAGE2D;\n  image_desc.image_width = image_width;\n  image_desc.image_height = image_height;\n  image_desc.image_depth = 1;\n  image_desc.image_array_size = 1;\n  image_desc.image_row_pitch = image_row_pitch;\n  image_desc.image_slice_pitch = 1;\n  image_desc.num_mip_levels = 1;\n  image_desc.num_samples = 1;\n  image_desc.buffer = NULL;\n\n  return clCreateImageIntelFPGA(context, flags, image_format, &image_desc,\n                                host_ptr, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateImage2D(\n    cl_context context, cl_mem_flags flags, const cl_image_format *image_format,\n    size_t image_width, size_t image_height, size_t image_row_pitch,\n    void *host_ptr, cl_int *errcode_ret) {\n  return clCreateImage2DIntelFPGA(context, flags, image_format, image_width,\n                                  image_height, image_row_pitch, host_ptr,\n                                  errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreateImage3DIntelFPGA(\n    cl_context context, cl_mem_flags flags, const cl_image_format *image_format,\n    size_t image_width, size_t image_height, size_t image_depth,\n    size_t image_row_pitch, size_t image_slice_pitch, void *host_ptr,\n    cl_int *errcode_ret) {\n  cl_image_desc image_desc;\n  image_desc.image_type = CL_MEM_OBJECT_IMAGE3D;\n  image_desc.image_width = image_width;\n  image_desc.image_height = image_height;\n  image_desc.image_depth = image_depth;\n  image_desc.image_array_size = 1;\n  image_desc.image_row_pitch = image_row_pitch;\n  image_desc.image_slice_pitch = image_slice_pitch;\n  image_desc.num_mip_levels = 1;\n  image_desc.num_samples = 1;\n  image_desc.buffer = NULL;\n\n  return clCreateImageIntelFPGA(context, flags, image_format, &image_desc,\n                                host_ptr, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL\nclCreateImage3D(cl_context context, cl_mem_flags flags,\n                const cl_image_format *image_format, size_t image_width,\n                size_t image_height, size_t image_depth, size_t image_row_pitch,\n                size_t image_slice_pitch, void *host_ptr, cl_int *errcode_ret) {\n  return clCreateImage3DIntelFPGA(context, flags, image_format, image_width,\n                                  image_height, image_depth, image_row_pitch,\n                                  image_slice_pitch, host_ptr, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetSupportedImageFormatsIntelFPGA(\n    cl_context context, cl_mem_flags flags, cl_mem_object_type image_type,\n    cl_uint num_entries, cl_image_format *image_formats,\n    cl_uint *num_image_formats) {\n#define NUM_SUPPORTED_IMAGE_FORMATS 34\n  cl_image_format supported_image_formats[NUM_SUPPORTED_IMAGE_FORMATS] = {\n      {CL_R, CL_UNORM_INT8},\n      {CL_R, CL_UNORM_INT16},\n      {CL_R, CL_SNORM_INT8},\n      {CL_R, CL_SNORM_INT16},\n      {CL_R, CL_SIGNED_INT8},\n      {CL_R, CL_SIGNED_INT16},\n      {CL_R, CL_SIGNED_INT32},\n      {CL_R, CL_UNSIGNED_INT8},\n      {CL_R, CL_UNSIGNED_INT16},\n      {CL_R, CL_UNSIGNED_INT32},\n      {CL_R, CL_FLOAT},\n      {CL_RG, CL_UNORM_INT8},\n      {CL_RG, CL_UNORM_INT16},\n      {CL_RG, CL_SNORM_INT8},\n      {CL_RG, CL_SNORM_INT16},\n      {CL_RG, CL_SIGNED_INT8},\n      {CL_RG, CL_SIGNED_INT16},\n      {CL_RG, CL_SIGNED_INT32},\n      {CL_RG, CL_UNSIGNED_INT8},\n      {CL_RG, CL_UNSIGNED_INT16},\n      {CL_RG, CL_UNSIGNED_INT32},\n      {CL_RG, CL_FLOAT},\n      {CL_RGBA, CL_UNORM_INT8},\n      {CL_RGBA, CL_UNORM_INT16},\n      {CL_RGBA, CL_SNORM_INT8},\n      {CL_RGBA, CL_SNORM_INT16},\n      {CL_RGBA, CL_SIGNED_INT8},\n      {CL_RGBA, CL_SIGNED_INT16},\n      {CL_RGBA, CL_SIGNED_INT32},\n      {CL_RGBA, CL_UNSIGNED_INT8},\n      {CL_RGBA, CL_UNSIGNED_INT16},\n      {CL_RGBA, CL_UNSIGNED_INT32},\n      {CL_RGBA, CL_FLOAT},\n      {CL_BGRA, CL_UNORM_INT8},\n  };\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n  if (num_entries == 0 && image_formats) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"num_entries is zero but image formats array is specified\");\n  }\n  if (num_entries > 0 && image_formats == 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"num_entries is non-zero but image_formats array is NULL\");\n  }\n  switch (image_type) {\n  case CL_MEM_OBJECT_IMAGE2D:\n  case CL_MEM_OBJECT_IMAGE3D:\n  case CL_MEM_OBJECT_IMAGE2D_ARRAY:\n  case CL_MEM_OBJECT_IMAGE1D:\n  case CL_MEM_OBJECT_IMAGE1D_ARRAY:\n  case CL_MEM_OBJECT_IMAGE1D_BUFFER:\n    break;\n  default:\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid or unsupported image type\");\n  }\n  if (flags &\n      ~(CL_MEM_READ_WRITE | CL_MEM_READ_ONLY | CL_MEM_WRITE_ONLY |\n        CL_MEM_USE_HOST_PTR | CL_MEM_ALLOC_HOST_PTR | CL_MEM_COPY_HOST_PTR)) {\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid flags\");\n  }\n\n  if (num_image_formats) {\n    *num_image_formats = NUM_SUPPORTED_IMAGE_FORMATS;\n  }\n\n  if (num_entries >= NUM_SUPPORTED_IMAGE_FORMATS && image_formats) {\n    int i;\n    for (i = 0; i < NUM_SUPPORTED_IMAGE_FORMATS; ++i) {\n      image_formats[i].image_channel_order =\n          supported_image_formats[i].image_channel_order;\n      image_formats[i].image_channel_data_type =\n          supported_image_formats[i].image_channel_data_type;\n    }\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetSupportedImageFormats(\n    cl_context context, cl_mem_flags flags, cl_mem_object_type image_type,\n    cl_uint num_entries, cl_image_format *image_formats,\n    cl_uint *num_image_formats) {\n  return clGetSupportedImageFormatsIntelFPGA(context, flags, image_type,\n                                             num_entries, image_formats,\n                                             num_image_formats);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetImageInfoIntelFPGA(\n    cl_mem image, cl_mem_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  cl_context context;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  RESULT_INIT;\n\n  if (!acl_mem_is_valid(image)) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  context = image->context;\n\n  if (!is_image(image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, context, \"Memory object is not an image\");\n  }\n\n  switch (param_name) {\n  case CL_IMAGE_FORMAT:\n    RESULT_IMAGE_FORMAT(*(image->fields.image_objs.image_format));\n    break;\n  case CL_IMAGE_ELEMENT_SIZE:\n    RESULT_SIZE_T(acl_get_image_element_size(\n        image->context, image->fields.image_objs.image_format, NULL));\n    break;\n  case CL_IMAGE_ROW_PITCH:\n    RESULT_SIZE_T(image->fields.image_objs.image_desc->image_row_pitch);\n    break;\n  case CL_IMAGE_SLICE_PITCH:\n    RESULT_SIZE_T(image->fields.image_objs.image_desc->image_slice_pitch);\n    break;\n  case CL_IMAGE_WIDTH:\n    RESULT_SIZE_T(image->fields.image_objs.image_desc->image_width);\n    break;\n  case CL_IMAGE_HEIGHT:\n    RESULT_SIZE_T(image->fields.image_objs.image_desc->image_height);\n    break;\n  case CL_IMAGE_DEPTH:\n    RESULT_SIZE_T(image->fields.image_objs.image_desc->image_depth);\n    break;\n  default:\n    break;\n  }\n\n  if (result.size == 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid or unsupported memory object query\");\n  }\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetImageInfo(cl_mem image,\n                                               cl_mem_info param_name,\n                                               size_t param_value_size,\n                                               void *param_value,\n                                               size_t *param_value_size_ret) {\n  return clGetImageInfoIntelFPGA(image, param_name, param_value_size,\n                                 param_value, param_value_size_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueReadImageIntelFPGA(\n    cl_command_queue command_queue, cl_mem image, cl_bool blocking_read,\n    const size_t *origin, const size_t *region, size_t row_pitch,\n    size_t slice_pitch, void *ptr, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  size_t tmp_row_pitch, tmp_slice_pitch;\n  cl_int errcode_ret;\n  size_t src_element_size;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (ptr == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n\n  if (image == NULL) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  src_element_size = acl_get_image_element_size(\n      image->context, image->fields.image_objs.image_format, &errcode_ret);\n  if (errcode_ret != CL_SUCCESS) {\n    return errcode_ret;\n  }\n\n  tmp_src_offset[0] = origin[0];\n  tmp_src_offset[1] = origin[1];\n  tmp_src_offset[2] = origin[2];\n  tmp_dst_offset[0] = (size_t)((char *)ptr - (char *)ACL_MEM_ALIGN);\n  tmp_dst_offset[1] = 0;\n  tmp_dst_offset[2] = 0;\n  // For images, have to multiply the first region by the size of each element\n  // because each element can be more than one byte wide.\n  tmp_cb[0] = region[0] * src_element_size;\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  if (row_pitch != 0) {\n    if (row_pitch <\n        image->fields.image_objs.image_desc->image_width * src_element_size) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Invalid row pitch provided\");\n    }\n    tmp_row_pitch = row_pitch;\n  } else {\n    tmp_row_pitch =\n        image->fields.image_objs.image_desc->image_width * src_element_size;\n  }\n\n  if (image->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n    tmp_slice_pitch = tmp_row_pitch;\n  } else {\n    tmp_slice_pitch =\n        image->fields.image_objs.image_desc->image_height * tmp_row_pitch;\n  }\n  // Allow the user to override the default slice pitch\n  if (slice_pitch != 0) {\n    if (slice_pitch < tmp_slice_pitch) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Invalid row pitch provided\");\n    }\n    tmp_slice_pitch = slice_pitch;\n  }\n\n  if (!is_image(image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Memory object is not an image\");\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, image)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, blocking_read, image, tmp_src_offset, 0, 0,\n        command_queue->context->unwrapped_host_mem,\n        tmp_dst_offset, // see creation of the unwrapped_host_mem\n        tmp_row_pitch, tmp_slice_pitch, tmp_cb, num_events_in_wait_list,\n        event_wait_list, event, CL_COMMAND_READ_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueReadImage(\n    cl_command_queue command_queue, cl_mem image, cl_bool blocking_read,\n    const size_t *origin, const size_t *region, size_t row_pitch,\n    size_t slice_pitch, void *ptr, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueReadImageIntelFPGA(\n      command_queue, image, blocking_read, origin, region, row_pitch,\n      slice_pitch, ptr, num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWriteImageIntelFPGA(\n    cl_command_queue command_queue, cl_mem image, cl_bool blocking_write,\n    const size_t *origin, const size_t *region, size_t input_row_pitch,\n    size_t input_slice_pitch, const void *ptr, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  size_t tmp_row_pitch, tmp_slice_pitch;\n  cl_int errcode_ret;\n  size_t dst_element_size;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (image == NULL) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  dst_element_size = acl_get_image_element_size(\n      image->context, image->fields.image_objs.image_format, &errcode_ret);\n  if (errcode_ret != CL_SUCCESS) {\n    return errcode_ret;\n  }\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  tmp_src_offset[0] = (size_t)((char *)ptr - (const char *)ACL_MEM_ALIGN);\n  tmp_src_offset[1] = 0;\n  tmp_src_offset[2] = 0;\n  tmp_dst_offset[0] = origin[0];\n  tmp_dst_offset[1] = origin[1];\n  tmp_dst_offset[2] = origin[2];\n  // For images, have to multiply the first region by the size of each element\n  // because each element can be more than one byte wide.\n  tmp_cb[0] = region[0] * dst_element_size;\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  if (input_row_pitch != 0) {\n    if (input_row_pitch <\n        image->fields.image_objs.image_desc->image_width * dst_element_size) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Invalid row pitch provided\");\n    }\n    tmp_row_pitch = input_row_pitch;\n  } else {\n    tmp_row_pitch =\n        image->fields.image_objs.image_desc->image_width * dst_element_size;\n  }\n\n  if (image->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n    tmp_slice_pitch = tmp_row_pitch;\n  } else {\n    tmp_slice_pitch =\n        image->fields.image_objs.image_desc->image_height * tmp_row_pitch;\n  }\n  // Allow the user to override the default slice pitch\n  if (input_slice_pitch != 0) {\n    if (input_slice_pitch < tmp_slice_pitch) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Invalid row pitch provided\");\n    }\n    tmp_slice_pitch = input_slice_pitch;\n  }\n\n  if (!is_image(image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Memory object is not an image\");\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, image)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, blocking_write,\n        command_queue->context->unwrapped_host_mem, tmp_src_offset,\n        tmp_row_pitch, tmp_slice_pitch, image, tmp_dst_offset, 0, 0, tmp_cb,\n        num_events_in_wait_list, event_wait_list, event,\n        CL_COMMAND_WRITE_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWriteImage(\n    cl_command_queue command_queue, cl_mem image, cl_bool blocking_write,\n    const size_t *origin, const size_t *region, size_t input_row_pitch,\n    size_t input_slice_pitch, const void *ptr, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueWriteImageIntelFPGA(\n      command_queue, image, blocking_write, origin, region, input_row_pitch,\n      input_slice_pitch, ptr, num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueFillImageIntelFPGA(\n    cl_command_queue command_queue, cl_mem image, const void *fill_color,\n    const size_t *origin, const size_t *region, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  size_t tmp_row_pitch, tmp_slice_pitch;\n  cl_int errcode_ret;\n  size_t dst_element_size;\n  char *ptr = NULL;\n  char converted_fill_color[16]; // Maximum number of bytes needed to keep a\n                                 // pixel.\n  cl_event tmp_event;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (image == NULL) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  dst_element_size = acl_get_image_element_size(\n      image->context, image->fields.image_objs.image_format, &errcode_ret);\n  if (errcode_ret != CL_SUCCESS) {\n    return errcode_ret;\n  }\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (!is_image(image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Memory object is not an image\");\n  }\n\n  // Replicating the color in the region allocated in host mem.\n  cl_image_format color_format;\n  color_format.image_channel_order = CL_RGBA;\n\n  if (fill_color == NULL)\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"fill_color cannot be NULL\");\n\n  size_t host_mem_size = region[0] * region[1] * region[2] * dst_element_size;\n\n  // Converting the given color to proper format.\n  // Note: The fill color is a single floating point value if the channel order\n  // is CL_DEPTH. This case is not implemented since we don't support CL_DEPTH\n  // yet\n  switch (image->fields.image_objs.image_format->image_channel_data_type) {\n  case CL_SNORM_INT8:\n  case CL_SNORM_INT16:\n  case CL_UNORM_INT8:\n  case CL_UNORM_INT16:\n  case CL_UNORM_SHORT_565:\n  case CL_UNORM_SHORT_555:\n  case CL_UNORM_INT_101010:\n  case CL_HALF_FLOAT:\n  case CL_FLOAT:\n    color_format.image_channel_data_type = CL_FLOAT;\n    errcode_ret =\n        acl_convert_image_format(fill_color, converted_fill_color, color_format,\n                                 *(image->fields.image_objs.image_format));\n    break;\n  case CL_SIGNED_INT8:\n  case CL_SIGNED_INT16:\n  case CL_SIGNED_INT32:\n    // Specs are vague about this, the correct image_channel_data_type of\n    // fill_color is either always int32, or int 8, 16 or 32 based on the image\n    // channel data type. Going for always int32.\n    color_format.image_channel_data_type = CL_SIGNED_INT32;\n    errcode_ret =\n        acl_convert_image_format(fill_color, converted_fill_color, color_format,\n                                 *(image->fields.image_objs.image_format));\n    break;\n  case CL_UNSIGNED_INT8:\n  case CL_UNSIGNED_INT16:\n  case CL_UNSIGNED_INT32:\n    // Specs are vague about this, the correct image_channel_data_type of\n    // fill_color is either always int32, or int 8, 16 or 32 based on the image\n    // channel data type. Going for always int32.\n    color_format.image_channel_data_type = CL_UNSIGNED_INT32;\n    errcode_ret =\n        acl_convert_image_format(fill_color, converted_fill_color, color_format,\n                                 *(image->fields.image_objs.image_format));\n    break;\n  default:\n    errcode_ret = -1;\n  }\n  if (errcode_ret != CL_SUCCESS)\n    ERR_RET(CL_IMAGE_FORMAT_NOT_SUPPORTED, command_queue->context,\n            \"Failed to convert fill_color to the appropriate image \"\n            \"channel format and order\");\n\n  // This array is passed to clSetEventCallback for releasing the\n  // allocated memory and releasing the event, if *event is null.\n  void **callback_data = (void **)acl_malloc(sizeof(void *) * 2);\n  if (!callback_data) {\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  acl_aligned_ptr_t *aligned_ptr =\n      (acl_aligned_ptr_t *)acl_malloc(sizeof(acl_aligned_ptr_t));\n  if (!aligned_ptr) {\n    acl_free(callback_data);\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  *aligned_ptr = acl_mem_aligned_malloc(host_mem_size);\n  ptr = (char *)(aligned_ptr->aligned_ptr);\n  if (!ptr) {\n    acl_free(aligned_ptr);\n    acl_free(callback_data);\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  for (cl_uint i = 0; i < region[0] * region[1] * region[2]; i++) {\n    safe_memcpy(ptr + i * dst_element_size, converted_fill_color,\n                dst_element_size, dst_element_size, 16);\n  }\n\n  tmp_src_offset[0] = (size_t)ptr - ACL_MEM_ALIGN;\n  tmp_src_offset[1] = 0;\n  tmp_src_offset[2] = 0;\n  tmp_dst_offset[0] = origin[0];\n  tmp_dst_offset[1] = origin[1];\n  tmp_dst_offset[2] = origin[2];\n  // For images, have to multiply the first region by the size of each element\n  // because each element can be more than one byte wide.\n  tmp_cb[0] = region[0] * dst_element_size;\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  tmp_row_pitch =\n      region[0] *\n      dst_element_size; // Width of each row of the memory that ptr points to.\n\n  if (image->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n    tmp_slice_pitch = tmp_row_pitch;\n  } else {\n    tmp_slice_pitch = region[1] * tmp_row_pitch;\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, image)) {\n    if (ptr) {\n      acl_mem_aligned_free(command_queue->context,\n                           aligned_ptr); // Cleaning up before failing.\n      acl_free(aligned_ptr);\n      acl_free(callback_data);\n    }\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, 0, command_queue->context->unwrapped_host_mem,\n        tmp_src_offset, tmp_row_pitch, tmp_slice_pitch, image, tmp_dst_offset,\n        0, 0, tmp_cb, num_events_in_wait_list, event_wait_list, &tmp_event,\n        CL_COMMAND_WRITE_BUFFER, 0);\n\n    if (ret != CL_SUCCESS) {\n      acl_mem_aligned_free(command_queue->context,\n                           aligned_ptr); // Cleaning up before failing.\n      acl_free(aligned_ptr);\n      acl_free(callback_data);\n      return ret;\n    }\n\n    callback_data[0] = (void *)(aligned_ptr);\n    if (event) {\n      *event = tmp_event;\n      callback_data[1] = NULL; // User needs the event, so we shouldn't release\n                               // it after the event completion.\n    } else {\n      callback_data[1] =\n          tmp_event; // Passing the event to release it when the event is done.\n    }\n    clSetEventCallback(tmp_event, CL_COMPLETE,\n                       acl_free_allocation_after_event_completion,\n                       (void *)callback_data);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueFillImage(\n    cl_command_queue command_queue, cl_mem image, const void *fill_color,\n    const size_t *origin, const size_t *region, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueFillImageIntelFPGA(command_queue, image, fill_color, origin,\n                                     region, num_events_in_wait_list,\n                                     event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyImageIntelFPGA(\n    cl_command_queue command_queue, cl_mem src_image, cl_mem dst_image,\n    const size_t *src_origin, const size_t *dst_origin, const size_t *region,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  size_t element_size;\n  cl_int errcode_ret;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (src_image == NULL || !is_image(src_image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Source memory object is not an image\");\n  }\n  if (dst_image == NULL || !is_image(dst_image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Source memory object is not an image\");\n  }\n\n  if ((src_image->fields.image_objs.image_format->image_channel_order !=\n       dst_image->fields.image_objs.image_format->image_channel_order) ||\n      (src_image->fields.image_objs.image_format->image_channel_data_type !=\n       dst_image->fields.image_objs.image_format->image_channel_data_type)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Source memory object is not an image\");\n  }\n\n  // Doesn't matter if we look at src or dst, already verified that they are the\n  // same above.\n  element_size = acl_get_image_element_size(\n      command_queue->context, src_image->fields.image_objs.image_format,\n      &errcode_ret);\n  if (errcode_ret != CL_SUCCESS) {\n    ERR_RET(errcode_ret, command_queue->context,\n            \"Source memory object is not an image\");\n  }\n\n  tmp_src_offset[0] = src_origin[0];\n  tmp_src_offset[1] = src_origin[1];\n  tmp_src_offset[2] = src_origin[2];\n  tmp_dst_offset[0] = dst_origin[0];\n  tmp_dst_offset[1] = dst_origin[1];\n  tmp_dst_offset[2] = dst_origin[2];\n  // For images, have to multiply the first region by the size of each element\n  // because each element can be more than one byte wide.\n  tmp_cb[0] = region[0] * element_size;\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  if ((src_image->fields.image_objs.image_format->image_channel_order !=\n       dst_image->fields.image_objs.image_format->image_channel_order) ||\n      (src_image->fields.image_objs.image_format->image_channel_data_type !=\n       dst_image->fields.image_objs.image_format->image_channel_data_type)) {\n    ERR_RET(CL_IMAGE_FORMAT_MISMATCH, command_queue->context,\n            \"Mismatch in image format between source & destination image\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, src_image)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, dst_image)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, 0, src_image, tmp_src_offset, 0, 0, dst_image,\n        tmp_dst_offset, 0, 0, tmp_cb, num_events_in_wait_list, event_wait_list,\n        event, CL_COMMAND_COPY_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyImage(\n    cl_command_queue command_queue, cl_mem src_image, cl_mem dst_image,\n    const size_t *src_origin, const size_t *dst_origin, const size_t *region,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  return clEnqueueCopyImageIntelFPGA(\n      command_queue, src_image, dst_image, src_origin, dst_origin, region,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyImageToBufferIntelFPGA(\n    cl_command_queue command_queue, cl_mem src_image, cl_mem dst_buffer,\n    const size_t *src_origin, const size_t *region, size_t dst_offset,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  size_t tmp_row_pitch, tmp_slice_pitch;\n  cl_int errcode_ret;\n  size_t src_element_size;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (!acl_mem_is_valid(src_image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Source image is invalid\");\n  }\n  if (!acl_mem_is_valid(dst_buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Destination buffer is invalid\");\n  }\n\n  if (src_image != NULL) {\n    src_element_size = acl_get_image_element_size(\n        src_image->context, src_image->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n  } else {\n    src_element_size = 0;\n  }\n\n  tmp_src_offset[0] = src_origin[0];\n  tmp_src_offset[1] = src_origin[1];\n  tmp_src_offset[2] = src_origin[2];\n  tmp_dst_offset[0] = dst_offset;\n  tmp_dst_offset[1] = 0;\n  tmp_dst_offset[2] = 0;\n  // For images, have to multiply the first region by the size of each element\n  // because each element can be more than one byte wide.\n  tmp_cb[0] = region[0] * src_element_size;\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  tmp_row_pitch =\n      src_image->fields.image_objs.image_desc->image_width * src_element_size;\n  if (src_image->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n      src_image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY ||\n      src_image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n    tmp_slice_pitch = tmp_row_pitch;\n  } else {\n    tmp_slice_pitch =\n        src_image->fields.image_objs.image_desc->image_height * tmp_row_pitch;\n  }\n\n  if (!is_image(src_image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Memory object is not an image\");\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, src_image)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, dst_buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, CL_FALSE, src_image, tmp_src_offset, 0, 0, dst_buffer,\n        tmp_dst_offset, // see creation of the unwrapped_host_mem\n        tmp_row_pitch, tmp_slice_pitch, tmp_cb, num_events_in_wait_list,\n        event_wait_list, event, CL_COMMAND_READ_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyImageToBuffer(\n    cl_command_queue command_queue, cl_mem src_image, cl_mem dst_buffer,\n    const size_t *src_origin, const size_t *region, size_t dst_offset,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  return clEnqueueCopyImageToBufferIntelFPGA(\n      command_queue, src_image, dst_buffer, src_origin, region, dst_offset,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyBufferToImageIntelFPGA(\n    cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_image,\n    size_t src_offset, const size_t *dst_origin, const size_t *region,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  size_t tmp_row_pitch, tmp_slice_pitch;\n  cl_int errcode_ret;\n  size_t dst_element_size;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (dst_image != NULL) {\n    dst_element_size = acl_get_image_element_size(\n        dst_image->context, dst_image->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n  } else {\n    dst_element_size = 0;\n  }\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (!acl_mem_is_valid(src_buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Source buffer is invalid\");\n  }\n  if (!acl_mem_is_valid(dst_image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Destination buffer is invalid\");\n  }\n\n  tmp_src_offset[0] = src_offset;\n  tmp_src_offset[1] = 0;\n  tmp_src_offset[2] = 0;\n  tmp_dst_offset[0] = dst_origin[0];\n  tmp_dst_offset[1] = dst_origin[1];\n  tmp_dst_offset[2] = dst_origin[2];\n  // For images, have to multiply the first region by the size of each element\n  // because each element can be more than one byte wide.\n  tmp_cb[0] = region[0] * dst_element_size;\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  tmp_row_pitch =\n      dst_image->fields.image_objs.image_desc->image_width * dst_element_size;\n  if (dst_image->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n      dst_image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY ||\n      dst_image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n    tmp_slice_pitch = tmp_row_pitch;\n  } else {\n    tmp_slice_pitch =\n        dst_image->fields.image_objs.image_desc->image_height * tmp_row_pitch;\n  }\n\n  if (!is_image(dst_image)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Memory object is not an image\");\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, src_buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, dst_image)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, CL_FALSE, src_buffer, tmp_src_offset, tmp_row_pitch,\n        tmp_slice_pitch, dst_image, tmp_dst_offset, 0, 0, tmp_cb,\n        num_events_in_wait_list, event_wait_list, event,\n        CL_COMMAND_WRITE_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyBufferToImage(\n    cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_image,\n    size_t src_offset, const size_t *dst_origin, const size_t *region,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  return clEnqueueCopyBufferToImageIntelFPGA(\n      command_queue, src_buffer, dst_image, src_offset, dst_origin, region,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clEnqueueMapImageIntelFPGA(\n    cl_command_queue command_queue, cl_mem image, cl_bool blocking_map,\n    cl_map_flags map_flags, const size_t *origin, const size_t *region,\n    size_t *image_row_pitch, size_t *image_slice_pitch,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event, cl_int *errcode_ret) {\n  void *result;\n  cl_event local_event = 0; // used for blocking\n  cl_context context;\n  cl_int status;\n  size_t element_size;\n  size_t tmp_row_pitch;\n  size_t tmp_slice_pitch = 0;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (image != NULL) {\n    element_size = acl_get_image_element_size(\n        image->context, image->fields.image_objs.image_format, errcode_ret);\n    if (*errcode_ret != CL_SUCCESS) {\n      return NULL;\n    }\n  } else {\n    element_size = 0;\n  }\n  if (!acl_command_queue_is_valid(command_queue)) {\n    BAIL(CL_INVALID_COMMAND_QUEUE);\n  }\n  context = command_queue->context;\n  if (!acl_mem_is_valid(image)) {\n    BAIL_INFO(CL_INVALID_MEM_OBJECT, context, \"Invalid memory object\");\n  }\n\n  if (command_queue->context != image->context) {\n    BAIL_INFO(\n        CL_INVALID_CONTEXT, context,\n        \"Command queue and memory object are not associated with the same \"\n        \"context\");\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, image)) {\n    BAIL_INFO(CL_MEM_OBJECT_ALLOCATION_FAILURE, context,\n              \"Deferred Allocation Failed\");\n  }\n\n  // Check if we can physically map the data into place.\n  // We can map it either if it already resides on the host, or we\n  // have backing store for it.\n  if (!image->block_allocation->region->is_host_accessible &&\n      !image->host_mem.aligned_ptr) {\n    BAIL_INFO(CL_MAP_FAILURE, context,\n              \"Could not map the image into host memory\");\n  }\n\n  if (!is_image(image)) {\n    BAIL_INFO(CL_INVALID_MEM_OBJECT, command_queue->context,\n              \"Memory object is not an image\");\n  }\n\n  if (image_row_pitch == NULL) {\n    BAIL_INFO(CL_INVALID_VALUE, command_queue->context,\n              \"Invalid row pitch provided\");\n  } else {\n    tmp_row_pitch =\n        image->fields.image_objs.image_desc->image_width * element_size;\n    *image_row_pitch = tmp_row_pitch;\n  }\n\n  if ((image->mem_object_type == CL_MEM_OBJECT_IMAGE3D ||\n       image->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY ||\n       image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) &&\n      image_slice_pitch == NULL) {\n    BAIL_INFO(CL_INVALID_VALUE, command_queue->context,\n              \"Invalid slice pitch provided\");\n  }\n\n  if (image->mem_object_type == CL_MEM_OBJECT_IMAGE2D ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n      image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n    if (image_slice_pitch != NULL) {\n      *image_slice_pitch = 0;\n    }\n  } else if (image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n    *image_slice_pitch = tmp_row_pitch;\n  } else {\n    *image_slice_pitch =\n        image->fields.image_objs.image_desc->image_height * tmp_row_pitch;\n  }\n\n  if (image_slice_pitch != NULL) {\n    tmp_slice_pitch = *image_slice_pitch;\n  }\n\n  if (!image->block_allocation->region->is_host_accessible) {\n    size_t tmp_src_offset[3];\n    size_t tmp_dst_offset[3];\n    size_t tmp_cb[3];\n\n    tmp_src_offset[0] = 0;\n    tmp_src_offset[1] = 0;\n    tmp_src_offset[2] = 0;\n    tmp_dst_offset[0] =\n        (size_t)((char *)image->host_mem.aligned_ptr - (char *)ACL_MEM_ALIGN);\n    tmp_dst_offset[1] = 0;\n    tmp_dst_offset[2] = 0;\n    // For images, have to multiply the first region by the size of each element\n    // because each element can be more than one byte wide.\n    tmp_cb[0] = image->fields.image_objs.image_desc->image_width * element_size;\n    if (image->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n      tmp_cb[1] = image->fields.image_objs.image_desc->image_array_size;\n    } else {\n      tmp_cb[1] = image->fields.image_objs.image_desc->image_height;\n    }\n    if (image->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY) {\n      tmp_cb[2] = image->fields.image_objs.image_desc->image_array_size;\n    } else {\n      tmp_cb[2] = image->fields.image_objs.image_desc->image_depth;\n    }\n\n    // We need to enqueue a memory transfer into the buffer->host_mem.\n    //\n    // It does not matter where the writable copy of the buffer is *right now*\n    // because that can change based on other enqueued commands.  We must\n    // always schedule the map, and then can elide the copy if at\n    // command run time the buffer is already mapped to the host.\n    //\n    // Since we only have 1 bit to indicate the logical location\n    // of the buffer (on the host or not), we copy the entire buffer.\n    //\n    // This will only move date if *at DMA time* the buffer is not\n    // mapped to the host,\n    status = l_enqueue_mem_transfer(command_queue, blocking_map,\n                                    // Source is buffer\n                                    image, tmp_src_offset, 0, 0,\n                                    // dest is host memory pointer\n                                    context->unwrapped_host_mem, tmp_dst_offset,\n                                    tmp_row_pitch, tmp_slice_pitch, tmp_cb,\n                                    num_events_in_wait_list, event_wait_list,\n                                    event, CL_COMMAND_MAP_BUFFER, map_flags);\n    acl_print_debug_msg(\" map: ref count is %u\\n\", acl_ref_count(image));\n\n    if (status != CL_SUCCESS)\n      BAIL(status); // already signalled callback\n\n    // The enqueue of the mem transfer will retain the buffer.\n  } else {\n    // We don't automatically move the live copy of a host buffer to the device.\n    // So there is no reason to copy the data.\n    status =\n        acl_create_event(command_queue, num_events_in_wait_list,\n                         event_wait_list, CL_COMMAND_MAP_BUFFER, &local_event);\n    if (status != CL_SUCCESS)\n      BAIL(status); // already signalled callback\n    // Mark it as the trivial map buffer case.\n    local_event->cmd.trivial = 1;\n    local_event->cmd.info.trivial_mem_mapping.mem = image;\n\n    // Should retain the memory object so that its metadata will stick around\n    // until at least the command is completed.\n    clRetainMemObject(image);\n    acl_print_debug_msg(\" map[%p] ref count is %u trivial case\\n\", image,\n                        acl_ref_count(image));\n  }\n\n  // If nothing's blocking, then complete right away\n  acl_idle_update(command_queue->context);\n\n  if (blocking_map) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n  // Returns pointer to the mapped location.\n  if (image->host_mem.aligned_ptr) {\n    // This case occurs for:\n    //    - host-malloc'd CL_MEM_ALLOC_HOST_PTR buffers,\n    //    - device-side buffer inaccessible from the host, but\n    //    which has backing store in host_mem.  For this case\n    //    the map event completion callback will actually copy the whole\n    //    buffer over.\n    result = ((char *)image->host_mem.aligned_ptr) + origin[0] * element_size;\n  } else {\n    // Can occur in legacy case where we never use host's malloc.\n    result =\n        ((char *)image->block_allocation->range.begin) +\n        get_offset_for_image_param(context, image->mem_object_type, \"data\") +\n        origin[0] * element_size;\n  }\n  acl_dump_mem_internal(image);\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clEnqueueMapImage(\n    cl_command_queue command_queue, cl_mem image, cl_bool blocking_map,\n    cl_map_flags map_flags, const size_t *origin, const size_t *region,\n    size_t *image_row_pitch, size_t *image_slice_pitch,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event, cl_int *errcode_ret) {\n  return clEnqueueMapImageIntelFPGA(command_queue, image, blocking_map,\n                                    map_flags, origin, region, image_row_pitch,\n                                    image_slice_pitch, num_events_in_wait_list,\n                                    event_wait_list, event, errcode_ret);\n}\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n// Map a part of given memory into the host address space.\n//\n// Can't fail if using host ptr or alloc host ptr.\n//\n// If the context requires that device side buffers have backing store on\n// the host size, then we'll honour those mapping requests too.\n// We'll copy the whole buffer back and forth as needed.\n//\n// Otherwise don't, i.e. assume that the device global memory is\n// inaccessible from the host.\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clEnqueueMapBufferIntelFPGA(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_map,\n    cl_map_flags map_flags, size_t offset, size_t cb, cl_uint num_events,\n    const cl_event *events, cl_event *event, cl_int *errcode_ret) {\n  void *result;\n  cl_event local_event = 0; // used for blocking\n  cl_context context;\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    BAIL(CL_INVALID_COMMAND_QUEUE);\n  }\n  context = command_queue->context;\n  if (!acl_mem_is_valid(buffer)) {\n    BAIL_INFO(CL_INVALID_MEM_OBJECT, context, \"Invalid memory object\");\n  }\n\n  if (command_queue->context != buffer->context) {\n    BAIL_INFO(\n        CL_INVALID_CONTEXT, context,\n        \"Command queue and memory object are not associated with the same \"\n        \"context\");\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, buffer)) {\n    BAIL_INFO(CL_MEM_OBJECT_ALLOCATION_FAILURE, context,\n              \"Deferred Allocation Failed\");\n  }\n\n  // Check flags\n  if (map_flags &\n      ~(CL_MAP_READ | CL_MAP_WRITE | CL_MAP_WRITE_INVALIDATE_REGION)) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid or unsupported flags\");\n  }\n  if (((map_flags & CL_MAP_READ) &&\n       (map_flags & CL_MAP_WRITE_INVALIDATE_REGION)) ||\n      ((map_flags & CL_MAP_WRITE) &&\n       (map_flags & CL_MAP_WRITE_INVALIDATE_REGION))) {\n    BAIL_INFO(\n        CL_INVALID_VALUE, context,\n        \"CL_MAP_READ or CL_MAP_WRITE and CL_MAP_WRITE_INVALIDATE_REGION are \"\n        \"specified but are mutually exclusive\");\n  }\n\n  // Check if we can physically map the data into place.\n  // We can map it either if it already resides on the host, or we\n  // have backing store for it.\n  cl_mem_flags flags = buffer->flags;\n  if (!buffer->block_allocation->region->is_host_accessible &&\n      !buffer->host_mem.aligned_ptr && !(flags & CL_MEM_USE_HOST_PTR)) {\n    BAIL_INFO(CL_MAP_FAILURE, context,\n              \"Could not map the buffer into host memory\");\n  }\n\n  if (offset + cb > buffer->size) {\n    BAIL_INFO(CL_INVALID_VALUE, context,\n              \"Requested offset and byte count exceeds the buffer size\");\n  }\n\n  if (flags & CL_MEM_USE_HOST_PTR) {\n    size_t tmp_src_offset[3];\n    size_t tmp_dst_offset[3];\n    size_t tmp_cb[3];\n\n    tmp_src_offset[0] = 0;\n    tmp_src_offset[1] = 0;\n    tmp_src_offset[2] = 0;\n    tmp_dst_offset[0] = (size_t)((char *)buffer->fields.buffer_objs.host_ptr -\n                                 (char *)ACL_MEM_ALIGN);\n    tmp_dst_offset[1] = 0;\n    tmp_dst_offset[2] = 0;\n    tmp_cb[0] = buffer->size;\n    tmp_cb[1] = 1;\n    tmp_cb[2] = 1;\n\n    status = l_enqueue_mem_transfer(\n        command_queue, blocking_map,\n        // Source is buffer\n        buffer, tmp_src_offset, 0, 0,\n        // dest is host memory pointer\n        context->unwrapped_host_mem, tmp_dst_offset, 0, 0, tmp_cb, num_events,\n        events, &local_event, CL_COMMAND_MAP_BUFFER, map_flags);\n    acl_print_debug_msg(\" map: ref count is %u\\n\", acl_ref_count(buffer));\n\n    if (status != CL_SUCCESS)\n      BAIL(status); // already signalled callback\n\n  } else if (!buffer->block_allocation->region->is_host_accessible) {\n    size_t tmp_src_offset[3];\n    size_t tmp_dst_offset[3];\n    size_t tmp_cb[3];\n\n    tmp_src_offset[0] = 0;\n    tmp_src_offset[1] = 0;\n    tmp_src_offset[2] = 0;\n    tmp_dst_offset[0] =\n        (size_t)((char *)buffer->host_mem.aligned_ptr - (char *)ACL_MEM_ALIGN);\n    tmp_dst_offset[1] = 0;\n    tmp_dst_offset[2] = 0;\n    tmp_cb[0] = buffer->size;\n    tmp_cb[1] = 1;\n    tmp_cb[2] = 1;\n\n    // We need to enqueue a memory transfer into the buffer->host_mem.\n    //\n    // It does not matter where the writable copy of the buffer is *right now*\n    // because that can change based on other enqueued commands.  We must\n    // always schedule the map, and then can elide the copy if at\n    // command run time the buffer is already mapped to the host.\n    //\n    // Since we only have 1 bit to indicate the logical location\n    // of the buffer (on the host or not), we copy the entire buffer.\n    //\n    // This will only move date if *at DMA time* the buffer is not\n    // mapped to the host,\n    status = l_enqueue_mem_transfer(\n        command_queue, blocking_map,\n        // Source is buffer\n        buffer, tmp_src_offset, 0, 0,\n        // dest is host memory pointer\n        context->unwrapped_host_mem, tmp_dst_offset, 0, 0, tmp_cb, num_events,\n        events, &local_event, CL_COMMAND_MAP_BUFFER, map_flags);\n    acl_print_debug_msg(\" map: ref count is %u\\n\", acl_ref_count(buffer));\n\n    if (status != CL_SUCCESS)\n      BAIL(status); // already signalled callback\n\n    // The enqueue of the mem transfer will retain the buffer.\n  } else {\n    // We don't automatically move the live copy of a host buffer to the device.\n    // So there is no reason to copy the data.\n    status = acl_create_event(command_queue, num_events, events,\n                              CL_COMMAND_MAP_BUFFER, &local_event);\n    if (status != CL_SUCCESS)\n      BAIL(status); // already signalled callback\n    // Mark it as the trivial map buffer case.\n    local_event->cmd.trivial = 1;\n    local_event->cmd.info.trivial_mem_mapping.mem = buffer;\n\n    // Should retain the memory object so that its metadata will stick around\n    // until at least the command is completed.\n    clRetainMemObject(buffer);\n    acl_print_debug_msg(\" map[%p] ref count is %u trivial case\\n\", buffer,\n                        acl_ref_count(buffer));\n  }\n\n  // If nothing's blocking, then complete right away\n  acl_idle_update(command_queue->context);\n\n  if (blocking_map) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n  // Returns pointer to the mapped location.\n  if (buffer->host_mem.aligned_ptr) {\n    // This case occurs for:\n    //    - host-malloc'd CL_MEM_ALLOC_HOST_PTR buffers,\n    //    - device-side buffer inaccessible from the host, but\n    //    which has backing store in host_mem.  For this case\n    //    the map event completion callback will actually copy the whole\n    //    buffer over.\n    if (flags & CL_MEM_USE_HOST_PTR) {\n      result = (char *)buffer->fields.buffer_objs.host_ptr + offset;\n    } else {\n      result = ((char *)buffer->host_mem.aligned_ptr) + offset;\n    }\n  } else {\n    if (flags & CL_MEM_USE_HOST_PTR) {\n      result = (char *)buffer->fields.buffer_objs.host_ptr + offset;\n    } else {\n      // Can occur in legacy case where we never use host's malloc\n      result = ((char *)buffer->block_allocation->range.begin) + offset;\n    }\n  }\n  acl_dump_mem_internal(buffer);\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clEnqueueMapBuffer(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_map,\n    cl_map_flags map_flags, size_t offset, size_t cb, cl_uint num_events,\n    const cl_event *events, cl_event *event, cl_int *errcode_ret) {\n  return clEnqueueMapBufferIntelFPGA(command_queue, buffer, blocking_map,\n                                     map_flags, offset, cb, num_events, events,\n                                     event, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueUnmapMemObjectIntelFPGA(\n    cl_command_queue command_queue, cl_mem mem, void *mapped_ptr,\n    cl_uint num_events, const cl_event *events, cl_event *event) {\n  cl_event local_event = 0;\n  cl_context context;\n  cl_int status;\n  char *valid_base_ptr;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  context = command_queue->context;\n  if (!acl_mem_is_valid(mem)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, context, \"Memory object is invalid\");\n  }\n\n  if (command_queue->context != mem->context) {\n    ERR_RET(CL_INVALID_CONTEXT, context,\n            \"Command queue and memory object are not associated with the \"\n            \"same context\");\n  }\n  cl_mem_flags flags = mem->flags;\n  if ((!mem->block_allocation->region->is_host_accessible &&\n       !mem->host_mem.aligned_ptr && !(flags & CL_MEM_USE_HOST_PTR)) ||\n      mem->allocation_deferred) {\n    ERR_RET(CL_MAP_FAILURE, context,\n            \"Could not have mapped the buffer into host memory\");\n  }\n\n  // Necessary sanity check on the pointer.\n  // This checks that the *start* of the mapped buffer is in bounds for\n  // the mem object.\n  if (flags & CL_MEM_USE_HOST_PTR) {\n    valid_base_ptr = (char *)mem->fields.buffer_objs.host_ptr;\n  } else {\n    valid_base_ptr = (char *)(mem->host_mem.aligned_ptr\n                                  ? mem->host_mem.aligned_ptr\n                                  : mem->block_allocation->range.begin);\n  }\n  if ((valid_base_ptr - (char *)mapped_ptr) > 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid mapped_ptr argument: it lies outside the buffer\");\n  }\n  if (((char *)mapped_ptr - (valid_base_ptr + mem->size)) >= 0) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid mapped_ptr argument: it lies outside the buffer\");\n  }\n\n  // This is the mirror image of mapping the buffer in the first place.\n  if (flags & CL_MEM_USE_HOST_PTR) {\n    size_t tmp_src_offset[3];\n    size_t tmp_dst_offset[3];\n    size_t tmp_cb[3];\n\n    // When unmapping an image, use the built in image sizes\n    tmp_src_offset[0] = (size_t)((char *)mem->fields.buffer_objs.host_ptr -\n                                 (char *)ACL_MEM_ALIGN);\n    tmp_src_offset[1] = 0;\n    tmp_src_offset[2] = 0;\n    tmp_dst_offset[0] = 0;\n    tmp_dst_offset[1] = 0;\n    tmp_dst_offset[2] = 0;\n    if (is_image(mem)) {\n\n      size_t image_element_size = acl_get_image_element_size(\n          mem->context, mem->fields.image_objs.image_format, &status);\n      if (status != CL_SUCCESS) {\n        return status;\n      }\n\n      tmp_cb[0] =\n          mem->fields.image_objs.image_desc->image_width * image_element_size;\n      tmp_cb[1] = mem->fields.image_objs.image_desc->image_height;\n      tmp_cb[2] = mem->fields.image_objs.image_desc->image_depth;\n    } else {\n      tmp_cb[0] = mem->size;\n      tmp_cb[1] = 1;\n      tmp_cb[2] = 1;\n    }\n\n    acl_print_debug_msg(\" unmapping case writable %d\\n\",\n                        mem->writable_copy_on_host);\n    // We need to enqueue a memory transfer into the buffer->host_mem.\n    // Since we only have 1 bit to indicate the logical location\n    // of the buffer (on the host or not), we copy the entire buffer.\n    //\n    // This will only move date if *at DMA time* the buffer is not\n    // mapped to the host *and* the mapping included the write flag.\n    status = l_enqueue_mem_transfer(\n        command_queue,\n        0, // not blocking\n        // Src is host memory pointer\n        context->unwrapped_host_mem, tmp_src_offset, 0, 0,\n        // Dest is buffer\n        mem, tmp_dst_offset, 0, 0, tmp_cb, num_events, events, &local_event,\n        CL_COMMAND_UNMAP_MEM_OBJECT,\n        // We don't need map flags.  We just return\n        // the writable copy back to the host.  So we'll only need\n        // to inspect cl_mem->writable_copy_on_host at command\n        // execution time.\n        0);\n    if (status != CL_SUCCESS)\n      return status; // already signalled callback\n    acl_print_debug_msg(\"mem[%p] enqueue unmap. refcount %u\\n\", mem,\n                        acl_ref_count(mem));\n\n  } else if (!mem->block_allocation->region->is_host_accessible) {\n    size_t tmp_src_offset[3];\n    size_t tmp_dst_offset[3];\n    size_t tmp_cb[3];\n\n    // When unmapping an image, use the built in image sizes\n    tmp_src_offset[0] =\n        (size_t)((char *)mem->host_mem.aligned_ptr - (char *)ACL_MEM_ALIGN);\n    tmp_src_offset[1] = 0;\n    tmp_src_offset[2] = 0;\n    tmp_dst_offset[0] = 0;\n    tmp_dst_offset[1] = 0;\n    tmp_dst_offset[2] = 0;\n    if (is_image(mem)) {\n\n      size_t image_element_size = acl_get_image_element_size(\n          mem->context, mem->fields.image_objs.image_format, &status);\n      if (status != CL_SUCCESS) {\n        return status;\n      }\n\n      tmp_cb[0] =\n          mem->fields.image_objs.image_desc->image_width * image_element_size;\n      tmp_cb[1] = mem->fields.image_objs.image_desc->image_height;\n      tmp_cb[2] = mem->fields.image_objs.image_desc->image_depth;\n    } else {\n      tmp_cb[0] = mem->size;\n      tmp_cb[1] = 1;\n      tmp_cb[2] = 1;\n    }\n\n    acl_print_debug_msg(\" unmapping case writable %d\\n\",\n                        mem->writable_copy_on_host);\n    // We need to enqueue a memory transfer into the buffer->host_mem.\n    // Since we only have 1 bit to indicate the logical location\n    // of the buffer (on the host or not), we copy the entire buffer.\n    //\n    // This will only move date if *at DMA time* the buffer is not\n    // mapped to the host *and* the mapping included the write flag.\n    status = l_enqueue_mem_transfer(\n        command_queue,\n        0, // not blocking\n        // Src is host memory pointer\n        context->unwrapped_host_mem, tmp_src_offset, 0, 0,\n        // Dest is buffer\n        mem, tmp_dst_offset, 0, 0, tmp_cb, num_events, events, &local_event,\n        CL_COMMAND_UNMAP_MEM_OBJECT,\n        // We don't need map flags.  We just return\n        // the writable copy back to the host.  So we'll only need\n        // to inspect cl_mem->writable_copy_on_host at command\n        // execution time.\n        0);\n    if (status != CL_SUCCESS)\n      return status; // already signalled callback\n    acl_print_debug_msg(\"mem[%p] enqueue unmap. refcount %u\\n\", mem,\n                        acl_ref_count(mem));\n  } else {\n    status = acl_create_event(command_queue, num_events, events,\n                              CL_COMMAND_UNMAP_MEM_OBJECT, &local_event);\n    if (status != CL_SUCCESS)\n      return status; // already signalled callback\n    local_event->cmd.trivial = 1;\n    local_event->cmd.info.trivial_mem_mapping.mem = mem;\n    // Should retain the memory object so that its metadata will stick around\n    // until at least the command is completed.\n    clRetainMemObject(mem);\n    acl_print_debug_msg(\"mem[%p] enqueue unmap trivial. refcount %u\\n\", mem,\n                        acl_ref_count(mem));\n  }\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueUnmapMemObject(\n    cl_command_queue command_queue, cl_mem mem, void *mapped_ptr,\n    cl_uint num_events, const cl_event *events, cl_event *event) {\n  return clEnqueueUnmapMemObjectIntelFPGA(command_queue, mem, mapped_ptr,\n                                          num_events, events, event);\n}\n\n// Schedule a buffer read into host memory.\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueReadBufferIntelFPGA(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_read,\n    size_t offset, size_t cb, void *ptr, cl_uint num_events,\n    const cl_event *events, cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  tmp_src_offset[0] = offset;\n  tmp_src_offset[1] = 0;\n  tmp_src_offset[2] = 0;\n  tmp_dst_offset[0] = (size_t)((char *)ptr - (char *)ACL_MEM_ALIGN);\n  tmp_dst_offset[1] = 0;\n  tmp_dst_offset[2] = 0;\n  tmp_cb[0] = cb;\n  tmp_cb[1] = 1;\n  tmp_cb[2] = 1;\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (ptr == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, blocking_read, buffer, tmp_src_offset, 0, 0,\n        command_queue->context->unwrapped_host_mem,\n        tmp_dst_offset, // see creation of the unwrapped_host_mem\n        0, 0, tmp_cb, num_events, events, event, CL_COMMAND_READ_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueReadBuffer(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_read,\n    size_t offset, size_t cb, void *ptr, cl_uint num_events,\n    const cl_event *events, cl_event *event) {\n  return clEnqueueReadBufferIntelFPGA(command_queue, buffer, blocking_read,\n                                      offset, cb, ptr, num_events, events,\n                                      event);\n}\n\n// Schedule a buffer read of a rectangular region into host memory.\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueReadBufferRectIntelFPGA(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_read,\n    const size_t *buffer_offset, const size_t *host_offset,\n    const size_t *region, size_t buffer_row_pitch, size_t buffer_slice_pitch,\n    size_t host_row_pitch, size_t host_slice_pitch, void *ptr,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (buffer_row_pitch == 0) {\n    buffer_row_pitch = region[0];\n  }\n  if (buffer_slice_pitch == 0) {\n    buffer_slice_pitch = region[1] * buffer_row_pitch;\n  }\n  if (host_row_pitch == 0) {\n    host_row_pitch = region[0];\n  }\n  if (host_slice_pitch == 0) {\n    host_slice_pitch = region[1] * host_row_pitch;\n  }\n\n  tmp_src_offset[0] = buffer_offset[0];\n  tmp_src_offset[1] = buffer_offset[1];\n  tmp_src_offset[2] = buffer_offset[2];\n  tmp_dst_offset[0] = (char *)ptr - (char *)ACL_MEM_ALIGN + host_offset[0];\n  tmp_dst_offset[1] = host_offset[1];\n  tmp_dst_offset[2] = host_offset[2];\n  tmp_cb[0] = region[0];\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (!acl_mem_is_valid(buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context, \"Buffer is invalid\");\n  }\n  if (ptr == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, blocking_read, buffer, tmp_src_offset, buffer_row_pitch,\n        buffer_slice_pitch, command_queue->context->unwrapped_host_mem,\n        tmp_dst_offset, // see creation of the unwrapped_host_mem\n        host_row_pitch, host_slice_pitch, tmp_cb, num_events_in_wait_list,\n        event_wait_list, event, CL_COMMAND_READ_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueReadBufferRect(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_read,\n    const size_t *buffer_origin, const size_t *host_origin,\n    const size_t *region, size_t buffer_row_pitch, size_t buffer_slice_pitch,\n    size_t host_row_pitch, size_t host_slice_pitch, void *ptr,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  return clEnqueueReadBufferRectIntelFPGA(\n      command_queue, buffer, blocking_read, buffer_origin, host_origin, region,\n      buffer_row_pitch, buffer_slice_pitch, host_row_pitch, host_slice_pitch,\n      ptr, num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWriteBufferIntelFPGA(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_write,\n    size_t offset, size_t cb, const void *ptr, cl_uint num_events,\n    const cl_event *events, cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  tmp_src_offset[0] = (size_t)((char *)ptr - (const char *)ACL_MEM_ALIGN);\n  tmp_src_offset[1] = 0;\n  tmp_src_offset[2] = 0;\n  tmp_dst_offset[0] = offset;\n  tmp_dst_offset[1] = 0;\n  tmp_dst_offset[2] = 0;\n  tmp_cb[0] = cb;\n  tmp_cb[1] = 1;\n  tmp_cb[2] = 1;\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, blocking_write,\n        command_queue->context->unwrapped_host_mem, tmp_src_offset, 0, 0,\n        buffer, tmp_dst_offset, 0, 0, tmp_cb, num_events, events, event,\n        CL_COMMAND_WRITE_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWriteBuffer(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_write,\n    size_t offset, size_t cb, const void *ptr, cl_uint num_events,\n    const cl_event *events, cl_event *event) {\n  return clEnqueueWriteBufferIntelFPGA(command_queue, buffer, blocking_write,\n                                       offset, cb, ptr, num_events, events,\n                                       event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWriteBufferRectIntelFPGA(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_write,\n    const size_t *buffer_offset, const size_t *host_offset,\n    const size_t *region, size_t buffer_row_pitch, size_t buffer_slice_pitch,\n    size_t host_row_pitch, size_t host_slice_pitch, const void *ptr,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (buffer_row_pitch == 0) {\n    buffer_row_pitch = region[0];\n  }\n  if (buffer_slice_pitch == 0) {\n    buffer_slice_pitch = region[1] * buffer_row_pitch;\n  }\n  if (host_row_pitch == 0) {\n    host_row_pitch = region[0];\n  }\n  if (host_slice_pitch == 0) {\n    host_slice_pitch = region[1] * host_row_pitch;\n  }\n\n  tmp_src_offset[0] = (char *)ptr - (char *)ACL_MEM_ALIGN + host_offset[0];\n  tmp_src_offset[1] = host_offset[1];\n  tmp_src_offset[2] = host_offset[2];\n  tmp_dst_offset[0] = buffer_offset[0];\n  tmp_dst_offset[1] = buffer_offset[1];\n  tmp_dst_offset[2] = buffer_offset[2];\n  tmp_cb[0] = region[0];\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (!acl_mem_is_valid(buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context, \"Buffer is invalid\");\n  }\n  if (ptr == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, blocking_write,\n        command_queue->context->unwrapped_host_mem, tmp_src_offset,\n        host_row_pitch, host_slice_pitch, buffer,\n        tmp_dst_offset, // see creation of the unwrapped_host_mem\n        buffer_row_pitch, buffer_slice_pitch, tmp_cb, num_events_in_wait_list,\n        event_wait_list, event, CL_COMMAND_WRITE_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWriteBufferRect(\n    cl_command_queue command_queue, cl_mem buffer, cl_bool blocking_write,\n    const size_t *buffer_offset, const size_t *host_offset,\n    const size_t *region, size_t buffer_row_pitch, size_t buffer_slice_pitch,\n    size_t host_row_pitch, size_t host_slice_pitch, const void *ptr,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  return clEnqueueWriteBufferRectIntelFPGA(\n      command_queue, buffer, blocking_write, buffer_offset, host_offset, region,\n      buffer_row_pitch, buffer_slice_pitch, host_row_pitch, host_slice_pitch,\n      ptr, num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int clEnqueueFillBufferIntelFPGA(\n    cl_command_queue command_queue, cl_mem buffer, const void *pattern,\n    size_t pattern_size, size_t offset, size_t size,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  char *ptr;\n\n  cl_event tmp_event;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (!acl_mem_is_valid(buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context, \"Buffer is invalid\");\n  }\n\n  // Pattern size can only be {1,2,4,8,...,1024 sizeof(double16)}.\n  if (pattern_size == 0 || pattern_size > 1024 ||\n      (pattern_size & (pattern_size - 1))) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context, \"Invalid pattern size\");\n  }\n\n  if (offset % pattern_size != 0 || size % pattern_size != 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Offset and size must be a multiple of pattern size\");\n  }\n\n  if (pattern == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context, \"pattern cannot be NULL\");\n  }\n\n  if (!acl_bind_buffer_to_device(command_queue->device, buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  // This array is passed to clSetEventCallback for releasing the\n  // allocated memory and releasing the event, if *event is null.\n  void **callback_data = (void **)acl_malloc(sizeof(void *) * 2);\n  if (!callback_data) {\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  acl_aligned_ptr_t *aligned_ptr =\n      (acl_aligned_ptr_t *)acl_malloc(sizeof(acl_aligned_ptr_t));\n  if (!aligned_ptr) {\n    acl_free(callback_data);\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  // Replicating the pattern, size/pattern_size times.\n  *aligned_ptr = acl_mem_aligned_malloc(size);\n  ptr = (char *)(aligned_ptr->aligned_ptr);\n  if (!ptr) {\n    acl_free(aligned_ptr);\n    acl_free(callback_data);\n    ERR_RET(CL_OUT_OF_HOST_MEMORY, command_queue->context,\n            \"Out of host memory\");\n  }\n\n  for (cl_uint i = 0; i < size / pattern_size; i++) {\n    safe_memcpy(&(ptr[i * pattern_size]), pattern, pattern_size, pattern_size,\n                pattern_size);\n  }\n\n  tmp_src_offset[0] = (size_t)((char *)ptr - (const char *)ACL_MEM_ALIGN);\n  tmp_src_offset[1] = 0;\n  tmp_src_offset[2] = 0;\n  tmp_dst_offset[0] = offset;\n  tmp_dst_offset[1] = 0;\n  tmp_dst_offset[2] = 0;\n  tmp_cb[0] = size;\n  tmp_cb[1] = 1;\n  tmp_cb[2] = 1;\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, 0, command_queue->context->unwrapped_host_mem,\n        tmp_src_offset, 0, 0, buffer, tmp_dst_offset, 0, 0, tmp_cb,\n        num_events_in_wait_list, event_wait_list, &tmp_event,\n        CL_COMMAND_WRITE_BUFFER, 0);\n\n    if (ret != CL_SUCCESS) {\n      acl_mem_aligned_free(command_queue->context, aligned_ptr);\n      acl_free(aligned_ptr);\n      acl_free(callback_data);\n      return ret;\n    }\n    callback_data[0] = (void *)(aligned_ptr);\n    if (event) {\n      *event = tmp_event;\n      callback_data[1] = NULL; // User needs the event, so we shouldn't release\n                               // it after the event completion.\n    } else {\n      callback_data[1] =\n          tmp_event; // Passing the event to release it when the event is done.\n    }\n    clSetEventCallback(tmp_event, CL_COMPLETE,\n                       acl_free_allocation_after_event_completion,\n                       (void *)callback_data);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int clEnqueueFillBuffer(cl_command_queue command_queue,\n                                        cl_mem buffer, const void *pattern,\n                                        size_t pattern_size, size_t offset,\n                                        size_t size,\n                                        cl_uint num_events_in_wait_list,\n                                        const cl_event *event_wait_list,\n                                        cl_event *event) {\n  return clEnqueueFillBufferIntelFPGA(\n      command_queue, buffer, pattern, pattern_size, offset, size,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyBufferIntelFPGA(\n    cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_buffer,\n    size_t src_offset, size_t dst_offset, size_t cb, cl_uint num_events,\n    const cl_event *events, cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  tmp_src_offset[0] = src_offset;\n  tmp_src_offset[1] = 0;\n  tmp_src_offset[2] = 0;\n  tmp_dst_offset[0] = dst_offset;\n  tmp_dst_offset[1] = 0;\n  tmp_dst_offset[2] = 0;\n  tmp_cb[0] = cb;\n  tmp_cb[1] = 1;\n  tmp_cb[2] = 1;\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (!acl_mem_is_valid(src_buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Source buffer is invalid\");\n  }\n  if (!acl_mem_is_valid(dst_buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Destination buffer is invalid\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, src_buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, dst_buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, 0, src_buffer, tmp_src_offset, 0, 0, dst_buffer,\n        tmp_dst_offset, 0, 0, tmp_cb, num_events, events, event,\n        CL_COMMAND_COPY_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyBuffer(\n    cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_buffer,\n    size_t src_offset, size_t dst_offset, size_t cb, cl_uint num_events,\n    const cl_event *events, cl_event *event) {\n  return clEnqueueCopyBufferIntelFPGA(command_queue, src_buffer, dst_buffer,\n                                      src_offset, dst_offset, cb, num_events,\n                                      events, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyBufferRectIntelFPGA(\n    cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_buffer,\n    const size_t *src_origin, const size_t *dst_origin, const size_t *region,\n    size_t src_row_pitch, size_t src_slice_pitch, size_t dst_row_pitch,\n    size_t dst_slice_pitch, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  size_t tmp_src_offset[3];\n  size_t tmp_dst_offset[3];\n  size_t tmp_cb[3];\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (src_row_pitch == 0) {\n    src_row_pitch = region[0];\n  }\n  if (src_slice_pitch == 0) {\n    src_slice_pitch = region[1] * src_row_pitch;\n  }\n  if (dst_row_pitch == 0) {\n    dst_row_pitch = region[0];\n  }\n  if (dst_slice_pitch == 0) {\n    dst_slice_pitch = region[1] * dst_row_pitch;\n  }\n\n  tmp_src_offset[0] = src_origin[0];\n  tmp_src_offset[1] = src_origin[1];\n  tmp_src_offset[2] = src_origin[2];\n  tmp_dst_offset[0] = dst_origin[0];\n  tmp_dst_offset[1] = dst_origin[1];\n  tmp_dst_offset[2] = dst_origin[2];\n  tmp_cb[0] = region[0];\n  tmp_cb[1] = region[1];\n  tmp_cb[2] = region[2];\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (!acl_mem_is_valid(src_buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Source buffer is invalid\");\n  }\n  if (!acl_mem_is_valid(dst_buffer)) {\n    ERR_RET(CL_INVALID_MEM_OBJECT, command_queue->context,\n            \"Destination buffer is invalid\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, src_buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n  if (!acl_bind_buffer_to_device(command_queue->device, dst_buffer)) {\n    ERR_RET(CL_MEM_OBJECT_ALLOCATION_FAILURE, command_queue->context,\n            \"Deferred Allocation Failed\");\n  }\n\n  if (src_buffer == dst_buffer) {\n    if (src_row_pitch != dst_row_pitch) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Source buffer and destination buffer are the same, but row \"\n              \"pitches do not match\");\n    }\n    if (src_slice_pitch != dst_slice_pitch) {\n      ERR_RET(CL_INVALID_VALUE, command_queue->context,\n              \"Source buffer and destination buffer are the same, but slice \"\n              \"pitches do not match\");\n    }\n    if (check_copy_overlap(tmp_src_offset, tmp_dst_offset, tmp_cb,\n                           src_row_pitch, src_slice_pitch)) {\n      ERR_RET(CL_MEM_COPY_OVERLAP, command_queue->context,\n              \"Source buffer and destination buffer are the same and regions \"\n              \"overlaps\");\n    }\n  }\n  {\n    cl_int ret = l_enqueue_mem_transfer(\n        command_queue, 0, src_buffer, tmp_src_offset, src_row_pitch,\n        src_slice_pitch, dst_buffer,\n        tmp_dst_offset, // see creation of the unwrapped_host_mem\n        dst_row_pitch, dst_slice_pitch, tmp_cb, num_events_in_wait_list,\n        event_wait_list, event, CL_COMMAND_COPY_BUFFER, 0);\n    return ret;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueCopyBufferRect(\n    cl_command_queue command_queue, cl_mem src_buffer, cl_mem dst_buffer,\n    const size_t *src_origin, const size_t *dst_origin, const size_t *region,\n    size_t src_row_pitch, size_t src_slice_pitch, size_t dst_row_pitch,\n    size_t dst_slice_pitch, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueCopyBufferRectIntelFPGA(\n      command_queue, src_buffer, dst_buffer, src_origin, dst_origin, region,\n      src_row_pitch, src_slice_pitch, dst_row_pitch, dst_slice_pitch,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL clCreatePipeIntelFPGA(\n    cl_context context, cl_mem_flags flags, cl_uint pipe_packet_size,\n    cl_uint pipe_max_packets, const cl_pipe_properties *properties,\n    cl_int *errcode_ret) {\n  cl_mem mem;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    BAIL(CL_INVALID_CONTEXT);\n  }\n\n  // Check flags\n  {\n    // Check for invalid enum bits\n    if (flags & ~(CL_MEM_READ_WRITE | CL_MEM_READ_ONLY | CL_MEM_WRITE_ONLY |\n                  CL_MEM_HOST_READ_ONLY | CL_MEM_HOST_WRITE_ONLY)) {\n      BAIL_INFO(CL_INVALID_VALUE, context, \"Invalid or unsupported flags\");\n    }\n\n    {\n      int num_rw_specs = 0;\n      int num_hostrw_specs = 0;\n\n      if (flags & CL_MEM_READ_WRITE)\n        num_rw_specs++;\n      if (flags & CL_MEM_READ_ONLY)\n        num_rw_specs++;\n      if (flags & CL_MEM_WRITE_ONLY)\n        num_rw_specs++;\n      if (flags & CL_MEM_HOST_READ_ONLY)\n        num_hostrw_specs++;\n      if (flags & CL_MEM_HOST_WRITE_ONLY)\n        num_hostrw_specs++;\n\n      // Check for exactly one read/write spec\n      if (num_rw_specs > 1) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"More than one read/write flag is specified\");\n      }\n      if (num_hostrw_specs > 1) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"More than one host read/write flag is specified\");\n      }\n\n      // Default to CL_MEM_READ_WRITE.\n      if ((num_rw_specs == 0) && (num_hostrw_specs == 0)) {\n        flags |= CL_MEM_READ_WRITE;\n      }\n\n      if (((flags & CL_MEM_HOST_READ_ONLY) && (flags & CL_MEM_READ_ONLY)) ||\n          ((flags & CL_MEM_HOST_WRITE_ONLY) && (flags & CL_MEM_WRITE_ONLY)) ||\n          (num_hostrw_specs && (flags & CL_MEM_READ_WRITE))) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"Conflicting read/write flags specified\");\n      }\n    }\n  }\n\n  if (pipe_packet_size == 0) {\n    BAIL_INFO(CL_INVALID_PIPE_SIZE, context, \"Pipe packet size is zero\");\n  }\n  if (pipe_packet_size > acl_platform.pipe_max_packet_size) {\n    BAIL_INFO(CL_INVALID_PIPE_SIZE, context,\n              \"Pipe packet size exceeds maximum allowed\");\n  }\n\n  if (properties != NULL) {\n    BAIL_INFO(CL_INVALID_VALUE, context, \"Properties must be NULL for pipes\");\n  }\n\n  mem = acl_alloc_cl_mem();\n  if (!mem) {\n    BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n              \"Could not allocate a cl_mem object\");\n  }\n\n  acl_reset_ref_count(mem);\n\n  mem->mem_object_type = CL_MEM_OBJECT_PIPE;\n  mem->dispatch = &acl_icd_dispatch;\n  mem->allocation_deferred = 0;\n  mem->host_mem.aligned_ptr = NULL;\n  mem->host_mem.raw = NULL;\n  mem->mem_cpy_host_ptr_pending = 0;\n  mem->destructor_callback_list = NULL;\n  mem->block_allocation = NULL;\n  mem->writable_copy_on_host = 0;\n\n  mem->fields.pipe_objs.pipe_packet_size = pipe_packet_size;\n  mem->fields.pipe_objs.pipe_max_packets = pipe_max_packets;\n\n  mem->context = context;\n  mem->flags = flags;\n  mem->size = 0;\n\n  mem->host_pipe_info = NULL;\n\n  /* If it is a host pipe, populate the host pipe data structure in the pipe  */\n  if (flags & (CL_MEM_HOST_READ_ONLY | CL_MEM_HOST_WRITE_ONLY)) {\n    host_pipe_t *host_pipe_info;\n\n    host_pipe_info = acl_new<host_pipe_t>();\n    if (!host_pipe_info) {\n      acl_free_cl_mem(mem);\n      BAIL_INFO(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not allocate memory for internal data structure\");\n    }\n    host_pipe_info->m_physical_device_id = 0;\n    host_pipe_info->m_channel_handle = -1;\n    host_pipe_info->size_buffered = 0;\n    host_pipe_info->m_binded_kernel = 0;\n    host_pipe_info->binded = false;\n    host_pipe_info->host_pipe_channel_id = \"\";\n    acl_mutex_init(&(host_pipe_info->m_lock), NULL);\n\n    mem->host_pipe_info = host_pipe_info;\n  }\n\n  // push_back the new cl_pipe\n  context->pipe_vec.push_back(mem);\n\n  mem->mapping_count = 0;\n\n  acl_retain(mem);\n  acl_retain(context);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_MEM_OBJECT, mem);\n\n  return mem;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_mem CL_API_CALL\nclCreatePipe(cl_context context, cl_mem_flags flags, cl_uint pipe_packet_size,\n             cl_uint pipe_max_packets, const cl_pipe_properties *properties,\n             cl_int *errcode_ret) {\n  return clCreatePipeIntelFPGA(context, flags, pipe_packet_size,\n                               pipe_max_packets, properties, errcode_ret);\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL clGetPipeInfoIntelFPGA(\n    cl_mem pipe, cl_pipe_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  RESULT_INIT;\n\n  if (!acl_mem_is_valid(pipe)) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  // Wrong object type\n  if (pipe->mem_object_type != CL_MEM_OBJECT_PIPE) {\n    return CL_INVALID_MEM_OBJECT;\n  }\n\n  switch (param_name) {\n  case CL_PIPE_PACKET_SIZE:\n    RESULT_UINT(pipe->fields.pipe_objs.pipe_packet_size);\n    break;\n  case CL_PIPE_MAX_PACKETS:\n    RESULT_UINT(pipe->fields.pipe_objs.pipe_max_packets);\n    break;\n  default:\n    break;\n  }\n\n  if (result.size == 0) {\n    // We didn't implement the enum. Error out semi-gracefully.\n    return CL_INVALID_VALUE;\n  }\n\n  if (param_value) {\n    // Actually try to return the string.\n    if (param_value_size < result.size) {\n      // Buffer is too small to hold the return value.\n      return CL_INVALID_VALUE;\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL\nclGetPipeInfo(cl_mem pipe, cl_pipe_info param_name, size_t param_value_size,\n              void *param_value, size_t *param_value_size_ret) {\n  return clGetPipeInfoIntelFPGA(pipe, param_name, param_value_size, param_value,\n                                param_value_size_ret);\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL clEnqueueMigrateMemObjectsIntelFPGA(\n    cl_command_queue command_queue, cl_uint num_mem_objects,\n    const cl_mem *mem_objects, cl_mem_migration_flags flags,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  size_t i;\n  cl_int status;\n  cl_event local_event = 0;\n  unsigned int physical_id;\n  unsigned int mem_id;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (num_mem_objects == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Number of memory objects is zero\");\n  }\n  if (mem_objects == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Array of memory objects is NULL\");\n  }\n\n  for (i = 0; i < num_mem_objects; ++i) {\n    if (!acl_mem_is_valid(mem_objects[i])) {\n      return CL_INVALID_MEM_OBJECT;\n    }\n\n    if (command_queue->context != mem_objects[i]->context) {\n      return CL_INVALID_CONTEXT;\n    }\n  }\n\n  if (flags != 0 && (flags & ~(CL_MIGRATE_MEM_OBJECT_HOST |\n                               CL_MIGRATE_MEM_OBJECT_CONTENT_UNDEFINED))) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context, \"Invalid flags provided\");\n  }\n\n  physical_id = command_queue->device->def.physical_device_id;\n  // SVM memory is not associated with any device.\n  // Migration should only be moving device global memories.\n  // First find the largest mem object.\n  size_t mem_objects_largest_size = 0;\n  for (i = 0; i < num_mem_objects; ++i) {\n    if (mem_objects[i]->size > mem_objects_largest_size) {\n      mem_objects_largest_size = mem_objects[i]->size;\n    }\n  }\n  int tmp_mem_id = acl_get_fit_device_global_memory(command_queue->device->def,\n                                                    mem_objects_largest_size);\n  if (tmp_mem_id < 0) {\n    ERR_RET(CL_OUT_OF_RESOURCES, command_queue->context,\n            \"Can not find default global memory system\");\n  }\n  mem_id = (unsigned int)tmp_mem_id;\n\n  // Try to reserve space for all the buffers to be moved. If we fail, we need\n  // to know which buffers to deallocate:\n  std::vector<bool> needs_release_on_fail(num_mem_objects, false);\n\n  status = CL_SUCCESS;\n  for (i = 0; i < num_mem_objects; ++i) {\n    if (mem_objects[i]->reserved_allocations[physical_id].size() == 0) {\n      acl_resize_reserved_allocations_for_device(mem_objects[i],\n                                                 command_queue->device->def);\n    }\n    if (mem_objects[i]->reserved_allocations[physical_id][mem_id] == NULL) {\n      if (!acl_reserve_buffer_block(mem_objects[i],\n                                    &(acl_get_platform()->global_mem),\n                                    physical_id, mem_id)) {\n        status = CL_MEM_OBJECT_ALLOCATION_FAILURE;\n        break;\n      }\n      needs_release_on_fail[i] = true;\n    }\n    mem_objects[i]->reserved_allocations_count[physical_id][mem_id]++;\n  }\n\n  if (status != CL_SUCCESS) {\n    // We failed, release memory that was reserved:\n    for (i = 0; i < num_mem_objects; ++i) {\n      if (needs_release_on_fail[i]) {\n        remove_mem_block_linked_list(\n            mem_objects[i]->reserved_allocations[physical_id][mem_id]);\n        acl_free(mem_objects[i]->reserved_allocations[physical_id][mem_id]);\n        mem_objects[i]->reserved_allocations[physical_id][mem_id] = NULL;\n      }\n      mem_objects[i]->reserved_allocations_count[physical_id][mem_id]--;\n    }\n    return status;\n  }\n\n  // All space is reserved, create an event/command to actually move the data at\n  // the appropriate time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_MIGRATE_MEM_OBJECTS, &local_event);\n\n  if (status != CL_SUCCESS) {\n    return status; // already signalled callback\n  }\n\n  local_event->cmd.info.memory_migration.num_mem_objects = num_mem_objects;\n\n  if (local_event->cmd.info.memory_migration.src_mem_list) {\n    acl_mem_migrate_wrapper_t *new_src_mem_list =\n        (acl_mem_migrate_wrapper_t *)acl_realloc(\n            local_event->cmd.info.memory_migration.src_mem_list,\n            num_mem_objects * sizeof(acl_mem_migrate_wrapper_t));\n\n    if (!new_src_mem_list) {\n      return CL_OUT_OF_RESOURCES;\n    }\n\n    local_event->cmd.info.memory_migration.src_mem_list = new_src_mem_list;\n  } else {\n    local_event->cmd.info.memory_migration.src_mem_list =\n        (acl_mem_migrate_wrapper_t *)acl_malloc(\n            num_mem_objects * sizeof(acl_mem_migrate_wrapper_t));\n\n    if (!local_event->cmd.info.memory_migration.src_mem_list) {\n      return CL_OUT_OF_RESOURCES;\n    }\n  }\n\n  for (i = 0; i < num_mem_objects; ++i) {\n    local_event->cmd.info.memory_migration.src_mem_list[i].src_mem =\n        mem_objects[i];\n    local_event->cmd.info.memory_migration.src_mem_list[i]\n        .destination_physical_device_id = physical_id;\n    local_event->cmd.info.memory_migration.src_mem_list[i].destination_mem_id =\n        mem_id;\n\n    clRetainMemObject(mem_objects[i]);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT CL_API_ENTRY cl_int CL_API_CALL clEnqueueMigrateMemObjects(\n    cl_command_queue command_queue, cl_uint num_mem_objects,\n    const cl_mem *mem_objects, cl_mem_migration_flags flags,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n  return clEnqueueMigrateMemObjectsIntelFPGA(\n      command_queue, num_mem_objects, mem_objects, flags,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\n/**\n * Read <size> bytes of data from device global\n *\n * @param command_queue the queue system this command will belong\n * @param program contains the device global\n * @param name name of device global, used to look up for device global address\n * in autodiscovery string\n * @param blocking_read whether the operation is blocking or not\n * @param size number of bytes to read / write\n * @param offset offset from the extracted address of device global\n * @param ptr pointer that will hold the data copied from device global\n * @param num_events_in_wait_list number of event that device global read depend\n * on\n * @param event_wait_list events that device global read depend on\n * @param event the info about the execution of device global read will be\n * stored in the event\n * @return status code, CL_SUCCESS if all operations are successful.\n */\nACL_EXPORT\nCL_API_ENTRY cl_int clEnqueueReadGlobalVariableINTEL(\n    cl_command_queue command_queue, cl_program program, const char *name,\n    cl_bool blocking_read, size_t size, size_t offset, void *ptr,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n\n  acl_print_debug_msg(\"Entering clEnqueueReadGlobalVariableINTEL function.\\n\");\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE; // There is no context yet.\n  }\n\n  cl_int status = 0;\n\n  // Get context from program, command_queue and event\n  cl_context context = program->context;\n  cl_device_id device = command_queue->device;\n  unsigned int physical_device_id = device->def.physical_device_id;\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n\n  if (!acl_program_is_valid(program)) {\n    ERR_RET(CL_INVALID_PROGRAM, context, \"Invalid program was provided\");\n  }\n\n  if (ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid pointer was provided to host data\");\n  }\n\n  if (name == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid Device Global Name\");\n  }\n\n  if ((num_events_in_wait_list > 0 && !event_wait_list) ||\n      (!num_events_in_wait_list && event_wait_list)) {\n    ERR_RET(CL_INVALID_EVENT_WAIT_LIST, context,\n            \"Invalid event_wait_list or num_events_in_wait_list\");\n  }\n\n  uint64_t device_global_addr;\n\n  std::unordered_map<std::string, acl_device_global_mem_def_t> dev_global_map =\n      command_queue->device->loaded_bin->get_devdef()\n          .autodiscovery_def.device_global_mem_defs;\n\n  std::unordered_map<std::string, acl_device_global_mem_def_t>::const_iterator\n      dev_global = dev_global_map.find(std::string(name));\n\n  if (dev_global != dev_global_map.end()) {\n    device_global_addr = dev_global->second.address;\n  } else {\n    // For unit test purpose\n    // Device global name not found in kernel dev_bin, try to find in the sysdef\n    // setup by unit tests.\n    dev_global_map = acl_present_board_def()\n                         ->device[0]\n                         .autodiscovery_def.device_global_mem_defs;\n    dev_global = dev_global_map.find(std::string(name));\n    if (dev_global != dev_global_map.end()) {\n      device_global_addr = dev_global->second.address;\n      return CL_SUCCESS; // This is for unit test purpose. Unit test would stop\n                         // here.\n    } else {\n      acl_print_debug_msg(\"clEnqueueReadGlobalVariableINTEL Cannot find Device \"\n                          \"Global address from the name %s\\n\",\n                          name);\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"Cannot find Device Global address from the name\");\n    }\n  }\n  cl_event local_event = 0; // used for blocking\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_READ_GLOBAL_VARIABLE_INTEL, &local_event);\n\n  if (status != CL_SUCCESS)\n    return status;\n\n  local_event->cmd.info.device_global_info.offset = offset;\n  local_event->cmd.info.device_global_info.read_ptr = ptr;\n  local_event->cmd.info.device_global_info.device_global_addr =\n      device_global_addr;\n  local_event->cmd.info.device_global_info.name = name;\n  local_event->cmd.info.device_global_info.size = size;\n  local_event->cmd.info.device_global_info.physical_device_id =\n      physical_device_id;\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n  if (blocking_read) {\n    status = clWaitForEvents(1, &local_event);\n  }\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  acl_print_debug_msg(\"Exiting clEnqueueReadGlobalVariableINTEL function\\n\");\n  return CL_SUCCESS;\n}\n\n/**\n * Write <size> bytes of data from user provided host pointer into device global\n *\n * @param command_queue the queue system this command belongs\n * @param program contains device global write\n * @param name name of device global, used to look up for device global address\n * in autodiscovery string\n * @param blocking_write whether the operation is blocking or not\n * @param size number of bytes to read / write\n * @param offset offset from the extracted address of device global\n * @param ptr pointer that will hold the data copied from device global\n * @param num_events_in_wait_list number of event that device global write\n * depend on\n * @param event_wait_list events that device global read depend on\n * @param event the info about the execution of device global write will be\n * stored in the event\n * @return status code, CL_SUCCESS if all operations are successful.\n */\nACL_EXPORT\nCL_API_ENTRY cl_int clEnqueueWriteGlobalVariableINTEL(\n    cl_command_queue command_queue, cl_program program, const char *name,\n    cl_bool blocking_write, size_t size, size_t offset, const void *ptr,\n    cl_uint num_events_in_wait_list, const cl_event *event_wait_list,\n    cl_event *event) {\n\n  acl_print_debug_msg(\"Entering clEnqueueWriteGlobalVariableINTEL function\\n\");\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE; // There is no context yet.\n  }\n\n  cl_int status = 0;\n  // Get context from program, command_queue and event\n  cl_context context = program->context;\n  cl_device_id device = command_queue->device;\n  unsigned int physical_device_id = device->def.physical_device_id;\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n\n  if (!acl_program_is_valid(program)) {\n    ERR_RET(CL_INVALID_PROGRAM, context, \"Invalid program was provided\");\n  }\n\n  if (ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid pointer was provided to host data\");\n  }\n\n  if (name == NULL) {\n    ERR_RET(CL_INVALID_VALUE, context, \"Invalid Device Global Name\");\n  }\n\n  if ((num_events_in_wait_list > 0 && !event_wait_list) ||\n      (!num_events_in_wait_list && event_wait_list)) {\n    ERR_RET(CL_INVALID_EVENT_WAIT_LIST, context,\n            \"Invalid event_wait_list or num_events_in_wait_list\");\n  }\n\n  uint64_t device_global_addr;\n\n  std::unordered_map<std::string, acl_device_global_mem_def_t> dev_global_map =\n      command_queue->device->loaded_bin->get_devdef()\n          .autodiscovery_def.device_global_mem_defs;\n\n  std::unordered_map<std::string, acl_device_global_mem_def_t>::const_iterator\n      dev_global = dev_global_map.find(std::string(name));\n\n  if (dev_global != dev_global_map.end()) {\n    device_global_addr = dev_global->second.address;\n  } else {\n    // For unit test purpose\n    // Device global name not found in kernel dev_bin, try to find in the sysdef\n    // setup by unit tests.\n    dev_global_map = acl_present_board_def()\n                         ->device[0]\n                         .autodiscovery_def.device_global_mem_defs;\n    dev_global = dev_global_map.find(std::string(name));\n    if (dev_global != dev_global_map.end()) {\n      device_global_addr = dev_global->second.address;\n      return CL_SUCCESS; // This is for unit test purpose. Unit test would stop\n                         // here.\n    } else {\n      acl_print_debug_msg(\"clEnqueueWriteGlobalVariableINTEL Cannot find \"\n                          \"Device Global address from the name %s\\n\",\n                          name);\n      ERR_RET(CL_INVALID_ARG_VALUE, context,\n              \"Cannot find Device Global address from the name\");\n    }\n  }\n\n  cl_event local_event = 0; // used for blocking\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_WRITE_GLOBAL_VARIABLE_INTEL, &local_event);\n\n  if (status != CL_SUCCESS)\n    return status;\n\n  local_event->cmd.info.device_global_info.offset = offset;\n  local_event->cmd.info.device_global_info.write_ptr = ptr;\n  local_event->cmd.info.device_global_info.device_global_addr =\n      device_global_addr;\n  local_event->cmd.info.device_global_info.name = name;\n  local_event->cmd.info.device_global_info.size = size;\n  local_event->cmd.info.device_global_info.physical_device_id =\n      physical_device_id;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (blocking_write) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  acl_print_debug_msg(\"Exiting clEnqueueWriteGlobalVariableINTEL function\\n\");\n  return CL_SUCCESS;\n}\n\n//////////////////////////////\n// Internals\n\nstatic void l_get_working_range(const acl_block_allocation_t *block_allocation,\n                                unsigned physical_device_id,\n                                unsigned target_mem_id, unsigned bank_id,\n                                acl_addr_range_t *working_range,\n                                void **initial_try) {\n  acl_assert_locked();\n\n  if (block_allocation->region == &(acl_platform.global_mem)) {\n    const auto *global_mem_defs = &(acl_platform.device[physical_device_id]\n                                        .def.autodiscovery_def.global_mem_defs);\n\n    assert(target_mem_id < acl_platform.device[physical_device_id]\n                               .def.autodiscovery_def.num_global_mem_systems);\n    const auto &global_mem_def = (*global_mem_defs)[target_mem_id];\n\n    acl_addr_range_t physical_range = global_mem_def.range;\n    *working_range = global_mem_def.get_usable_range();\n    size_t num_banks = global_mem_def.num_global_banks;\n    num_banks = num_banks ? num_banks : 1;\n    // Bank size must be calculated using the physical size of the memory since\n    // it's a physical property of the memory.\n    size_t bank_size =\n        ((size_t)physical_range.next - (size_t)physical_range.begin) /\n        num_banks;\n\n    working_range->begin =\n        ACL_CREATE_DEVICE_ADDRESS(physical_device_id, working_range->begin);\n    working_range->next =\n        ACL_CREATE_DEVICE_ADDRESS(physical_device_id, working_range->next);\n    *initial_try = working_range->begin;\n\n    // If user requested a certain bank within global_mem, start there\n    // instead of working_range.begin. If in acl_allocate_block it doesn't find\n    // a free block somewhere between the requested bank and end of memory, it\n    // will try again from the beginning of memory. WARNING: Nothing prevents\n    // the block from straddling across two banks\n    if (bank_id > 0) {\n      // Bank start and end addresses are calculated using the physical_range\n      // since banks correspond to the physical layout of the memory.\n      *initial_try =\n          ACL_CREATE_DEVICE_ADDRESS(physical_device_id, physical_range.begin);\n      *initial_try = (void *)((size_t)(*initial_try) +\n                              ((bank_id - 1) % num_banks) * bank_size);\n      if (*initial_try < working_range->begin) {\n        // Some of the first bank's memory might not be usable so initial_try\n        // needs to be adjusted to the first usable memory address.\n        *initial_try = working_range->begin;\n      }\n    }\n  } else {\n    *working_range = block_allocation->region->range;\n    *initial_try = working_range->begin;\n  }\n}\n\n// Do device allocation on target device and memory.\n// Try allocating first on target DIMM, then try entire memory range.\nstatic int acl_allocate_block(acl_block_allocation_t *block_allocation,\n                              const cl_mem mem, unsigned physical_device_id,\n                              unsigned target_mem_id) {\n  acl_assert_locked();\n\n  int result = 0;\n  acl_addr_range_t where; // The candidate range of addresses\n  char trying_preferred_bank = 0;\n  char tried_preferred_bank = 0;\n\n  size_t alloc_size = mem->size;\n  // For images, need to allocate additional memory for meta data (width,\n  // height, etc)\n  if (is_image(mem)) {\n    alloc_size +=\n        get_offset_for_image_param(mem->context, mem->mem_object_type, \"data\");\n  }\n\n  // Find a free range inside the region.\n  // Must keep it aligned, so up the alloc_size if necessary.\n  if (alloc_size & (ACL_MEM_ALIGN - 1)) {\n    alloc_size += ACL_MEM_ALIGN - (alloc_size & (ACL_MEM_ALIGN - 1));\n  }\n\n  acl_addr_range_t working_range = {0, 0};\n  void *initial_try = NULL;\n\n  l_get_working_range(block_allocation, physical_device_id, target_mem_id,\n                      mem->bank_id, &working_range, &initial_try);\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"acl_allocate_block size:%zx, working_range:%zx - %zx, initial \"\n         \"try:%zx \\n\",\n         alloc_size, (size_t)(working_range.begin),\n         (size_t)(working_range.next), (size_t)initial_try);\n#endif\n\n  // Use a first-fit algorithm:  We walk the list of allocated blocks.\n  // They are stored in allocated-address order.\n  // In each iteration we look at the unallocated gap before the next\n  // allocated block.\n#define TRY_AT(P) (where.begin = (P)), (where.next = ((char *)P) + alloc_size)\n\n  if (initial_try != working_range.begin) {\n    trying_preferred_bank = 1;\n  }\n\n  TRY_AT(initial_try);\n\n  // The candidate range could overlap an allocated block. Traverse the\n  // allocation chain for this region to check that the candidate range does\n  // not overlap.\n  do {\n    acl_block_allocation_t **block_ptr =\n        &(block_allocation->region->first_block);\n    acl_block_allocation_t *block = *block_ptr;\n    while ((char *)working_range.next - (char *)where.next >= 0) {\n      // We haven't yet run off the end of the region. Keep going.\n      if (block == NULL) {\n        // No allocated buffers span higher addresses.\n        // So we're at the gap between the end of the last allocated\n        // block and the end of the region.\n        // Use this gap.\n        block_allocation->range = where;\n        block_allocation->next_block_in_region = NULL; // link out\n        *block_ptr = block_allocation;                 // link in\n        result = 1;                                    // Signal success.\n        break;\n      } else {\n        // Look at the gap before this allocated block.\n        // The first address of the gap is where.begin.\n        if ((char *)block->range.begin - (char *)where.next >= 0) {\n          // We found a gap big enough for the new allocation.\n          // Insert it here.\n          block_allocation->range = where;\n          block_allocation->next_block_in_region = block; // link out\n          *block_ptr = block_allocation;                  // link in\n          result = 1;\n          break;\n        } else {\n          if ((char *)block->range.next > (char *)where.begin) {\n            // If user specified a preferred bank, don't try any\n            // regions before its address.\n            TRY_AT(block->range.next);\n          }\n          block_ptr = &(block->next_block_in_region);\n          block = *block_ptr;\n        }\n      }\n    }\n\n    // If we started looking for memory in a preferred bank but didn't find\n    // anything, try again searching all of global memory.\n    TRY_AT(working_range.begin);\n    tried_preferred_bank = trying_preferred_bank;\n    trying_preferred_bank = 0;\n  } while (result == 0 && tried_preferred_bank);\n\n#ifdef MEM_DEBUG_MSG\n  printf(\n      \"acl_allocate_block finished: result:%i, block_allocation:%zx - %zx \\n\",\n      result, (size_t)(block_allocation->range.begin),\n      (size_t)(block_allocation->range.next));\n#endif\n  return result;\n}\n\nvoid acl_resize_reserved_allocations_for_device(cl_mem mem,\n                                                acl_device_def_t &def) {\n  acl_assert_locked();\n\n  unsigned int physical_device_id = def.physical_device_id;\n  unsigned int num_global_mem_systems =\n      def.autodiscovery_def.num_global_mem_systems;\n\n#ifdef MEM_DEBUG_MSG\n  printf(\n      \"resizing reserved_allocations, physical_device_id:%u, target_size:%u \\n\",\n      physical_device_id, num_global_mem_systems);\n#endif\n\n  mem->reserved_allocations[physical_device_id].resize(num_global_mem_systems);\n  mem->reserved_allocations_count[physical_device_id].resize(\n      num_global_mem_systems);\n}\n\ncl_int acl_reserve_buffer_block(cl_mem mem, acl_mem_region_t *region,\n                                unsigned physical_device_id,\n                                unsigned target_mem_id) {\n  acl_assert_locked();\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"acl_reserve_buffer_block mem:%zx, region:%zx, physical_device_id:%u, \"\n         \"target_mem_id:%u \\n\",\n         (size_t)mem, (size_t)(region), physical_device_id, target_mem_id);\n#endif\n\n  acl_block_allocation_t *block_allocation = acl_new<acl_block_allocation_t>();\n  if (block_allocation == nullptr)\n    return -1;\n  block_allocation->region = region;\n\n  // Can't call this function if there's already a reserved block for this\n  // device:\n  assert(mem->reserved_allocations[physical_device_id].size() > target_mem_id);\n  assert(mem->reserved_allocations[physical_device_id][target_mem_id] == NULL);\n  assert(mem->reserved_allocations_count[physical_device_id][target_mem_id] ==\n         0);\n\n  int result = acl_allocate_block(block_allocation, mem, physical_device_id,\n                                  target_mem_id);\n\n  // For images, copy the additional meta data (width, height, etc) after\n  // allocating\n  if (result && is_image(mem)) {\n    result = copy_image_metadata(mem);\n  }\n\n  if (!result) {\n    acl_delete(block_allocation);\n    return result;\n  }\n\n  mem->reserved_allocations[physical_device_id][target_mem_id] =\n      block_allocation;\n  block_allocation->mem_obj = mem;\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"acl_reserve_buffer_block finished block_allocation:%zx, range:%zx - \"\n         \"%zx \\n\",\n         (size_t)block_allocation, (size_t)(block_allocation->range.begin),\n         (size_t)(block_allocation->range.next));\n#endif\n  return result;\n}\n\nstatic int copy_image_metadata(cl_mem mem) {\n  acl_assert_locked();\n  {\n    cl_int errcode;\n    size_t element_size;\n    const acl_hal_t *hal = acl_get_hal();\n\n    element_size = acl_get_image_element_size(\n        mem->context, mem->fields.image_objs.image_format, &errcode);\n    if (errcode != CL_SUCCESS) {\n      return 0;\n    }\n\n    void *local_meta_data = malloc(\n        get_offset_for_image_param(mem->context, mem->mem_object_type, \"data\"));\n\n    safe_memcpy((char *)local_meta_data +\n                    get_offset_for_image_param(mem->context,\n                                               mem->mem_object_type, \"width\"),\n                &(mem->fields.image_objs.image_desc->image_width), 4, 4, 4);\n    if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D ||\n        mem->mem_object_type == CL_MEM_OBJECT_IMAGE3D ||\n        mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY) {\n      safe_memcpy((char *)local_meta_data +\n                      get_offset_for_image_param(\n                          mem->context, mem->mem_object_type, \"height\"),\n                  &(mem->fields.image_objs.image_desc->image_height), 4, 4, 4);\n    }\n    if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE3D) {\n      safe_memcpy((char *)local_meta_data +\n                      get_offset_for_image_param(mem->context,\n                                                 mem->mem_object_type, \"depth\"),\n                  &(mem->fields.image_objs.image_desc->image_depth), 4, 4, 4);\n    }\n    if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY ||\n        mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n      safe_memcpy((char *)local_meta_data +\n                      get_offset_for_image_param(\n                          mem->context, mem->mem_object_type, \"array_size\"),\n                  &(mem->fields.image_objs.image_desc->image_array_size), 4, 4,\n                  4);\n    }\n    safe_memcpy((char *)local_meta_data +\n                    get_offset_for_image_param(mem->context,\n                                               mem->mem_object_type,\n                                               \"channel_data_type\"),\n                &(mem->fields.image_objs.image_format->image_channel_data_type),\n                4, 4, 4);\n    safe_memcpy((char *)local_meta_data +\n                    get_offset_for_image_param(\n                        mem->context, mem->mem_object_type, \"channel_order\"),\n                &(mem->fields.image_objs.image_format->image_channel_order), 4,\n                4, 4);\n    safe_memcpy((char *)local_meta_data +\n                    get_offset_for_image_param(\n                        mem->context, mem->mem_object_type, \"element_size\"),\n                &(element_size), 8, 8, 8);\n\n    hal->copy_hostmem_to_globalmem(\n        0, local_meta_data, (char *)mem->block_allocation->range.begin,\n        get_offset_for_image_param(mem->context, mem->mem_object_type, \"data\"));\n\n    free(local_meta_data);\n  }\n  return 1;\n}\n\n// Remove the block from the region's allocation list.\n// (Need to keep this condition lined up with the behaviour\n// in clCreateBuffer.\nstatic void remove_mem_block_linked_list(acl_block_allocation_t *block) {\n  acl_block_allocation_t **region_block_ptr = &(block->region->first_block);\n  while (*region_block_ptr) {\n    if (*region_block_ptr == block) {\n      // Remove block from linked list\n      *region_block_ptr = block->next_block_in_region;\n      // Break loop since the block will appear only once\n      break;\n    }\n    // Advance to next cl_mem in the region\n    region_block_ptr = &((*region_block_ptr)->next_block_in_region);\n  }\n}\n\nvoid acl_mem_destructor_callback(cl_mem memobj) {\n  acl_mem_destructor_user_callback *cb_head, *temp;\n  acl_assert_locked();\n  // Call the notification function registered via\n  // clSetMemObjectDestructorCallback calls. All of the callbacks in the list\n  // are called in the reverse order they were registered. Note we can't check\n  // if acl_mem_is_valid here, since the ref. count is zero. The function\n  // calling this function is responsible for passing valid mem objects.\n  cb_head = memobj->destructor_callback_list;\n  while (cb_head) {\n    acl_mem_destructor_notify_fn_t mem_destructor_notify_fn =\n        cb_head->mem_destructor_notify_fn;\n    void *notify_user_data = cb_head->notify_user_data;\n    // Removing that callback from the list and calling it.\n    temp = cb_head;\n    cb_head = cb_head->next;\n    memobj->destructor_callback_list = cb_head;\n    acl_free(temp);\n    {\n      acl_suspend_lock_guard lock{acl_mutex_wrapper};\n      mem_destructor_notify_fn(memobj, notify_user_data);\n    }\n  }\n}\n\nint acl_mem_is_valid(cl_mem mem) {\n  acl_assert_locked();\n#ifdef REMOVE_VALID_CHECKS\n  return 1;\n#else\n  if (!acl_is_valid_ptr(mem)) {\n    return 0;\n  }\n  if (!acl_ref_count(mem)) {\n    return 0;\n  }\n  if (!acl_context_is_valid(mem->context)) {\n    return 0;\n  }\n  if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER &&\n      acl_ref_count(mem) <= mem->fields.buffer_objs.num_subbuffers) {\n    return 0;\n  }\n  return 1;\n#endif\n}\n\n// Iterate through device's global memories and return the ID of the first\n// device private global memory\n// TODO: Used for ARM board device query, need to verify correctness\nint acl_get_default_device_global_memory(const acl_device_def_t &dev) {\n  int lowest_gmem_idx = -1;\n  cl_ulong lowest_gmem_begin = 0xFFFFFFFFFFFFFFFF;\n  acl_assert_locked();\n\n  // If the device has no physical memory then clCreateBuffer will fall back to\n  // allocating device global memory in the default memory.\n  if (!acl_svm_device_supports_physical_memory(dev.physical_device_id))\n    return acl_get_default_memory(dev);\n  for (unsigned gmem_idx = 0;\n       gmem_idx < dev.autodiscovery_def.num_global_mem_systems; gmem_idx++) {\n    if (dev.autodiscovery_def.global_mem_defs[gmem_idx].type ==\n            ACL_GLOBAL_MEM_DEVICE_PRIVATE &&\n        (size_t)dev.autodiscovery_def.global_mem_defs[gmem_idx].range.begin <\n            lowest_gmem_begin) {\n      lowest_gmem_begin =\n          (size_t)dev.autodiscovery_def.global_mem_defs[gmem_idx].range.begin;\n      lowest_gmem_idx = static_cast<int>(gmem_idx);\n    }\n  }\n\n  // This can return -1, but that means there's no device private memory\n  return lowest_gmem_idx;\n}\n\n// Iterate through device's global memories and return the ID of the first\n// device private global memory that has enough capacity to fit the allocation.\n// This is needed over acl_get_default_memory(dev) because device can have\n// both device private and shared virtual memory.\n// Returns the id of the memory that clCreateBuffer would allocate in by default\n// (i.e. when CL_MEM_USE_HOST_PTR is not used) or -1 if no such memory exists.\nint acl_get_fit_device_global_memory(const acl_device_def_t &dev,\n                                     const size_t size) {\n  int lowest_gmem_idx = -1;\n  cl_ulong lowest_gmem_begin = 0xFFFFFFFFFFFFFFFF;\n  acl_assert_locked();\n\n  // If the device has no physical memory then clCreateBuffer will fall back to\n  // allocating device global memory in the default memory.\n  if (!acl_svm_device_supports_physical_memory(dev.physical_device_id))\n    return acl_get_default_memory(dev);\n  for (unsigned gmem_idx = 0;\n       gmem_idx < dev.autodiscovery_def.num_global_mem_systems; gmem_idx++) {\n    acl_system_global_mem_allocation_type_t alloc_type =\n        dev.autodiscovery_def.global_mem_defs[gmem_idx].allocation_type;\n    bool is_device_alloc =\n        !alloc_type || (alloc_type & ACL_GLOBAL_MEM_DEVICE_ALLOCATION);\n    bool is_device_private =\n        dev.autodiscovery_def.global_mem_defs[gmem_idx].type ==\n        ACL_GLOBAL_MEM_DEVICE_PRIVATE;\n    if (is_device_private && is_device_alloc &&\n        (size_t)dev.autodiscovery_def.global_mem_defs[gmem_idx].range.begin <\n            lowest_gmem_begin &&\n        ACL_RANGE_SIZE(dev.autodiscovery_def.global_mem_defs[gmem_idx]\n                           .get_usable_range()) >= size) {\n      lowest_gmem_begin =\n          (size_t)dev.autodiscovery_def.global_mem_defs[gmem_idx].range.begin;\n      lowest_gmem_idx = static_cast<int>(gmem_idx);\n    }\n  }\n\n  // This can return -1, but that means there's no device private memory\n  return lowest_gmem_idx;\n}\n\n// Memory systems are listed with the default memory at the first index\nint acl_get_default_memory(const acl_device_def_t &) { return 0; }\n\nstatic void *l_get_address_of_writable_copy(cl_mem mem,\n                                            unsigned int physical_device_id,\n                                            int *on_host_ptr,\n                                            cl_bool is_dest_unmap) {\n  acl_assert_locked();\n  // If this is an SVM buffer and the device supports SVM then the user expects\n  // us to use SVM. Use the host memory.\n  if (mem->is_svm && acl_svm_device_supports_any_svm(physical_device_id)) {\n    if (on_host_ptr)\n      (*on_host_ptr) = 1;\n    return mem->host_mem.aligned_ptr;\n    // If the device only supports SVM, we have to use SVM. Use the host memory.\n  } else if (!acl_svm_device_supports_physical_memory(physical_device_id)) {\n    if (mem->block_allocation->region->is_host_accessible) {\n      // We never automatically map host buffers to the device, so the writable\n      // copy is always on the host.\n      // Need to use range.begin instead of host_mem.aligned_ptr in case either\n      //    (a) this buffer was created with CL_MEM_USE_HOST_PTR,\n      // or (b) ACL is not using host malloc.\n      if (on_host_ptr)\n        (*on_host_ptr) = 1;\n      return mem->block_allocation->range.begin;\n    } else {\n      if (on_host_ptr)\n        (*on_host_ptr) = 1;\n      return mem->host_mem.aligned_ptr;\n    }\n  } else {\n    // When unmapping a mem object, always take the device address\n    if (is_dest_unmap) {\n      if (on_host_ptr)\n        (*on_host_ptr) = 0;\n      return mem->block_allocation->range.begin;\n    } else {\n      if (mem->block_allocation->region->is_host_accessible) {\n        // We never automatically map host buffers to the device, so the\n        // writable copy is always on the host. Need to use range.begin instead\n        // of host_mem.aligned_ptr in case either\n        //    (a) this buffer was created with CL_MEM_USE_HOST_PTR,\n        // or (b) ACL is not using host malloc.\n        if (on_host_ptr)\n          (*on_host_ptr) = 1;\n        return mem->block_allocation->range.begin;\n      } else if (mem->writable_copy_on_host) {\n        // It's home is on the device, but it's mapped to the host right now.\n        if (on_host_ptr)\n          (*on_host_ptr) = 1;\n        return mem->host_mem.aligned_ptr;\n      } else {\n        // It's home is on the device, and it's not mapped to the host.\n        if (on_host_ptr)\n          (*on_host_ptr) = 0;\n        return mem->block_allocation->range.begin;\n      }\n    }\n  }\n}\n\n// clEnqueueReadBuffer and clEnqueueWriteBuffer are almost entirely the\n// same.\ncl_int l_enqueue_mem_transfer(cl_command_queue command_queue, cl_bool blocking,\n                              cl_mem src_buffer, size_t src_offset[3],\n                              size_t src_row_pitch, size_t src_slice_pitch,\n                              cl_mem dst_buffer, size_t dst_offset[3],\n                              size_t dst_row_pitch, size_t dst_slice_pitch,\n                              size_t cb[3], cl_uint num_events,\n                              const cl_event *events, cl_event *event,\n                              cl_command_type type, cl_map_flags map_flags) {\n  acl_assert_locked();\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  cl_context context = command_queue->context;\n\n  if ((num_events > 0 && !events) || (!num_events && events)) {\n    ERR_RET(CL_INVALID_EVENT_WAIT_LIST, context,\n            \"Invalid event_wait_list or num_events_in_wait_list\");\n  }\n\n  if (!acl_mem_is_valid(src_buffer))\n    ERR_RET(CL_INVALID_MEM_OBJECT, context, \"Source buffer is invalid\");\n  if (!acl_mem_is_valid(dst_buffer))\n    ERR_RET(CL_INVALID_MEM_OBJECT, context, \"Destination buffer is invalid\");\n\n  if (command_queue->context != src_buffer->context)\n    ERR_RET(CL_INVALID_CONTEXT, context,\n            \"Source buffer is not in the same context as the command queue\");\n  if (command_queue->context != dst_buffer->context)\n    ERR_RET(\n        CL_INVALID_CONTEXT, context,\n        \"Destination buffer is not in the same context as the command queue\");\n\n  if (cb[0] == 0)\n    ERR_RET(CL_INVALID_VALUE, context, \"Region 'x' size is zero\");\n  if (cb[1] == 0)\n    ERR_RET(CL_INVALID_VALUE, context, \"Region 'y' size is zero\");\n  if (cb[2] == 0)\n    ERR_RET(CL_INVALID_VALUE, context, \"Region 'z' size is zero\");\n\n  // Check that the requested operation is compatible with the flags the src/dst\n  // buffers were created with. Checks for image related operations have not\n  // been implemented.\n  switch (type) {\n  case CL_COMMAND_READ_BUFFER:\n  case CL_COMMAND_READ_BUFFER_RECT:\n    if (src_buffer->flags & CL_MEM_HOST_WRITE_ONLY ||\n        src_buffer->flags & CL_MEM_HOST_NO_ACCESS) {\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"clEnqueueReadBuffer cannot be called on a buffer \"\n              \"created with CL_MEM_HOST_WRITE_ONLY or CL_MEM_HOST_NO_ACCESS\");\n    }\n    break;\n  case CL_COMMAND_WRITE_BUFFER:\n  case CL_COMMAND_WRITE_BUFFER_RECT:\n    if (dst_buffer->flags & CL_MEM_COPY_HOST_PTR &&\n        dst_buffer->copy_host_ptr_skip_check) {\n      // Don't block the call to clEnqueueWriteBuffer when we're initializing\n      // a read-only or no-access buffer created with CL_MEM_COPY_HOST_PTR since\n      // it's an internal usage of the API.\n      dst_buffer->copy_host_ptr_skip_check = CL_FALSE;\n      break;\n    }\n\n    if (dst_buffer->flags & CL_MEM_HOST_READ_ONLY ||\n        dst_buffer->flags & CL_MEM_HOST_NO_ACCESS) {\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"clEnqueueWriteBuffer cannot be called on a buffer \"\n              \"created with CL_MEM_HOST_READ_ONLY or CL_MEM_HOST_NO_ACCESS\");\n    }\n    break;\n  case CL_COMMAND_COPY_BUFFER:\n  case CL_COMMAND_COPY_BUFFER_RECT:\n  case CL_COMMAND_FILL_BUFFER:\n  case CL_COMMAND_UNMAP_MEM_OBJECT:\n    break;\n  case CL_COMMAND_MAP_BUFFER:\n    if ((src_buffer->flags & CL_MEM_HOST_WRITE_ONLY ||\n         src_buffer->flags & CL_MEM_HOST_NO_ACCESS) &&\n        map_flags & CL_MAP_READ) {\n      ERR_RET(\n          CL_INVALID_OPERATION, context,\n          \"clEnqeueueMapBuffer with CL_MAP_READ cannot be called on a buffer \"\n          \"created with CL_MEM_HOST_WRITE_ONLY or CL_MEM_HOST_NO_ACCESS\");\n    }\n\n    if ((src_buffer->flags & CL_MEM_HOST_READ_ONLY ||\n         src_buffer->flags & CL_MEM_HOST_NO_ACCESS) &&\n        (map_flags & CL_MAP_WRITE ||\n         map_flags & CL_MAP_WRITE_INVALIDATE_REGION)) {\n      ERR_RET(CL_INVALID_OPERATION, context,\n              \"clEnqeueueMapBuffer with CL_MAP_WRITE or \"\n              \"CL_MAP_WRITE_INVALIDATE_REGION \"\n              \"cannot be called on a buffer created with CL_MEM_HOST_READ_ONLY \"\n              \"or CL_MEM_HOST_NO_ACCESS\");\n    }\n    break;\n  case CL_COMMAND_READ_IMAGE:\n  case CL_COMMAND_WRITE_IMAGE:\n  case CL_COMMAND_COPY_IMAGE:\n  case CL_COMMAND_COPY_IMAGE_TO_BUFFER:\n  case CL_COMMAND_COPY_BUFFER_TO_IMAGE:\n  case CL_COMMAND_MAP_IMAGE:\n  case CL_COMMAND_FILL_IMAGE:\n#ifndef ACL_SUPPORT_IMAGES\n    ERR_RET(CL_INVALID_OPERATION, context, \"Device does not support images\");\n#endif\n    break;\n  default:\n    assert(0 && \"Command is not a memory transfer related command type\");\n  }\n\n  cl_int errcode_ret;\n  size_t src_element_size;\n\n  // Check that the copy area of the source buffer is within the allocated\n  // memory\n  switch (src_buffer->mem_object_type) {\n    size_t default_slice_pitch;\n  case CL_MEM_OBJECT_BUFFER:\n    if ((src_offset[2] + cb[2] - 1) * src_slice_pitch +\n            (src_offset[1] + cb[1] - 1) * src_row_pitch + src_offset[0] +\n            cb[0] >\n        src_buffer->size)\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Source buffer offset plus byte count exceeds source buffer size\");\n    if (src_row_pitch != 0 && src_row_pitch < cb[0])\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source buffer row pitch is less than region 'x' size\");\n\n    if (dst_buffer->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n      default_slice_pitch = src_row_pitch;\n    } else {\n      default_slice_pitch = cb[1] * src_row_pitch;\n    }\n    if (src_slice_pitch != 0 && src_slice_pitch < default_slice_pitch) {\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Source buffer slice pitch is less than region 'y' size * row pitch\");\n    }\n    break;\n  case CL_MEM_OBJECT_IMAGE1D:\n  case CL_MEM_OBJECT_IMAGE1D_BUFFER:\n    src_element_size = acl_get_image_element_size(\n        src_buffer->context, src_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (src_offset[0] + cb[0] >\n        src_buffer->fields.image_objs.image_desc->image_width *\n            src_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image width\");\n    if (src_offset[1] != 0 || src_offset[2] != 0)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Non-zero offset specified for invalid index in source image\");\n    if (cb[1] != 1 || cb[2] != 1)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Non-one region specified for invalid index in source image\");\n    break;\n  case CL_MEM_OBJECT_IMAGE1D_ARRAY:\n    src_element_size = acl_get_image_element_size(\n        src_buffer->context, src_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (src_offset[0] + cb[0] >\n        src_buffer->fields.image_objs.image_desc->image_width *\n            src_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image width\");\n    if (src_offset[1] + cb[1] >\n        src_buffer->fields.image_objs.image_desc->image_array_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image array size\");\n    if (src_offset[2] != 0)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Non-zero offset specified for invalid index in source image\");\n    if (cb[2] != 1)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Non-one region specified for invalid index in source image\");\n    break;\n  case CL_MEM_OBJECT_IMAGE2D:\n    src_element_size = acl_get_image_element_size(\n        src_buffer->context, src_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (src_offset[0] + cb[0] >\n        src_buffer->fields.image_objs.image_desc->image_width *\n            src_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image width\");\n    if (src_offset[1] + cb[1] >\n        src_buffer->fields.image_objs.image_desc->image_height)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image height\");\n    if (src_offset[2] != 0)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Non-zero offset specified for invalid index in source image\");\n    if (cb[2] != 1)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Non-one region specified for invalid index in source image\");\n    break;\n  case CL_MEM_OBJECT_IMAGE2D_ARRAY:\n    src_element_size = acl_get_image_element_size(\n        src_buffer->context, src_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (src_offset[0] + cb[0] >\n        src_buffer->fields.image_objs.image_desc->image_width *\n            src_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image width\");\n    if (src_offset[1] + cb[1] >\n        src_buffer->fields.image_objs.image_desc->image_height)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image height\");\n    if (src_offset[2] + cb[2] >\n        src_buffer->fields.image_objs.image_desc->image_array_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image array size\");\n    break;\n  case CL_MEM_OBJECT_IMAGE3D:\n    src_element_size = acl_get_image_element_size(\n        src_buffer->context, src_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (src_offset[0] + cb[0] >\n        src_buffer->fields.image_objs.image_desc->image_width *\n            src_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image width\");\n    if (src_offset[1] + cb[1] >\n        src_buffer->fields.image_objs.image_desc->image_height)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image height\");\n    if (src_offset[2] + cb[2] >\n        src_buffer->fields.image_objs.image_desc->image_depth)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Source image offset plus region exceeds image depth\");\n    break;\n  default:\n    std::stringstream ss;\n    ss << \"Memory transfers of source mem object type not supported 0x\"\n       << std::hex << src_buffer->mem_object_type;\n    ERR_RET(CL_INVALID_VALUE, context, ss.str().c_str());\n    break;\n  }\n\n  size_t dst_element_size;\n\n  // Check that the copy area of the destination buffer is within the allocated\n  // memory\n  switch (dst_buffer->mem_object_type) {\n  case CL_MEM_OBJECT_BUFFER:\n    if ((dst_offset[2] + cb[2] - 1) * dst_slice_pitch +\n            (dst_offset[1] + cb[1] - 1) * dst_row_pitch + dst_offset[0] +\n            cb[0] >\n        dst_buffer->size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination buffer offset plus byte count exceeds destination \"\n              \"buffer size\");\n    if (dst_row_pitch != 0 && dst_row_pitch < cb[0])\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination buffer row pitch is less than region 'x' size\");\n    if (dst_slice_pitch != 0 && dst_slice_pitch < cb[1] * dst_row_pitch)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination buffer slice pitch is less than region 'y' size * \"\n              \"row pitch\");\n    break;\n  case CL_MEM_OBJECT_IMAGE1D:\n  case CL_MEM_OBJECT_IMAGE1D_BUFFER:\n    dst_element_size = acl_get_image_element_size(\n        dst_buffer->context, dst_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (dst_offset[0] + cb[0] >\n        dst_buffer->fields.image_objs.image_desc->image_width *\n            dst_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image width\");\n    if (dst_offset[1] != 0 || dst_offset[2] != 0)\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Non-zero offset specified for invalid index in destination image\");\n    if (cb[1] != 1 || cb[2] != 1)\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Non-one region specified for invalid index in destination image\");\n    break;\n  case CL_MEM_OBJECT_IMAGE1D_ARRAY:\n    dst_element_size = acl_get_image_element_size(\n        dst_buffer->context, dst_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (dst_offset[0] + cb[0] >\n        dst_buffer->fields.image_objs.image_desc->image_width *\n            dst_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image width\");\n    if (dst_offset[1] + cb[1] >\n        dst_buffer->fields.image_objs.image_desc->image_array_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image array size\");\n    if (dst_offset[2] != 0)\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Non-zero offset specified for invalid index in destination image\");\n    if (cb[2] != 1)\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Non-one region specified for invalid index in destination image\");\n    break;\n  case CL_MEM_OBJECT_IMAGE2D:\n    dst_element_size = acl_get_image_element_size(\n        dst_buffer->context, dst_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (dst_offset[0] + cb[0] >\n        dst_buffer->fields.image_objs.image_desc->image_width *\n            dst_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image width\");\n    if (dst_offset[1] + cb[1] >\n        dst_buffer->fields.image_objs.image_desc->image_height)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image height\");\n    if (dst_offset[2] != 0)\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Non-zero offset specified for invalid index in destination image\");\n    if (cb[2] != 1)\n      ERR_RET(\n          CL_INVALID_VALUE, context,\n          \"Non-one region specified for invalid index in destination image\");\n    break;\n  case CL_MEM_OBJECT_IMAGE2D_ARRAY:\n    dst_element_size = acl_get_image_element_size(\n        dst_buffer->context, dst_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (dst_offset[0] + cb[0] >\n        dst_buffer->fields.image_objs.image_desc->image_width *\n            dst_element_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image width\");\n    if (dst_offset[1] + cb[1] >\n        dst_buffer->fields.image_objs.image_desc->image_height)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image height\");\n    if (dst_offset[2] + cb[2] >\n        dst_buffer->fields.image_objs.image_desc->image_array_size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image array size\");\n    break;\n  case CL_MEM_OBJECT_IMAGE3D:\n    dst_element_size = acl_get_image_element_size(\n        dst_buffer->context, dst_buffer->fields.image_objs.image_format,\n        &errcode_ret);\n    if (errcode_ret != CL_SUCCESS) {\n      return errcode_ret;\n    }\n    if (dst_offset[0] + cb[0] >\n        dst_buffer->fields.image_objs.image_desc->image_width *\n            dst_element_size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image width\");\n    }\n    if (dst_offset[1] + cb[1] >\n        dst_buffer->fields.image_objs.image_desc->image_height) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image height\");\n    }\n    if (dst_offset[2] + cb[2] >\n        dst_buffer->fields.image_objs.image_desc->image_depth) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Destination image offset plus region exceeds image depth\");\n    }\n    break;\n  default:\n    std::stringstream ss;\n    ss << \"Memory transfers of destination mem object type not supported 0x\"\n       << std::hex << dst_buffer->mem_object_type;\n    ERR_RET(CL_INVALID_VALUE, context, ss.str().c_str());\n    break;\n  }\n\n  cl_event local_event = 0; // used for blocking\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  cl_int status =\n      acl_create_event(command_queue, num_events, events, type, &local_event);\n  if (status != CL_SUCCESS)\n    return status; // already signalled callback\n  local_event->cmd.info.mem_xfer.src_mem = src_buffer;\n  clRetainMemObject(src_buffer);\n  local_event->cmd.info.mem_xfer.dst_mem = dst_buffer;\n  clRetainMemObject(dst_buffer);\n  for (size_t i = 0; i < 3; ++i) {\n    local_event->cmd.info.mem_xfer.src_offset[i] = src_offset[i];\n    local_event->cmd.info.mem_xfer.dst_offset[i] = dst_offset[i];\n    local_event->cmd.info.mem_xfer.cb[i] = cb[i];\n  }\n  local_event->cmd.info.mem_xfer.src_row_pitch = src_row_pitch;\n  local_event->cmd.info.mem_xfer.src_slice_pitch = src_slice_pitch;\n  local_event->cmd.info.mem_xfer.dst_row_pitch = dst_row_pitch;\n  local_event->cmd.info.mem_xfer.dst_slice_pitch = dst_slice_pitch;\n  local_event->cmd.info.mem_xfer.map_flags = map_flags;\n  local_event->cmd.info.mem_xfer.is_auto_map = 0;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (blocking) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n\n  if (blocking && status == CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST)\n    return status;\n\n  return CL_SUCCESS;\n}\n\nvoid acl_forcibly_release_all_memory_for_context(cl_context context) {\n  acl_assert_locked();\n  acl_forcibly_release_all_memory_for_context_in_region(\n      context, &acl_platform.host_auto_mem);\n  acl_forcibly_release_all_memory_for_context_in_region(\n      context, &acl_platform.host_user_mem);\n  acl_forcibly_release_all_memory_for_context_in_region(context,\n                                                        context->global_mem);\n}\n\nstatic void acl_forcibly_release_all_memory_for_context_in_region(\n    cl_context context, acl_mem_region_t *region) {\n  // Remove all memory blocks associated with the given context from the\n  // given region.\n\n  acl_block_allocation_t **block_ptr = &(region->first_block);\n  acl_assert_locked();\n\n  while (*block_ptr) {\n    acl_block_allocation_t *block = *block_ptr;\n    // Calling the user registered callbacks for when memory is destroyed,\n    // before freeing resources.\n    acl_mem_destructor_callback(block->mem_obj);\n\n    if (block->mem_obj->context == context) {\n      // This mem is associated with the given context.\n\n      // Change pointer to instead point to the next memory in order to\n      // remove this memory from the linked list\n      *block_ptr = block->next_block_in_region;\n\n      // Avoid memory leak of host mallocs.\n      if (block->mem_obj->host_mem.raw) {\n        acl_mem_aligned_free(block->mem_obj->context,\n                             &block->mem_obj->host_mem);\n      }\n\n      acl_free_cl_mem(block->mem_obj);\n\n    } else {\n      // Advance to next block on the allocation list.\n      block_ptr = &(block->next_block_in_region);\n    }\n  }\n}\n\n//////////////////////////////\n// Internals -- Command completion\n\nacl_aligned_ptr_t acl_mem_aligned_malloc(size_t size) {\n  acl_aligned_ptr_t result;\n\n  // Use malloc\n  // Allocate something slightly larger to ensure alignment.\n  size_t offset;\n  const size_t alloc_size = size + ACL_MEM_ALIGN;\n\n  acl_assert_locked();\n\n  result.size = size;\n  result.alignment = ACL_MEM_ALIGN;\n  if (alloc_size < size) // watch for wraparound!\n    return result;\n  result.raw = acl_malloc(alloc_size);\n\n  if (result.raw == 0)\n    return result;\n  offset = ((uintptr_t)(char *)result.raw) & (ACL_MEM_ALIGN - 1);\n  if (offset)\n    offset = ACL_MEM_ALIGN - offset;\n  result.aligned_ptr = (void *)(((char *)result.raw) + offset);\n  result.size = alloc_size;\n  return result;\n}\n\nvoid acl_mem_aligned_free(cl_context context, acl_aligned_ptr_t *ptr) {\n  acl_assert_locked();\n\n  if (ptr->device_addr != 0L) {\n    acl_get_hal()->legacy_shared_free(context, ptr->raw, ptr->size);\n  } else {\n    acl_free(ptr->raw);\n  }\n  *ptr = acl_aligned_ptr_t();\n}\n\n// Map a buffer into host memory.\n// Return 1 if we made forward progress, 0 otherwise.\nint acl_mem_map_buffer(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n\n  if (event->cmd.trivial) {\n    // The buffer is defined to always be host accessible.\n    // So just count the mappings.\n    cl_mem mem = event->cmd.info.trivial_mem_mapping.mem;\n    acl_set_execution_status(event, CL_SUBMITTED);\n    acl_set_execution_status(event, CL_RUNNING);\n    mem->mapping_count++;\n    acl_set_execution_status(event, CL_COMPLETE);\n    acl_print_debug_msg(\"mem[%p] map trivial. refcount %u\\n\", mem,\n                        acl_ref_count(mem));\n    result = 1;\n  } else {\n    // Otherwise we might have to move data.\n    result = acl_submit_mem_transfer_device_op(event);\n  }\n  return result;\n}\n\n// Unmap a buffer from host memory.\n// Return 1 if we made forward progress, 0 otherwise.\nint acl_mem_unmap_mem_object(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n\n  if (event->cmd.trivial) {\n    // We only grant the Map request if the buffer is already in the host\n    // address space.\n    // So just count the mappings.\n    cl_mem mem = event->cmd.info.trivial_mem_mapping.mem;\n    acl_set_execution_status(event, CL_SUBMITTED);\n    acl_set_execution_status(event, CL_RUNNING);\n    mem->mapping_count--;\n    acl_print_debug_msg(\"mem[%p] unmap trivial ->refcount %u\\n\", mem,\n                        acl_ref_count(mem));\n    acl_set_execution_status(event, CL_COMPLETE);\n    result = 1;\n  } else {\n    // Otherwise we might have to move data.\n    result = acl_submit_mem_transfer_device_op(event);\n  }\n  return result;\n}\n\n// Submit an op to the device op queue to copy memory.\n// Return 1 if we made forward progress, 0 otherwise.\nint acl_submit_mem_transfer_device_op(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n\n  // No user-level scheduling blocks this memory transfer.\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  if (event->last_device_op) {\n    return result;\n  }\n\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n  acl_device_op_t *last_op = 0;\n  int src_on_host;\n  int dst_on_host;\n\n  // Precautionary, but it also nudges the device scheduler to try\n  // to free up old operation slots.\n  acl_forget_proposed_device_ops(doq);\n\n  // Figure out where the memory is and going\n  // Similar check is done in l_mem_transfer_buffer_explicitly\n  l_get_address_of_writable_copy(\n      event->cmd.info.mem_xfer.src_mem,\n      event->command_queue->device->def.physical_device_id, &src_on_host,\n      CL_FALSE);\n  l_get_address_of_writable_copy(\n      event->cmd.info.mem_xfer.dst_mem,\n      event->command_queue->device->def.physical_device_id, &dst_on_host,\n      (cl_bool)(event->cmd.type == CL_COMMAND_UNMAP_MEM_OBJECT));\n  if (src_on_host) {\n    if (dst_on_host) {\n      last_op =\n          acl_propose_device_op(doq, ACL_DEVICE_OP_MEM_TRANSFER_COPY, event);\n    } else {\n      last_op =\n          acl_propose_device_op(doq, ACL_DEVICE_OP_MEM_TRANSFER_WRITE, event);\n    }\n  } else {\n    if (dst_on_host) {\n      last_op =\n          acl_propose_device_op(doq, ACL_DEVICE_OP_MEM_TRANSFER_READ, event);\n    } else {\n      last_op =\n          acl_propose_device_op(doq, ACL_DEVICE_OP_MEM_TRANSFER_COPY, event);\n    }\n  }\n\n  if (last_op) {\n    // We managed to enqueue everything.\n    event->last_device_op = last_op;\n    acl_commit_proposed_device_ops(doq);\n    result = 1;\n  } else {\n    // Back off, and wait until later when we have more space in the\n    // device op queue.\n    acl_forget_proposed_device_ops(doq);\n  }\n  return result;\n}\n\nint acl_is_device_accessible(cl_mem mem) {\n  if (mem->mem_object_type == CL_MEM_OBJECT_PIPE) {\n    return 0;\n  }\n\n#if defined(ACL_HOST_MEMORY_SHARED)\n  if (mem->block_allocation->region->is_user_provided) {\n    // even though the memory host_ptr points to is physically accessible\n    // to device, it's not usable because it was probably allocated with\n    // malloc(), hence it's not paged and not physically contiguous.\n    return 0;\n  }\n#endif\n\n  return mem->block_allocation->region->is_device_accessible;\n}\n\nACL_EXPORT\nvoid acl_mem_transfer_buffer(void *user_data, acl_device_op_t *op) {\n  cl_event event = op->info.event;\n  acl_assert_locked();\n\n  user_data = user_data; // Only used by the test mock.\n  if (acl_event_is_valid(event) &&\n      acl_command_queue_is_valid(event->command_queue)) {\n    cl_context context = event->command_queue->context;\n    l_mem_transfer_buffer_explicitly(\n        context, op, event->command_queue->device->def.physical_device_id,\n        event->cmd);\n  } else {\n    acl_set_device_op_execution_status(op, -1);\n  }\n}\n\nACL_EXPORT\nvoid acl_mem_migrate_buffer(void *user_data, acl_device_op_t *op) {\n#ifdef MEM_DEBUG_MSG\n  printf(\"acl_mem_migrate_buffer\\n\");\n#endif\n\n  cl_event event = op->info.event;\n\n  user_data = user_data; // Disable compile warnings.\n\n  acl_set_device_op_execution_status(op, CL_SUBMITTED);\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  if (!acl_event_is_valid(event) ||\n      !acl_command_queue_is_valid(event->command_queue)) {\n    acl_set_device_op_execution_status(op, -1);\n  }\n  acl_mem_migrate_t memory_migration;\n  unsigned int physical_id =\n      event->command_queue->device->def.physical_device_id;\n  cl_bool device_supports_any_svm =\n      acl_svm_device_supports_any_svm(physical_id);\n  cl_bool device_supports_physical_memory =\n      acl_svm_device_supports_physical_memory(physical_id);\n  unsigned int index = op->info.index;\n\n  if (event->cmd.type == CL_COMMAND_MIGRATE_MEM_OBJECTS) {\n    memory_migration = event->cmd.info.memory_migration;\n  } else {\n    memory_migration = event->cmd.info.ndrange_kernel.memory_migration;\n  }\n\n  assert(index < memory_migration.num_mem_objects);\n\n  {\n    const cl_mem src_mem = memory_migration.src_mem_list[index].src_mem;\n    const unsigned int dest_device =\n        memory_migration.src_mem_list[index].destination_physical_device_id;\n    const unsigned int dest_mem_id =\n        memory_migration.src_mem_list[index].destination_mem_id;\n\n#ifdef MEM_DEBUG_MSG\n    printf(\"object %d, mem %zx, count %d:\\n\", index, (size_t)src_mem,\n           src_mem->reserved_allocations_count[dest_device][dest_mem_id]);\n#endif\n\n    assert(src_mem->reserved_allocations[dest_device].size() > dest_mem_id);\n    assert(src_mem->reserved_allocations[dest_device][dest_mem_id] != NULL);\n\n    // Handle deferred allocations first, as it's a special case\n    // If the allocation was deferred then just update the block allocation.\n    if (src_mem->allocation_deferred) {\n#ifdef MEM_DEBUG_MSG\n      printf(\"allocation was deferred\\n\");\n#endif\n\n      // The empty block allocation that src_mem started with must be freed\n      // (since it isn't in the reserved_allocations list):\n      acl_delete(src_mem->block_allocation);\n\n      src_mem->block_allocation =\n          src_mem->reserved_allocations[dest_device][dest_mem_id];\n      src_mem->mem_id = dest_mem_id;\n      src_mem->allocation_deferred = 0;\n      // Reserved allocations count is decremented below in the \"already at\n      // destination\" branch\n    }\n\n    // If the src_mem is auto_mapped we must first unmap it\n    // Note: This guarantees we have the live copy when dealing with sub or\n    // parent buffers. If we already had the live copy, then auto_mapped would\n    // be false. If we didn't, the auto_unmap_mem command, once complete, will\n    // ensure that this copy is the live copy.\n    if (src_mem->auto_mapped) {\n#ifdef MEM_DEBUG_MSG\n      printf(\"(auto mapped) \");\n#endif\n\n      auto_unmap_mem(event->command_queue->context, physical_id, src_mem, NULL);\n    }\n\n    // Do nothing for SVM:\n    if ((src_mem->is_svm && device_supports_any_svm) ||\n        (!device_supports_physical_memory)) {\n#ifdef MEM_DEBUG_MSG\n      printf(\"svm\\n\");\n#endif\n      acl_set_device_op_execution_status(\n          op, CL_COMPLETE); // There will be no mem transfer, so we must set the\n                            // op status ourselves\n\n      // If memory is already at the destination, do nothing unless there's a\n      // pending copy from the host:\n    } else if (src_mem->block_allocation ==\n               src_mem->reserved_allocations[dest_device][dest_mem_id]) {\n#ifdef MEM_DEBUG_MSG\n      printf(\"already at dest\\n\");\n#endif\n\n      if (src_mem->mem_cpy_host_ptr_pending == 1) {\n// A host->device copy was deferred at some point and is still pending\n#ifdef MEM_DEBUG_MSG\n        printf(\"copy host pointer pending\\n\");\n#endif\n        const acl_hal_t *const hal = acl_get_hal();\n        void *host_mem_address = src_mem->host_mem.aligned_ptr;\n        void *device_mem_address = src_mem->block_allocation->range.begin;\n\n        // Do a blocking copy. Before, we had a non-blocking copy and\n        // that caused a bug.\n        hal->copy_hostmem_to_globalmem(0 /* blocking */, host_mem_address,\n                                       device_mem_address, src_mem->size);\n\n        // The copy to the device is done, so it is no longer pending (i.e.\n        // don't enter this if-block again) and the writable copy is no\n        // longer on the host, it is on the device.\n        src_mem->mem_cpy_host_ptr_pending = 0;\n        src_mem->writable_copy_on_host = 0;\n\n// We did a blocking copy, so this operation is done!\n#ifdef MEM_DEBUG_MSG\n        printf(\"Done hostmem->globalmem copy, setting status to complete\\n\");\n#endif\n        acl_set_device_op_execution_status(op, CL_COMPLETE);\n      } else {\n// We can fall into this else-block if we are trying to do a\n// deferred migration, again. This time around there is no migration\n// to do, so mark the operation as complete.\n#ifdef MEM_DEBUG_MSG\n        printf(\"Memory op is already complete\\n\");\n#endif\n        acl_set_device_op_execution_status(op, CL_COMPLETE);\n      }\n\n      src_mem->reserved_allocations_count[dest_device][dest_mem_id]--;\n      // Otherwise the memory needs to be moved:\n    } else {\n#ifdef MEM_DEBUG_MSG\n      printf(\"moving \");\n#endif\n      if (src_mem->block_allocation->region != NULL) {\n        bool is_user_provided_flag =\n            src_mem->block_allocation->region\n                ->is_user_provided; // This flag is used for guarding some of\n                                    // the code and will be changed in the\n                                    // middle\n\n        const unsigned int src_device =\n            ACL_GET_PHYSICAL_ID(src_mem->block_allocation->range.begin);\n        const unsigned int src_mem_id = src_mem->mem_id;\n\n        if (!is_user_provided_flag) {\n          assert(src_device != dest_device ||\n                 src_mem_id !=\n                     dest_mem_id); // We shouldn't get here if the source and\n                                   // destination are the same place\n        }\n\n        {\n          int mem_on_host;\n          void *old_mem_address = l_get_address_of_writable_copy(\n              src_mem, physical_id, &mem_on_host, CL_FALSE);\n          void *new_mem_address =\n              src_mem->reserved_allocations[dest_device][dest_mem_id]\n                  ->range.begin;\n          const acl_hal_t *const hal = acl_get_hal();\n\n#ifdef MEM_DEBUG_MSG\n          printf(\"from %u:%u:%zx to %u:%u:%zx \", src_device, src_mem_id,\n                 (size_t)(ACL_STRIP_PHYSICAL_ID(old_mem_address)), dest_device,\n                 dest_mem_id, (size_t)(ACL_STRIP_PHYSICAL_ID(new_mem_address)));\n#endif\n\n          if (!is_user_provided_flag && !mem_on_host) {\n\n            // Assert that the memory is actually at its source:\n            assert(src_mem->reserved_allocations[src_device].size() >\n                   src_mem_id);\n            assert(src_mem->reserved_allocations[src_device][src_mem_id] !=\n                   NULL);\n            assert(src_mem->reserved_allocations[src_device][src_mem_id] ==\n                   src_mem->block_allocation);\n          }\n\n          src_mem->reserved_allocations_count\n              [dest_device]\n              [dest_mem_id]--; // this reserved allocation has been used\n          src_mem->block_allocation =\n              src_mem->reserved_allocations[dest_device][dest_mem_id];\n          src_mem->mem_id = dest_mem_id;\n\n          if (!is_user_provided_flag) {\n            if (!mem_on_host) {\n              // Transfer the memory:\n              hal->copy_globalmem_to_globalmem(event, old_mem_address,\n                                               new_mem_address, src_mem->size);\n            } else {\n#ifdef MEM_DEBUG_MSG\n              printf(\"(on host) \");\n#endif\n              hal->copy_hostmem_to_globalmem(event, old_mem_address,\n                                             new_mem_address, src_mem->size);\n            }\n            if (src_mem->reserved_allocations_count[src_device][src_mem_id] <=\n                0) { //\"<=\" instead of \"==\" just in case\n\n#ifdef MEM_DEBUG_MSG\n              printf(\"release block %zx (%u:%u) \",\n                     (size_t)(src_mem->reserved_allocations[src_device]\n                                                           [src_mem_id]),\n                     src_device, src_mem_id);\n#endif\n              remove_mem_block_linked_list(\n                  src_mem->reserved_allocations[src_device][src_mem_id]);\n              acl_delete(src_mem->reserved_allocations[src_device][src_mem_id]);\n              src_mem->reserved_allocations[src_device][src_mem_id] = NULL;\n            }\n          } else {\n#ifdef MEM_DEBUG_MSG\n            printf(\"\\nhostmem->globalmem copy with user pointer\\n\");\n#endif\n            // POTENTIAL RACE CONDITION\n            // 'copy_hostmem_to_globalmem' is non-blocking if event != 0.\n            // What happens if another migration is performed on this same\n            // memory in the (near) future? E.g. if two kernels share the\n            // same pointer created with CL_MEM_USE_HOST_PTR. Based on my\n            // debugging, if a second migration happens on this memory, we\n            // go into the previous else-if block (i.e. the memory is\n            // already at the destination) and since the copy is not\n            // pending (we just did it) the operation gets marked as\n            // CL_COMPLETE. This may be a race condition!\n            hal->copy_hostmem_to_globalmem(event, old_mem_address,\n                                           new_mem_address, src_mem->size);\n            src_mem->writable_copy_on_host = 0;\n          }\n          // If nobody else has reserved the region we just moved from we can\n          // free it:\n        }\n      } else {\n// Bad event\n#ifdef MEM_DEBUG_MSG\n        printf(\"\\t acl_mem_migrate_buffer: bad event\");\n#endif\n        acl_set_device_op_execution_status(op, -1);\n      }\n#ifdef MEM_DEBUG_MSG\n      printf(\"\\n\");\n#endif\n    }\n  }\n\n#ifdef MEM_DEBUG_MSG\n  printf(\"acl_mem_migrate_buffer finished\\n\");\n#endif\n}\n\nstatic void auto_unmap_mem(cl_context context, unsigned int physical_id,\n                           cl_mem src_mem, acl_device_op_t *op) {\n  acl_command_info_t dst_unmap_cmd;\n  dst_unmap_cmd.type = CL_COMMAND_UNMAP_MEM_OBJECT;\n  dst_unmap_cmd.info.mem_xfer.is_auto_map = 1;\n  if (src_mem->flags & CL_MEM_READ_ONLY) {\n    dst_unmap_cmd.info.mem_xfer.map_flags = CL_MAP_READ;\n  } else {\n    dst_unmap_cmd.info.mem_xfer.map_flags = CL_MAP_WRITE;\n  }\n  dst_unmap_cmd.info.mem_xfer.src_mem = context->unwrapped_host_mem;\n  dst_unmap_cmd.info.mem_xfer.src_offset[0] =\n      (size_t)((char *)src_mem->host_mem.aligned_ptr - (char *)ACL_MEM_ALIGN);\n  dst_unmap_cmd.info.mem_xfer.src_offset[1] = 0;\n  dst_unmap_cmd.info.mem_xfer.src_offset[2] = 0;\n  dst_unmap_cmd.info.mem_xfer.dst_mem = src_mem;\n  dst_unmap_cmd.info.mem_xfer.dst_offset[0] = 0;\n  dst_unmap_cmd.info.mem_xfer.dst_offset[1] = 0;\n  dst_unmap_cmd.info.mem_xfer.dst_offset[2] = 0;\n  dst_unmap_cmd.info.mem_xfer.cb[0] = src_mem->size;\n  dst_unmap_cmd.info.mem_xfer.cb[1] = 1;\n  dst_unmap_cmd.info.mem_xfer.cb[2] = 1;\n  dst_unmap_cmd.info.mem_xfer.src_row_pitch = dst_unmap_cmd.info.mem_xfer.cb[0];\n  dst_unmap_cmd.info.mem_xfer.src_slice_pitch = 1;\n  dst_unmap_cmd.info.mem_xfer.dst_row_pitch = dst_unmap_cmd.info.mem_xfer.cb[0];\n  dst_unmap_cmd.info.mem_xfer.dst_slice_pitch = 1;\n  l_mem_transfer_buffer_explicitly(context, op, physical_id, dst_unmap_cmd);\n}\n\nstatic void sync_subbuffers(cl_mem mem, cl_context context, acl_device_op_t *op,\n                            unsigned int physical_device_id) {\n  int other_buffer_on_host;\n  cl_mem other_buffer_mem;\n  // For sub-buffers, first check the parent.\n  if (mem->fields.buffer_objs.is_subbuffer) {\n\n    other_buffer_mem = mem->fields.buffer_objs.parent;\n    // For parent buffers, just go through the sub buffers\n  } else {\n    other_buffer_mem = mem->fields.buffer_objs.next_sub;\n  }\n\n  while (other_buffer_mem != NULL) {\n    // Overlaps\n    if (!((mem->fields.buffer_objs.sub_origin + mem->size <=\n           other_buffer_mem->fields.buffer_objs.sub_origin) ||\n          (mem->fields.buffer_objs.sub_origin >=\n           other_buffer_mem->fields.buffer_objs.sub_origin +\n               other_buffer_mem->size))) {\n\n      l_get_address_of_writable_copy(other_buffer_mem, physical_device_id,\n                                     &other_buffer_on_host, CL_FALSE);\n      // If other buffer is on the device and writable, it is considered the\n      // live version. If there are 2 or more overlapping sub buffers that meet\n      // this, then 2 overlapping sub buffers were writable at the same time and\n      // the behaviour is undefined. If this is host accessible, the live data\n      // is immediately copied back to the device as soon as the event is\n      // complete. If the event is not complete, we have 2 copies of writable\n      // data at the same time and the behaviour is undefined.\n      if (!other_buffer_on_host &&\n          !(other_buffer_mem->flags & CL_MEM_READ_ONLY) &&\n          !other_buffer_mem->block_allocation->region->is_host_accessible) {\n        acl_command_info_t other_cmd;\n        if (!other_buffer_mem->host_mem.aligned_ptr) {\n          acl_context_callback(context, \"Could not allocate backing store for \"\n                                        \"a device buffer with a sub buffer.\");\n          acl_set_device_op_execution_status(op, -1);\n          return;\n        }\n        other_cmd.type = CL_COMMAND_MAP_BUFFER;\n        other_cmd.info.mem_xfer.is_auto_map = 1;\n        if (other_buffer_mem->flags & CL_MEM_READ_ONLY) {\n          other_cmd.info.mem_xfer.map_flags = CL_MAP_READ;\n        } else {\n          other_cmd.info.mem_xfer.map_flags = CL_MAP_WRITE;\n        }\n        other_cmd.info.mem_xfer.src_mem = other_buffer_mem;\n        other_cmd.info.mem_xfer.src_offset[0] = 0;\n        other_cmd.info.mem_xfer.src_offset[1] = 0;\n        other_cmd.info.mem_xfer.src_offset[2] = 0;\n        other_cmd.info.mem_xfer.dst_mem = context->unwrapped_host_mem;\n        other_cmd.info.mem_xfer.dst_offset[0] =\n            (size_t)((char *)other_buffer_mem->host_mem.aligned_ptr -\n                     (char *)ACL_MEM_ALIGN);\n        other_cmd.info.mem_xfer.dst_offset[1] = 0;\n        other_cmd.info.mem_xfer.dst_offset[2] = 0;\n        other_cmd.info.mem_xfer.cb[0] = other_buffer_mem->size;\n        other_cmd.info.mem_xfer.cb[1] = 1;\n        other_cmd.info.mem_xfer.cb[2] = 1;\n        other_cmd.info.mem_xfer.src_row_pitch = other_cmd.info.mem_xfer.cb[0];\n        other_cmd.info.mem_xfer.src_slice_pitch = 1;\n        other_cmd.info.mem_xfer.dst_row_pitch = other_cmd.info.mem_xfer.cb[0];\n        other_cmd.info.mem_xfer.dst_slice_pitch = 1;\n\n        // Setting other_cmd.trivial to avoid coverity warning. Trivial is not\n        // used in this instance.\n        other_cmd.trivial = 0;\n        l_mem_transfer_buffer_explicitly(context, NULL, physical_device_id,\n                                         other_cmd);\n      }\n    }\n    other_buffer_mem = other_buffer_mem->fields.buffer_objs.next_sub;\n  }\n}\n\n// Determine if memory transfer operation requires data transfer.\n// RTE can make various optimizations based on flags the buffer was created\n// with and/or mapped with.\n// Furthermore, if they don't result in transfers, they can be executed along\n// with other memory transfer ops without conflict in device-op queue.\nint acl_mem_op_requires_transfer(const acl_command_info_t &cmd) {\n  cl_mem dst_mem = cmd.info.mem_xfer.dst_mem;\n\n  // We should skip the memory transfer when mapping and the\n  // map flag is CL_MAP_WRITE_INVALIDATE_REGION since we don't care about the\n  // contents of the mapped region. We should also skip the memory transfer when\n  // unmapping if the buffer was previously mapped with CL_MAP_READ (if the\n  // destination memory's writable copy is not the host)\n  return !(cmd.type == CL_COMMAND_MAP_BUFFER &&\n           (cmd.info.mem_xfer.map_flags & CL_MAP_WRITE_INVALIDATE_REGION)) &&\n         !(cmd.type == CL_COMMAND_UNMAP_MEM_OBJECT &&\n           !dst_mem->writable_copy_on_host);\n}\n\n// Transfer memory, using an explicit context and cmd.\n// Signal command state transitions to the given event, which may be NULL.\n// If event is NULL, then do the transfer in a blocking manner.\nstatic void l_mem_transfer_buffer_explicitly(cl_context context,\n                                             acl_device_op_t *op,\n                                             unsigned int physical_device_id,\n                                             const acl_command_info_t &cmd) {\n  void *src_base;\n  void *dst_base;\n  void *src_data_base;\n  void *dst_data_base;\n  int src_on_host;\n  int dst_on_host;\n  cl_mem src_mem = cmd.info.mem_xfer.src_mem;\n  cl_mem dst_mem = cmd.info.mem_xfer.dst_mem;\n  acl_assert_locked();\n\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  switch (cmd.type) {\n  case CL_COMMAND_MAP_BUFFER:\n  case CL_COMMAND_UNMAP_MEM_OBJECT:\n  case CL_COMMAND_READ_BUFFER:\n  case CL_COMMAND_WRITE_BUFFER:\n  case CL_COMMAND_COPY_BUFFER:\n    break;\n  default:\n    acl_context_callback(\n        context, \"Internal error: invalid memory transfer completion type. \"\n                 \"Corrupt host memory?\");\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n\n  if (!acl_mem_is_valid(src_mem)) {\n    acl_context_callback(\n        context, \"Internal error: Invalid source buffer for memory transfer \"\n                 \"completion. Corrupt host memory?\");\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n  if (!acl_mem_is_valid(dst_mem)) {\n    acl_context_callback(\n        context, \"Internal error: Invalid destination \"\n                 \"buffer for memory transfer completion. Corrupt host memory?\");\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n\n  // Need to know the base address of both buffers.\n  // Note that either of these can be a host address or a device address.\n  //\n  // The tricky bit is the map (respectively unmap) case, but the caller\n  // has already set the destination (respectively source) address to the host\n  // side pointer.\n\n  src_base = l_get_address_of_writable_copy(src_mem, physical_device_id,\n                                            &src_on_host, CL_FALSE);\n  dst_base = l_get_address_of_writable_copy(\n      dst_mem, physical_device_id, &dst_on_host,\n      (cl_bool)(cmd.type == CL_COMMAND_UNMAP_MEM_OBJECT));\n\n  bool should_transfer = acl_mem_op_requires_transfer(cmd);\n\n  if (should_transfer) {\n    // We'll perform the transfer.\n    void **src;\n    void **dst;\n    size_t src_element_size;\n    size_t dst_element_size;\n    cl_int errcode_ret;\n    const acl_hal_t *const hal = acl_get_hal();\n    cl_bool single_copy = CL_TRUE;\n    size_t src_row_pitch;\n    size_t src_slice_pitch;\n    size_t dst_row_pitch;\n    size_t dst_slice_pitch;\n\n    void (*hal_dma_fn)(cl_event, const void *, void *, size_t) = 0;\n    size_t *size, num_memory_transfers, imem_xfr;\n\n    if (is_image(src_mem)) {\n      src_element_size = acl_get_image_element_size(\n          src_mem->context, src_mem->fields.image_objs.image_format,\n          &errcode_ret);\n      if (errcode_ret != CL_SUCCESS) {\n        acl_context_callback(context, \"Invalid image for memory transfer.\");\n        acl_set_device_op_execution_status(op, -1);\n        return;\n      }\n    } else {\n      src_element_size = 1;\n    }\n    if (is_image(dst_mem)) {\n      dst_element_size = acl_get_image_element_size(\n          dst_mem->context, dst_mem->fields.image_objs.image_format,\n          &errcode_ret);\n      if (errcode_ret != CL_SUCCESS) {\n        acl_context_callback(context, \"Invalid image for memory transfer.\");\n        acl_set_device_op_execution_status(op, -1);\n        return;\n      }\n    } else {\n      dst_element_size = 1;\n    }\n    // If this is an image, element size was set above. Otherwise, element size\n    // is one.\n    if (is_image(src_mem) && is_image(dst_mem) &&\n        src_element_size != dst_element_size) {\n      acl_context_callback(context,\n                           \"Invalid image for memory transfer. Element size \"\n                           \"between source and destination don't line up.\");\n      acl_set_device_op_execution_status(op, -1);\n      return;\n    }\n\n    if (src_mem->mem_object_type == CL_MEM_OBJECT_BUFFER) {\n      src_row_pitch = cmd.info.mem_xfer.src_row_pitch;\n      src_slice_pitch = cmd.info.mem_xfer.src_slice_pitch;\n    } else if (is_image(src_mem)) {\n      src_row_pitch =\n          src_mem->fields.image_objs.image_desc->image_width * src_element_size;\n      if (src_mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n        src_slice_pitch = src_row_pitch;\n      } else {\n        src_slice_pitch =\n            src_mem->fields.image_objs.image_desc->image_height * src_row_pitch;\n      }\n    } else {\n      acl_context_callback(context, \"Invalid memory type for memory transfer.\");\n      acl_set_device_op_execution_status(op, -1);\n      return;\n    }\n    if (dst_mem->mem_object_type == CL_MEM_OBJECT_BUFFER) {\n      dst_row_pitch = cmd.info.mem_xfer.dst_row_pitch;\n      dst_slice_pitch = cmd.info.mem_xfer.dst_slice_pitch;\n    } else if (is_image(dst_mem)) {\n      dst_row_pitch =\n          dst_mem->fields.image_objs.image_desc->image_width * dst_element_size;\n      if (dst_mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n        dst_slice_pitch = dst_row_pitch;\n      } else {\n        dst_slice_pitch =\n            dst_mem->fields.image_objs.image_desc->image_height * dst_row_pitch;\n      }\n    } else {\n      acl_context_callback(context, \"Invalid memory type for memory transfer.\");\n      acl_set_device_op_execution_status(op, -1);\n      return;\n    }\n\n    // Check if we can do this in a single copy. We can if the sizes line up.\n    if (((src_mem == src_mem->context->unwrapped_host_mem ||\n          cmd.info.mem_xfer.src_offset[0] == 0) &&\n         (dst_mem == dst_mem->context->unwrapped_host_mem ||\n          cmd.info.mem_xfer.dst_offset[0] == 0) &&\n         cmd.info.mem_xfer.cb[0] == src_row_pitch &&\n         src_row_pitch == dst_row_pitch) &&\n        (cmd.info.mem_xfer.cb[2] == 1 ||\n         (cmd.info.mem_xfer.src_offset[1] == 0 &&\n          cmd.info.mem_xfer.dst_offset[1] == 0 &&\n          cmd.info.mem_xfer.cb[1] * cmd.info.mem_xfer.cb[0] ==\n              src_slice_pitch &&\n          src_slice_pitch == dst_slice_pitch))) {\n      single_copy = CL_TRUE;\n    } else {\n      single_copy = CL_FALSE;\n    }\n\n    // If this is a buffer with sub-buffers or a sub-buffer, need to make sure\n    // all overlapping regions are in sync first For buffers & sub-buffers,\n    // there can only be one active writable copy on any device at a time. If\n    // this buffer/sub-buffer is writable and on a device, it _must_ be the live\n    // copy so we don't need to do anything (or there are multiple live copies,\n    // in which case behaviour is undefined, and we still don't need to do\n    // anything). Only check for buffers & sub-buffers if the source is on the\n    // host or this memory is read only\n    if ((src_mem->mem_object_type == CL_MEM_OBJECT_BUFFER &&\n         (src_on_host || (src_mem->flags & CL_MEM_READ_ONLY))) &&\n        acl_is_sub_or_parent_buffer(src_mem)) {\n      sync_subbuffers(src_mem, context, op, physical_device_id);\n\n      // If the buffer is read only, we just copied back any buffers that\n      // changed and the writable copy is now on the host. This is important for\n      // sub buffers, where we don't want to copy the data back from multiple\n      // buffers & subbuffers if they are read only. (For dst_mem, this flag\n      // will be set based on its ultimate destination)\n      if (!src_on_host && (src_mem->flags & CL_MEM_READ_ONLY)) {\n        src_mem->writable_copy_on_host = 1;\n        src_base = l_get_address_of_writable_copy(src_mem, physical_device_id,\n                                                  &src_on_host, CL_FALSE);\n      }\n    }\n\n    // Repeat for the destination buffer if this is a destination buffer that\n    // has been auto-mapped and it is a sub buffer or has sub buffers that may\n    // have changed the live data\n    if (dst_mem->mem_object_type == CL_MEM_OBJECT_BUFFER &&\n        dst_mem->auto_mapped) {\n      if (acl_is_sub_or_parent_buffer(dst_mem)) {\n        sync_subbuffers(dst_mem, context, op, physical_device_id);\n      }\n\n      // If the destination has been automapped, and this operation will not\n      // unmap the object, then undo the automap before continuing.\n      if (cmd.type != CL_COMMAND_UNMAP_MEM_OBJECT) {\n        acl_command_info_t dst_unmap_cmd;\n        dst_unmap_cmd.type = CL_COMMAND_UNMAP_MEM_OBJECT;\n        dst_unmap_cmd.info.mem_xfer.is_auto_map = 1;\n        if (dst_mem->flags & CL_MEM_READ_ONLY) {\n          dst_unmap_cmd.info.mem_xfer.map_flags = CL_MAP_READ;\n        } else {\n          dst_unmap_cmd.info.mem_xfer.map_flags = CL_MAP_WRITE;\n        }\n        dst_unmap_cmd.info.mem_xfer.src_mem = context->unwrapped_host_mem;\n        dst_unmap_cmd.info.mem_xfer.src_offset[0] =\n            (size_t)((char *)dst_mem->host_mem.aligned_ptr -\n                     (char *)ACL_MEM_ALIGN);\n        dst_unmap_cmd.info.mem_xfer.src_offset[1] = 0;\n        dst_unmap_cmd.info.mem_xfer.src_offset[2] = 0;\n        dst_unmap_cmd.info.mem_xfer.dst_mem = dst_mem;\n        dst_unmap_cmd.info.mem_xfer.dst_offset[0] = 0;\n        dst_unmap_cmd.info.mem_xfer.dst_offset[1] = 0;\n        dst_unmap_cmd.info.mem_xfer.dst_offset[2] = 0;\n        dst_unmap_cmd.info.mem_xfer.cb[0] = dst_mem->size;\n        dst_unmap_cmd.info.mem_xfer.cb[1] = 1;\n        dst_unmap_cmd.info.mem_xfer.cb[2] = 1;\n        dst_unmap_cmd.info.mem_xfer.src_row_pitch =\n            dst_unmap_cmd.info.mem_xfer.cb[0];\n        dst_unmap_cmd.info.mem_xfer.src_slice_pitch = 1;\n        dst_unmap_cmd.info.mem_xfer.dst_row_pitch =\n            dst_unmap_cmd.info.mem_xfer.cb[0];\n        dst_unmap_cmd.info.mem_xfer.dst_slice_pitch = 1;\n        l_mem_transfer_buffer_explicitly(context, NULL, physical_device_id,\n                                         dst_unmap_cmd);\n        dst_base = l_get_address_of_writable_copy(dst_mem, physical_device_id,\n                                                  &dst_on_host, CL_FALSE);\n      }\n    }\n\n    // Determine the base address of the data, excluding any metadata\n    if (is_image(src_mem)) {\n      src_data_base =\n          ((char *)src_base) +\n          get_offset_for_image_param(context, src_mem->mem_object_type, \"data\");\n    } else {\n      src_data_base = src_base;\n    }\n    if (is_image(dst_mem)) {\n      dst_data_base =\n          ((char *)dst_base) +\n          get_offset_for_image_param(context, dst_mem->mem_object_type, \"data\");\n    } else {\n      dst_data_base = dst_base;\n    }\n\n    if (is_image(dst_mem) && !dst_on_host) {\n      // If we are copying to an image on a device, make sure we have copied the\n      // meta data\n      copy_image_metadata(dst_mem);\n    }\n\n    // Can do a single copy. Determine starting address & size of this copy\n    if (single_copy) {\n      num_memory_transfers = 1;\n      src = (void **)acl_malloc(sizeof(void *) * num_memory_transfers);\n      dst = (void **)acl_malloc(sizeof(void *) * num_memory_transfers);\n      size = (size_t *)acl_malloc(sizeof(size_t) * num_memory_transfers);\n      size[0] = cmd.info.mem_xfer.cb[0] * cmd.info.mem_xfer.cb[1] *\n                cmd.info.mem_xfer.cb[2];\n\n      src[0] = ((char *)src_data_base) +\n               cmd.info.mem_xfer.src_offset[0] * src_element_size +\n               cmd.info.mem_xfer.src_offset[1] * src_row_pitch +\n               cmd.info.mem_xfer.src_offset[2] * src_slice_pitch;\n      dst[0] = ((char *)dst_data_base) +\n               cmd.info.mem_xfer.dst_offset[0] * dst_element_size +\n               cmd.info.mem_xfer.dst_offset[1] * dst_row_pitch +\n               cmd.info.mem_xfer.dst_offset[2] * dst_slice_pitch;\n      // Need to do multiple copies to cover all affected regions of memory.\n      // Currently we treat each row as a separate region. It may be possible to\n      // optimize this more in the future. For example we may be able to copy\n      // slices if we are copying complete rows.\n    } else {\n      size_t deep;\n      size_t row;\n      num_memory_transfers = cmd.info.mem_xfer.cb[1] * cmd.info.mem_xfer.cb[2];\n      assert(num_memory_transfers > 0);\n      src = (void **)acl_malloc(sizeof(void *) * num_memory_transfers);\n      dst = (void **)acl_malloc(sizeof(void *) * num_memory_transfers);\n      size = (size_t *)acl_malloc(sizeof(size_t) * num_memory_transfers);\n      for (deep = 0; deep < cmd.info.mem_xfer.cb[2]; ++deep) {\n        for (row = 0; row < cmd.info.mem_xfer.cb[1]; ++row) {\n          src[row + deep * cmd.info.mem_xfer.cb[1]] =\n              ((char *)src_data_base) +\n              cmd.info.mem_xfer.src_offset[0] * src_element_size +\n              (cmd.info.mem_xfer.src_offset[1] + row) * src_row_pitch +\n              (cmd.info.mem_xfer.src_offset[2] + deep) * src_slice_pitch;\n          dst[row + deep * cmd.info.mem_xfer.cb[1]] =\n              ((char *)dst_data_base) +\n              cmd.info.mem_xfer.dst_offset[0] * dst_element_size +\n              (cmd.info.mem_xfer.dst_offset[1] + row) * dst_row_pitch +\n              (cmd.info.mem_xfer.dst_offset[2] + deep) * dst_slice_pitch;\n          size[row + deep * cmd.info.mem_xfer.cb[1]] = cmd.info.mem_xfer.cb[0];\n        }\n      }\n    }\n\n    // Determine which HAL copy function to use, depending on where the memory\n    // is\n    if (src_on_host) {\n      if (dst_on_host) {\n        hal_dma_fn = hal->copy_hostmem_to_hostmem;\n      } else {\n        hal_dma_fn = hal->copy_hostmem_to_globalmem;\n      }\n    } else {\n      if (dst_on_host) {\n        hal_dma_fn = hal->copy_globalmem_to_hostmem;\n      } else {\n        hal_dma_fn = hal->copy_globalmem_to_globalmem;\n      }\n    }\n\n    for (imem_xfr = 0; imem_xfr < num_memory_transfers; ++imem_xfr) {\n      acl_print_debug_msg(\n          \"      l_mem_transfer_buffer_explicitly %d %d (%p,%p,%lu)\\n\",\n          src_on_host, dst_on_host, src[imem_xfr], dst[imem_xfr],\n          (unsigned long)size[imem_xfr]);\n// Go for it!\n#ifdef MEM_DEBUG_MSG\n      printf(\"l_mem_transfer_buffer_explicitly src %zx dest %zx ([%d]%zx -> \"\n             \"[%d]%zx)\\n\",\n             (size_t)src_mem, (size_t)dst_mem, src_on_host,\n             (size_t)src[imem_xfr], dst_on_host, (size_t)dst[imem_xfr]);\n#endif\n      hal_dma_fn((op ? op->info.event : 0), src[imem_xfr], dst[imem_xfr],\n                 size[imem_xfr]);\n    }\n\n    if (is_image(src_mem)) {\n      if (op == NULL) {\n        size_t element_size;\n        void *local_meta_data = malloc(get_offset_for_image_param(\n            src_mem->context, src_mem->mem_object_type, \"data\"));\n        int errcode;\n        element_size = acl_get_image_element_size(\n            src_mem->context, src_mem->fields.image_objs.image_format,\n            &errcode);\n\n        if (errcode != CL_SUCCESS) {\n          acl_context_callback(context, \"Could not determine image type.\");\n          acl_set_device_op_execution_status(op, -1);\n\n          acl_free(src);\n          acl_free(dst);\n          acl_free(size);\n          free(local_meta_data);\n          return;\n        }\n\n        safe_memcpy(\n            (char *)local_meta_data +\n                get_offset_for_image_param(src_mem->context,\n                                           src_mem->mem_object_type, \"width\"),\n            &(src_mem->fields.image_objs.image_desc->image_width), 4, 4, 4);\n        if (src_mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D ||\n            src_mem->mem_object_type == CL_MEM_OBJECT_IMAGE3D ||\n            src_mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY) {\n          safe_memcpy(\n              (char *)local_meta_data +\n                  get_offset_for_image_param(\n                      src_mem->context, src_mem->mem_object_type, \"height\"),\n              &(src_mem->fields.image_objs.image_desc->image_height), 4, 4, 4);\n        }\n        if (src_mem->mem_object_type == CL_MEM_OBJECT_IMAGE3D) {\n          safe_memcpy(\n              (char *)local_meta_data +\n                  get_offset_for_image_param(src_mem->context,\n                                             src_mem->mem_object_type, \"depth\"),\n              &(src_mem->fields.image_objs.image_desc->image_depth), 4, 4, 4);\n        }\n        if (src_mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY ||\n            src_mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n          safe_memcpy(\n              (char *)local_meta_data +\n                  get_offset_for_image_param(\n                      src_mem->context, src_mem->mem_object_type, \"array_size\"),\n              &(src_mem->fields.image_objs.image_desc->image_array_size), 4, 4,\n              4);\n        }\n        safe_memcpy(\n            (char *)local_meta_data +\n                get_offset_for_image_param(src_mem->context,\n                                           src_mem->mem_object_type,\n                                           \"channel_data_type\"),\n            &(src_mem->fields.image_objs.image_format->image_channel_data_type),\n            4, 4, 4);\n        safe_memcpy(\n            (char *)local_meta_data +\n                get_offset_for_image_param(src_mem->context,\n                                           src_mem->mem_object_type,\n                                           \"channel_order\"),\n            &(src_mem->fields.image_objs.image_format->image_channel_order), 4,\n            4, 4);\n        safe_memcpy((char *)local_meta_data +\n                        get_offset_for_image_param(src_mem->context,\n                                                   src_mem->mem_object_type,\n                                                   \"element_size\"),\n                    &(element_size), 8, 8, 8);\n\n        hal_dma_fn((op ? op->info.event : 0), local_meta_data, dst_base,\n                   get_offset_for_image_param(context, src_mem->mem_object_type,\n                                              \"data\"));\n\n        free(local_meta_data);\n      }\n    }\n\n    acl_free(src);\n    acl_free(dst);\n    acl_free(size);\n  } else {\n    // If the HAL isn't doing anything for us, then it won't signal\n    // completion either.\n    // Must do that for ourselves.\n    acl_set_device_op_execution_status(op, CL_COMPLETE);\n  }\n\n  // Track the mapping count and the location of the writable copy.\n  if (cmd.type == CL_COMMAND_MAP_BUFFER) {\n    if (!cmd.info.mem_xfer.is_auto_map) {\n      src_mem->mapping_count++;\n    } else {\n      src_mem->auto_mapped = 1;\n    }\n    if (cmd.info.mem_xfer.map_flags & CL_MAP_WRITE ||\n        cmd.info.mem_xfer.map_flags & CL_MAP_WRITE_INVALIDATE_REGION) {\n      // The writable copy is now on the host.\n      src_mem->writable_copy_on_host = 1;\n    }\n  } else if (cmd.type == CL_COMMAND_UNMAP_MEM_OBJECT) {\n    if (!cmd.info.mem_xfer.is_auto_map) {\n      dst_mem->mapping_count--;\n    }\n    // If this memory is explicitly unmapped by the user, we can release\n    // the auto-mapping as well.\n    dst_mem->auto_mapped = 0;\n    if (dst_mem->mapping_count == 0) {\n      // No more mappings\n      // The writable copy is back in the home location.\n      dst_mem->writable_copy_on_host =\n          dst_mem->block_allocation->region->is_host_accessible;\n    }\n  }\n}\n\ncl_bool acl_is_sub_or_parent_buffer(cl_mem mem) {\n  return (cl_bool)(mem->fields.buffer_objs.is_subbuffer ||\n                   mem->fields.buffer_objs.next_sub != NULL);\n}\n\nstatic cl_bool is_image(cl_mem mem) {\n  return (cl_bool)(mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D ||\n                   mem->mem_object_type == CL_MEM_OBJECT_IMAGE3D ||\n                   mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY ||\n                   mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n                   mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY ||\n                   mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER);\n}\n\nstatic void l_free_image_members(cl_mem mem) {\n  if (mem->fields.image_objs.image_format != NULL) {\n    acl_free(mem->fields.image_objs.image_format);\n  }\n  if (mem->fields.image_objs.image_desc != NULL) {\n    if (mem->fields.image_objs.image_desc->buffer != NULL) {\n      clReleaseMemObject(mem->fields.image_objs.image_desc->buffer);\n      mem->fields.image_objs.image_desc->buffer = NULL;\n    }\n    if (mem->fields.image_objs.image_desc->mem_object != NULL) {\n      clReleaseMemObject(mem->fields.image_objs.image_desc->mem_object);\n      mem->fields.image_objs.image_desc->mem_object = NULL;\n    }\n    acl_free(mem->fields.image_objs.image_desc);\n  }\n}\n\nvoid acl_copy_device_buffers_to_host_before_programming(\n    cl_context _context, unsigned int physical_device_id,\n    void(CL_CALLBACK *read_callback)(cl_mem, int)) {\n  // Copy all device buffers into host memory.\n  // Regardless of which context they belong to.\n  //\n  // We need this because reprogramming the device wipes out DDR\n  // memory, where device global buffers live.\n  // After reprogramming, we'll unmap the buffers again.\n  // The reason this works is quite subtle:\n  //\n  //  a. It's the programmer's responsibility to fully unmap any\n  //     buffers used by a kernel *before* the kernel launches.\n  //     The OpenCL 1.2 spec makes this clear in section 5.4.3\n  //     \"Accessing mapped regions of a memory object\".\n  //\n  //  b. By the waiting done prior to programming the device, no other\n  //     kernels are running on this device.\n  //\n  //  c. There may be buffers in device memory (not mapped into host\n  //     address space) that are not arguments to this kernel.  They will be\n  //     mapped by these calls, and then unmapped (and thus put back\n  //     into device memory after reprogramming).\n  //\n  //  d. There may be buffers mapped into the host that are not\n  //     arguments to this kernel. So they don't fall under case (a).\n  //     So they have a positive map count before mapping as\n  //     writable here.  After programming, they will be unmapped\n  //     again.\n  //     We split into two cases:\n  //       d.1.  The buffers were originally mapped for write\n  //             (CL_MAP_WRITE).\n  //             After mapping again for write, here, then\n  //             programming, then unmapping, they will continue to\n  //             be mapped with CL_MAP_WRITE.  Just exactly as the\n  //             user expects.  (Our automatic mapping was invisible\n  //             to the user.)\n  //       d.2.  The buffers were originally mapped read-only, CL_MAP_READ.\n  //             After mapping as CL_MAP_WRITE here, then reprogramming,\n  //             then unmapped, those buffers will *stay*\n  //             as CL_MAP_WRITE after the unmap.\n  //             But this is ok because:\n  //                d.2.i.   The programmer promised not to update the data.\n  //                d.2.ii.  This buffer is not valid to use in a\n  //                         kernel until it is completely unmapped again, by\n  //                         user unmap calls.\n  //             So it is ok for the host to have the writable copy.\n  cl_mem mem = 0;\n  acl_block_allocation_t *block = 0;\n  acl_assert_locked();\n\n  if (debug_mode > 0) {\n    printf(\" Explicit read of all device side buffers\\n\");\n    printf(\" Context %p gm %p. ... p gm %p \\n\", _context, _context->global_mem,\n           &acl_platform.global_mem);\n    printf(\" first_mem %p\\n\", _context->global_mem->first_block);\n  }\n\n  for (block = _context->global_mem->first_block; block != NULL;\n       block = block->next_block_in_region) {\n    mem = block->mem_obj;\n\n    if (debug_mode > 0) {\n      printf(\"   Consider:\\n\");\n      acl_dump_mem_internal(mem);\n    }\n\n    if (mem->allocation_deferred || mem->mem_cpy_host_ptr_pending ||\n        ACL_GET_PHYSICAL_ID(mem->block_allocation->range.begin) !=\n            physical_device_id) {\n      // If the memory isn't actually allocated yet OR it's not on the device\n      // being reprogrammed then just skip OR we haven't even dont the memory\n      // copied to the device yet\n      continue;\n    }\n    // Copy to host only if the writable copy is not on the host.\n    if (!mem->writable_copy_on_host) {\n      cl_context context2 = mem->context;\n      acl_command_info_t cmd;\n\n      if (debug_mode > 0) {\n        printf(\" Explicit read of mem [%p]\\n\", mem);\n        acl_dump_mem_internal(mem);\n      }\n\n      cmd.type = CL_COMMAND_READ_BUFFER;\n      cmd.trivial = 0; // not used\n      cmd.info.mem_xfer.src_mem = mem;\n      cmd.info.mem_xfer.src_offset[0] = 0;\n      cmd.info.mem_xfer.src_offset[1] = 0;\n      cmd.info.mem_xfer.src_offset[2] = 0;\n      cmd.info.mem_xfer.dst_mem = context2->unwrapped_host_mem;\n      if (mem->flags & CL_MEM_USE_HOST_PTR) {\n        cmd.info.mem_xfer.dst_offset[0] =\n            (size_t)((char *)mem->fields.buffer_objs.host_ptr -\n                     (char *)ACL_MEM_ALIGN);\n      } else {\n        cmd.info.mem_xfer.dst_offset[0] =\n            (size_t)((char *)mem->host_mem.aligned_ptr - (char *)ACL_MEM_ALIGN);\n      }\n      cmd.info.mem_xfer.dst_offset[1] = 0;\n      cmd.info.mem_xfer.dst_offset[2] = 0;\n      cmd.info.mem_xfer.is_auto_map = 0;\n      if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER) {\n        cmd.info.mem_xfer.cb[0] = mem->size;\n        cmd.info.mem_xfer.cb[1] = 1;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_height;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE3D) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_height;\n        cmd.info.mem_xfer.cb[2] =\n            mem->fields.image_objs.image_desc->image_depth;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_height;\n        cmd.info.mem_xfer.cb[2] =\n            mem->fields.image_objs.image_desc->image_array_size;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n                 mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] = 1;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_array_size;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else {\n        acl_context_callback(context2,\n                             \"Invalid cl_mem object mirrored to device.\");\n        cmd.info.mem_xfer.cb[0] = mem->size;\n        cmd.info.mem_xfer.cb[1] = 1;\n        cmd.info.mem_xfer.cb[2] = 1;\n      }\n      cmd.info.mem_xfer.map_flags = 0; // not used\n      cmd.info.mem_xfer.src_row_pitch = cmd.info.mem_xfer.cb[0];\n      cmd.info.mem_xfer.src_slice_pitch = 1;\n      cmd.info.mem_xfer.dst_row_pitch = cmd.info.mem_xfer.cb[0];\n      cmd.info.mem_xfer.dst_slice_pitch = 1;\n\n      if (read_callback)\n        read_callback(mem, 0);\n      l_mem_transfer_buffer_explicitly(context2, 0 /*blocking*/,\n                                       physical_device_id, cmd);\n      if (read_callback)\n        read_callback(mem, 1);\n    }\n  }\n}\n\nvoid acl_copy_device_buffers_from_host_after_programming(\n    cl_context _context, unsigned int physical_device_id,\n    void(CL_CALLBACK *write_callback)(cl_mem, int)) {\n  // Copy all device buffers back from host memory.\n  cl_mem mem = 0;\n  acl_block_allocation_t *block = 0;\n  acl_assert_locked();\n\n  acl_print_debug_msg(\" Explicit write of all device side buffers\\n\");\n\n  for (block = _context->global_mem->first_block; block != NULL;\n       block = block->next_block_in_region) {\n    mem = block->mem_obj;\n\n    acl_print_debug_msg(\"   write mem[%p]?\\n\", mem);\n\n    if (mem->allocation_deferred || mem->mem_cpy_host_ptr_pending ||\n        ACL_GET_PHYSICAL_ID(mem->block_allocation->range.begin) !=\n            physical_device_id) {\n      // If the memory isn't actually allocated yet OR it's not on the device\n      // being reprogrammed then just skip OR we haven't even dont the memory\n      // copied to the device yet\n      continue;\n    }\n    // Copy back from host only if the \"writable\" copy is not on the host.\n    if (!mem->writable_copy_on_host) {\n      cl_context context2 = mem->context;\n      acl_command_info_t cmd;\n      acl_print_debug_msg(\" Explicit write of mem[%p]\\n\", mem);\n\n      if (debug_mode > 0) {\n        acl_dump_mem_internal(mem);\n      }\n\n      cmd.type = CL_COMMAND_WRITE_BUFFER;\n      cmd.info.mem_xfer.src_mem = context2->unwrapped_host_mem;\n      if (mem->flags & CL_MEM_USE_HOST_PTR) {\n        cmd.info.mem_xfer.src_offset[0] =\n            (size_t)((char *)mem->fields.buffer_objs.host_ptr -\n                     (char *)ACL_MEM_ALIGN);\n      } else {\n        cmd.info.mem_xfer.src_offset[0] =\n            (size_t)((char *)mem->host_mem.aligned_ptr - (char *)ACL_MEM_ALIGN);\n      }\n      cmd.info.mem_xfer.src_offset[1] = 0;\n      cmd.info.mem_xfer.src_offset[2] = 0;\n      cmd.info.mem_xfer.dst_mem = mem;\n      cmd.info.mem_xfer.dst_offset[0] = 0;\n      cmd.info.mem_xfer.dst_offset[1] = 0;\n      cmd.info.mem_xfer.dst_offset[2] = 0;\n      cmd.info.mem_xfer.is_auto_map = 0;\n      cmd.trivial = 0; // not used\n      if (mem->mem_object_type == CL_MEM_OBJECT_BUFFER) {\n        cmd.info.mem_xfer.cb[0] = mem->size;\n        cmd.info.mem_xfer.cb[1] = 1;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_height;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE3D) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_height;\n        cmd.info.mem_xfer.cb[2] =\n            mem->fields.image_objs.image_desc->image_depth;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE2D_ARRAY) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_height;\n        cmd.info.mem_xfer.cb[2] =\n            mem->fields.image_objs.image_desc->image_array_size;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D ||\n                 mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_BUFFER) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] = 1;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else if (mem->mem_object_type == CL_MEM_OBJECT_IMAGE1D_ARRAY) {\n        cl_int local_errcode_ret;\n        size_t element_size = acl_get_image_element_size(\n            mem->context, mem->fields.image_objs.image_format,\n            &local_errcode_ret);\n        if (local_errcode_ret != CL_SUCCESS) {\n          acl_context_callback(context2,\n                               \"Invalid cl_mem object mirrored to device.\");\n        }\n        cmd.info.mem_xfer.cb[0] =\n            mem->fields.image_objs.image_desc->image_width * element_size;\n        cmd.info.mem_xfer.cb[1] =\n            mem->fields.image_objs.image_desc->image_array_size;\n        cmd.info.mem_xfer.cb[2] = 1;\n      } else {\n        acl_context_callback(context2,\n                             \"Invalid cl_mem object mirrored to device.\");\n        cmd.info.mem_xfer.cb[0] = mem->size;\n        cmd.info.mem_xfer.cb[1] = 1;\n        cmd.info.mem_xfer.cb[2] = 1;\n      }\n      cmd.info.mem_xfer.map_flags = 0; // not used\n\n      cmd.info.mem_xfer.src_row_pitch = cmd.info.mem_xfer.cb[0];\n      cmd.info.mem_xfer.src_slice_pitch = 1;\n      cmd.info.mem_xfer.dst_row_pitch = cmd.info.mem_xfer.cb[0];\n      cmd.info.mem_xfer.dst_slice_pitch = 1;\n\n      if (write_callback)\n        write_callback(mem, 0);\n      l_mem_transfer_buffer_explicitly(context2, 0 /*blocking*/,\n                                       physical_device_id, cmd);\n      if (write_callback)\n        write_callback(mem, 1);\n    }\n  }\n}\n\nstatic void acl_print_all_mem_in_region(acl_mem_region_t *region);\nvoid acl_print_all_mem(void) {\n  acl_assert_locked();\n\n  acl_print_debug_msg(\"===============================================Current \"\n                      \"memory allocations=====\\n\");\n\n  acl_print_debug_msg(\"Host_auto_mem:\");\n  acl_print_all_mem_in_region(&acl_platform.host_auto_mem);\n\n  acl_print_debug_msg(\"Host user mem:\");\n  acl_print_all_mem_in_region(&acl_platform.host_user_mem);\n\n  acl_print_debug_msg(\"Global mem:\");\n  acl_print_all_mem_in_region(&acl_platform.global_mem);\n\n  acl_print_debug_msg(\"Warning: if using an offline device, then not showing \"\n                      \"emulated host mem\\n\");\n\n  acl_print_debug_msg(\"========================================================\"\n                      \"======================\\n\\n\");\n}\n\nstatic void acl_print_all_mem_in_region(acl_mem_region_t *region) {\n  int num_prints = 0;\n  acl_block_allocation_t *block;\n  cl_mem mem;\n  acl_assert_locked();\n\n  for (block = region->first_block; block != NULL;\n       block = block->next_block_in_region) {\n    mem = block->mem_obj;\n    if (num_prints == 0)\n      acl_print_debug_msg(\"\\n\");\n\n    acl_print_debug_msg(\"\\tAt %p: Range from %p to %p, size %lu\", mem,\n                        block->range.begin, block->range.next,\n                        mem->size <= 10000 ? mem->size : 9999);\n    if (mem->size > 10000)\n      acl_print_debug_msg(\"+\\n\");\n    else\n      acl_print_debug_msg(\"\\n\");\n    ++num_prints;\n  }\n  if (num_prints == 0)\n    acl_print_debug_msg(\"\\tN/A\\n\");\n}\n\nvoid acl_print_mem(cl_mem mem) {\n  acl_assert_locked();\n\n  if (debug_mode > 0) {\n    printf(\"mem at %p:\\n\", mem);\n    printf(\"\\t- Range from %p to %p\\n\", mem->block_allocation->range.begin,\n           mem->block_allocation->range.next);\n    printf(\"\\t- Size %zu\\n\", mem->size);\n    printf(\"\\t- Refcount = %d\\n\", acl_ref_count(mem));\n    printf(\"\\t- Context %p\\n\", mem->context);\n    printf(\"\\t- Host ptr = %p\\n\", mem->fields.buffer_objs.host_ptr);\n  }\n}\n\nstatic void acl_print_all_detailed_mem_in_region(acl_mem_region_t *region);\nvoid acl_print_all_mem_detail(void) {\n  acl_assert_locked();\n\n  if (acl_platform.host_auto_mem.first_block == NULL &&\n      acl_platform.host_user_mem.first_block == NULL &&\n      acl_platform.global_mem.first_block == NULL) {\n    acl_print_debug_msg(\"=====================================================\"\n                        \"No mem allocated.========\\n\\n\");\n    return;\n  }\n\n  acl_print_debug_msg(\"===============================================Current \"\n                      \"memory allocations=====\\n\");\n\n  acl_print_debug_msg(\"Host_auto_mem:\");\n  acl_print_all_detailed_mem_in_region(&acl_platform.host_auto_mem);\n\n  acl_print_debug_msg(\"Host user mem:\");\n  acl_print_all_detailed_mem_in_region(&acl_platform.host_user_mem);\n\n  acl_print_debug_msg(\"Global mem:\");\n  acl_print_all_detailed_mem_in_region(&acl_platform.global_mem);\n\n  acl_print_debug_msg(\"Warning: if using an offline device, then not showing \"\n                      \"emulated host mem\\n\");\n\n  acl_print_debug_msg(\"========================================================\"\n                      \"======================\\n\\n\");\n}\n\nstatic void acl_print_all_detailed_mem_in_region(acl_mem_region_t *region) {\n  int num_prints = 0;\n  acl_block_allocation_t *block;\n  cl_mem mem;\n  acl_assert_locked();\n\n  for (block = region->first_block; block != NULL;\n       block = block->next_block_in_region) {\n    mem = block->mem_obj;\n    if (num_prints == 0)\n      acl_print_debug_msg(\"\\n\");\n    acl_print_debug_msg(\"   \");\n    acl_print_mem(mem);\n    ++num_prints;\n  }\n  if (num_prints == 0)\n    acl_print_debug_msg(\"\\tN/A\\n\");\n}\n\n#ifdef ACL_DEBUG\nvoid acl_dump_mem(cl_mem mem) { acl_dump_mem_internal(mem); }\n#endif\n\nstatic void acl_dump_mem_internal(cl_mem mem) {\n  acl_assert_locked();\n\n  if (debug_mode > 0) {\n    printf(\"           Mem[%p] = {\\n\", mem);\n    printf(\"              .refcnt             %d\\n\", acl_ref_count(mem));\n    printf(\"              .flags              0x%x\\n\",\n           (unsigned int)mem->flags);\n    printf(\"              %s \\n\",\n           (mem->writable_copy_on_host ? \"writable copy on host\"\n                                       : \"writable copy on device\"));\n    if (mem->block_allocation != NULL) {\n      printf(\"              .region             %p\\n\",\n             mem->block_allocation->region);\n      printf(\"              %s \\n\",\n             (mem->block_allocation->region->is_user_provided\n                  ? \"user provided\"\n                  : \"not user provided\"));\n      printf(\"              %s \\n\",\n             (mem->block_allocation->region->is_host_accessible\n                  ? \"host accessible\"\n                  : \"not host accessible\"));\n      printf(\"              %s \\n\",\n             (mem->block_allocation->region->is_device_accessible\n                  ? \"device accessible\"\n                  : \"not device accessible\"));\n      printf(\"              %s \\n\",\n             (mem->block_allocation->region->uses_host_system_malloc\n                  ? \"is malloc\"\n                  : \"not malloc\"));\n      printf(\"              .begin             %p\\n\",\n             mem->block_allocation->range.begin);\n      printf(\"              .end               %p\\n\",\n             mem->block_allocation->range.next);\n    }\n    printf(\"              .mappings          %d\\n\", mem->mapping_count);\n    acl_print_debug_msg(\"              .size              %lu\\n\", mem->size);\n    printf(\"              .host_ptr          %p\\n\",\n           mem->fields.buffer_objs.host_ptr);\n    printf(\"              .host_mem.aligned_ptr  %p\\n\",\n           mem->host_mem.aligned_ptr);\n    printf(\"              .host_mem.raw      %p\\n\", mem->host_mem.raw);\n    printf(\"           }\\n\");\n  }\n}\n\nsize_t acl_get_image_element_size(cl_context context,\n                                  const cl_image_format *image_format,\n                                  cl_int *errcode_ret) {\n  size_t channel_data_type_size;\n  cl_bool multiply_by_channel_order;\n  size_t channel_order_size;\n  acl_assert_locked();\n\n  switch (image_format->image_channel_data_type) {\n  case CL_SNORM_INT8:\n  case CL_SIGNED_INT8:\n    channel_data_type_size = 1;\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_SNORM_INT16:\n  case CL_SIGNED_INT16:\n    channel_data_type_size = 2;\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_SIGNED_INT32:\n    channel_data_type_size = 4;\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_UNORM_INT8:\n  case CL_UNSIGNED_INT8:\n    channel_data_type_size = 1;\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_UNORM_INT16:\n  case CL_UNSIGNED_INT16:\n    channel_data_type_size = 2;\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_UNSIGNED_INT32:\n    channel_data_type_size = 4;\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_HALF_FLOAT:\n    channel_data_type_size = sizeof(float) / 2;\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_FLOAT:\n    channel_data_type_size = sizeof(float);\n    multiply_by_channel_order = CL_TRUE;\n    break;\n  case CL_UNORM_SHORT_565:\n    channel_data_type_size = 2;\n    multiply_by_channel_order = CL_FALSE;\n    break;\n  case CL_UNORM_SHORT_555:\n    channel_data_type_size = 2;\n    multiply_by_channel_order = CL_FALSE;\n    break;\n  case CL_UNORM_INT_101010:\n    channel_data_type_size = 4;\n    multiply_by_channel_order = CL_FALSE;\n    break;\n  default:\n    BAIL_INFO(CL_INVALID_IMAGE_FORMAT_DESCRIPTOR, context,\n              \"image_channel_data_type not valid\");\n  }\n\n  switch (image_format->image_channel_order) {\n  case CL_R:\n  case CL_A:\n    channel_order_size = 1;\n    break;\n  case CL_INTENSITY:\n  case CL_LUMINANCE:\n    if (image_format->image_channel_data_type != CL_UNORM_INT8 &&\n        image_format->image_channel_data_type != CL_UNORM_INT16 &&\n        image_format->image_channel_data_type != CL_SNORM_INT8 &&\n        image_format->image_channel_data_type != CL_SNORM_INT16 &&\n        image_format->image_channel_data_type != CL_FLOAT &&\n        image_format->image_channel_data_type != CL_HALF_FLOAT) {\n      BAIL_INFO(CL_INVALID_IMAGE_FORMAT_DESCRIPTOR, context,\n                \"Combination of image_channel_order & image_channel_data_type \"\n                \"not supported\");\n    }\n    channel_order_size = 1;\n    break;\n  case CL_RG:\n  case CL_RA:\n    channel_order_size = 2;\n    break;\n  case CL_RGB:\n    channel_order_size = 3;\n    if (image_format->image_channel_data_type != CL_UNORM_SHORT_565 &&\n        image_format->image_channel_data_type != CL_UNORM_SHORT_555 &&\n        image_format->image_channel_data_type != CL_UNORM_INT_101010) {\n      BAIL_INFO(CL_INVALID_IMAGE_FORMAT_DESCRIPTOR, context,\n                \"Combination of image_channel_order & image_channel_data_type \"\n                \"not supported\");\n    }\n    break;\n  case CL_RGBA:\n    channel_order_size = 4;\n    break;\n  case CL_ARGB:\n  case CL_BGRA:\n    channel_order_size = 4;\n    if (image_format->image_channel_data_type != CL_UNORM_INT8 &&\n        image_format->image_channel_data_type != CL_SNORM_INT8 &&\n        image_format->image_channel_data_type != CL_SIGNED_INT8 &&\n        image_format->image_channel_data_type != CL_UNSIGNED_INT8) {\n      BAIL_INFO(CL_INVALID_IMAGE_FORMAT_DESCRIPTOR, context,\n                \"Combination of image_channel_order & image_channel_data_type \"\n                \"not supported\");\n    }\n    break;\n  default: {\n    std::stringstream ss;\n    ss << \"image_channel_order 0x\" << std::hex\n       << image_format->image_channel_order << \" not valid\";\n    BAIL_INFO(CL_INVALID_IMAGE_FORMAT_DESCRIPTOR, context, ss.str().c_str());\n  }\n  }\n  if (!multiply_by_channel_order) {\n    channel_order_size = 1;\n  }\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  return channel_data_type_size * channel_order_size;\n}\n\nsize_t get_offset_for_image_param(cl_context context,\n                                  cl_mem_object_type mem_object_type,\n                                  const char *name) {\n  acl_assert_locked();\n\n  switch (mem_object_type) {\n  case CL_MEM_OBJECT_IMAGE2D:\n    if (strncmp(name, \"width\", MAX_NAME_SIZE) == 0) {\n      return 0;\n    } else if (strncmp(name, \"height\", MAX_NAME_SIZE) == 0) {\n      return 4;\n    } else if (strncmp(name, \"channel_data_type\", MAX_NAME_SIZE) == 0) {\n      return 8;\n    } else if (strncmp(name, \"channel_order\", MAX_NAME_SIZE) == 0) {\n      return 12;\n    } else if (strncmp(name, \"element_size\", MAX_NAME_SIZE) == 0) {\n      return 16;\n    } else if (strncmp(name, \"data\", MAX_NAME_SIZE) == 0) {\n      // Check that the aligned location is at least as big\n      // as the minimum location we can fit data\n      assert(24 <= ACL_MEM_ALIGN);\n      return ACL_MEM_ALIGN;\n    } else {\n      acl_context_callback(context, \"Invalid or unsupported image field\");\n      return 0;\n    }\n    break;\n  case CL_MEM_OBJECT_IMAGE3D:\n    if (strncmp(name, \"width\", MAX_NAME_SIZE) == 0) {\n      return 0;\n    } else if (strncmp(name, \"height\", MAX_NAME_SIZE) == 0) {\n      return 4;\n    } else if (strncmp(name, \"depth\", MAX_NAME_SIZE) == 0) {\n      return 8;\n    } else if (strncmp(name, \"channel_data_type\", MAX_NAME_SIZE) == 0) {\n      return 12;\n    } else if (strncmp(name, \"channel_order\", MAX_NAME_SIZE) == 0) {\n      return 16;\n    } else if (strncmp(name, \"element_size\", MAX_NAME_SIZE) == 0) {\n      return 24;\n    } else if (strncmp(name, \"data\", MAX_NAME_SIZE) == 0) {\n      // Check that the aligned location is at least as big\n      // as the minimum location we can fit data\n      assert(32 <= ACL_MEM_ALIGN);\n      return ACL_MEM_ALIGN;\n    } else {\n      acl_context_callback(context, \"Invalid or unsupported image field\");\n      return 0;\n    }\n    break;\n  case CL_MEM_OBJECT_IMAGE2D_ARRAY:\n    if (strncmp(name, \"width\", MAX_NAME_SIZE) == 0) {\n      return 0;\n    } else if (strncmp(name, \"height\", MAX_NAME_SIZE) == 0) {\n      return 4;\n    } else if (strncmp(name, \"array_size\", MAX_NAME_SIZE) == 0) {\n      return 8;\n    } else if (strncmp(name, \"channel_data_type\", MAX_NAME_SIZE) == 0) {\n      return 12;\n    } else if (strncmp(name, \"channel_order\", MAX_NAME_SIZE) == 0) {\n      return 16;\n    } else if (strncmp(name, \"element_size\", MAX_NAME_SIZE) == 0) {\n      return 24;\n    } else if (strncmp(name, \"data\", MAX_NAME_SIZE) == 0) {\n      // Check that the aligned location is at least as big\n      // as the minimum location we can fit data\n      assert(32 <= ACL_MEM_ALIGN);\n      return ACL_MEM_ALIGN;\n    } else {\n      acl_context_callback(context, \"Invalid or unsupported image field\");\n      return 0;\n    }\n    break;\n  case CL_MEM_OBJECT_IMAGE1D:\n    if (strncmp(name, \"width\", MAX_NAME_SIZE) == 0) {\n      return 0;\n    } else if (strncmp(name, \"channel_data_type\", MAX_NAME_SIZE) == 0) {\n      return 12;\n    } else if (strncmp(name, \"channel_order\", MAX_NAME_SIZE) == 0) {\n      return 16;\n    } else if (strncmp(name, \"element_size\", MAX_NAME_SIZE) == 0) {\n      return 24;\n    } else if (strncmp(name, \"data\", MAX_NAME_SIZE) == 0) {\n      // Check that the aligned location is at least as big\n      // as the minimum location we can fit data\n      assert(32 <= ACL_MEM_ALIGN);\n      return ACL_MEM_ALIGN;\n    } else {\n      acl_context_callback(context, \"Invalid or unsupported image field\");\n      return 0;\n    }\n    break;\n  case CL_MEM_OBJECT_IMAGE1D_ARRAY:\n    if (strncmp(name, \"width\", MAX_NAME_SIZE) == 0) {\n      return 0;\n    } else if (strncmp(name, \"array_size\", MAX_NAME_SIZE) == 0) {\n      return 8;\n    } else if (strncmp(name, \"channel_data_type\", MAX_NAME_SIZE) == 0) {\n      return 12;\n    } else if (strncmp(name, \"channel_order\", MAX_NAME_SIZE) == 0) {\n      return 16;\n    } else if (strncmp(name, \"element_size\", MAX_NAME_SIZE) == 0) {\n      return 24;\n    } else if (strncmp(name, \"data\", MAX_NAME_SIZE) == 0) {\n      // Check that the aligned location is at least as big\n      // as the minimum location we can fit data\n      assert(32 <= ACL_MEM_ALIGN);\n      return ACL_MEM_ALIGN;\n    } else {\n      acl_context_callback(context, \"Invalid or unsupported image field\");\n      return 0;\n    }\n    break;\n  case CL_MEM_OBJECT_IMAGE1D_BUFFER:\n    if (strncmp(name, \"width\", MAX_NAME_SIZE) == 0) {\n      return 0;\n    } else if (strncmp(name, \"channel_data_type\", MAX_NAME_SIZE) == 0) {\n      return 12;\n    } else if (strncmp(name, \"channel_order\", MAX_NAME_SIZE) == 0) {\n      return 16;\n    } else if (strncmp(name, \"element_size\", MAX_NAME_SIZE) == 0) {\n      return 24;\n    } else if (strncmp(name, \"data\", MAX_NAME_SIZE) == 0) {\n      // Check that the aligned location is at least as big\n      // as the minimum location we can fit data\n      assert(32 <= ACL_MEM_ALIGN);\n      return ACL_MEM_ALIGN;\n    } else {\n      acl_context_callback(context, \"Invalid or unsupported image field\");\n      return 0;\n    }\n    break;\n  default:\n    acl_context_callback(context, \"Invalid image type used\");\n    return 0;\n    break;\n  }\n}\n\nint acl_submit_migrate_mem_device_op(cl_event event) {\n  int result = 0;\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  if (!acl_command_queue_is_valid(event->command_queue)) {\n    return result;\n  }\n  // No user-level scheduling blocks this memory transfer.\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  if (!event->last_device_op &&\n      event->cmd.type == CL_COMMAND_MIGRATE_MEM_OBJECTS) {\n    unsigned int ibuf;\n    int ok = 1;\n    acl_mem_migrate_t memory_migration = event->cmd.info.memory_migration;\n    acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n    acl_device_op_t *last_op = 0;\n\n    // Precautionary, but it also nudges the device scheduler to try\n    // to free up old operation slots.\n    acl_forget_proposed_device_ops(doq);\n\n    // Arrange for mem migration\n    for (ibuf = 0; ok && ibuf < memory_migration.num_mem_objects; ibuf++) {\n      acl_device_op_t *last_op2 = acl_propose_indexed_device_op(\n          doq, ACL_DEVICE_OP_MEM_MIGRATION, event, ibuf);\n      ok = (last_op2 != NULL);\n    }\n\n    if (ok) {\n      // We managed to enqueue everything.\n      event->last_device_op = last_op;\n      acl_commit_proposed_device_ops(doq);\n      result = 1;\n    } else {\n      // Back off, and wait until later when we have more space in the\n      // device op queue.\n      acl_forget_proposed_device_ops(doq);\n    }\n  }\n  return result;\n}\n\n// This function takes an image element with a cl_image_format format_from and\n// outputs an image element with cl_image_format format_to. The implementation\n// does not cover all of the cases (yet), as it is written to only cover the use\n// cases by clEnqueueFillImage and currently supported image formats. It is\n// straight forward to add other cases, as per needed.\ncl_int acl_convert_image_format(const void *input_element, void *output_element,\n                                cl_image_format format_from,\n                                cl_image_format format_to) {\n  cl_int status = 0;\n  if (format_from.image_channel_order == CL_RGBA) {\n    switch (format_to.image_channel_order) {\n    case CL_R:\n      switch (format_from.image_channel_data_type) {\n      case CL_FLOAT:\n        switch (format_to.image_channel_data_type) {\n        case CL_SNORM_INT8: // { CL_RGBA, CL_FLOAT } ->  { CL_R, CL_SNORM_INT8 }\n          ((cl_char *)output_element)[0] =\n              (cl_char)(((cl_float *)input_element)[0] * 127.0f);\n          break;\n        case CL_SNORM_INT16: // { CL_RGBA, CL_FLOAT } ->  { CL_R, CL_SNORM_INT16\n                             // }\n          ((cl_short *)output_element)[0] =\n              (cl_short)(((cl_float *)input_element)[0] * 32767.0f);\n          break;\n        case CL_UNORM_INT8: // { CL_RGBA, CL_FLOAT } ->  { CL_R, CL_UNORM_INT8 }\n          ((cl_uchar *)output_element)[0] =\n              (cl_uchar)(((cl_float *)input_element)[0] * 255.0f);\n          break;\n        case CL_UNORM_INT16: // { CL_RGBA, CL_FLOAT } ->  { CL_R, CL_UNORM_INT16\n                             // }\n          ((cl_ushort *)output_element)[0] =\n              (cl_ushort)(((cl_float *)input_element)[0] * 65535.0f);\n          break;\n        case CL_FLOAT: // { CL_RGBA, CL_FLOAT } ->  { CL_R, CL_FLOAT }\n          ((cl_float *)output_element)[0] =\n              (cl_float)(((cl_float *)input_element)[0]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      case CL_SIGNED_INT32:\n        switch (format_to.image_channel_data_type) {\n        case CL_SIGNED_INT8: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_R,\n                             // CL_SIGNED_INT8 }\n          ((cl_char *)output_element)[0] =\n              (cl_char)(((cl_int *)input_element)[0] & 0xFF);\n          break;\n        case CL_SIGNED_INT16: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_R,\n                              // CL_SIGNED_INT16 }\n          ((cl_short *)output_element)[0] =\n              (cl_short)(((cl_int *)input_element)[0] & 0xFFFF);\n          break;\n        case CL_SIGNED_INT32: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_R,\n                              // CL_SIGNED_INT32 }\n          ((cl_int *)output_element)[0] =\n              (cl_int)(((cl_int *)input_element)[0]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      case CL_UNSIGNED_INT32:\n        switch (format_to.image_channel_data_type) {\n        case CL_UNSIGNED_INT8: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_R,\n                               // CL_UNSIGNED_INT8 }\n          ((cl_uchar *)output_element)[0] =\n              (cl_uchar)(((cl_uint *)input_element)[0] & 0xFF);\n          break;\n        case CL_UNSIGNED_INT16: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_R,\n                                // CL_UNSIGNED_INT16 }\n          ((cl_ushort *)output_element)[0] =\n              (cl_ushort)(((cl_uint *)input_element)[0] & 0xFFFF);\n          break;\n        case CL_UNSIGNED_INT32: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_R,\n                                // CL_UNSIGNED_INT32 }\n          ((cl_uint *)output_element)[0] =\n              (cl_uint)(((cl_uint *)input_element)[0]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      default:\n        status = -1; // Returning -1 for conversions not supported (yet).\n      }              // 2nd switch (format_from.image_channel_data_type)\n      break;\n    //**********************************************\n    case CL_RG:\n      switch (format_from.image_channel_data_type) {\n      case CL_FLOAT:\n        switch (format_to.image_channel_data_type) {\n        case CL_SNORM_INT8: // { CL_RGBA, CL_FLOAT } ->  { CL_RG, CL_SNORM_INT8\n                            // }\n          ((cl_char *)output_element)[0] =\n              (cl_char)(((cl_float *)input_element)[0] * 127.0f);\n          ((cl_char *)output_element)[1] =\n              (cl_char)(((cl_float *)input_element)[1] * 127.0f);\n          break;\n        case CL_SNORM_INT16: // { CL_RGBA, CL_FLOAT } ->  { CL_RG,\n                             // CL_SNORM_INT16 }\n          ((cl_short *)output_element)[0] =\n              (cl_short)(((cl_float *)input_element)[0] * 32767.0f);\n          ((cl_short *)output_element)[1] =\n              (cl_short)(((cl_float *)input_element)[1] * 32767.0f);\n          break;\n        case CL_UNORM_INT8: // { CL_RGBA, CL_FLOAT } ->  { CL_RG, CL_UNORM_INT8\n                            // }\n          ((cl_uchar *)output_element)[0] =\n              (cl_uchar)(((cl_float *)input_element)[0] * 255.0f);\n          ((cl_uchar *)output_element)[1] =\n              (cl_uchar)(((cl_float *)input_element)[1] * 255.0f);\n          break;\n        case CL_UNORM_INT16: // { CL_RGBA, CL_FLOAT } ->  { CL_RG,\n                             // CL_UNORM_INT16 }\n          ((cl_ushort *)output_element)[0] =\n              (cl_ushort)(((cl_float *)input_element)[0] * 65535.0f);\n          ((cl_ushort *)output_element)[1] =\n              (cl_ushort)(((cl_float *)input_element)[1] * 65535.0f);\n          break;\n        case CL_FLOAT: // { CL_RGBA, CL_FLOAT } ->  { CL_RG, CL_FLOAT }\n          ((cl_float *)output_element)[0] =\n              (cl_float)(((cl_float *)input_element)[0]);\n          ((cl_float *)output_element)[1] =\n              (cl_float)(((cl_float *)input_element)[1]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      case CL_SIGNED_INT32:\n        switch (format_to.image_channel_data_type) {\n        case CL_SIGNED_INT8: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_RG,\n                             // CL_SIGNED_INT8 }\n          ((cl_char *)output_element)[0] =\n              (cl_char)(((cl_int *)input_element)[0] & 0xFF);\n          ((cl_char *)output_element)[1] =\n              (cl_char)(((cl_int *)input_element)[1] & 0xFF);\n          break;\n        case CL_SIGNED_INT16: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_RG,\n                              // CL_SIGNED_INT16 }\n          ((cl_short *)output_element)[0] =\n              (cl_short)(((cl_int *)input_element)[0] & 0xFFFF);\n          ((cl_short *)output_element)[1] =\n              (cl_short)(((cl_int *)input_element)[1] & 0xFFFF);\n          break;\n        case CL_SIGNED_INT32: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_RG,\n                              // CL_SIGNED_INT32 }\n          ((cl_int *)output_element)[0] =\n              (cl_int)(((cl_int *)input_element)[0]);\n          ((cl_int *)output_element)[1] =\n              (cl_int)(((cl_int *)input_element)[1]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      case CL_UNSIGNED_INT32:\n        switch (format_to.image_channel_data_type) {\n        case CL_UNSIGNED_INT8: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_RG,\n                               // CL_UNSIGNED_INT8 }\n          ((cl_uchar *)output_element)[0] =\n              (cl_uchar)(((cl_uint *)input_element)[0] & 0xFF);\n          ((cl_uchar *)output_element)[1] =\n              (cl_uchar)(((cl_uint *)input_element)[1] & 0xFF);\n          break;\n        case CL_UNSIGNED_INT16: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_RG,\n                                // CL_UNSIGNED_INT16 }\n          ((cl_ushort *)output_element)[0] =\n              (cl_ushort)(((cl_uint *)input_element)[0] & 0xFFFF);\n          ((cl_ushort *)output_element)[1] =\n              (cl_ushort)(((cl_uint *)input_element)[1] & 0xFFFF);\n          break;\n        case CL_UNSIGNED_INT32: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_RG,\n                                // CL_UNSIGNED_INT32 }\n          ((cl_uint *)output_element)[0] =\n              (cl_uint)(((cl_uint *)input_element)[0]);\n          ((cl_uint *)output_element)[1] =\n              (cl_uint)(((cl_uint *)input_element)[1]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      default:\n        status = -1; // Returning -1 for conversions not supported (yet).\n      }              // 2nd switch\n      break;\n    //**********************************************\n    case CL_RGBA:\n      switch (format_from.image_channel_data_type) {\n      case CL_FLOAT:\n        switch (format_to.image_channel_data_type) {\n        case CL_SNORM_INT8: // { CL_RGBA, CL_FLOAT } ->  { CL_RGBA,\n                            // CL_SNORM_INT8 }\n          ((cl_char *)output_element)[0] =\n              (cl_char)(((cl_float *)input_element)[0] * 127.0f);\n          ((cl_char *)output_element)[1] =\n              (cl_char)(((cl_float *)input_element)[1] * 127.0f);\n          ((cl_char *)output_element)[2] =\n              (cl_char)(((cl_float *)input_element)[2] * 127.0f);\n          ((cl_char *)output_element)[3] =\n              (cl_char)(((cl_float *)input_element)[3] * 127.0f);\n          break;\n        case CL_SNORM_INT16: // { CL_RGBA, CL_FLOAT } ->  { CL_RGBA,\n                             // CL_SNORM_INT16 }\n          ((cl_short *)output_element)[0] =\n              (cl_short)(((cl_float *)input_element)[0] * 32767.0f);\n          ((cl_short *)output_element)[1] =\n              (cl_short)(((cl_float *)input_element)[1] * 32767.0f);\n          ((cl_short *)output_element)[2] =\n              (cl_short)(((cl_float *)input_element)[2] * 32767.0f);\n          ((cl_short *)output_element)[3] =\n              (cl_short)(((cl_float *)input_element)[3] * 32767.0f);\n          break;\n        case CL_UNORM_INT8: // { CL_RGBA, CL_FLOAT } ->  { CL_RGBA,\n                            // CL_UNORM_INT8 }\n          ((cl_uchar *)output_element)[0] =\n              (cl_uchar)(((cl_float *)input_element)[0] * 255.0f);\n          ((cl_uchar *)output_element)[1] =\n              (cl_uchar)(((cl_float *)input_element)[1] * 255.0f);\n          ((cl_uchar *)output_element)[2] =\n              (cl_uchar)(((cl_float *)input_element)[2] * 255.0f);\n          ((cl_uchar *)output_element)[3] =\n              (cl_uchar)(((cl_float *)input_element)[3] * 255.0f);\n          break;\n        case CL_UNORM_INT16: // { CL_RGBA, CL_FLOAT } ->  { CL_RGBA,\n                             // CL_UNORM_INT16 }\n          ((cl_ushort *)output_element)[0] =\n              (cl_ushort)(((cl_float *)input_element)[0] * 65535.0f);\n          ((cl_ushort *)output_element)[1] =\n              (cl_ushort)(((cl_float *)input_element)[1] * 65535.0f);\n          ((cl_ushort *)output_element)[2] =\n              (cl_ushort)(((cl_float *)input_element)[2] * 65535.0f);\n          ((cl_ushort *)output_element)[3] =\n              (cl_ushort)(((cl_float *)input_element)[3] * 65535.0f);\n          break;\n        case CL_FLOAT: // { CL_RGBA, CL_FLOAT } ->  { CL_RGBA, CL_FLOAT }\n          ((cl_float *)output_element)[0] =\n              (cl_float)(((cl_float *)input_element)[0]);\n          ((cl_float *)output_element)[1] =\n              (cl_float)(((cl_float *)input_element)[1]);\n          ((cl_float *)output_element)[2] =\n              (cl_float)(((cl_float *)input_element)[2]);\n          ((cl_float *)output_element)[3] =\n              (cl_float)(((cl_float *)input_element)[3]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      case CL_SIGNED_INT32:\n        switch (format_to.image_channel_data_type) {\n        case CL_SIGNED_INT8: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_RGBA,\n                             // CL_SIGNED_INT8 }\n          ((cl_char *)output_element)[0] =\n              (cl_char)(((cl_int *)input_element)[0] & 0xFF);\n          ((cl_char *)output_element)[1] =\n              (cl_char)(((cl_int *)input_element)[1] & 0xFF);\n          ((cl_char *)output_element)[2] =\n              (cl_char)(((cl_int *)input_element)[2] & 0xFF);\n          ((cl_char *)output_element)[3] =\n              (cl_char)(((cl_int *)input_element)[3] & 0xFF);\n          break;\n        case CL_SIGNED_INT16: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_RGBA,\n                              // CL_SIGNED_INT16 }\n          ((cl_short *)output_element)[0] =\n              (cl_short)(((cl_int *)input_element)[0] & 0xFFFF);\n          ((cl_short *)output_element)[1] =\n              (cl_short)(((cl_int *)input_element)[1] & 0xFFFF);\n          ((cl_short *)output_element)[2] =\n              (cl_short)(((cl_int *)input_element)[2] & 0xFFFF);\n          ((cl_short *)output_element)[3] =\n              (cl_short)(((cl_int *)input_element)[3] & 0xFFFF);\n          break;\n        case CL_SIGNED_INT32: // { CL_RGBA, CL_SIGNED_INT32 } ->  { CL_RGBA,\n                              // CL_SIGNED_INT32 }\n          ((cl_int *)output_element)[0] =\n              (cl_int)(((cl_int *)input_element)[0]);\n          ((cl_int *)output_element)[1] =\n              (cl_int)(((cl_int *)input_element)[1]);\n          ((cl_int *)output_element)[2] =\n              (cl_int)(((cl_int *)input_element)[2]);\n          ((cl_int *)output_element)[3] =\n              (cl_int)(((cl_int *)input_element)[3]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      case CL_UNSIGNED_INT32:\n        switch (format_to.image_channel_data_type) {\n        case CL_UNSIGNED_INT8: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_RGBA,\n                               // CL_UNSIGNED_INT8 }\n          ((cl_uchar *)output_element)[0] =\n              (cl_uchar)(((cl_uint *)input_element)[0] & 0xFF);\n          ((cl_uchar *)output_element)[1] =\n              (cl_uchar)(((cl_uint *)input_element)[1] & 0xFF);\n          ((cl_uchar *)output_element)[2] =\n              (cl_uchar)(((cl_uint *)input_element)[2] & 0xFF);\n          ((cl_uchar *)output_element)[3] =\n              (cl_uchar)(((cl_uint *)input_element)[3] & 0xFF);\n          break;\n        case CL_UNSIGNED_INT16: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_RGBA,\n                                // CL_UNSIGNED_INT16 }\n          ((cl_ushort *)output_element)[0] =\n              (cl_ushort)(((cl_uint *)input_element)[0] & 0xFFFF);\n          ((cl_ushort *)output_element)[1] =\n              (cl_ushort)(((cl_uint *)input_element)[1] & 0xFFFF);\n          ((cl_ushort *)output_element)[2] =\n              (cl_ushort)(((cl_uint *)input_element)[2] & 0xFFFF);\n          ((cl_ushort *)output_element)[3] =\n              (cl_ushort)(((cl_uint *)input_element)[3] & 0xFFFF);\n          break;\n        case CL_UNSIGNED_INT32: // { CL_RGBA, CL_UNSIGNED_INT32 } ->  { CL_RGBA,\n                                // CL_UNSIGNED_INT32 }\n          ((cl_uint *)output_element)[0] =\n              (cl_uint)(((cl_uint *)input_element)[0]);\n          ((cl_uint *)output_element)[1] =\n              (cl_uint)(((cl_uint *)input_element)[1]);\n          ((cl_uint *)output_element)[2] =\n              (cl_uint)(((cl_uint *)input_element)[2]);\n          ((cl_uint *)output_element)[3] =\n              (cl_uint)(((cl_uint *)input_element)[3]);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n\n      default:\n        status = -1; // Returning -1 for conversions not supported (yet).\n      }              // 2nd switch\n      break;\n    //**********************************************\n    case CL_BGRA:\n      switch (format_from.image_channel_data_type) {\n      case CL_FLOAT:\n        switch (format_to.image_channel_data_type) {\n        case CL_UNORM_INT8: // { CL_RGBA, CL_FLOAT } ->  { CL_BGRA,\n                            // CL_UNORM_INT8 }\n          ((cl_uchar *)output_element)[0] =\n              (cl_uchar)(((cl_float *)input_element)[2] * 255.0f);\n          ((cl_uchar *)output_element)[1] =\n              (cl_uchar)(((cl_float *)input_element)[1] * 255.0f);\n          ((cl_uchar *)output_element)[2] =\n              (cl_uchar)(((cl_float *)input_element)[0] * 255.0f);\n          ((cl_uchar *)output_element)[3] =\n              (cl_uchar)(((cl_float *)input_element)[3] * 255.0f);\n          break;\n        default:\n          status = -1;\n        } // 3rd switch (format_to.image_channel_data_type)\n        break;\n      default:\n        status = -1;\n      } // 2nd switch (format_from.image_channel_data_type)\n      break;\n    default:\n      status = -1;\n    } // 1st switch (format_to.image_channel_order)\n  }   // if (format_from.image_channel_order == CL_RGBA)\n  else {\n    status = -1; // Returning -1 for conversions not supported (yet).\n  }\n  return status;\n}\n\nvoid *acl_get_physical_address(cl_mem mem, cl_device_id device) {\n  assert(!mem->writable_copy_on_host && \"Writable copy is not on device\");\n  return l_get_address_of_writable_copy(mem, device->def.physical_device_id,\n                                        NULL, CL_FALSE);\n}\n\n// Submit an op to the device op queue to read device global.\n// Return 1 if we made forward progress, 0 otherwise.\ncl_int acl_submit_read_device_global_device_op(cl_event event) {\n  acl_print_debug_msg(\n      \"Entering acl_submit_read_device_global_device_op function\\n\");\n  int result = 0;\n  acl_assert_locked();\n\n  // No user-level scheduling blocks this device global read\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  // Already enqueued.\n  if (event->last_device_op) {\n    return result;\n  }\n\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n  acl_device_op_t *last_op = 0;\n\n  // Precautionary, but it also nudges the device scheduler to try\n  // to free up old operation slots.\n  acl_forget_proposed_device_ops(doq);\n\n  last_op = acl_propose_device_op(doq, ACL_DEVICE_OP_DEVICE_GLOBAL_READ, event);\n\n  if (last_op) {\n    // We managed to enqueue everything.\n    event->last_device_op = last_op;\n    acl_commit_proposed_device_ops(doq);\n    result = 1;\n  } else {\n    // Back off, and wait until later when we have more space in the\n    // device op queue.\n    acl_forget_proposed_device_ops(doq);\n  }\n  acl_print_debug_msg(\n      \"Exiting acl_submit_read_device_global_device_op function\\n\");\n  return result;\n}\n\n// Submit a device global write device operation to the device op queue\ncl_int acl_submit_write_device_global_device_op(cl_event event) {\n  acl_print_debug_msg(\n      \"Entering acl_submit_write_device_global_device_op function\\n\");\n  int result = 0;\n  acl_assert_locked();\n\n  // No user-level scheduling blocks this device global write\n  // So submit it to the device op queue.\n  // But only if it isn't already enqueued there.\n  if (!acl_event_is_valid(event)) {\n    return result;\n  }\n  // Already enqueued.\n  if (event->last_device_op) {\n    return result;\n  }\n\n  acl_device_op_queue_t *doq = &(acl_platform.device_op_queue);\n  acl_device_op_t *last_op = 0;\n\n  // Precautionary, but it also nudges the device scheduler to try\n  // to free up old operation slots.\n  acl_forget_proposed_device_ops(doq);\n\n  last_op =\n      acl_propose_device_op(doq, ACL_DEVICE_OP_DEVICE_GLOBAL_WRITE, event);\n\n  if (last_op) {\n    // We managed to enqueue everything.\n    event->last_device_op = last_op;\n    acl_commit_proposed_device_ops(doq);\n    result = 1;\n  } else {\n    // Back off, and wait until later when we have more space in the\n    // device op queue.\n    acl_forget_proposed_device_ops(doq);\n  }\n  acl_print_debug_msg(\n      \"Exiting acl_submit_write_device_global_device_op function\\n\");\n  return result;\n}\n\n// Read from a device global\nvoid acl_read_device_global(void *user_data, acl_device_op_t *op) {\n  acl_print_debug_msg(\"Entering acl_read_device_global function\\n\");\n  cl_event event = op->info.event;\n  cl_int status = 0;\n\n  acl_assert_locked();\n\n  if (!acl_event_is_valid(event) ||\n      !acl_command_queue_is_valid(event->command_queue)) {\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n\n  acl_set_device_op_execution_status(op, CL_SUBMITTED);\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  status = acl_get_hal()->simulation_device_global_interface_read(\n      event->cmd.info.device_global_info.physical_device_id,\n      event->cmd.info.device_global_info.name,\n      event->cmd.info.device_global_info.read_ptr,\n      (size_t)event->cmd.info.device_global_info.device_global_addr +\n          event->cmd.info.device_global_info.offset,\n      event->cmd.info.device_global_info.size);\n  if (status == 0) {\n    acl_set_device_op_execution_status(op, CL_COMPLETE);\n  } else {\n    acl_set_device_op_execution_status(op, -1);\n  }\n  acl_print_debug_msg(\"Exiting acl_read_device_global function\\n\");\n}\n\n// Write into a device global\nvoid acl_write_device_global(void *user_data, acl_device_op_t *op) {\n\n  acl_print_debug_msg(\"Entering acl_write_device_global function\\n\");\n  cl_event event = op->info.event;\n  cl_int status = 0;\n\n  acl_assert_locked();\n\n  if (!acl_event_is_valid(event) ||\n      !acl_command_queue_is_valid(event->command_queue)) {\n    acl_set_device_op_execution_status(op, -1);\n    return;\n  }\n\n  acl_set_device_op_execution_status(op, CL_SUBMITTED);\n  acl_set_device_op_execution_status(op, CL_RUNNING);\n\n  status = acl_get_hal()->simulation_device_global_interface_write(\n      event->cmd.info.device_global_info.physical_device_id,\n      event->cmd.info.device_global_info.name,\n      event->cmd.info.device_global_info.write_ptr,\n      (size_t)event->cmd.info.device_global_info.device_global_addr +\n          event->cmd.info.device_global_info.offset,\n      event->cmd.info.device_global_info.size);\n\n  if (status == 0) {\n    acl_set_device_op_execution_status(op, CL_COMPLETE);\n  } else {\n    acl_set_device_op_execution_status(op, -1);\n  }\n  acl_print_debug_msg(\"Exiting acl_write_device_global function\\n\");\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_shared_aligned_ptr.cpp",
        "data": "// Copyright (C) 2019-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <cstdint>\n\n// Internal headers.\n#include <acl_shared_aligned_ptr.h>\n\nacl_shared_aligned_ptr::acl_shared_aligned_ptr(const std::size_t size) {\n  const auto alloc_size = size + ACL_MEM_ALIGN;\n  if (alloc_size < size)\n    return; // watch for wraparound!\n\n  m_raw = std::shared_ptr<char>(new char[alloc_size],\n                                std::default_delete<char[]>());\n  if (m_raw == nullptr)\n    return;\n\n  auto raw_uint = reinterpret_cast<std::uintptr_t>(m_raw.get());\n  auto offset = raw_uint & (ACL_MEM_ALIGN - 1);\n  if (offset)\n    offset = ACL_MEM_ALIGN - offset;\n  m_aligned_ptr = reinterpret_cast<void *>(raw_uint + offset);\n  m_requested_size = size;\n  m_size = alloc_size;\n}\n"
    },
    {
        "label": "acl_context.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <stddef.h>\n#include <stdlib.h>\n#include <string.h>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl_command_queue.h>\n#include <acl_context.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_icd_dispatch.h>\n#include <acl_mem.h>\n#include <acl_platform.h>\n#include <acl_printf.h>\n#include <acl_profiler.h>\n#include <acl_program.h>\n#include <acl_support.h>\n#include <acl_svm.h>\n#include <acl_thread.h>\n#include <acl_util.h>\n\n#ifndef MAX_NAME_LENGTH\n#define MAX_NAME_LENGTH 1024\n#endif\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\nstatic cl_context l_create_context(const cl_context_properties *properties,\n                                   acl_notify_fn_t pfn_notify, void *user_data,\n                                   cl_int *errcode_ret);\nstatic cl_int l_finalize_context(cl_context context, cl_uint num_devices,\n                                 const cl_device_id *devices);\nstatic cl_int l_load_properties(cl_context context,\n                                const cl_context_properties *properties);\nstatic cl_int l_init_context_with_devices(cl_context context,\n                                          cl_uint num_devices,\n                                          const cl_device_id *devices);\nstatic void\nl_init_kernel_invocation_wrapper(acl_kernel_invocation_wrapper_t *wrapper,\n                                 unsigned i);\nstatic void l_forcibly_release_allocations(cl_context context);\nstatic cl_device_id l_find_device_by_name(const std::string &name);\nstatic cl_int l_update_program_library_root(cl_context context,\n                                            const char *new_root);\nstatic cl_int l_update_compile_command(cl_context context, const char *new_cmd);\nstatic std::set<cl_context>::iterator l_release_context(cl_context context);\n\nACL_DEFINE_CL_OBJECT_ALLOC_FUNCTIONS(cl_context);\n\n// Conditionally override the normal definition of BAIL() from acl_util.h\n#ifdef ACL_DEBUG_BAIL\n#define BAIL(STATUS)                                                           \\\n  do {                                                                         \\\n    debug_mode++;                                                              \\\n    acl_print_debug_msg(\"%s:%d bailing with status %d\\n\", __FILE__, __LINE__,  \\\n                        STATUS);                                               \\\n    if (errcode_ret) {                                                         \\\n      *errcode_ret = (STATUS);                                                 \\\n    }                                                                          \\\n    return 0;                                                                  \\\n  } while (0)\n#endif\n\nextern int platform_owner_pid; // Used to detect if user is creating contexts in\n                               // multiple processes.\n\n//////////////////////////////\n// OpenCL API\n\n// Create a context\nACL_EXPORT\nCL_API_ENTRY cl_context CL_API_CALL clCreateContextIntelFPGA(\n    const cl_context_properties *properties, cl_uint num_devices,\n    const cl_device_id *devices, acl_notify_fn_t pfn_notify, void *user_data,\n    cl_int *errcode_ret) {\n  cl_context context;\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  context = l_create_context(properties, pfn_notify, user_data, &status);\n  if (context == NULL || status != CL_SUCCESS) {\n    acl_free_cl_context(context);\n    BAIL(status);\n  }\n\n  // Now check the devices.\n  if (num_devices == 0) {\n    acl_context_callback(context, \"No devices specified\");\n    acl_free_cl_context(context);\n    BAIL(CL_INVALID_VALUE);\n  }\n  if (devices == 0) {\n    acl_context_callback(context, \"No device array specified\");\n    acl_free_cl_context(context);\n    BAIL(CL_INVALID_VALUE);\n  }\n\n  // Make sure all mentioned devices are valid.\n  for (cl_uint i = 0; i < num_devices; i++) {\n    if (!acl_device_is_valid_ptr(devices[i])) {\n      acl_context_callback(context, \"Invalid device specified\");\n      acl_free_cl_context(context);\n      BAIL(CL_INVALID_DEVICE);\n    }\n\n    if (devices[i]->opened_count) {\n      if (context->uses_dynamic_sysdef && devices[i]->mode_lock == BUILT_IN) {\n        acl_context_callback(\n            context, \"Could not create context with reprogramming enabled. A \"\n                     \"device in the device list is currently in use in another \"\n                     \"context created with reprogramming disabled.\");\n        acl_free_cl_context(context);\n        BAIL(CL_INVALID_VALUE);\n      } else if (!context->uses_dynamic_sysdef &&\n                 devices[i]->mode_lock == BINARY) {\n        acl_context_callback(\n            context, \"Could not create context with reprogramming disabled. A \"\n                     \"device in the device list is currently in use in another \"\n                     \"context created with reprogramming enabled.\");\n        acl_free_cl_context(context);\n        BAIL(CL_INVALID_VALUE);\n      }\n    } else {\n      // Since this is the first time creating a context for this device, we\n      // lock all future context creation to the mode of the current setting\n      if (context->uses_dynamic_sysdef) {\n        devices[i]->mode_lock = BINARY;\n      } else {\n        devices[i]->mode_lock = BUILT_IN;\n      }\n    }\n  }\n\n  status = l_finalize_context(context, num_devices, devices);\n\n  if (status != CL_SUCCESS) {\n    BAIL(status);\n  }\n\n  // Open the profiler output file after the first context creation\n  acl_open_profiler_file();\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n  // the context is created successfully, add it to the set\n  acl_platform.contexts_set.insert(context);\n  return context;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_context CL_API_CALL\nclCreateContext(const cl_context_properties *properties, cl_uint num_devices,\n                const cl_device_id *devices, acl_notify_fn_t pfn_notify,\n                void *user_data, cl_int *errcode_ret) {\n  return clCreateContextIntelFPGA(properties, num_devices, devices, pfn_notify,\n                                  user_data, errcode_ret);\n}\nACL_EXPORT\nCL_API_ENTRY cl_context CL_API_CALL clCreateContextFromTypeIntelFPGA(\n    const cl_context_properties *properties, cl_device_type device_type,\n    acl_notify_fn_t pfn_notify, void *user_data, cl_int *errcode_ret) {\n  cl_context context = 0;\n  cl_uint num_devices = 0;\n  cl_int status;\n  cl_device_id devices[ACL_MAX_DEVICE];\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  context = l_create_context(properties, pfn_notify, user_data, &status);\n  if (context == NULL || status != CL_SUCCESS) {\n    acl_free_cl_context(context);\n    BAIL(status);\n  }\n\n  // Determine device IDs.\n  status = clGetDeviceIDs(acl_get_platform(), device_type, ACL_MAX_DEVICE,\n                          &devices[0], &num_devices);\n  if (status != CL_SUCCESS || num_devices == 0) {\n    acl_context_callback(context, \"Device not found\");\n    acl_free_cl_context(context);\n    BAIL(CL_DEVICE_NOT_FOUND);\n  }\n\n  // Filter out devices.\n  cl_uint i = 0;\n  for (cl_uint j = 0; j < num_devices; j++) {\n    if (devices[j]->opened_count) {\n      if ((context->uses_dynamic_sysdef && devices[j]->mode_lock == BINARY) ||\n          (!context->uses_dynamic_sysdef &&\n           devices[j]->mode_lock == BUILT_IN)) {\n        devices[i] = devices[j];\n        i++;\n      }\n    } else {\n      // Since this is the first time creating a context for this device, we\n      // lock all future context creation to the mode of the current setting\n      if (context->uses_dynamic_sysdef) {\n        devices[j]->mode_lock = BINARY;\n      } else {\n        devices[j]->mode_lock = BUILT_IN;\n      }\n      devices[i] = devices[j];\n      i++;\n    }\n  }\n  num_devices = i;\n  // Error out accordingly if all the devices got filtered out.\n  if (!num_devices) {\n    if (context->uses_dynamic_sysdef) {\n      acl_context_callback(\n          context, \"Could not create context with reprogramming enabled. All \"\n                   \"devices of the given device type are currently in use in \"\n                   \"other contexts created with reprogramming disabled.\");\n      acl_free_cl_context(context);\n      BAIL(CL_DEVICE_NOT_AVAILABLE);\n    } else {\n      acl_context_callback(\n          context, \"Could not create context with reprogramming disabled. All \"\n                   \"devices of the given device type are currently in use in \"\n                   \"other contexts created with reprogramming enabled.\");\n      acl_free_cl_context(context);\n      BAIL(CL_DEVICE_NOT_AVAILABLE);\n    }\n  }\n\n  status = l_finalize_context(context, num_devices, devices);\n\n  if (status != CL_SUCCESS) {\n    BAIL(status);\n  }\n\n  // Open the profiler output file after the first context creation\n  acl_open_profiler_file();\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n  // the context is created successfully, add it to the set\n  acl_platform.contexts_set.insert(context);\n  return context;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_context CL_API_CALL clCreateContextFromType(\n    const cl_context_properties *properties, cl_device_type device_type,\n    acl_notify_fn_t pfn_notify, void *user_data, cl_int *errcode_ret) {\n  return clCreateContextFromTypeIntelFPGA(properties, device_type, pfn_notify,\n                                          user_data, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainContextIntelFPGA(cl_context context) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Note: Context creation uses acl_retain<> directly, but users must use\n  // clRetainContext.\n  // So it's ok for us to error out if the current reference count is 0.\n  // That's why we use acl_context_is_valid() here instead of just\n  // acl_is_valid_ptr().\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n  acl_retain(context);\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainContext(cl_context context) {\n  return clRetainContextIntelFPGA(context);\n}\n\nstatic std::set<cl_context>::iterator l_release_context(cl_context context) {\n  // Make sure we clean up the elf files.  This should already be done by\n  // clReleaseProgram.\n  for (unsigned i = 0; i < context->num_devices; i++) {\n    if (context->device[i]->loaded_bin != nullptr)\n      context->device[i]->loaded_bin->unload_content();\n    if (context->device[i]->last_bin != nullptr)\n      context->device[i]->last_bin->unload_content();\n  }\n\n  // We have to close all devices associated with this context so they can be\n  // opened by other processes\n  acl_get_hal()->close_devices(context->num_devices, context->device);\n\n  // remove the context from the context set in the platform\n  auto cur_it = acl_platform.contexts_set.find(context);\n  assert(cur_it != acl_platform.contexts_set.end() &&\n         \"Context to be erased is not in the contexts set!\");\n  std::set<cl_context>::iterator it = acl_platform.contexts_set.erase(cur_it);\n\n  context->notify_fn = 0;\n  context->notify_user_data = 0;\n\n  if (context->auto_queue) {\n    // Really subtle.\n    // The command queue release needs to see the context as being valid.\n    // That's why we had to defer the acl_release call.\n    clReleaseCommandQueue(context->auto_queue);\n    context->auto_queue = 0; // for clarity\n  }\n\n  if (context->user_event_queue) {\n    clReleaseCommandQueue(context->user_event_queue);\n    context->user_event_queue = 0; // for clarity\n  }\n\n  if (context->command_queue) {\n    acl_free(context->command_queue);\n  }\n\n  clReleaseMemObject(context->unwrapped_host_mem);\n\n  l_forcibly_release_allocations(context);\n\n  acl_untrack_object(context);\n\n  // all that should be left now is the single implicit retain from when\n  // the cl_context was created\n  assert(acl_ref_count(context) == 1);\n  acl_free_cl_context(context);\n\n  // Close the profiler output file after the last context release\n  acl_close_profiler_file();\n\n  return it;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseContextIntelFPGA(cl_context context) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Error out if the reference count is already 0\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n\n  // Must mirror what is retained in clRetainContext.\n  // Since that method doesn't retain the constituent objects, we don't\n  // release them until our own reference count is 0.\n\n  if (acl_ref_count(context) > context->num_automatic_references) {\n    acl_release(context);\n  } else if (context->is_being_updated) {\n    assert(context->is_being_freed == 0 &&\n           \"Context is being released more than retained count!\");\n    context->is_being_freed = 1; // Defer freeing context;\n    return CL_SUCCESS;\n  } else {\n    // num_automatic_references is the ref count the context had when it was\n    // given to the user. So, if we have a ref count equal to that, the user\n    // must have run release and retain an equal number of times (before the\n    // current release). Therefore, the current release will destroy the\n    // context. Any remaining ref count are from circular references from the\n    // command queues and memory objects attached to this context.\n\n    // When we call clReleaseCommandQueue and clReleaseMemObject below,\n    // they're gonna call clReleaseContext themselves. This stops us from\n    // recursively trying to delete them again.\n    if (context->is_being_freed) {\n      acl_release(context);\n      return CL_SUCCESS;\n    }\n    context->is_being_freed = 1;\n\n    l_release_context(context);\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseContext(cl_context context) {\n  return clReleaseContextIntelFPGA(context);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetContextInfoIntelFPGA(\n    cl_context context, cl_context_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_context_is_valid(context)) {\n    return CL_INVALID_CONTEXT;\n  }\n  VALIDATE_ARRAY_OUT_ARGS(param_value_size, param_value, param_value_size_ret,\n                          context);\n\n  RESULT_INIT;\n\n  switch (param_name) {\n  case CL_CONTEXT_REFERENCE_COUNT:\n    RESULT_UINT(acl_ref_count(context));\n    break;\n  case CL_CONTEXT_DEVICES:\n    RESULT_BUF(&(context->device[0]),\n               context->num_devices * sizeof(context->device[0]));\n    break;\n  case CL_CONTEXT_NUM_DEVICES:\n    RESULT_UINT(context->num_devices);\n    break;\n  case CL_CONTEXT_PLATFORM:\n    RESULT_PTR(acl_get_platform());\n    break;\n\n  // When returning the context properties, the size includes the\n  // terminating NULL pointer.\n  case CL_CONTEXT_PROPERTIES:\n    RESULT_BUF(context->properties,\n               context->num_property_entries * sizeof(cl_context_properties));\n    break;\n  default:\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid or unsupported context info query\");\n  }\n\n  if (param_value) {\n    if (param_value_size < result.size) {\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    }\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetContextInfo(cl_context context,\n                                                 cl_context_info param_name,\n                                                 size_t param_value_size,\n                                                 void *param_value,\n                                                 size_t *param_value_size_ret) {\n  return clGetContextInfoIntelFPGA(context, param_name, param_value_size,\n                                   param_value, param_value_size_ret);\n}\n//////////////////////////////\n// Internals\n\nint acl_context_is_valid(cl_context context) {\n  acl_assert_locked();\n\n  if (!acl_is_valid_ptr(context)) {\n    return 0;\n  }\n  if (!acl_ref_count(context)) {\n    return 0;\n  }\n  return 1;\n}\n\nint acl_context_uses_device(cl_context context, cl_device_id device) {\n  unsigned int i;\n  acl_assert_locked();\n\n  // Assumes both context and device are valid.\n  for (i = 0; i < context->num_devices; i++) {\n    if (context->device[i] == device) {\n      return 1;\n    }\n  }\n  return 0;\n}\n\nstatic cl_context l_create_context(const cl_context_properties *properties,\n                                   acl_notify_fn_t pfn_notify, void *user_data,\n                                   cl_int *errcode_ret) {\n  cl_context context = 0;\n  cl_int status;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (user_data && !pfn_notify) {\n    BAIL(CL_INVALID_VALUE);\n  }\n\n  {\n    // Blocking multiple_processing\n    const char *allow_mp = NULL;\n    // Check if user wants to disable the check.\n    allow_mp = acl_getenv(ENV_CL_CONTEXT_ALLOW_MULTIPROCESSING_INTELFPGA);\n    if (!allow_mp && platform_owner_pid != 0 &&\n        platform_owner_pid != acl_get_pid()) {\n      if (pfn_notify) {\n        {\n          acl_suspend_lock_guard lock{acl_mutex_wrapper};\n          (pfn_notify)(\"Cannot create contexts in more than one process\", 0, 0,\n                       user_data);\n        }\n      }\n      BAIL(CL_OUT_OF_RESOURCES);\n    }\n  }\n\n  // Get the context, but don't retain it yet.\n  context = acl_alloc_cl_context();\n  if (context == 0) {\n    if (pfn_notify) {\n      {\n        acl_suspend_lock_guard lock{acl_mutex_wrapper};\n        (pfn_notify)(\"Could not allocate a context object\", 0, 0, user_data);\n      }\n    }\n    BAIL(CL_OUT_OF_HOST_MEMORY);\n  }\n\n  context->notify_fn = pfn_notify;\n  context->notify_user_data = user_data;\n\n  // Load the platform and compiler mode.\n  status = l_load_properties(context, properties);\n  if (status != CL_SUCCESS) {\n    acl_free_cl_context(context);\n    BAIL(status);\n  } // already called context error callback\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  return context;\n}\n\nstatic cl_int l_finalize_context(cl_context context, cl_uint num_devices,\n                                 const cl_device_id *devices) {\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  status = acl_get_hal()->try_devices(num_devices, devices, &acl_platform);\n  if (status) {\n    acl_context_callback(context, \"Could not open devices\");\n    acl_free_cl_context(context);\n    return status;\n  }\n\n  acl_retain(context);\n\n  status = l_init_context_with_devices(context, num_devices, devices);\n  if (status != CL_SUCCESS) {\n    l_forcibly_release_allocations(context);\n    acl_free_cl_context(context);\n    return status; // already signaled callback\n  }\n\n  acl_track_object(ACL_OBJ_CONTEXT, context);\n\n  return CL_SUCCESS;\n}\n\n// Analyze and load the context properties.\n// Populates the given context's\n//    properties\n//    platform\n//    compiler mode\n// If ok, return CL_SUCCESS.\n// Otherwise, it calls the callback function and returns an error.\nstatic cl_int l_load_properties(cl_context context,\n                                const cl_context_properties *properties) {\n  const char *default_compile_cmd = 0;\n  acl_assert_locked();\n\n  // Set defaults.\n  context->dispatch = &acl_icd_dispatch;\n  context->compiler_mode = static_cast<acl_compiler_mode_t>(\n      CL_CONTEXT_COMPILER_MODE_OFFLINE_INTELFPGA);\n  context->split_kernel = 0;\n  context->is_being_freed = 0;\n  context->is_being_updated = 0;\n  context->eagerly_program_device_with_first_binary = 1;\n\n  // This one is only overridden with an environment variable.\n  {\n    const char *override = acl_getenv(ENV_AOCL_EAGERLY_LOAD_FIRST_BINARY);\n    if (override) {\n      // There was a string.\n      char *endptr = 0;\n      long value = strtol(override, &endptr, 10);\n      if (endptr != override) { // we parsed something\n        context->eagerly_program_device_with_first_binary = (int)value;\n      }\n    }\n  }\n\n  // Environment variable can provide a default for compiler mode.\n  {\n    const char *override = 0;\n    const char *override_deprecated = 0;\n    override = acl_getenv(ENV_CL_CONTEXT_COMPILER_MODE_INTELFPGA);\n    override_deprecated = acl_getenv(ENV_CL_CONTEXT_COMPILER_MODE_ALTERA);\n    if (!override && override_deprecated) {\n      override = override_deprecated;\n      fprintf(stderr,\n              \"Warning: %s has been deprecated. \"\n              \"Use %s instead.\\n\",\n              ENV_CL_CONTEXT_COMPILER_MODE_ALTERA,\n              ENV_CL_CONTEXT_COMPILER_MODE_INTELFPGA);\n    }\n\n    if (override) {\n      // There was a string.\n      char *endptr = 0;\n      long value = strtol(override, &endptr, 10);\n      if (endptr == override // no valid characters\n          || *endptr         // an invalid character\n          || (value < 0 || value >= (long)ACL_COMPILER_MODE_NUM_MODES)) {\n        // the value of \"ACL_COMPILER_MODE_NUM_MODES\" is set in \"acl_types.h\"\n        ERR_RET(CL_INVALID_VALUE, context,\n                \"Invalid compiler mode in environment variable \"\n                \"CL_CONTEXT_COMPILER_MODE_INTELFPGA\");\n      }\n      // Was ok.\n      context->compiler_mode = static_cast<acl_compiler_mode_t>(value);\n    }\n  }\n\n  // Environment variable can specify we always an offline device.\n  if (!acl_platform.offline_device.empty()) {\n    if (!l_find_device_by_name(acl_platform.offline_device))\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Invalid offline device specified by environment variable \"\n              \"CL_CONTEXT_OFFLINE_DEVICE_INTELFPGA\");\n  }\n\n  // Get default for program_library_root.\n  // The directory does not have to exist.\n  // If the env var is not set, then use \"aocl_program_library\" in the\n  // current directory.\n  const auto *default_root = acl_getenv(\n      ENV_CL_CONTEXT_PROGRAM_EXE_LIBRARY_ROOT_INTELFPGA); // this one is public,\n                                                          // in\n                                                          // cl_ext_intelfpga.h\n  if (!default_root) {\n    default_root = \"aocl_program_library\";\n  }\n  auto status = l_update_program_library_root(context, default_root);\n  if (status != CL_SUCCESS) {\n    return status;\n  } // already signaled.\n\n  // Get user-specified compile command.\n  // The default command depends on effective compiler mode, so defer\n  // until later.\n  const char *default_cmd =\n      acl_getenv(ENV_CL_CONTEXT_COMPILE_COMMAND_INTELFPGA);\n  if (default_cmd) {\n    status = l_update_compile_command(context, default_cmd);\n    if (status != CL_SUCCESS)\n      return status; // already signaled.\n  }\n\n  context->num_property_entries = 0;\n\n  if (properties) {\n    const cl_context_properties *curr_prop;\n\n    for (curr_prop = properties; curr_prop && *curr_prop; ++curr_prop) {\n\n      // Check overflow.\n      // Properties always come in pairs.\n      // And we should account for the terminating 0.\n      if (context->num_property_entries + 3 >\n          ACL_MAX_NUM_CONTEXT_PROPERTY_ENTRIES)\n        ERR_RET(CL_OUT_OF_HOST_MEMORY, context,\n                \"Could not save context properties\");\n\n      switch (*curr_prop) {\n      case CL_CONTEXT_PLATFORM: {\n        // There's only one valid option for platform, so just check\n        // that they've passed the right one\n        cl_platform_id selected_platform = *((cl_platform_id *)(++curr_prop));\n        if (selected_platform != acl_get_platform()) {\n          // In later OpenCL 1.1 this is CL_INVALID_PROPERTY\n          ERR_RET(\n              CL_INVALID_PLATFORM, context,\n              \"Invalid platform specified with CL_CONTEXT_PLATFORM property\");\n        }\n        context->properties[context->num_property_entries++] =\n            CL_CONTEXT_PLATFORM;\n        context->properties[context->num_property_entries++] =\n            (cl_context_properties)selected_platform;\n      } break;\n\n      case CL_CONTEXT_COMPILER_MODE_INTELFPGA: {\n        cl_context_properties proposed = *(++curr_prop);\n        if (proposed < 0 || proposed >= ACL_COMPILER_MODE_NUM_MODES)\n          ERR_RET(CL_INVALID_VALUE, context,\n                  \"Invalid CL_CONTEXT_COMPILER_MODE_INTELFPGA property\");\n        context->compiler_mode = static_cast<acl_compiler_mode_t>(proposed);\n        context->properties[context->num_property_entries++] =\n            CL_CONTEXT_COMPILER_MODE_INTELFPGA;\n        context->properties[context->num_property_entries++] = proposed;\n      } break;\n\n      case CL_CONTEXT_COMPILE_COMMAND_INTELFPGA: {\n        cl_context_properties proposed = *(++curr_prop);\n        const char *name = (const char *)proposed;\n        status = l_update_compile_command(context, name);\n        if (status != CL_SUCCESS)\n          return status;\n\n        context->properties[context->num_property_entries++] =\n            CL_CONTEXT_COMPILE_COMMAND_INTELFPGA;\n        context->properties[context->num_property_entries++] = proposed;\n      } break;\n\n      case CL_CONTEXT_PROGRAM_EXE_LIBRARY_ROOT_INTELFPGA: {\n        cl_context_properties proposed = *(++curr_prop);\n        const char *name = (const char *)proposed;\n        status = l_update_program_library_root(context, name);\n        if (status != CL_SUCCESS)\n          return status;\n\n        context->properties[context->num_property_entries++] =\n            CL_CONTEXT_PROGRAM_EXE_LIBRARY_ROOT_INTELFPGA;\n        context->properties[context->num_property_entries++] = proposed;\n      } break;\n\n      default:\n        ERR_RET(\n            CL_INVALID_VALUE, // In later OpenCL 1.1 this is CL_INVALID_PROPERTY\n            context, \"Invalid context property\");\n      }\n    }\n  }\n  // Always terminate list. After all, 'properties' might be empty!\n  context->properties[context->num_property_entries++] = 0;\n\n  context->compiles_programs_incompletely = 0;\n  switch (context->compiler_mode) {\n  case static_cast<acl_compiler_mode_t>(\n      CL_CONTEXT_COMPILER_MODE_OFFLINE_INTELFPGA):\n    context->compiles_programs = 0; // need to generate sys_description.txt\n    context->programs_devices = 1;\n    context->saves_and_restores_buffers_for_reprogramming = 1;\n    context->uses_dynamic_sysdef = 1;\n    context->uses_program_library = 0;\n    break;\n\n  case static_cast<acl_compiler_mode_t>(\n      CL_CONTEXT_COMPILER_MODE_OFFLINE_CREATE_EXE_LIBRARY_INTELFPGA):\n    context->uses_program_library = 1;\n    context->compiles_programs = 1;\n    context->compiles_programs_incompletely = 1; // Only mode that does this.\n    default_compile_cmd = \"aoc -rtl -tidy\";\n    context->saves_and_restores_buffers_for_reprogramming =\n        1; // for test purposes\n    context->uses_dynamic_sysdef = 1;\n    context->programs_devices = 0; // does not switch devices: might be offline\n    break;\n\n  case static_cast<acl_compiler_mode_t>(\n      CL_CONTEXT_COMPILER_MODE_ONLINE_INTELFPGA):\n    // Emulate having an online compiler.\n    // Not practical.\n    context->uses_program_library = 1;\n    context->compiles_programs = 1;\n    context->saves_and_restores_buffers_for_reprogramming = 1;\n    context->uses_dynamic_sysdef = 1;\n    context->programs_devices = 1;\n    default_compile_cmd = \"aoc -tidy\"; // Yes, the full compile.\n    break;\n\n  case static_cast<acl_compiler_mode_t>(\n      CL_CONTEXT_COMPILER_MODE_OFFLINE_USE_EXE_LIBRARY_INTELFPGA):\n    context->uses_program_library = 1;\n    context->compiles_programs = 0;\n    context->saves_and_restores_buffers_for_reprogramming = 1;\n    context->uses_dynamic_sysdef = 1;\n    context->programs_devices = 1; // This is production mode. Must swap SOFs\n    break;\n\n  case static_cast<acl_compiler_mode_t>(\n      CL_CONTEXT_COMPILER_MODE_SPLIT_KERNEL_INTELFPGA):\n    context->split_kernel = 1;\n    context->uses_program_library = 1;\n    context->compiles_programs = 0;\n    context->saves_and_restores_buffers_for_reprogramming = 1;\n    context->uses_dynamic_sysdef = 1;\n    context->programs_devices = 1;\n    // In split_kernel mode it is difficult to determine which\n    // binary should be loaded first.\n    context->eagerly_program_device_with_first_binary = 0;\n    break;\n\n  default: // Embedded mode does none of these special things.\n    context->compiles_programs = 0;\n    context->programs_devices = 0;\n    context->saves_and_restores_buffers_for_reprogramming = 0;\n    context->uses_dynamic_sysdef = 0;\n    context->uses_program_library = 0;\n    break;\n  }\n\n  // We need backing store for the buffers.\n  context->device_buffers_have_backing_store = 1;\n\n  if (acl_platform.offline_mode == ACL_CONTEXT_MPSIM) {\n    //  Simulator should support save/restore buffers around programming if\n    //  reprogramming on-the-fly is supported\n    context->saves_and_restores_buffers_for_reprogramming = 1;\n\n    // Required to support compiler mode 3 in simulation mode.\n    context->uses_dynamic_sysdef = 1;\n    context->programs_devices = 1;\n  }\n\n  if (default_compile_cmd && context->compile_command == \"\") {\n    // Need to supply a compile command.\n    status = l_update_compile_command(context, default_compile_cmd);\n    if (status != CL_SUCCESS)\n      return status; // already signaled error.\n  }\n\n  if (context->uses_program_library) {\n    // Canonicalize the program library root path.\n    // This means we also make path too it.\n    if (!acl_make_path_to_dir(context->program_library_root))\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Could not make path to program libary root\");\n    auto root = acl_realpath_existing(context->program_library_root);\n    status = l_update_program_library_root(context, root.c_str());\n    if (status != CL_SUCCESS)\n      return status; // already signaled error\n  }\n\n  return CL_SUCCESS;\n}\n\nstatic cl_device_id l_find_device_by_name(const std::string &name) {\n  acl_assert_locked();\n\n  for (unsigned i = 0; i < acl_platform.num_devices; ++i) {\n    if (name == acl_platform.device[i].def.autodiscovery_def.name) {\n      return &(acl_platform.device[i]);\n    }\n  }\n  return 0;\n}\n\n// Initialize the given context.\n// Yes, this is like a \"placement new\".\n//\n// We perform two kinds of allocations here:\n//    1. A command queue to serialize memory transfers\n//    2. Several memory buffers.\n// We've already checked for command queue allocation. That should not fail.\n//\n// The memory object allocations can fail.  Since those objects are drawn\n// from the platform, we have to perform cleanup on failure.\n//\n// We can easily roll back the mem buffer allocations via\n// acl_forcibly_release_all_memory_for_context.\nstatic cl_int l_init_context_with_devices(cl_context context,\n                                          cl_uint num_devices,\n                                          const cl_device_id *devices) {\n  acl_assert_locked();\n\n  acl_finalize_init_platform(num_devices, devices);\n\n  context->command_queue = (cl_command_queue *)acl_malloc(\n      ACL_INIT_COMMAND_QUEUE_ALLOC * sizeof(cl_command_queue));\n  if (!context->command_queue) {\n    return CL_OUT_OF_HOST_MEMORY;\n  };\n  context->num_command_queues_allocated = ACL_INIT_COMMAND_QUEUE_ALLOC;\n  context->num_command_queues = 0;\n  context->last_command_queue_idx_with_update = -1;\n  context->auto_queue = 0;       // will get overwritten later\n  context->user_event_queue = 0; // will get overwritten later\n  context->num_automatic_references = 0;\n  context->reprogram_buf_read_callback = 0;\n  context->reprogram_buf_write_callback = 0;\n\n  // Initialize the emulated memory region.\n  // Do it early so we have an easier time on early exit conditions.\n  // The release of memory is trivial.  (.first_block = NULL is key here).\n  context->emulated_global_mem.is_user_provided = 0;\n  context->emulated_global_mem.is_host_accessible = 1;\n  context->emulated_global_mem.is_device_accessible = 1;\n  context->emulated_global_mem.uses_host_system_malloc = 1;\n  context->emulated_global_mem.range.begin = 0; // not used\n  context->emulated_global_mem.range.next = 0;  // not used\n  context->emulated_global_mem.first_block = NULL;\n  // For now, just point global mem at ourselves.\n  // We'll override this later if this context uses real (present) devcies.\n  context->global_mem = &(context->emulated_global_mem);\n\n  context->svm_list = NULL;\n\n  // Add the devices.\n  context->num_devices = 0;\n  int num_present = 0;\n  int num_absent = 0;\n  for (cl_uint i = 0; i < num_devices; i++) {\n    int usable = devices[i]->present;\n\n    // Can't mix both (actually) present and absent devices because there\n    // is no consistent way to place device global memory.\n    if (devices[i]->present) {\n      num_present++;\n      // We should use real device memory, not emulated memory.\n      context->global_mem = &(acl_platform.global_mem);\n    } else {\n      num_absent++;\n    }\n    if (num_present && num_absent) {\n      acl_free(context->command_queue);\n      ERR_RET(CL_INVALID_DEVICE, context,\n              \"Can't create a context with both offline and online devices\");\n    }\n\n    usable = usable || acl_platform.offline_device ==\n                           devices[i]->def.autodiscovery_def.name;\n\n    if (!usable)\n      ERR_RET(CL_DEVICE_NOT_AVAILABLE, context, \"Device not available\");\n\n    // Mark the device(s) as opened\n    devices[i]->has_been_opened = 1;\n\n    context->device[i] = devices[i];\n    acl_retain(devices[i]);\n\n    // Determine *minimum* max alloc across all devices.\n    cl_ulong max_alloc = 0;\n    clGetDeviceInfo(devices[i], CL_DEVICE_MAX_MEM_ALLOC_SIZE, sizeof(max_alloc),\n                    &max_alloc, 0);\n    if (i == 0 || context->max_mem_alloc_size > max_alloc) {\n      context->max_mem_alloc_size = max_alloc;\n    }\n\n    context->num_devices++;\n  }\n  assert(!(num_present && num_absent));\n\n  cl_int status = CL_SUCCESS;\n\n  // Create the command queue for internal purposes.\n  // For example, profiling buffers are transferred over it.\n  // Just attach it to the first device.\n  // We know we can't run out of resources because we checked earlier if\n  // we could allocate the command queue.\n  // Enable profiling, but not out-of-order execution.\n  // Out-of-order on mem objects is a bad bad bad thing.\n  context->auto_queue = clCreateCommandQueue(\n      context, devices[0], CL_QUEUE_PROFILING_ENABLE, &status);\n  if (status != CL_SUCCESS) {\n    acl_free(context->command_queue);\n    return CL_OUT_OF_HOST_MEMORY;\n  } // already signalled callback\n\n  // Create the user event queue.\n  // The OpenCL spec says clGetEventInfo with query property\n  // CL_EVENT_COMMAND_QUEUE should return NULL so logically we are not supposed\n  // to associate user events with a command queue. However, internally we\n  // associate user events with a special out of order command queue which\n  // should never be exposed to the end user. Profiling is disabled on this\n  // special command queue because it is an error to call\n  // clGetEventProfilingInfo on a user event.\n  context->user_event_queue =\n      clCreateCommandQueue(context, devices[0], 0, &status);\n  if (status != CL_SUCCESS || context->user_event_queue == NULL) {\n    acl_free(context->command_queue);\n    return CL_OUT_OF_HOST_MEMORY;\n  } // already signalled callback\n  // This queue is special.\n  // It does not submit commands.\n  context->user_event_queue->submits_commands = 0;\n  // It is out of order.\n  context->user_event_queue->properties |=\n      static_cast<cl_command_queue_properties>(\n          CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE);\n\n  // Create the cl_mem handle for all of host memory.\n  // This is used internally by buffer read/write commands\n  // to talk about host memory that the caller has given\n  // us as a plain void pointer.\n  //\n  // The clCreateBuffer API doesn't allow us to use a host pointer\n  // of 0.  So use ACL_MEM_ALIGN as the base pointer.\n  // When we wrap a raw pointer, we have to subtract ACL_MEM_ALIGN\n  // from its value.\n  context->unwrapped_host_mem = 0; // not actually an allocated buffer.\n  context->unwrapped_host_mem =\n      clCreateBuffer(context, CL_MEM_READ_WRITE | CL_MEM_USE_HOST_PTR,\n                     (size_t)-1 - ACL_MEM_ALIGN, // All of memory!\n                     (void *)ACL_MEM_ALIGN, &status);\n  if (status != CL_SUCCESS || context->unwrapped_host_mem == NULL) {\n    acl_free(context->command_queue);\n    return CL_OUT_OF_HOST_MEMORY;\n  } // already signalled callback\n  // This memory is never considered SVM memory, regardless of what devices are\n  // available\n  context->unwrapped_host_mem->is_svm = CL_FALSE;\n\n  // Initialize invocation image wrappers and images.\n  context->invocation_wrapper = 0;\n  context->num_invocation_wrappers_allocated = 0;\n\n  // Remember how many automatic references were attached to this context.\n  // This tells clReleaseContext when to really shut down the context\n  // object.\n  context->num_automatic_references = acl_ref_count(context);\n\n  return CL_SUCCESS;\n}\n\n// Release resources owned by this context.\n// This is called when the context has been retained, and\n// context->global_mem is valid.\n// Does not release the context\nstatic void l_forcibly_release_allocations(cl_context context) {\n  acl_assert_locked();\n\n  if (context) {\n    unsigned int idevice;\n    // Double check: should not have failed this allocation because we\n    // pre-check.\n    context->auto_queue = 0;\n\n    if (context->user_event_queue) {\n      clReleaseCommandQueue(context->user_event_queue);\n      context->user_event_queue = 0;\n    }\n\n    if (context->invocation_wrapper) {\n      for (unsigned int i = 0; i < context->num_invocation_wrappers_allocated;\n           i++) {\n        auto *const wrapper = context->invocation_wrapper[i];\n        if (wrapper->image->arg_value) {\n          acl_delete_arr(wrapper->image->arg_value);\n        }\n        // invoke non-default destructors of non-POD types, e.g., std::vector\n        wrapper->~acl_kernel_invocation_wrapper_t();\n        acl_free(wrapper);\n      }\n      acl_free(context->invocation_wrapper);\n      context->invocation_wrapper = 0;\n    }\n    context->num_invocation_wrappers_allocated = 0;\n\n    for (cl_event event : context->used_events) {\n      acl_free_cl_event(event);\n    }\n    context->used_events.clear();\n\n    while (!context->free_events.empty()) {\n      cl_event event = context->free_events.front();\n      context->free_events.pop_front();\n      acl_free_cl_event(event);\n    }\n\n    // During initialization we retained all the devices before first\n    // possible failure.\n    // During release, num_devices is still valid\n    for (idevice = 0; idevice < context->num_devices; idevice++) {\n      acl_release(context->device[idevice]);\n    }\n\n    // Buffers might have been allocated.\n    acl_forcibly_release_all_memory_for_context(context);\n    acl_forcibly_release_all_svm_memory_for_context(context);\n  }\n}\n\n// Update the storage for program libary root, but don't try to\n// canonicalize into an absolute pathname.\n// We make a copy of new_root\nstatic cl_int l_update_program_library_root(cl_context context,\n                                            const char *new_root) {\n  acl_assert_locked();\n\n  // In OpenCL 1.2, use CL_INVALID_PROPERTY for errors.\n  // But not available in 1.1\n\n  if (!new_root)\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"NULL pointer provided for program library root property\");\n\n  context->program_library_root = new_root;\n\n  return CL_SUCCESS;\n}\n\n// Update the storage for compile command.\n// We make a copy of new_cmd\nstatic cl_int l_update_compile_command(cl_context context,\n                                       const char *new_cmd) {\n  acl_assert_locked();\n\n  // In OpenCL 1.2, use CL_INVALID_PROPERTY for errors.\n  // But not available in 1.1\n\n  if (!new_cmd)\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"NULL pointer provided for program compile command property\");\n\n  context->compile_command = new_cmd;\n  return CL_SUCCESS;\n}\n\n// Update internal data structures in response to external messages.\n// Do this as part of a waiting loop, along with acl_hal_yield();\nvoid acl_update_context(cl_context context) {\n  int num_updates = 0;\n  unsigned iters = 0;\n  const unsigned max_iters = 10000; // This is unreasonably high.\n  acl_assert_locked();\n\n  do {\n    num_updates = 0;\n    // Check if an ecc interrupt occured (platform flag is raised first for a\n    // quick check)\n    if (acl_platform.device_exception_platform_counter) {\n      // Perform a run through all devices to find the one causing the ecc\n      // interrupt\n      for (cl_uint device_iter = 0; device_iter < context->num_devices;\n           device_iter++) {\n        if (context->device[device_iter]->device_exception_status) {\n          cl_device_id device = context->device[device_iter];\n\n          if (device->exception_notify_fn) {\n            acl_exception_notify_fn_t notify_fn = device->exception_notify_fn;\n            void *notify_user_data = device->exception_notify_user_data;\n\n            for (unsigned int i = 0; i < sizeof(CL_EXCEPTION_TYPE_INTEL) * 8;\n                 ++i) {\n              CL_EXCEPTION_TYPE_INTEL exception_type = 1ULL << i;\n              if (device->device_exception_status & exception_type) {\n                {\n                  acl_suspend_lock_guard lock{acl_mutex_wrapper};\n                  notify_fn(exception_type, device->exception_private_info[i],\n                            device->exception_cb[i], notify_user_data);\n                }\n              }\n            }\n\n          } else {\n            acl_context_callback(\n                context, \"Device exception has occured, however the device \"\n                         \"callback function was not registered\\n\");\n          }\n          // All exceptions for the device were processed\n          device->device_exception_status = 0;\n          acl_platform.device_exception_platform_counter -= 1;\n          for (unsigned int i = 0; i < sizeof(CL_EXCEPTION_TYPE_INTEL) * 8;\n               ++i) {\n            if (device->exception_private_info[i]) {\n              acl_free(device->exception_private_info[i]);\n              device->exception_private_info[i] = NULL;\n              device->exception_cb[i] = 0;\n            }\n          }\n        }\n      }\n    }\n\n    // Ensure forward progress is made on devices that uses yield\n    if (acl_get_hal()->yield) {\n      acl_get_hal()->yield(context->num_devices, context->device);\n    }\n\n    // Starting the update round from right after the last command_queue with\n    // update. Dont forget to wrap around.\n    const int starting_idx = context->last_command_queue_idx_with_update + 1;\n    for (int count = 0; count < context->num_command_queues; count++) {\n\n      int index = (count + starting_idx) % context->num_command_queues;\n      const int updates = acl_update_queue(context->command_queue[index]);\n      if (updates) {\n        context->last_command_queue_idx_with_update = index;\n      }\n\n      if (!acl_is_retained(context->command_queue[index])) {\n        // the command queue is not retained, is marked for deleted\n        acl_delete_command_queue(context->command_queue[index]);\n        // deleting a command queue places a live command queue at this index\n        // itterate on this index again\n        if (--count < 0) {\n          count = 0;\n        }\n      }\n\n      acl_print_debug_msg(\" cq[%d] had %d updates\\n\", index, updates);\n\n      num_updates += updates;\n    }\n  } while ((num_updates > 0) && (++iters < max_iters));\n\n  if (num_updates) {\n    acl_print_debug_msg(\" context[%p] idle update: still had %d 'updates' \"\n                        \"after %u iters. Breaking out\\n\",\n                        context, num_updates, iters);\n  }\n}\n\n// Functionality: acl_idle_update firstly updates the current context, and then\n// updates the sibling contexts within the same platform Reason for the\n// extension:\n//   The previous implementation only takes a single context into account. In\n//   the multiple contexts scenario, when there are dependencies among contexts,\n//   updating the current context is insufficient if the dependency relies on a\n//   callback when an event in another context runs into a specific exec status\n//   after the context switch. To resolve this dependency issue, a feasible way\n//   is by updating every context when acl_idle_update is called.\n//\nvoid acl_idle_update(cl_context context) {\n  // firstly update the current context\n  acl_update_context(context);\n  // update the other contexts from the platform\n  for (auto it = acl_platform.contexts_set.begin();\n       it != acl_platform.contexts_set.end();) {\n    auto _context = *it;\n    if (context != _context) {\n      _context->is_being_updated = 1;\n      acl_update_context(_context);\n      _context->is_being_updated = 0;\n      if (_context->is_being_freed) {\n        it = l_release_context(_context);\n        continue;\n      }\n    }\n    ++it;\n  }\n  // if there are any new updates on the current contect done by updating the\n  // other it will be handled in clflush/clfinish or acl_wait_for_event where\n  // acl_idle_update are called\n}\n\nstatic void\nl_init_kernel_invocation_wrapper(acl_kernel_invocation_wrapper_t *wrapper,\n                                 unsigned i) {\n  acl_assert_locked();\n\n  if (wrapper) {\n    // invoke non-default constructors of non-POD types, e.g., std::vector\n    new (wrapper) acl_kernel_invocation_wrapper_t{};\n    wrapper->id = i;\n    wrapper->image = &(wrapper->image_storage);\n    wrapper->image->arg_value = nullptr;\n    wrapper->event = 0;\n    acl_reset_ref_count(wrapper);\n  }\n}\n\nacl_kernel_invocation_wrapper_t *\nacl_get_unused_kernel_invocation_wrapper(cl_context context) {\n  acl_assert_locked();\n\n  for (unsigned ii = 0; ii < context->num_invocation_wrappers_allocated; ii++) {\n    acl_kernel_invocation_wrapper_t *candidate =\n        context->invocation_wrapper[ii];\n    if (candidate == 0) {\n      // Could have happened on a partial allocation from an earlier step.\n      context->invocation_wrapper[ii] =\n          (acl_kernel_invocation_wrapper_t *)acl_malloc(\n              sizeof(acl_kernel_invocation_wrapper_t));\n      candidate = context->invocation_wrapper[ii];\n\n      l_init_kernel_invocation_wrapper(candidate, ii);\n    }\n    if (candidate && !acl_is_retained(candidate))\n      return candidate;\n  }\n  // If we got here, then need to allocate.\n  {\n    unsigned first_new, limit, ij;\n    if (0 == context->invocation_wrapper) {\n      // First time allocation\n      const unsigned initial_alloc = 50;\n      context->invocation_wrapper =\n          (acl_kernel_invocation_wrapper_t **)acl_malloc(\n              initial_alloc * sizeof(acl_kernel_invocation_wrapper_t *));\n      if (!context->invocation_wrapper) {\n        acl_context_callback(context,\n                             \"Cannot allocate space for kernel invocations\");\n        return 0;\n      }\n      first_new = 0;\n      limit = initial_alloc;\n    } else {\n      const unsigned next_size =\n          context->num_invocation_wrappers_allocated * 2 + 50;\n      if (next_size < context->num_invocation_wrappers_allocated) {\n        return 0;\n      }\n      {\n\n        acl_kernel_invocation_wrapper_t **newloc =\n            (acl_kernel_invocation_wrapper_t **)acl_realloc(\n                context->invocation_wrapper,\n                next_size * sizeof(acl_kernel_invocation_wrapper_t *));\n        if (!newloc) {\n          acl_context_callback(\n              context, \"Cannot allocate space for more kernel invocations\");\n          return 0;\n        }\n        context->invocation_wrapper = newloc;\n        first_new = context->num_invocation_wrappers_allocated;\n        limit = next_size;\n      }\n    }\n    // Initialize the new entries.\n    context->num_invocation_wrappers_allocated = limit;\n    for (ij = first_new; ij < limit; ij++) {\n      context->invocation_wrapper[ij] = 0;\n    }\n    // Now fill them in.\n    for (ij = first_new; ij < limit; ij++) {\n      context->invocation_wrapper[ij] =\n          (acl_kernel_invocation_wrapper_t *)acl_malloc(\n              sizeof(acl_kernel_invocation_wrapper_t));\n      if (!context->invocation_wrapper[ij]) {\n        acl_context_callback(\n            context, \"Cannot allocate space for another kernel invocation\");\n        return 0;\n      }\n      l_init_kernel_invocation_wrapper(context->invocation_wrapper[ij], ij);\n    }\n    // If we get here, then the first one must be available.\n    return context->invocation_wrapper[first_new];\n  }\n}\n\nvoid acl_context_callback(cl_context context, const std::string errinfo) {\n  // Call the notification function registered during context creation.\n  // We don't support the special info (middle two arguments).\n  if (context && context->notify_fn) {\n    acl_notify_fn_t notify_fn = context->notify_fn;\n    void *notify_user_data = context->notify_user_data;\n    {\n      acl_suspend_lock_guard lock{acl_mutex_wrapper};\n      notify_fn(errinfo.c_str(), 0, 0, notify_user_data);\n    }\n  }\n\n  if (context && acl_getenv(ENV_ACL_CONTEXT_CALLBACK_DEBUG)) {\n    std::cout << \"[acl_context_callback] Error Info: \" << errinfo << std::endl;\n  }\n}\n\n// Called when runtime has been waiting for device update for a while\nvoid acl_context_print_hung_device_status(cl_context context) {\n  acl_get_hal()->get_device_status(context->num_devices, context->device);\n}\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "check_copy_overlap.c",
        "data": "// clang-format off\n\n// The function `check_copy_overlap` was copied verbatim from the Khronos\n// OpenCL Specification Version 2.1 Appendix D - CL_MEM_COPY_OVERLAP.\n// https://www.khronos.org/registry/OpenCL/specs/opencl-2.1.pdf\n\n/*\n * Copyright (c) 2011 The Khronos Group Inc.\n * \n * Permission is hereby granted, free of charge, to any person obtaining a copy of this\n * software and /or associated documentation files (the \"Materials \"), to deal in the Materials\n * without restriction, including without limitation the rights to use, copy, modify, merge,\n * publish, distribute, sublicense, and/or sell copies of the Materials, and to permit persons to\n * whom the Materials are furnished to do so, subject to\n * the following conditions:\n * \n * The above copyright notice and this permission notice shall be included\n * in all copies or substantial portions of the Materials.\n * \n * THE MATERIALS ARE PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n * FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n * AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n * LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n * OUT OF OR IN CONNECTION WITH THE MATERIALS OR THE USE OR OTHER DEALINGS IN\n * THE MATERIALS.\n */\n\n#include <check_copy_overlap.h>\n\nunsigned int\ncheck_copy_overlap( const size_t src_origin[],\n                    const size_t dst_origin[],\n                    const size_t region[],\n                    const size_t row_pitch,\n                    const size_t slice_pitch )\n{\n      const size_t slice_size = (region[1] - 1) * row_pitch   + region[0];\n      const size_t block_size = (region[2] - 1) * slice_pitch + slice_size;\n\n      const size_t src_start  = src_origin[2] * slice_pitch +\n                                src_origin[1] * row_pitch +\n                                src_origin[0];\n      const size_t src_end    = src_start + block_size;\n\n      const size_t dst_start  = dst_origin[2] * slice_pitch +\n                                dst_origin[1] * row_pitch +\n                                dst_origin[0];\n      const size_t dst_end    = dst_start + block_size;\n\n      /* No overlap if dst ends before src starts or if src ends\n       * before dst starts.\n       */\n      if( (dst_end <= src_start) || (src_end <= dst_start) )\n      {\n            return 0;\n      }\n\n      /* No overlap if region[0] for dst or src fits in the gap\n       * between region[0] and row_pitch.\n       */\n      {\n            const size_t src_dx = src_origin[0] % row_pitch;\n            const size_t dst_dx = dst_origin[0] % row_pitch;\n\n            if( ((dst_dx >= src_dx + region[0]) &&\n                 (dst_dx + region[0] <= src_dx + row_pitch)) ||\n                ((src_dx >= dst_dx + region[0]) &&\n                 (src_dx + region[0] <= dst_dx + row_pitch)) )\n            {\n                  return 0;\n            }\n      }\n\n      /* No overlap if region[1] for dst or src fits in the gap\n       * between region[1] and slice_pitch.\n       */\n      {\n            const size_t src_dy =\n                  (src_origin[1] * row_pitch + src_origin[0]) % slice_pitch;\n            const size_t dst_dy =\n                   (dst_origin[1] * row_pitch + dst_origin[0]) % slice_pitch;\n\n            if( ((dst_dy >= src_dy + slice_size) &&\n                 (dst_dy + slice_size <= src_dy + slice_pitch)) ||\n                ((src_dy >= dst_dy + slice_size) &&\n                 (src_dy + slice_size <= dst_dy + slice_pitch)) )\n            {\n                  return 0;\n            }\n      }\n\n      /* Otherwise src and dst overlap. */\n      return 1;\n}\n"
    },
    {
        "label": "check_copy_overlap.h",
        "data": "// Copyright (C) 2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n#ifndef CHECK_COPY_OVERLAP_H\n#define CHECK_COPY_OVERLAP_H\n\n#include <stddef.h>\n\n#ifdef __cplusplus\nextern \"C\" {\n#endif\n\n// Use internal visibility since this symbol is never called from another\n// dynamic shared object, neither directly nor through a function pointer.\n#ifdef __GNUC__\n#pragma GCC visibility push(internal)\n#endif\n\nunsigned int check_copy_overlap(const size_t src_origin[],\n                                const size_t dst_origin[],\n                                const size_t region[], const size_t row_pitch,\n                                const size_t slice_pitch);\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n\n#ifdef __cplusplus\n}\n#endif\n\n#endif // CHECK_COPY_OVERLAP_H\n"
    },
    {
        "label": "acl_globals.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <stddef.h>\n\n// External library headers.\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_auto.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_offline_hal.h>\n#include <acl_platform.h>\n#include <acl_profiler.h>\n#include <acl_support.h>\n#include <acl_types.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n////////////////////////////////////////////////////\n// Global data\n\nstd::unordered_map<std::string, bool> l_allow_invalid_type{};\n\n// This variable exists only to ensure that the user has linked the host\n// library with a matching HAL.\nint ACL_VERSION_COMPATIBILITY_SYMBOL = 0;\n\n////////////////////////////////////////////////////\n// Static data\n\n// This is the board that is plugged into the host system.\n// It might be invalid in the offline compilation scenario.\nstatic int acl_present_board_is_valid_value = 0;\nstatic acl_system_def_t builtin_prog_def_value;\n\n// Static functions\nstatic int l_is_valid_system_def(const acl_system_def_t *sys);\nstatic void l_reset_present_board();\n\n////////////////////////////////////////////////////\n// Global functions\n\nacl_system_def_t *acl_present_board_def(void) {\n  acl_assert_locked();\n\n  if (acl_present_board_is_valid_value) {\n    return &builtin_prog_def_value;\n  } else {\n    return 0;\n  }\n}\n\nint acl_present_board_is_valid(void) {\n  acl_assert_locked();\n\n  return acl_present_board_is_valid_value;\n}\n\nstatic void l_reset_present_board() {\n  acl_assert_locked();\n\n  acl_present_board_is_valid_value = 0;\n}\n\n// Determine user's offline device setting from the environment variable.\n// If it's prefixed by \"+\", then it's in addition to any auto-discovered\n// devices.\n// If not, then we don't even probe for auto-discovered devices.\nconst char *acl_get_offline_device_user_setting(int *use_offline_only_ret) {\n  int use_offline_only = 0;\n  const char *setting = 0;\n  const char *setting_deprecated = 0;\n  const char *result = 0;\n  static char warn_depr1 = 0;\n\n  setting = acl_getenv(\"CL_CONTEXT_OFFLINE_DEVICE_INTELFPGA\");\n  setting_deprecated = acl_getenv(\"CL_CONTEXT_OFFLINE_DEVICE_ALTERA\");\n  if (!setting && setting_deprecated) {\n    setting = setting_deprecated;\n    if (0 == warn_depr1) {\n      fprintf(stderr, \"[Runtime Warning]: CL_CONTEXT_OFFLINE_DEVICE_ALTERA has \"\n                      \"been deprecated. Use \"\n                      \"CL_CONTEXT_OFFLINE_DEVICE_INTELFPGA instead.\\n\");\n      warn_depr1 = 1;\n    }\n  }\n\n  if (setting) {\n    if (*setting == '+') {\n      use_offline_only = ACL_CONTEXT_OFFLINE_AND_AUTODISCOVERY;\n      result = setting + 1; // Skip over the leading \"+\"\n    } else {\n      use_offline_only = ACL_CONTEXT_OFFLINE_ONLY;\n      result = setting;\n    }\n    // Treat empty string like no setting at all.\n    if (*result == 0) {\n      result = 0;\n      use_offline_only = ACL_CONTEXT_OFFLINE_AND_AUTODISCOVERY;\n    }\n  } else {\n    // Look for multi-process simulator before old simulator.\n    setting = acl_getenv(\"CL_CONTEXT_MPSIM_DEVICE_INTELFPGA\");\n    // Check if simulation board spec directory is set, which implies\n    // CL_CONTEXT_MPSIM_DEVICE_INTELFPGA if that is not set\n    if (!setting && acl_getenv(INTELFPGA_SIM_DEVICE_SPEC_DIR)) {\n      setting = \"1\"; // Use 1 simulator device by default\n    }\n    if (setting) {\n      use_offline_only = ACL_CONTEXT_MPSIM;\n      result = 0;\n    }\n  }\n\n  *use_offline_only_ret = use_offline_only;\n  return result;\n}\n\nint acl_init(const acl_system_def_t *newsys) {\n  acl_assert_locked();\n\n  // The user/tester really knows what they're doing.\n  acl_reset();\n\n  // Must have set the HAL first, so we can print errors if necessary.\n  if (acl_get_hal() && l_is_valid_system_def(newsys)) {\n    builtin_prog_def_value = *newsys;\n    acl_present_board_is_valid_value =\n        1; // Must come before acl_init_platform because that *uses* the defined\n           // system\n  }\n  acl_init_platform();\n  acl_init_profiler();\n  return acl_present_board_is_valid_value;\n}\n\n// Initialize the HAL and load the builtin system definition.\n//\n// It will handle setting up an offline device, either exclusively (by\n// default), or in addition to devices found via HAL probing.\n//\n// This function returns CL_TRUE if a hal is initialized and CL_FALSE\n// if it is not.\ncl_bool acl_init_from_hal_discovery(void) {\n  int use_offline_only = 0;\n  const acl_hal_t *board_hal;\n  acl_assert_locked();\n\n  (void)acl_get_offline_device_user_setting(&use_offline_only);\n\n  // Two jobs:\n  // 1. Set the HAL from the linked-in HAL library.\n  // 2. Discover the device parameters by probing.\n  acl_reset();\n\n  switch (use_offline_only) {\n  case ACL_CONTEXT_OFFLINE_AND_AUTODISCOVERY:\n  case ACL_CONTEXT_MPSIM:\n    board_hal = acl_mmd_get_system_definition(&builtin_prog_def_value,\n                                              libraries_to_load);\n    break;\n  case ACL_CONTEXT_OFFLINE_ONLY:\n    board_hal = acl_get_offline_hal();\n    break;\n  default:\n    // Not a valid setting so don't know which HAL to load\n    return CL_FALSE;\n  }\n\n  if (board_hal == NULL) {\n    return CL_FALSE;\n  }\n  // Probe the HAL for a device.\n  if (!acl_set_hal(board_hal)) {\n    return CL_FALSE;\n  }\n\n  if (use_offline_only != ACL_CONTEXT_OFFLINE_ONLY) {\n    acl_present_board_is_valid_value = 1;\n  }\n  acl_init_platform();\n  acl_init_profiler();\n\n  if (acl_get_hal() == nullptr) {\n    return CL_FALSE;\n  }\n\n  return CL_TRUE;\n}\n\nvoid acl_reset(void) {\n  acl_assert_locked();\n\n  l_reset_present_board();\n\n  acl_platform.offline_device = \"\";\n  acl_platform.offline_mode = ACL_CONTEXT_OFFLINE_AND_AUTODISCOVERY;\n  acl_platform.num_devices = 0;\n  for (unsigned i = 0; i < ACL_MAX_DEVICE; ++i) {\n    acl_platform.device[i] = _cl_device_id();\n  }\n  acl_platform.initialized = 0;\n}\n\n////////////////////////////////////////////////////\n// Static functions\n\nstatic int l_is_valid_range(int (*msg)(const char *, ...),\n                            const std::string &name,\n                            const acl_addr_range_t *range) {\n  ptrdiff_t begin_addr = ((char *)range->begin) - ((char *)0);\n  ptrdiff_t next_addr = ((char *)range->next) - ((char *)0);\n  acl_assert_locked();\n\n  if ((char *)range->next < (char *)range->begin) {\n    msg(\"Invalid address range for %s: beginning of storage %x comes after \"\n        \"next storage %x\\n\",\n        name.c_str(), range->begin, range->next);\n    return 0;\n  }\n\n  // Check alignment.\n  if ((begin_addr & (ACL_MEM_ALIGN - 1)) & ~(ACL_MEM_ALIGN)) {\n    msg(\"Invalid address range for %s: begin pointer %x should be aligned to \"\n        \"%d bytes\\n\",\n        name.c_str(), range->begin, ACL_MEM_ALIGN);\n    return 0;\n  }\n  if ((next_addr & (ACL_MEM_ALIGN - 1)) & ~(ACL_MEM_ALIGN)) {\n    msg(\"Invalid address range for %s: next pointer %x should be aligned to %d \"\n        \"bytes\\n\",\n        name.c_str(), range->next, ACL_MEM_ALIGN);\n    return 0;\n  }\n  return 1;\n}\n\nint l_is_valid_system_def(const acl_system_def_t *sys) {\n  int is_ok = 1;\n  int (*msg)(const char *, ...) = acl_print_debug_msg;\n  acl_assert_locked();\n\n#define AND_ALSO(COND, ISSUE_MSG)                                              \\\n  do {                                                                         \\\n    if (is_ok) {                                                               \\\n      is_ok = COND;                                                            \\\n      if (!is_ok) {                                                            \\\n        ISSUE_MSG;                                                             \\\n      }                                                                        \\\n    }                                                                          \\\n  } while (0)\n\n  AND_ALSO(sys != 0, msg(\"System definition is NULL pointer\\n\"));\n  if (!is_ok) {\n    return 0;\n  }\n\n  AND_ALSO(sys->num_devices <= ACL_MAX_DEVICE,\n           msg(\"Too many devices %d > %d\\n\", sys->num_devices, ACL_MAX_DEVICE));\n\n  if (!is_ok) {\n    return 0;\n  }\n\n  // Check each device in turn\n  for (cl_uint idev = 0; idev < sys->num_devices; idev++) {\n    AND_ALSO(sys->device[idev].autodiscovery_def.num_global_mem_systems > 0,\n             msg(\"Currently loaded device binary version is not supported\\n\"));\n    for (unsigned imem = 0;\n         imem < sys->device[idev].autodiscovery_def.num_global_mem_systems;\n         imem++) {\n      AND_ALSO(l_is_valid_range(msg, \"global memory\",\n                                &sys->device[idev]\n                                     .autodiscovery_def.global_mem_defs[imem]\n                                     .range), );\n      AND_ALSO(sys->device[idev]\n                       .autodiscovery_def.global_mem_defs[imem]\n                       .num_global_banks > 0,\n               msg(\"num global banks (%d) should be positive.\\n\",\n                   sys->device[idev]\n                       .autodiscovery_def.global_mem_defs[imem]\n                       .num_global_banks));\n    }\n\n    AND_ALSO(!sys->device[idev].autodiscovery_def.name.empty(),\n             msg(\"Accelerator %d does not have a name\\n\", idev));\n\n    // Check each accelerator block in turn.\n    for (const auto &accel : sys->device[idev].autodiscovery_def.accel) {\n\n      AND_ALSO(l_is_valid_range(msg, accel.iface.name.c_str(), &(accel.mem)), );\n\n      AND_ALSO(!accel.iface.name.empty(),\n               msg(\"Accelerator [%d][%d] has no name\\n\", idev, accel.id));\n\n      // Now check arg specs.\n      unsigned iarg = 0;\n      for (const auto &arg_info : accel.iface.args) {\n        switch (arg_info.addr_space) {\n        case ACL_ARG_ADDR_LOCAL:\n        case ACL_ARG_ADDR_GLOBAL:\n        case ACL_ARG_ADDR_CONSTANT:\n        case ACL_ARG_ADDR_NONE:\n          break;\n        default:\n          AND_ALSO(0, msg(\"Accelerator [%d] \\\"%s\\\" arg %d is in wrong address \"\n                          \"space %d\\n\",\n                          accel.id, accel.iface.name.c_str(), iarg,\n                          static_cast<int>(arg_info.addr_space)));\n          break;\n        }\n        switch (arg_info.category) {\n        case ACL_ARG_BY_VALUE:\n        case ACL_ARG_MEM_OBJ:\n        case ACL_ARG_SAMPLER:\n          break;\n        default:\n          AND_ALSO(0,\n                   msg(\"Accelerator [%d] \\\"%s\\\" arg %d has wrong category %d\\n\",\n                       accel.id, accel.iface.name.c_str(), iarg,\n                       static_cast<int>(arg_info.category)));\n          break;\n        }\n        AND_ALSO(\n            (arg_info.addr_space == ACL_ARG_ADDR_LOCAL || arg_info.size > 0),\n            msg(\"Accelerator [%d] \\\"%s\\\" arg %d is not a __local argument but \"\n                \"has 0 or negative size:  %d\\n\",\n                accel.id, accel.iface.name.c_str(), iarg,\n                static_cast<int>(arg_info.size)));\n        // Support devices that use 64-bit pointers or 32-bit pointers\n        AND_ALSO((arg_info.addr_space != ACL_ARG_ADDR_LOCAL ||\n                  arg_info.size == 4 || arg_info.size == 8),\n                 msg(\"Accelerator [%d] \\\"%s\\\" __local arg %d should have size \"\n                     \"= 4 or 8  but is size:  %d\\n\",\n                     accel.id, accel.iface.name.c_str(), iarg,\n                     static_cast<int>(arg_info.size)));\n        ++iarg;\n      }\n    }\n  }\n  return is_ok;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_svm.cpp",
        "data": "// Copyright (C) 2014-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <assert.h>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string.h>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_context.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_svm.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n//////////////////////////////\n// OpenCL API\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clSVMAllocIntelFPGA(cl_context context,\n                                                   cl_svm_mem_flags flags,\n                                                   size_t size,\n                                                   unsigned int alignment) {\n  void *result = NULL;\n  acl_svm_entry_t *svm_entry;\n  int num_rw_specs = 0;\n\n#ifdef __linux__\n  int mem_result;\n#endif\n#ifndef SYSTEM_SVM\n  unsigned int idevice;\n  // this context supports SVM\n  cl_bool context_has_svm;\n#endif\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  // Valid context\n#ifndef REMOVE_VALID_CHECKS\n\n  if (!acl_context_is_valid(context))\n    return NULL;\n\n  // Check for invalid enum bits\n  if (flags & ~(CL_MEM_READ_WRITE | CL_MEM_READ_ONLY | CL_MEM_WRITE_ONLY |\n                CL_MEM_SVM_FINE_GRAIN_BUFFER | CL_MEM_SVM_ATOMICS)) {\n    return NULL;\n  }\n\n  // Check for exactly one read/write spec\n  if (flags & CL_MEM_READ_WRITE)\n    num_rw_specs++;\n  if (flags & CL_MEM_READ_ONLY)\n    num_rw_specs++;\n  if (flags & CL_MEM_WRITE_ONLY)\n    num_rw_specs++;\n  // Default to CL_MEM_READ_WRITE.\n  if (num_rw_specs > 1)\n    return NULL;\n  if (num_rw_specs == 0)\n    flags |= CL_MEM_READ_WRITE;\n\n  // Cannot specify SVM atomics without fine grain\n  if ((flags & CL_MEM_SVM_ATOMICS) && !(flags & CL_MEM_SVM_FINE_GRAIN_BUFFER)) {\n    return NULL;\n  }\n\n  // If SVM atomics specified, check if any device in context supports SVM\n  // atomics Right now though, we don't support SVM atomics so just return NULL\n  if (flags & CL_MEM_SVM_ATOMICS) {\n    return NULL;\n  }\n\n  // If fine grain specified, check if any device in context supports fine grain\n  // Right now though, we don't support SVM fine grain so just return NULL\n  if (flags & CL_MEM_SVM_FINE_GRAIN_BUFFER) {\n    return NULL;\n  }\n\n  // size is 0 or > CL_DEVICE_MAX_MEM_ALLOC_SIZE value for any device in context\n  if (size == 0)\n    return NULL;\n  if (size > context->max_mem_alloc_size) {\n    return NULL;\n  }\n\n  // alignment is not a power of two or the OpenCL implementation cannot support\n  // the specified alignment for at least one device in context. For now, we\n  // only allow alignment to ACL_MEM_ALIGN. If we ever change this, we also need\n  // to update the alignment check in clSetKernelArgSVMPointerIntelFPGA.\n  // Alignment of '0' means use the default\n  if (alignment == 0)\n    alignment = ACL_MEM_ALIGN;\n  if (alignment != ACL_MEM_ALIGN)\n    return NULL;\n\n#endif // !REMOVE_VALID_CHECKS\n\n#ifdef SYSTEM_SVM\n#ifdef _WIN32\n  result = _aligned_malloc(size, alignment);\n#else // LINUX\n  mem_result = posix_memalign(&result, alignment, size);\n  if (mem_result != 0) {\n    return NULL;\n  }\n#endif\n#else\n  // Determine if this is SVM memory and, if so, allocate the memory for it\n  context_has_svm = CL_FALSE;\n  if (acl_get_hal()) {\n    for (idevice = 0; idevice < context->num_devices; ++idevice) {\n      context_has_svm =\n          (cl_bool)(context_has_svm ||\n                    acl_svm_device_supports_any_svm(\n                        context->device[idevice]->def.physical_device_id));\n    }\n  }\n  // Valid context\n  // Allocate\n  if (!context_has_svm || acl_get_hal() == NULL ||\n      acl_get_hal()->legacy_shared_alloc == NULL) {\n// if hal function is not provided, use system alloc\n#ifdef _WIN32\n    result = _aligned_malloc(size, alignment);\n#else // LINUX\n    mem_result = posix_memalign(&result, alignment, size);\n    if (mem_result != 0) {\n      return NULL;\n    }\n#endif\n  } else {\n    long long unsigned int offset;\n    result = acl_get_hal()->legacy_shared_alloc(context, size, &offset);\n  }\n#endif // SYSTEM_SVM\n  svm_entry = context->svm_list;\n  context->svm_list = (acl_svm_entry_t *)malloc(sizeof(acl_svm_entry_t));\n  assert(context->svm_list);\n\n  context->svm_list->next = svm_entry;\n  if (flags & CL_MEM_READ_ONLY) {\n    context->svm_list->read_only = CL_TRUE;\n  } else {\n    context->svm_list->read_only = CL_FALSE;\n  }\n  if (flags & CL_MEM_WRITE_ONLY) {\n    context->svm_list->write_only = CL_TRUE;\n  } else {\n    context->svm_list->write_only = CL_FALSE;\n  }\n  context->svm_list->is_mapped = CL_FALSE;\n  context->svm_list->ptr = result;\n  context->svm_list->size = size;\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY void *CL_API_CALL clSVMAlloc(cl_context context,\n                                          cl_svm_mem_flags flags, size_t size,\n                                          unsigned int alignment) {\n  return clSVMAllocIntelFPGA(context, flags, size, alignment);\n}\n\nACL_EXPORT\nCL_API_ENTRY void clSVMFreeIntelFPGA(cl_context context, void *svm_pointer) {\n  acl_svm_entry_t *last_entry;\n  acl_svm_entry_t *next_entry;\n  unsigned int idevice;\n  cl_bool context_has_svm;\n  std::scoped_lock lock{acl_mutex_wrapper};\n  context_has_svm = CL_FALSE;\n  if (acl_get_hal()) {\n    for (idevice = 0; idevice < context->num_devices; ++idevice) {\n      context_has_svm =\n          (cl_bool)(context_has_svm ||\n                    acl_svm_device_supports_any_svm(\n                        context->device[idevice]->def.physical_device_id));\n    }\n  }\n#ifndef REMOVE_VALID_CHECKS\n  if (!acl_context_is_valid(context))\n    return;\n\n  if (svm_pointer == NULL)\n    return;\n\n#endif // !REMOVE_VALID_CHECKS\n  // Only free the SVM pointer if it is from this context\n  if (context->svm_list == NULL)\n    return;\n\n  last_entry = NULL;\n  next_entry = context->svm_list;\n  while (next_entry != NULL) {\n    if (next_entry->ptr == svm_pointer) {\n      if (last_entry == NULL) {\n        context->svm_list = next_entry->next;\n      } else {\n        last_entry->next = next_entry->next;\n      }\n#ifdef SYSTEM_SVM\n#ifdef _WIN32\n      _aligned_free(svm_pointer);\n#else // LINUX\n      free(svm_pointer);\n#endif\n#else\n      if (!context_has_svm || acl_get_hal() == NULL ||\n          acl_get_hal()->legacy_shared_free == NULL) {\n#ifdef _WIN32\n        _aligned_free(svm_pointer);\n#else // LINUX\n        free(svm_pointer);\n#endif\n      } else {\n        acl_get_hal()->legacy_shared_free(context, svm_pointer,\n                                          next_entry->size);\n      }\n#endif // SYSTEM_SVM\n      free(next_entry);\n      break;\n    }\n    last_entry = next_entry;\n    next_entry = next_entry->next;\n  }\n}\n\nACL_EXPORT\nCL_API_ENTRY void clSVMFree(cl_context context, void *svm_pointer) {\n  clSVMFreeIntelFPGA(context, svm_pointer);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMMemcpyIntelFPGA(\n    cl_command_queue command_queue, cl_bool blocking_copy, void *dst_ptr,\n    const void *src_ptr, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  cl_event local_event = 0; // used for blocking\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (src_ptr == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (dst_ptr == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer size cannot be 0\");\n  }\n\n  if (((char *)src_ptr < (char *)dst_ptr &&\n       (char *)src_ptr + size > (char *)dst_ptr) ||\n      ((char *)dst_ptr < (char *)src_ptr &&\n       (char *)dst_ptr + size > (char *)src_ptr)) {\n    ERR_RET(CL_MEM_COPY_OVERLAP, command_queue->context,\n            \"Source and destination memory overlaps\");\n  }\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_SVM_MEMCPY, &local_event);\n  if (status != CL_SUCCESS)\n    return status; // already signalled callback\n  local_event->cmd.info.svm_xfer.src_ptr = src_ptr;\n  local_event->cmd.info.svm_xfer.dst_ptr = dst_ptr;\n  local_event->cmd.info.svm_xfer.src_size = size;\n  local_event->cmd.info.svm_xfer.dst_size = size;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (blocking_copy) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMMemcpy(\n    cl_command_queue command_queue, cl_bool blocking_copy, void *dst_ptr,\n    const void *src_ptr, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueSVMMemcpyIntelFPGA(command_queue, blocking_copy, dst_ptr,\n                                     src_ptr, size, num_events_in_wait_list,\n                                     event_wait_list, event);\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMMemFillIntelFPGA(\n    cl_command_queue command_queue, void *svm_ptr, const void *pattern,\n    size_t pattern_size, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  cl_event local_event = 0; // used for blocking\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (svm_ptr == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (((uintptr_t)svm_ptr) % (pattern_size * 8) != 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer not aligned with pattern size\");\n  }\n  if (pattern == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pattern argument cannot be NULL\");\n  }\n  if (pattern_size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pattern size argument cannot be 0\");\n  }\n  if (pattern_size != 1 && pattern_size != 2 && pattern_size != 4 &&\n      pattern_size != 8 && pattern_size != 16 && pattern_size != 32 &&\n      pattern_size != 64 && pattern_size != 128) {\n    ERR_RET(\n        CL_INVALID_VALUE, command_queue->context,\n        \"Pattern size argument must be one of {1, 2, 4, 8, 16, 32, 64, 128}\");\n  }\n  if (size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer size cannot be 0\");\n  }\n  if (size % pattern_size != 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer size must be multiple of pattern size\");\n  }\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_SVM_MEMFILL, &local_event);\n  if (status != CL_SUCCESS)\n    return status; // already signalled callback\n  local_event->cmd.info.svm_xfer.src_ptr = pattern;\n  local_event->cmd.info.svm_xfer.dst_ptr = svm_ptr;\n  local_event->cmd.info.svm_xfer.src_size = pattern_size;\n  local_event->cmd.info.svm_xfer.dst_size = size;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  return CL_SUCCESS;\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMMemFill(\n    cl_command_queue command_queue, void *svm_ptr, const void *pattern,\n    size_t pattern_size, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueSVMMemFillIntelFPGA(\n      command_queue, svm_ptr, pattern, pattern_size, size,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMMapIntelFPGA(\n    cl_command_queue command_queue, cl_bool blocking_map, cl_map_flags flags,\n    void *svm_ptr, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  cl_event local_event = 0; // used for blocking\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (svm_ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n  if (size == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer size cannot be 0\");\n  }\n  if (flags & ~(CL_MAP_READ | CL_MAP_WRITE)) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Invalid or unsupported flags\");\n  }\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status = acl_create_event(command_queue, num_events_in_wait_list,\n                            event_wait_list, CL_COMMAND_SVM_MAP, &local_event);\n  if (status != CL_SUCCESS)\n    return status; // already signalled callback\n  // We don't use this right now, but if we ever have to sync up caches we will\n  // need this.\n  local_event->cmd.info.svm_xfer.dst_ptr = svm_ptr;\n  local_event->cmd.info.svm_xfer.dst_size = size;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (blocking_map) {\n    status = clWaitForEvents(1, &local_event);\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  return CL_SUCCESS;\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMMap(\n    cl_command_queue command_queue, cl_bool blocking_map, cl_map_flags flags,\n    void *svm_ptr, size_t size, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueSVMMapIntelFPGA(command_queue, blocking_map, flags, svm_ptr,\n                                  size, num_events_in_wait_list,\n                                  event_wait_list, event);\n}\n\nCL_API_ENTRY cl_int CL_API_CALL\nclEnqueueSVMUnmapIntelFPGA(cl_command_queue command_queue, void *svm_ptr,\n                           cl_uint num_events_in_wait_list,\n                           const cl_event *event_wait_list, cl_event *event) {\n  cl_event local_event = 0; // used for blocking\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (svm_ptr == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Pointer argument cannot be NULL\");\n  }\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status =\n      acl_create_event(command_queue, num_events_in_wait_list, event_wait_list,\n                       CL_COMMAND_SVM_UNMAP, &local_event);\n  if (status != CL_SUCCESS)\n    return status; // already signalled callback\n  // We don't use this right now, but if we ever have to sync up caches we will\n  // need this.\n  local_event->cmd.info.svm_xfer.dst_ptr = svm_ptr;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  return CL_SUCCESS;\n}\n\nCL_API_ENTRY cl_int CL_API_CALL\nclEnqueueSVMUnmap(cl_command_queue command_queue, void *svm_ptr,\n                  cl_uint num_events_in_wait_list,\n                  const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueSVMUnmapIntelFPGA(\n      command_queue, svm_ptr, num_events_in_wait_list, event_wait_list, event);\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMFreeIntelFPGA(\n    cl_command_queue command_queue, cl_uint num_svm_pointers,\n    void *svm_pointers[],\n    void(CL_CALLBACK *pfn_free_func)(cl_command_queue /* queue */,\n                                     cl_uint /* num_svm_pointers */,\n                                     void *[] /* svm_pointers[] */,\n                                     void * /* user_data */),\n    void *user_data, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  cl_event local_event = 0; // used for blocking\n  cl_int status;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (svm_pointers == NULL) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"List of SVM pointers argument cannot be NULL\");\n  }\n  if (num_svm_pointers == 0) {\n    ERR_RET(CL_INVALID_VALUE, command_queue->context,\n            \"Number of SVM pointers cannot be 0\");\n  }\n\n  // Create an event/command to actually move the data at the appropriate\n  // time.\n  status = acl_create_event(command_queue, num_events_in_wait_list,\n                            event_wait_list, CL_COMMAND_SVM_FREE, &local_event);\n  if (status != CL_SUCCESS)\n    return status; // already signalled callback\n  // We don't use this right now, but if we ever have to sync up caches we will\n  // need this.\n  local_event->cmd.info.svm_free.pfn_free_func = pfn_free_func;\n  local_event->cmd.info.svm_free.num_svm_pointers = num_svm_pointers;\n  local_event->cmd.info.svm_free.svm_pointers = svm_pointers;\n  local_event->cmd.info.svm_free.user_data = user_data;\n\n  acl_idle_update(\n      command_queue\n          ->context); // If nothing's blocking, then complete right away\n\n  if (event) {\n    *event = local_event;\n  } else {\n    // User didn't care, so forget about the event.\n    clReleaseEvent(local_event);\n    acl_idle_update(command_queue->context); // Clean up early\n  }\n  return CL_SUCCESS;\n}\n\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueSVMFree(\n    cl_command_queue command_queue, cl_uint num_svm_pointers,\n    void *svm_pointers[],\n    void(CL_CALLBACK *pfn_free_func)(cl_command_queue /* queue */,\n                                     cl_uint /* num_svm_pointers */,\n                                     void *[] /* svm_pointers[] */,\n                                     void * /* user_data */),\n    void *user_data, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueSVMFreeIntelFPGA(\n      command_queue, num_svm_pointers, svm_pointers, pfn_free_func, user_data,\n      num_events_in_wait_list, event_wait_list, event);\n}\n\nvoid acl_forcibly_release_all_svm_memory_for_context(cl_context context) {\n  acl_assert_locked();\n  while (context->svm_list != NULL) {\n    clSVMFreeIntelFPGA(context, context->svm_list->ptr);\n  }\n}\n\ncl_bool acl_ptr_is_exactly_in_context_svm(cl_context context, const void *ptr) {\n  acl_svm_entry_t *next_svm_entry;\n  acl_assert_locked();\n  next_svm_entry = context->svm_list;\n\n  while (next_svm_entry != NULL) {\n    if (next_svm_entry->ptr == ptr) {\n      return CL_TRUE;\n    }\n    next_svm_entry = next_svm_entry->next;\n  }\n\n  return CL_FALSE;\n}\n\ncl_bool acl_ptr_is_contained_in_context_svm(cl_context context,\n                                            const void *ptr) {\n  acl_svm_entry_t *next_svm_entry;\n  acl_assert_locked();\n  next_svm_entry = context->svm_list;\n\n  while (next_svm_entry != NULL) {\n    if ((char *)ptr >= (char *)next_svm_entry->ptr &&\n        (char *)ptr < (char *)next_svm_entry->ptr + next_svm_entry->size) {\n      return CL_TRUE;\n    }\n    next_svm_entry = next_svm_entry->next;\n  }\n\n  return CL_FALSE;\n}\n\nacl_svm_entry_t *acl_get_svm_entry(cl_context context, void *ptr) {\n  acl_svm_entry_t *result = NULL;\n  acl_svm_entry_t *next_svm_entry;\n  acl_assert_locked();\n\n  next_svm_entry = context->svm_list;\n\n  while (next_svm_entry != NULL) {\n    if ((char *)ptr >= (char *)next_svm_entry->ptr &&\n        (char *)ptr < (char *)next_svm_entry->ptr + next_svm_entry->size) {\n      result = next_svm_entry;\n      break;\n    }\n    next_svm_entry = next_svm_entry->next;\n  }\n\n  return result;\n}\n\n// Submit an op to the device op queue to copy memory.\n// Return 1 if we made forward progress, 0 otherwise.\nint acl_svm_op(cl_event event) {\n  int result = 0;\n  // The buffer is defined to always be host accessible.\n  // So just count the mappings.\n  const void *src_ptr = event->cmd.info.svm_xfer.src_ptr;\n  void *dst_ptr = event->cmd.info.svm_xfer.dst_ptr;\n  size_t src_size = event->cmd.info.svm_xfer.src_size;\n  size_t dst_size = event->cmd.info.svm_xfer.dst_size;\n  acl_assert_locked();\n\n  acl_set_execution_status(event, CL_SUBMITTED);\n  acl_set_execution_status(event, CL_RUNNING);\n  if (event->cmd.type == CL_COMMAND_SVM_MEMCPY) {\n    memcpy(dst_ptr, src_ptr, src_size);\n  } else if (event->cmd.type == CL_COMMAND_SVM_MEMFILL) {\n    size_t current_pos = 0;\n    while (current_pos < dst_size) {\n      memcpy((char *)dst_ptr + current_pos, (char *)src_ptr, src_size);\n      current_pos += src_size;\n    }\n  } else if (event->cmd.type == CL_COMMAND_SVM_MAP) {\n    // Do nothing. One day we may need to do something if we need to sync up\n    // caches.\n  } else if (event->cmd.type == CL_COMMAND_SVM_UNMAP) {\n    // Do nothing. One day we may need to do something if we need to sync up\n    // caches.\n  } else if (event->cmd.type == CL_COMMAND_SVM_FREE) {\n    // If a user function was defined, pass in the provided values\n    if (event->cmd.info.svm_free.pfn_free_func != NULL) {\n      event->cmd.info.svm_free.pfn_free_func(\n          event->command_queue, event->cmd.info.svm_free.num_svm_pointers,\n          event->cmd.info.svm_free.svm_pointers,\n          event->cmd.info.svm_free.user_data);\n      // Otherwise, just call the regular SVM function on each SVM pointer\n      // provided\n    } else {\n      size_t i;\n\n      for (i = 0; i < event->cmd.info.svm_free.num_svm_pointers; ++i) {\n        clSVMFreeIntelFPGA(event->context,\n                           event->cmd.info.svm_free.svm_pointers[i]);\n      }\n    }\n  }\n  acl_set_execution_status(event, CL_COMPLETE);\n  acl_print_debug_msg(\"SVM operation\\n\");\n\n  result = 1;\n\n  return result;\n}\n\ncl_bool acl_svm_device_supports_any_svm(unsigned int physical_device_id) {\n  acl_assert_locked();\n  int memories_supported;\n  int supports = acl_get_hal()->has_svm_memory_support(physical_device_id,\n                                                       &memories_supported);\n\n  if (supports != 0) {\n    return CL_TRUE;\n  } else {\n    return CL_FALSE;\n  }\n}\n\ncl_bool\nacl_svm_device_supports_physical_memory(unsigned int physical_device_id) {\n  acl_assert_locked();\n\n  int supports = acl_get_hal()->has_physical_mem(physical_device_id);\n\n  if (supports != 0) {\n    return CL_TRUE;\n  } else {\n    return CL_FALSE;\n  }\n}\n"
    },
    {
        "label": "acl_sampler.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <sstream>\n\n// External library headers.\n#include <CL/opencl.h>\n\n// Internal headers.\n#include <acl_globals.h>\n#include <acl_util.h>\n\n// Samplers and images are not supported.\n\n// In case we ever fill this out, make sure our internals are somewhat\n// hidden from user's code.  For 14.0, make it protected.\n// Later, strongly consider upgrading to \"hidden\".\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n//////////////////////////////\n// OpenCL API\n\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4100)\n#endif\nACL_EXPORT\nCL_API_ENTRY cl_sampler clCreateSamplerWithPropertiesIntelFPGA(\n    cl_context context, const cl_sampler_properties *sampler_properties,\n    cl_int *errcode_ret) {\n  cl_sampler result = 0;\n  size_t iprop;\n  unsigned int idevice;\n  cl_bool some_device_supports_images = CL_FALSE;\n  int sampler_id;\n  cl_sampler sampler;\n  int next_free_sampler_head;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  sampler_id = acl_platform.free_sampler_head;\n  sampler = &(acl_platform.sampler[sampler_id]);\n  next_free_sampler_head = acl_platform.sampler[sampler_id].link;\n\n  // This should be redundant\n  acl_reset_ref_count(sampler);\n  sampler->normalized_coords = 0xFFFFFFFF;\n  sampler->addressing_mode = 0xFFFFFFFF;\n  sampler->filter_mode = 0xFFFFFFFF;\n\n  if (!acl_context_is_valid(context)) {\n    BAIL(CL_INVALID_CONTEXT);\n  }\n\n  sampler->context = context;\n\n  for (idevice = 0; idevice < context->num_devices; ++idevice) {\n    cl_bool support_images;\n    cl_int error_status;\n    error_status =\n        clGetDeviceInfo(context->device[idevice], CL_DEVICE_IMAGE_SUPPORT,\n                        sizeof(cl_bool), &support_images, NULL);\n    if (error_status != CL_SUCCESS) {\n      BAIL(CL_OUT_OF_RESOURCES);\n    }\n    if (support_images) {\n      some_device_supports_images = CL_TRUE;\n      break;\n    }\n  }\n  if (!some_device_supports_images) {\n    BAIL_INFO(CL_INVALID_OPERATION, context,\n              \"No devices in context support images\");\n  }\n\n  iprop = 0;\n  while (sampler_properties[iprop] != 0) {\n    if (sampler_properties[iprop] == CL_SAMPLER_NORMALIZED_COORDS) {\n      ++iprop;\n      if (sampler->normalized_coords != 0xFFFFFFFF) {\n        BAIL_INFO(\n            CL_INVALID_VALUE, context,\n            \"Normalized coords property specified more than once for sampler\");\n      }\n      if (sampler_properties[iprop] != CL_FALSE &&\n          sampler_properties[iprop] != CL_TRUE) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"Invalid value for normalized coords property of sampler\");\n      }\n      sampler->normalized_coords = sampler_properties[iprop];\n    } else if (sampler_properties[iprop] == CL_SAMPLER_ADDRESSING_MODE) {\n      ++iprop;\n      if (sampler->addressing_mode != 0xFFFFFFFF) {\n        BAIL_INFO(\n            CL_INVALID_VALUE, context,\n            \"Addressing mode property specified more than once for sampler\");\n      }\n      if (sampler_properties[iprop] != CL_ADDRESS_MIRRORED_REPEAT &&\n          sampler_properties[iprop] != CL_ADDRESS_REPEAT &&\n          sampler_properties[iprop] != CL_ADDRESS_CLAMP_TO_EDGE &&\n          sampler_properties[iprop] != CL_ADDRESS_CLAMP &&\n          sampler_properties[iprop] != CL_ADDRESS_NONE) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"Invalid value for addressing mode property of sampler\");\n      }\n      sampler->addressing_mode = sampler_properties[iprop];\n    } else if (sampler_properties[iprop] == CL_SAMPLER_FILTER_MODE) {\n      ++iprop;\n      if (sampler->filter_mode != 0xFFFFFFFF) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"Filter mode property specified more than once for sampler\");\n      }\n      if (sampler_properties[iprop] != CL_FILTER_NEAREST &&\n          sampler_properties[iprop] != CL_FILTER_LINEAR) {\n        BAIL_INFO(CL_INVALID_VALUE, context,\n                  \"Invalid value for filter mode property of sampler\");\n      }\n      sampler->filter_mode = sampler_properties[iprop];\n    } else {\n      std::stringstream msg;\n      msg << \"Invalid sampler property name \" << sampler_properties[iprop]\n          << \"\\n\";\n      BAIL_INFO(CL_INVALID_VALUE, context, msg.str().c_str());\n    }\n    ++iprop;\n  }\n\n  if (sampler->normalized_coords == 0xFFFFFFFF) {\n    sampler->normalized_coords = CL_TRUE;\n  }\n  if (sampler->addressing_mode == 0xFFFFFFFF) {\n    sampler->addressing_mode = CL_ADDRESS_CLAMP;\n  }\n  if (sampler->filter_mode == 0xFFFFFFFF) {\n    sampler->filter_mode = CL_FILTER_NEAREST;\n  }\n\n  acl_platform.free_sampler_head = next_free_sampler_head;\n\n  result = sampler;\n\n  acl_retain(result);\n  acl_retain(context);\n\n  if (errcode_ret) {\n    *errcode_ret = CL_SUCCESS;\n  }\n\n  acl_track_object(ACL_OBJ_MEM_OBJECT, result);\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_sampler clCreateSamplerWithProperties(\n    cl_context context, const cl_sampler_properties *sampler_properties,\n    cl_int *errcode_ret) {\n  return clCreateSamplerWithPropertiesIntelFPGA(context, sampler_properties,\n                                                errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_sampler CL_API_CALL\nclCreateSamplerIntelFPGA(cl_context context, cl_bool normalized_coords,\n                         cl_addressing_mode addressing_mode,\n                         cl_filter_mode filter_mode, cl_int *errcode_ret) {\n  cl_sampler_properties sampler_properties[7];\n\n  sampler_properties[0] = CL_SAMPLER_NORMALIZED_COORDS;\n  sampler_properties[1] = normalized_coords;\n  sampler_properties[2] = CL_SAMPLER_ADDRESSING_MODE;\n  sampler_properties[3] = addressing_mode;\n  sampler_properties[4] = CL_SAMPLER_FILTER_MODE;\n  sampler_properties[5] = filter_mode;\n  sampler_properties[6] = 0;\n\n  return clCreateSamplerWithPropertiesIntelFPGA(context, sampler_properties,\n                                                errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_sampler CL_API_CALL\nclCreateSampler(cl_context context, cl_bool normalized_coords,\n                cl_addressing_mode addressing_mode, cl_filter_mode filter_mode,\n                cl_int *errcode_ret) {\n  return clCreateSamplerIntelFPGA(context, normalized_coords, addressing_mode,\n                                  filter_mode, errcode_ret);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainSamplerIntelFPGA(cl_sampler sampler) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_sampler_is_valid(sampler)) {\n    return CL_INVALID_SAMPLER;\n  }\n  acl_retain(sampler);\n\n#if 1\n  acl_print_debug_msg(\"Retain sampler[%d] %p now %u\\n\", sampler->id, sampler,\n                      acl_ref_count(sampler));\n#endif\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clRetainSampler(cl_sampler sampler) {\n  return clRetainSamplerIntelFPGA(sampler);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseSamplerIntelFPGA(cl_sampler sampler) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n  // In the double-free case, we'll error out here because the reference count\n  // will be 0.\n  if (!acl_sampler_is_valid(sampler)) {\n    return CL_INVALID_SAMPLER;\n  }\n\n  acl_release(sampler);\n\n  acl_print_debug_msg(\"Release sampler[%d] %p now %u\\n\", sampler->id, sampler,\n                      acl_ref_count(sampler));\n\n  if (!acl_is_retained(sampler)) {\n    cl_context context = sampler->context;\n\n    // Put the cl_sampler object back onto the free list.\n    sampler->link = acl_platform.free_sampler_head;\n    acl_platform.free_sampler_head = sampler->id;\n\n    // Allow later check of double-free, and correct calls to\n    // clReleaseSampler and clRetainSampler in all cases.\n    sampler->context = 0;\n\n    sampler->normalized_coords = 0xFFFFFFFF;\n    sampler->addressing_mode = 0xFFFFFFFF;\n    sampler->filter_mode = 0xFFFFFFFF;\n\n    acl_untrack_object(sampler);\n\n    clReleaseContext(context);\n  }\n\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clReleaseSampler(cl_sampler sampler) {\n  return clReleaseSamplerIntelFPGA(sampler);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetSamplerInfoIntelFPGA(\n    cl_sampler sampler, cl_sampler_info param_name, size_t param_value_size,\n    void *param_value, size_t *param_value_size_ret) {\n  acl_result_t result;\n  cl_context context;\n  RESULT_INIT;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n  if (!acl_sampler_is_valid(sampler)) {\n    return CL_INVALID_SAMPLER;\n  }\n\n  context = sampler->context;\n\n  switch (param_name) {\n  case CL_SAMPLER_REFERENCE_COUNT:\n    RESULT_UINT(acl_ref_count(sampler));\n    break;\n  case CL_SAMPLER_CONTEXT:\n    RESULT_PTR(sampler->context);\n    break;\n  case CL_SAMPLER_NORMALIZED_COORDS:\n    RESULT_BOOL((cl_bool)((sampler->normalized_coords == CL_TRUE) ? CL_TRUE\n                                                                  : CL_FALSE));\n    break;\n  case CL_SAMPLER_ADDRESSING_MODE:\n    RESULT_UINT((unsigned int)sampler->addressing_mode);\n    break;\n  case CL_SAMPLER_FILTER_MODE:\n    RESULT_UINT((unsigned int)sampler->filter_mode);\n    break;\n  default:\n    break;\n  }\n\n  if (result.size == 0)\n    ERR_RET(CL_INVALID_VALUE, context,\n            \"Invalid or unsupported sampler object query\");\n\n  if (param_value) {\n    if (param_value_size < result.size)\n      ERR_RET(CL_INVALID_VALUE, context,\n              \"Parameter return buffer is too small\");\n    RESULT_COPY(param_value, param_value_size);\n  }\n\n  if (param_value_size_ret) {\n    *param_value_size_ret = result.size;\n  }\n  return CL_SUCCESS;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clGetSamplerInfo(cl_sampler sampler,\n                                                 cl_sampler_info param_name,\n                                                 size_t param_value_size,\n                                                 void *param_value,\n                                                 size_t *param_value_size_ret) {\n  return clGetSamplerInfoIntelFPGA(sampler, param_name, param_value_size,\n                                   param_value, param_value_size_ret);\n}\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n//////////////////////////////\n// Internals\nint acl_sampler_is_valid(cl_sampler sampler) {\n#ifdef REMOVE_VALID_CHECKS\n  return 1;\n#else\n  if (!acl_sampler_is_valid_ptr(sampler)) {\n    return 0;\n  }\n  if (!acl_is_retained(sampler)) {\n    return 0;\n  }\n  if (!acl_context_is_valid(sampler->context)) {\n    return 0;\n  }\n  return 1;\n#endif\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_kernel_if.cpp",
        "data": "// Copyright (C) 2013-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <cassert>\n#include <cinttypes>\n#include <cmath>\n#include <cstdint>\n#include <stdio.h>\n#include <stdlib.h>\n#include <string>\n#include <vector>\n\n// Internal headers.\n#include <acl_auto_configure.h>\n#include <acl_globals.h>\n#include <acl_kernel_if.h>\n#include <acl_profiler.h>\n#include <acl_shipped_board_cfgs.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_util.h>\n\n#undef TEST_PROFILING_HARDWARE\n#ifdef TEST_PROFILING_HARDWARE\nextern int acl_hal_mmd_reset_profile_counters(unsigned int physical_device_id,\n                                              unsigned int accel_id);\nextern int acl_hal_mmd_get_profile_data(unsigned int physical_device_id,\n                                        unsigned int accel_id, uint64_t *data,\n                                        unsigned int length);\nextern int acl_hal_mmd_disable_profile_counters(unsigned int physical_device_id,\n                                                unsigned int accel_id);\nextern int acl_hal_mmd_enable_profile_counters(unsigned int physical_device_id,\n                                               unsigned int accel_id);\n#endif\nextern int acl_process_profiler_scan_chain(acl_device_op_t *op);\nextern int\nacl_process_autorun_profiler_scan_chain(unsigned int physical_device_id,\n                                        unsigned int accel_id);\n\n#define ACL_KERNEL_READ_BIT(w, b) (((w) >> (b)) & 1)\n#define ACL_KERNEL_READ_BIT_RANGE(w, h, l)                                     \\\n  (((w) >> (l)) & ((1 << ((h) - (l) + 1)) - 1))\n#define ACL_KERNEL_SET_BIT(w, b) ((w) |= (1 << (b)))\n#define ACL_KERNEL_CLEAR_BIT(w, b) ((w) &= (~(1 << (b))))\n#define ACL_KERNEL_GET_BIT(b) (unsigned)(1 << (b))\n\n#define ACL_KERNEL_IF_DEBUG_MSG_VERBOSE(k, verbosity, m, ...)                  \\\n  if (k->io.printf && (k->io.debug_verbosity) >= verbosity)                    \\\n    do {                                                                       \\\n      k->io.printf((m), ##__VA_ARGS__);                                        \\\n  } while (0)\n#define ACL_KERNEL_IF_DEBUG_MSG(k, m, ...)                                     \\\n  if (k->io.printf && k->io.debug_verbosity > 0)                               \\\n    do {                                                                       \\\n      k->io.printf((m), ##__VA_ARGS__);                                        \\\n  } while (0)\n\ntypedef time_ns time_us;\n\n// Function declarations\nstatic int acl_kernel_if_read_32b(acl_kernel_if *kern, unsigned int addr,\n                                  unsigned int *val);\nstatic int acl_kernel_if_read_64b(acl_kernel_if *kern, unsigned int addr,\n                                  uint64_t *val);\nstatic int acl_kernel_if_write_32b(acl_kernel_if *kern, unsigned int addr,\n                                   unsigned int val);\nstatic int acl_kernel_if_write_64b(acl_kernel_if *kern, unsigned int addr,\n                                   uint64_t val);\nstatic int acl_kernel_rom_cra_read_32b(acl_kernel_if *kern, unsigned int addr,\n                                       unsigned int *val);\nstatic int acl_kernel_rom_cra_read_block(acl_kernel_if *kern, unsigned int addr,\n                                         char *config_rom, size_t size);\n\n// **************************************************************************\n// **************************** Callback Functions **************************\n// **************************************************************************\n\nacl_kernel_update_callback acl_kernel_if_update_fn = NULL;\nacl_profile_callback acl_kernel_profile_fn = NULL;\nacl_process_printf_buffer_callback acl_process_printf_buffer_fn = NULL;\n\nvoid acl_kernel_if_register_callbacks(\n    acl_kernel_update_callback kernel_update,\n    acl_profile_callback profile_callback,\n    acl_process_printf_buffer_callback process_printf) {\n  acl_assert_locked();\n  acl_kernel_if_update_fn = kernel_update;\n  acl_kernel_profile_fn = profile_callback;\n  acl_process_printf_buffer_fn = process_printf;\n}\n\n// **************************************************************************\n// **************************** Utility Functions ***************************\n// **************************************************************************\nvoid print_invocation_image(acl_kernel_if *kern, char *image_ptr,\n                            size_t image_size, size_t size_to_write,\n                            unsigned int csr_offset, bool is_static,\n                            bool is_write = true, size_t print_offset = 0) {\n  std::string image_type = is_static ? \"stat\" : \"args\";\n  std::string overwrite = is_write ? \"Writing\" : \"Keeping\";\n  size_t print_end = print_offset + size_to_write;\n  assert(print_end <= image_size && \"printing invocation image out of bound\");\n  for (uintptr_t p = print_offset; p < print_end; p += sizeof(int)) {\n    unsigned int pword = 0;\n    uintptr_t cpy_size =\n        (print_end - p > sizeof(int)) ? sizeof(int) : (print_end - p);\n    safe_memcpy(((char *)(&pword)), image_ptr + p, cpy_size * sizeof(char),\n                sizeof(int), (print_end - p) * sizeof(char));\n    ACL_KERNEL_IF_DEBUG_MSG_VERBOSE(\n        kern, 2, \"::   %s inv image (%s) [%2d] @%8p := %4x\\n\",\n        overwrite.c_str(), image_type.c_str(), (int)(p),\n        (void *)(csr_offset + p), pword);\n  }\n}\n\n// Returns 0 on success, -1 on failure\nstatic int check_version_id(acl_kernel_if *kern) {\n  unsigned int version = 0;\n  int r;\n  acl_assert_locked();\n\n  r = acl_kernel_if_read_32b(kern, OFFSET_KERNEL_VERSION_ID, &version);\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \"Version ID check, read: %08x\\n\", version);\n  if (r != 0 ||\n      (version != KERNEL_VERSION_ID && version != KERNEL_ROM_VERSION_ID)) {\n    kern->io.printf(\n        \"  HAL Kern: Version mismatch! Expected 0x%x but read 0x%x\\n\",\n        KERNEL_VERSION_ID, version);\n    return -1;\n  } else\n    return 0;\n}\n\n// Loads auto-discovery config string. Returns 0 if was successful.\nstatic int get_auto_discovery_string(acl_kernel_if *kern, char *config_str,\n                                     size_t config_str_len) {\n  unsigned int version, rom_address, rom_size;\n  size_t r;\n  int result;\n  acl_assert_locked();\n\n  kern->cur_segment = 0xffffffff;\n\n  version = 0;\n  acl_kernel_if_read_32b(kern, OFFSET_KERNEL_VERSION_ID, &version);\n  if (version == KERNEL_ROM_VERSION_ID) {\n    // Find beginning address of ROM\n    rom_size =\n        1 << ((unsigned)log2((double)config_str_len + KERNEL_ROM_SIZE_BYTES) +\n              1);\n    rom_address = OFFSET_KERNEL_MAX_ADDRESS - rom_size + 1;\n    config_str[0] = '\\0';\n    result = acl_kernel_rom_cra_read_block(kern, rom_address, config_str,\n                                           (size_t)config_str_len);\n\n    if (result < 0)\n      return -1;\n    // Pad autodiscovery with NULL\n    *(config_str + (unsigned int)config_str_len) = '\\0';\n    // Read is verified above\n    // Set rom_size and r equal to support old flow and big-endian shuffle\n    rom_size = (unsigned int)config_str_len;\n    r = rom_size;\n  } else if (version == KERNEL_VERSION_ID) {\n    rom_size = (unsigned int)config_str_len;\n    r = kern->io.read(&kern->io, (dev_addr_t)OFFSET_CONFIGURATION_ROM,\n                      config_str, config_str_len);\n  } else {\n    kern->io.printf(\"  HAL Kern: Version ID incorrect\\n\");\n    return -1;\n  }\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \"Read %zu bytes from kernel auto discovery\", r);\n\n  return (r == rom_size) ? 0 : -1;\n}\n\nint acl_kernel_if_is_valid(acl_kernel_if *kern) {\n  acl_assert_locked_or_sig();\n  if (kern == NULL || !acl_bsp_io_is_valid(&kern->io))\n    return 0;\n  return 1;\n}\n\ntime_us acl_kernel_if_get_time_us(acl_kernel_if *kern) {\n  acl_assert_locked_or_sig();\n  // Cheaply divide by 1000 by actually dividing by 1024\n  // Could be Nios II's out there without the div enabled\n  return kern->io.get_time_ns() >> 10;\n}\n\n// **************************************************************************\n// *************************** Read/write Functions *************************\n// **************************************************************************\n\n// 32-bit read and write calls with error checking\n\n// returns 0 on success, -ve on error\nstatic int acl_kernel_if_read_32b(acl_kernel_if *kern, unsigned int addr,\n                                  unsigned int *val) {\n  size_t size;\n  size_t r;\n  acl_assert_locked_or_sig();\n\n  if (!acl_kernel_if_is_valid(kern)) {\n    kern->io.printf(\"HAL Kern Error: Invalid kernel handle used\");\n    return -1;\n  }\n\n  size = sizeof(unsigned int);\n  r = kern->io.read(&kern->io, (dev_addr_t)addr, (char *)val, (size_t)size);\n  if (r < size) {\n    kern->io.printf(\n        \"HAL Kern Error: Read failed from addr %x, read %zu expected %zu\\n\",\n        addr, r, size);\n    return -1;\n  }\n  return 0;\n}\n\n// 64-bit read and write calls with error checking\n\n// returns 0 on success, -ve on error\nstatic int acl_kernel_if_read_64b(acl_kernel_if *kern, unsigned int addr,\n                                  uint64_t *val) {\n  int size;\n  int r;\n  acl_assert_locked_or_sig();\n\n  if (!acl_kernel_if_is_valid(kern)) {\n    kern->io.printf(\"HAL Kern Error: Invalid kernel handle used\");\n    return -1;\n  }\n\n  size = sizeof(uint64_t);\n  r = (int)kern->io.read(&kern->io, (dev_addr_t)addr, (char *)val,\n                         (size_t)size);\n  if (r < size) {\n    kern->io.printf(\n        \"HAL Kern Error: Read failed from addr %x, read %d expected %d\\n\", addr,\n        r, size);\n    return -1;\n  }\n  return 0;\n}\n\n// returns 0 on success, -ve on error\nstatic int acl_kernel_rom_read_block(acl_kernel_if *kern, unsigned int addr,\n                                     char *config_rom, size_t size) {\n  size_t r;\n\n  if (!acl_kernel_if_is_valid(kern)) {\n    kern->io.printf(\"HAL Kern Error: Invalid kernel handle used\");\n    return -1;\n  }\n\n  r = kern->io.read(&kern->io, (dev_addr_t)addr, config_rom, (size_t)size);\n  if (r < size) {\n    kern->io.printf(\n        \"HAL Kern Error: Read failed from addr %x, read %zu expected %zu\\n\",\n        addr, r, size);\n    return -1;\n  }\n  return 0;\n}\n\n// returns 0 on success, -ve on error\nstatic int acl_kernel_if_write_32b(acl_kernel_if *kern, unsigned int addr,\n                                   unsigned int val) {\n  int size;\n  int r;\n  acl_assert_locked_or_sig();\n\n  if (!acl_kernel_if_is_valid(kern)) {\n    kern->io.printf(\"HAL Kern Error: Invalid kernel handle used\");\n    return -1;\n  }\n\n  size = sizeof(unsigned int);\n  r = (int)kern->io.write(&kern->io, (dev_addr_t)addr, (char *)&val,\n                          (size_t)size);\n  if (r < size) {\n    kern->io.printf(\"HAL Kern Error: Write failed to addr %x with value %x, \"\n                    \"wrote %d expected %d\\n\",\n                    addr, val, r, size);\n    return -1;\n  }\n  return 0;\n}\n\n// returns 0 on success, -ve on error\nstatic int acl_kernel_if_write_64b(acl_kernel_if *kern, unsigned int addr,\n                                   uint64_t val) {\n  int size;\n  int r;\n  acl_assert_locked();\n\n  if (!acl_kernel_if_is_valid(kern)) {\n    kern->io.printf(\"HAL Kern Error: Invalid kernel handle used\");\n    return -1;\n  }\n\n  size = sizeof(uint64_t);\n  r = (int)kern->io.write(&kern->io, (dev_addr_t)addr, (char *)&val,\n                          (size_t)size);\n  if (r < size) {\n    kern->io.printf(\n        \"HAL Kern Error: Write failed to addr %x with value %\" PRIx64 \", \"\n        \"wrote %d, expected %d\\n\",\n        addr, val, r, size);\n    return -1;\n  }\n  return 0;\n}\n\n// return 0 on success, -ve on error\nstatic int acl_kernel_if_write_block(acl_kernel_if *kern, unsigned int addr,\n                                     unsigned int *val, size_t size) {\n  size_t r;\n  unsigned int *temp_val = val;\n  size_t aligned_size = size;\n\n  if (!acl_kernel_if_is_valid(kern)) {\n    kern->io.printf(\"HAL Kern Error: Invalid kernel handle used\");\n    return -1;\n  }\n\n// In Windows S5 MMD, block writes to kernel interface must have write size\n// divisible by 4 bytes If not divisible by 4, subtract the modulos and add 4\n// bytes to the write size.\n#ifdef _WIN32\n  if (size % 4) {\n    aligned_size = size - (size % 4) + 4;\n    temp_val = (unsigned int *)acl_malloc(aligned_size + 1);\n    memcpy(temp_val, val, size);\n  }\n#endif\n\n  r = kern->io.write(&kern->io, (dev_addr_t)addr, (char *)temp_val,\n                     (size_t)aligned_size);\n\n#ifdef _WIN32\n  if (size % 4) {\n    acl_free(temp_val);\n  }\n#endif\n\n  if (r < aligned_size) {\n    kern->io.printf(\"HAL Kern Error: Write failed to addr %x with value %x, \"\n                    \"wrote %zu expected %zu\\n\",\n                    addr, *val, r, aligned_size);\n    return -1;\n  }\n  return 0;\n}\n\n// The kernel cra is segmented (or windowed) using a Qsys address expander.\n// We must first write the upper bits of the address we want to access to the\n// extender, then perform our read/write operation using the offset within\n// that segment as the address.\n//\n// Sets the segment window bits and returns the offset needed within\nstatic uintptr_t acl_kernel_cra_set_segment(acl_kernel_if *kern,\n                                            unsigned int accel_id,\n                                            unsigned int addr) {\n  uintptr_t logical_addr =\n      kern->accel_csr[accel_id].address + addr - OFFSET_KERNEL_CRA;\n  uintptr_t segment = logical_addr & ((size_t)0 - (KERNEL_CRA_SEGMENT_SIZE));\n  uintptr_t segment_offset = logical_addr % KERNEL_CRA_SEGMENT_SIZE;\n  acl_assert_locked_or_sig();\n\n  // The kernel cra master is hardcoded to 30 addr bits, so we can use 32-bit\n  // interface here.\n  if (kern->cur_segment != segment) {\n    acl_kernel_if_write_32b(kern, OFFSET_KERNEL_CRA_SEGMENT,\n                            (unsigned int)segment);\n    kern->cur_segment = segment;\n  }\n\n  return segment_offset;\n}\n\n// Set CRA segment for ROM. No accel_id\nstatic uintptr_t acl_kernel_cra_set_segment_rom(acl_kernel_if *kern,\n                                                unsigned int addr) {\n  uintptr_t segment = addr & ((size_t)0 - (KERNEL_CRA_SEGMENT_SIZE));\n  uintptr_t segment_offset = addr % KERNEL_CRA_SEGMENT_SIZE;\n\n  if (kern->cur_segment != segment) {\n    acl_kernel_if_write_32b(kern, OFFSET_KERNEL_CRA_SEGMENT,\n                            (unsigned int)segment);\n    kern->cur_segment = segment;\n  }\n\n  return segment_offset;\n}\n\nstatic int acl_kernel_cra_read(acl_kernel_if *kern, unsigned int accel_id,\n                               unsigned int addr, unsigned int *val) {\n  assert(kern->cra_ring_root_exist);\n\n  // Need to block signals before acquiring mutex and unblock after releasing\n  acl_signal_blocker sig_blocker;\n  std::lock_guard<std::mutex> lock(kern->segment_mutex);\n\n  uintptr_t segment_offset = acl_kernel_cra_set_segment(kern, accel_id, addr);\n  acl_assert_locked_or_sig();\n  return acl_kernel_if_read_32b(\n      kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset, val);\n}\n\nint acl_kernel_cra_read_64b(acl_kernel_if *kern, unsigned int accel_id,\n                            unsigned int addr, uint64_t *val) {\n  assert(kern->cra_ring_root_exist);\n\n  // Need to block signals before acquiring mutex and unblock after releasing\n  acl_signal_blocker sig_blocker;\n  std::lock_guard<std::mutex> lock(kern->segment_mutex);\n\n  uintptr_t segment_offset = acl_kernel_cra_set_segment(kern, accel_id, addr);\n  acl_assert_locked_or_sig();\n  return acl_kernel_if_read_64b(\n      kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset, val);\n}\n\n// Read 32b from kernel ROM\nstatic int acl_kernel_rom_cra_read_32b(acl_kernel_if *kern, unsigned int addr,\n                                       unsigned int *val) {\n  uintptr_t segment_offset = acl_kernel_cra_set_segment_rom(kern, addr);\n  return acl_kernel_if_read_32b(\n      kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset, val);\n}\n\n// Read size number of bytes from ROM\nstatic int acl_kernel_rom_cra_read_block(acl_kernel_if *kern, unsigned int addr,\n                                         char *config_rom, size_t size) {\n  uintptr_t segment_offset = acl_kernel_cra_set_segment_rom(kern, addr);\n  uintptr_t segment = addr & ((size_t)0 - (KERNEL_CRA_SEGMENT_SIZE));\n  uintptr_t segment_end =\n      (addr + size) & ((size_t)0 - (KERNEL_CRA_SEGMENT_SIZE));\n\n  unsigned int step = 0;\n  unsigned int transfer = 0;\n  int r = 0;\n  if (segment != segment_end) {\n    while (step < size) {\n      transfer = ((unsigned int)size > (step + KERNEL_CRA_SEGMENT_SIZE))\n                     ? KERNEL_CRA_SEGMENT_SIZE\n                     : (unsigned int)size - step;\n\n      r = acl_kernel_rom_read_block(\n          kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset,\n          config_rom, transfer);\n      config_rom += transfer;\n      step += transfer;\n      segment_offset = acl_kernel_cra_set_segment_rom(kern, addr + step);\n      if (r < 0)\n        return r;\n    }\n  } else {\n    r = acl_kernel_rom_read_block(\n        kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset,\n        config_rom, size);\n  }\n\n  return r;\n}\n\nstatic int acl_kernel_cra_write(acl_kernel_if *kern, unsigned int accel_id,\n                                unsigned int addr, unsigned int val) {\n  assert(kern->cra_ring_root_exist);\n\n  // Need to block signals before acquiring mutex and unblock after releasing\n  acl_signal_blocker sig_blocker;\n  std::lock_guard<std::mutex> lock(kern->segment_mutex);\n\n  uintptr_t segment_offset = acl_kernel_cra_set_segment(kern, accel_id, addr);\n  acl_assert_locked_or_sig();\n  return acl_kernel_if_write_32b(\n      kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset, val);\n}\n\nstatic int acl_kernel_cra_write_64b(acl_kernel_if *kern, unsigned int accel_id,\n                                    unsigned int addr, uint64_t val) {\n  assert(kern->cra_ring_root_exist);\n\n  // Need to block signals before acquiring mutex and unblock after releasing\n  acl_signal_blocker sig_blocker;\n  std::lock_guard<std::mutex> lock(kern->segment_mutex);\n\n  uintptr_t segment_offset = acl_kernel_cra_set_segment(kern, accel_id, addr);\n  acl_assert_locked();\n  return acl_kernel_if_write_64b(\n      kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset, val);\n}\n\nstatic int acl_kernel_cra_write_block(acl_kernel_if *kern,\n                                      unsigned int accel_id, unsigned int addr,\n                                      unsigned int *val, size_t size) {\n  assert(kern->cra_ring_root_exist);\n\n  // Need to block signals before acquiring mutex and unblock after releasing\n  acl_signal_blocker sig_blocker;\n  std::lock_guard<std::mutex> lock(kern->segment_mutex);\n\n  uintptr_t segment_offset = acl_kernel_cra_set_segment(kern, accel_id, addr);\n  uintptr_t logical_addr =\n      kern->accel_csr[accel_id].address + addr - OFFSET_KERNEL_CRA;\n  uintptr_t segment = logical_addr & ((size_t)0 - (KERNEL_CRA_SEGMENT_SIZE));\n\n  uintptr_t logical_addr_end =\n      kern->accel_csr[accel_id].address + addr + size - OFFSET_KERNEL_CRA;\n  uintptr_t segment_end =\n      logical_addr_end & ((size_t)0 - (KERNEL_CRA_SEGMENT_SIZE));\n\n  unsigned int step = 0;\n  if (segment != segment_end) {\n    ACL_KERNEL_IF_DEBUG_MSG_VERBOSE(\n        kern, 2, \":: Segment change during block write detected.\\n\");\n    while (step < size) {\n      segment = (logical_addr + step) & ((size_t)0 - (KERNEL_CRA_SEGMENT_SIZE));\n      if (kern->cur_segment != segment) {\n        acl_kernel_if_write_block(\n            kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset, val,\n            step);\n        segment_offset =\n            acl_kernel_cra_set_segment(kern, accel_id, addr + step);\n        logical_addr =\n            kern->accel_csr[accel_id].address + addr + step - OFFSET_KERNEL_CRA;\n        val += step;\n        size -= step;\n        step = 0;\n      } else {\n        step += (unsigned)sizeof(int);\n      }\n    }\n  }\n\n  return acl_kernel_if_write_block(\n      kern, (unsigned)OFFSET_KERNEL_CRA + (unsigned)segment_offset, val, size);\n}\n\n// Private utility function to issue a command to the profile hardware\nstatic int acl_kernel_if_issue_profile_hw_command(acl_kernel_if *kern,\n                                                  cl_uint accel_id,\n                                                  unsigned bit_id,\n                                                  int set_bit) {\n  unsigned int profile_ctrl_val;\n  int status;\n  acl_assert_locked_or_sig();\n  assert(acl_kernel_if_is_valid(kern));\n  status = acl_kernel_cra_read(\n      kern, accel_id, KERNEL_OFFSET_CSR_PROFILE_CTRL + kern->cra_address_offset,\n      &profile_ctrl_val);\n  if (status)\n    return status;\n  ACL_KERNEL_IF_DEBUG_MSG(\n      kern, \"::   Issue profile HW command:: Accelerator %d old csr is %x.\\n\",\n      accel_id, profile_ctrl_val);\n\n  if (set_bit) {\n    ACL_KERNEL_SET_BIT(profile_ctrl_val, bit_id);\n  } else { // Clear bit\n    ACL_KERNEL_CLEAR_BIT(profile_ctrl_val, bit_id);\n  }\n  ACL_KERNEL_IF_DEBUG_MSG(\n      kern, \"::   Issue profile HW command:: Accelerator %d new csr is %x.\\n\",\n      accel_id, profile_ctrl_val);\n  status = acl_kernel_cra_write(\n      kern, accel_id, KERNEL_OFFSET_CSR_PROFILE_CTRL + kern->cra_address_offset,\n      profile_ctrl_val);\n  if (status)\n    return status;\n  return 0;\n}\n\n// Private utility function to issue a command to the profile hardware\n// See ip/src/common/lsu_top.v.tmpl, ip/src/common/hld_iord.sv.tmpl, &\n// ip/src/common/hld_iowr.sv.tmpl for what 0-3 represents\nint acl_kernel_if_set_profile_shared_control(acl_kernel_if *kern,\n                                             cl_uint accel_id) {\n  acl_assert_locked();\n  ACL_KERNEL_IF_DEBUG_MSG(\n      kern,\n      \":: Set the shared control for the profile counters:: Accelerator %d.\\n\",\n      accel_id);\n  int return_val = 0;\n  int set_to = get_env_profile_shared_counter_val();\n\n  // Valid control options are 0-3. If set to -1 shared counters are off.\n  // Anything else the control hasn't been pulled from the ENV variable\n  if (set_to >= 0 && set_to <= 3) {\n    if (set_to == 0) {\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT1, 0);\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT2, 0);\n    } else if (set_to == 1) {\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT1, 1);\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT2, 0);\n    } else if (set_to == 2) {\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT1, 0);\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT2, 1);\n    } else if (set_to == 3) {\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT1, 1);\n      return_val |= acl_kernel_if_issue_profile_hw_command(\n          kern, accel_id, KERNEL_CSR_PROFILE_SHARED_CONTROL_BIT2, 1);\n    } else {\n      ACL_KERNEL_IF_DEBUG_MSG(kern,\n                              \":: Setting shared control value - env variable \"\n                              \"was not in the range 0-3 %d.\\n\",\n                              accel_id);\n    }\n  } else if (set_to == -1) {\n    ACL_KERNEL_IF_DEBUG_MSG(kern,\n                            \":: Not setting shared control value - shared \"\n                            \"counters not enabled %d.\\n\",\n                            accel_id);\n  } else {\n    ACL_KERNEL_IF_DEBUG_MSG(\n        kern,\n        \":: Not setting shared control value - env variable was not set %d.\\n\",\n        accel_id);\n  }\n  return return_val;\n}\n\n// **************************************************************************\n// ************** Utility Functions that use Read/Write calls ***************\n// **************************************************************************\n\n// **************************************************************************\n// ***************************** Public Functions ***************************\n// **************************************************************************\n\n// This routine queries the PCIe bus for devices that match our ACL supported\n// board configurations and populates the acl_system_def and\n// ACL_PCIE_DEVICE_DESRIPTION structures in the ACL and HAL layers respectively.\n//\n// Returns 0 on success, -ve otherwise\nint acl_kernel_if_init(acl_kernel_if *kern, acl_bsp_io bsp_io,\n                       acl_system_def_t *sysdef, bool sim_mmd_dispatch) {\n  char description_size_msb[KERNEL_ROM_SIZE_BYTES_READ + 1];\n  char description_size_lsb[KERNEL_ROM_SIZE_BYTES_READ + 1];\n  unsigned int size_location, version, size;\n  int result = 0;\n  acl_assert_locked();\n\n  assert(acl_bsp_io_is_valid(&bsp_io));\n  kern->io = bsp_io;\n\n  kern->num_accel = 0;\n  kern->cur_segment = 0xffffffff;\n\n  kern->autorun_profiling_kernel_id = -1;\n\n  if (check_version_id(kern) != 0) {\n    kern->io.printf(\"Hardware version ID differs from version expected by \"\n                    \"software.  Either:\\n\");\n    kern->io.printf(\"   a) Ensure your compiled design was generated by the \"\n                    \"same ACL build\\n\");\n    kern->io.printf(\"      currently in use, OR\\n\");\n    kern->io.printf(\n        \"   b) The host can not communicate with the compiled kernel.\\n\");\n    assert(0);\n  }\n\n  version = 0;\n  acl_kernel_if_read_32b(kern, OFFSET_KERNEL_VERSION_ID, &version);\n  ACL_KERNEL_IF_DEBUG_MSG_VERBOSE(kern, 2, \"::   Kernel version 0x%x\\n\",\n                                  version);\n  size_t config_str_len = 0;\n  if (version == KERNEL_ROM_VERSION_ID) {\n    acl_kernel_if_reset(kern);\n    size_location = OFFSET_KERNEL_ROM_LOCATION_MSB;\n    result = acl_kernel_rom_cra_read_32b(kern, size_location,\n                                         (unsigned int *)description_size_msb);\n    description_size_msb[KERNEL_ROM_SIZE_BYTES_READ] = '\\0';\n\n    if (result < 0 || ((unsigned int)*description_size_msb == 0xFFFF)) {\n      ACL_KERNEL_IF_DEBUG_MSG(\n          kern, \"MSB ROM size not found. No response from PCIe.\\n\");\n      return -1;\n    }\n\n    size = (unsigned int)strtol(description_size_msb, NULL, 16);\n    size = size << 16;\n\n    size_location = OFFSET_KERNEL_ROM_LOCATION_LSB;\n    result = acl_kernel_rom_cra_read_32b(kern, size_location,\n                                         (unsigned int *)description_size_lsb);\n    description_size_lsb[KERNEL_ROM_SIZE_BYTES_READ] = '\\0';\n\n    if (result < 0 || ((unsigned int)*description_size_lsb == 0xFFFF)) {\n      ACL_KERNEL_IF_DEBUG_MSG(\n          kern, \"LSB ROM size not found. No response from PCIe.\\n\");\n      return -1;\n    }\n\n    size += (unsigned int)strtol(description_size_lsb, NULL, 16);\n\n    if ((size == 0) || (size == 0xffffffff)) {\n      ACL_KERNEL_IF_DEBUG_MSG(kern,\n                              \"ROM size is invalid. ROM does not exist\\n\");\n      ACL_KERNEL_IF_DEBUG_MSG(kern, \"ROM size read was: %i\\n\", size);\n      return -1;\n    }\n\n    ACL_KERNEL_IF_DEBUG_MSG_VERBOSE(kern, 2, \"::   ROM Size is: 0x%s%s : %i\\n\",\n                                    description_size_msb, description_size_lsb,\n                                    size);\n\n    config_str_len = size;\n  } else if (version == KERNEL_VERSION_ID) {\n    config_str_len = CONFIGURATION_ROM_BYTES;\n  } else {\n    kern->io.printf(\"  HAL Kern: Version ID incorrect\\n\");\n    return -1;\n  }\n\n  std::vector<char> config_str(config_str_len + 1); // +1 for null byte padding\n  result |= get_auto_discovery_string(kern, config_str.data(), config_str_len);\n  std::string config_string{config_str.data()};\n\n  if (result != 0) {\n    ACL_KERNEL_IF_DEBUG_MSG(kern, \"Failed to read from kernel auto discovery\");\n    return -1;\n  }\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \"\\nPCIE Auto-Discovered Param String: %s\\n\",\n                          config_str.data());\n\n  // Returns 1 if success\n  std::string auto_config_err_str;\n  auto load_result = acl_load_device_def_from_str(\n      config_string, sysdef->device[kern->physical_device_id].autodiscovery_def,\n      auto_config_err_str);\n  result |= load_result ? 0 : -1;\n\n  if (result != 0) {\n    kern->io.printf(\"%s\\n\", auto_config_err_str.c_str());\n    ACL_KERNEL_IF_DEBUG_MSG(kern, \"First 16 values:\\n  \");\n    for (unsigned i = 0; i < 16; i++)\n      ACL_KERNEL_IF_DEBUG_MSG(kern, \"%02x \", config_str[i]);\n    ACL_KERNEL_IF_DEBUG_MSG(kern, \"\\n\");\n    return -1;\n  }\n\n  result = acl_kernel_if_update(\n      sysdef->device[kern->physical_device_id].autodiscovery_def, kern);\n\n  if (sim_mmd_dispatch) {\n    sysdef->device[kern->physical_device_id].autodiscovery_def.name =\n        ACL_MPSIM_DEVICE_NAME;\n  }\n\n  return result;\n}\n\n// Given a devdef, update the kernel's internal state\n// Returns 0 on success\nint acl_kernel_if_update(const acl_device_def_autodiscovery_t &devdef,\n                         acl_kernel_if *kern) {\n  acl_assert_locked();\n\n  // Setup the accelerators\n  if (kern->io.debug_verbosity > 0) {\n    for (const auto &acc : devdef.accel) {\n      ACL_KERNEL_IF_DEBUG_MSG(kern, \"Found Kernel {%s}\\n\",\n                              acc.iface.name.c_str());\n      // Show info about the arguments themselves\n      for (const auto &arg : acc.iface.args) {\n        // These are strictly for debug, but we need a way to keep this up to\n        // date\n        static const char *acl_addr_space_names[] = {\n            \"ACL_ARG_ADDR_NONE\", \"ACL_ARG_ADDR_LOCAL\", \"ACL_ARG_ADDR_GLOBAL\",\n            \"ACL_ARG_ADDR_CONSTANT\"};\n        static const char *acl_arg_categories[] = {\n            \"ACL_ARG_BY_VALUE\", \"ACL_ARG_MEM_OBJ\", \"ACL_ARG_SAMPLER\"};\n\n        ACL_KERNEL_IF_DEBUG_MSG(\n            kern, \" ... param %s (addr_space=%s,category=%s,size=%d)\\n\",\n            arg.name.c_str(), acl_addr_space_names[arg.addr_space],\n            acl_arg_categories[arg.category], arg.size);\n      }\n\n      // Show other info about kernel.\n      ACL_KERNEL_IF_DEBUG_MSG(kern,\n                              \" ... compile work-group size = (%d, %d, %d)\\n\",\n                              (int)acc.compile_work_group_size[0],\n                              (int)acc.compile_work_group_size[1],\n                              (int)acc.compile_work_group_size[2]);\n      ACL_KERNEL_IF_DEBUG_MSG(kern, \" ... is work-group invariant = %d\\n\",\n                              acc.is_workgroup_invariant);\n      ACL_KERNEL_IF_DEBUG_MSG(kern, \" ... num vectors lanes = %d\\n\",\n                              acc.num_vector_lanes);\n      ACL_KERNEL_IF_DEBUG_MSG(kern, \" ... max work-group size = %d\\n\",\n                              acc.max_work_group_size);\n    }\n  }\n\n  acl_kernel_if_close(kern);\n\n  // Initialize whether the cra_ring_root exist in the design\n  kern->cra_ring_root_exist = devdef.cra_ring_root_exist;\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \"Number of cra_ring_root : %d\\n\",\n                          kern->cra_ring_root_exist);\n\n  // Setup the PCIe HAL Structures\n  kern->num_accel = static_cast<unsigned>(devdef.accel.size());\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \"Number of Accelerators : %d\\n\",\n                          kern->num_accel);\n\n  if (kern->num_accel > 0) {\n    // Allocations for each kernel\n    kern->accel_csr.resize(kern->num_accel);\n    kern->accel_perf_mon.resize(kern->num_accel);\n    kern->accel_num_printfs.resize(kern->num_accel);\n\n    // The Kernel CSR registers\n    // The new and improved config ROM give us the address *offsets* from\n    // the first kernel CSR, and the range of each kernel CSR.\n    std::vector<uintptr_t> kernel_csr_address_map;\n    for (unsigned ii = 0; ii < devdef.accel.size(); ++ii) {\n      kern->accel_csr[ii].address =\n          OFFSET_KERNEL_CRA + devdef.hal_info[ii].csr.address;\n      kern->accel_csr[ii].bytes = devdef.hal_info[ii].csr.num_bytes;\n      kernel_csr_address_map.push_back(kern->accel_csr[ii].address);\n\n      ACL_KERNEL_IF_DEBUG_MSG(\n          kern, \"Kernel_%s CSR { 0x%08\" PRIxPTR \", 0x%08\" PRIxPTR \" }\\n\",\n          devdef.accel[ii].iface.name.c_str(), kern->accel_csr[ii].address,\n          kern->accel_csr[ii].bytes);\n    }\n    acl_get_hal()->simulation_set_kernel_cra_address_map(\n        kern->physical_device_id, kernel_csr_address_map);\n\n    // The Kernel performance monitor registers\n    for (unsigned ii = 0; ii < devdef.accel.size(); ++ii) {\n      kern->accel_perf_mon[ii].address =\n          OFFSET_KERNEL_CRA + devdef.hal_info[ii].perf_mon.address;\n      kern->accel_perf_mon[ii].bytes = devdef.hal_info[ii].perf_mon.num_bytes;\n\n      ACL_KERNEL_IF_DEBUG_MSG(\n          kern, \"Kernel_%s perf_mon { 0x%08\" PRIxPTR \", 0x%08\" PRIxPTR \" }\\n\",\n          devdef.accel[ii].iface.name.c_str(), kern->accel_perf_mon[ii].address,\n          kern->accel_perf_mon[ii].bytes);\n\n      // printf info\n      kern->accel_num_printfs[ii] =\n          static_cast<unsigned>(devdef.accel[ii].printf_format_info.size());\n\n      ACL_KERNEL_IF_DEBUG_MSG(kern, \"Kernel_%s(id=%d) num_printfs is %d\\n\",\n                              devdef.accel[ii].iface.name.c_str(), ii,\n                              kern->accel_num_printfs[ii]);\n    }\n\n    // Find which of the kernels (if any) is the autorun profiling kernel\n    for (unsigned ii = 0; ii < devdef.accel.size(); ++ii) {\n      if (devdef.accel[ii].iface.name == ACL_PROFILE_AUTORUN_KERNEL_NAME) {\n        kern->autorun_profiling_kernel_id = (int)ii;\n        break;\n      }\n    }\n\n    kern->streaming_control_signal_names.clear();\n    kern->streaming_control_signal_names.reserve(devdef.accel.size());\n    for (const auto &accel : devdef.accel) {\n      std::optional<acl_streaming_kernel_control_info> signal_names;\n      if (accel.streaming_control_info_available) {\n        signal_names = accel.streaming_control_info;\n      }\n      kern->streaming_control_signal_names.emplace_back(signal_names);\n    }\n  }\n\n  // Do reset\n  acl_kernel_if_reset(kern);\n\n  // Set interleaving mode based on autodiscovery\n  for (unsigned ii = 0; ii < devdef.num_global_mem_systems; ii++) {\n    if ((unsigned int)devdef.global_mem_defs[ii].config_addr > 0 &&\n        (unsigned int)devdef.global_mem_defs[ii].num_global_banks > 1) {\n      ACL_KERNEL_IF_DEBUG_MSG(kern,\n                              \"Configuring interleaving for memory %d (%s), \"\n                              \"burst_interleaved = %d\\n\",\n                              ii, devdef.global_mem_defs[ii].name.c_str(),\n                              devdef.global_mem_defs[ii].burst_interleaved);\n      acl_kernel_if_write_32b(\n          kern, (unsigned int)devdef.global_mem_defs[ii].config_addr,\n          devdef.global_mem_defs[ii].burst_interleaved ? 0x0 : 0x1u);\n    }\n  }\n\n  kern->last_kern_update = 0;\n\n  // Set up the structures to store state information about the device\n  if (kern->num_accel > 0) {\n    kern->accel_job_ids.resize(kern->num_accel);\n    kern->accel_invoc_queue_depth.resize(kern->num_accel);\n    kern->static_img_cache.resize(kern->num_accel);\n    kern->accel_arg_cache.resize(kern->num_accel);\n\n    // Kernel IRQ is a separate thread. Need to use circular buffer to make this\n    // multithread safe.\n    kern->accel_queue_front.resize(kern->num_accel);\n    kern->accel_queue_back.resize(kern->num_accel);\n\n    acl_dev_kernel_invocation_image_t default_invocation;\n    size_t image_size_static =\n        (size_t)((uintptr_t) & (default_invocation.arg_value) - (uintptr_t) &\n                 (default_invocation.work_dim));\n\n    for (unsigned a = 0; a < kern->num_accel; ++a) {\n      kern->static_img_cache[a] = std::make_unique<char[]>(image_size_static);\n      memcpy(kern->static_img_cache[a].get(),\n             (char *)(&(default_invocation.work_dim)), image_size_static);\n      unsigned int max_same_accel_launches =\n          devdef.accel[a].fast_launch_depth + 1;\n      // +1, because fast launch depth does not account for the running kernel\n      kern->accel_invoc_queue_depth[a] = max_same_accel_launches;\n      kern->accel_job_ids[a].resize(max_same_accel_launches);\n      kern->accel_queue_front[a] = -1;\n      kern->accel_queue_back[a] = -1;\n      for (unsigned b = 0; b < max_same_accel_launches; ++b) {\n        kern->accel_job_ids[a][b] = -1;\n      }\n    }\n  }\n\n  return 0;\n}\n\n// Post-PLL config init function - at this point it's safe to talk to\n// the kernel CSR registers.\n// Returns 0 on success\nint acl_kernel_if_post_pll_config_init(acl_kernel_if *kern) {\n  unsigned int csr, version;\n  unsigned k;\n  char *profile_start_cycle = getenv(\"ACL_PROFILE_START_CYCLE\");\n  char *profile_stop_cycle = getenv(\"ACL_PROFILE_STOP_CYCLE\");\n  acl_assert_locked();\n\n  // Readback kernel version from status register and store in kernel data\n  // structure.  We cache this value now so that don't do PCIe readback on\n  // every kernel launch (when we actually check the value).\n  // Safe to read only first kernel because all will have the same version\n  if (!acl_kernel_if_is_valid(kern)) {\n    kern->io.printf(\"HAL Kern Error: Post PLL init: Invalid kernel handle\");\n    assert(0 && \"Invalid kernel handle\");\n  }\n\n  if (kern->num_accel > 0 && kern->cra_ring_root_exist) {\n    acl_kernel_cra_read(kern, 0, KERNEL_OFFSET_CSR, &csr);\n    version = ACL_KERNEL_READ_BIT_RANGE(csr, KERNEL_CSR_LAST_VERSION_BIT,\n                                        KERNEL_CSR_FIRST_VERSION_BIT);\n    kern->csr_version = version;\n    ACL_KERNEL_IF_DEBUG_MSG(kern,\n                            \"Read CSR version from kernel 0: Version = %u\\n\",\n                            kern->csr_version.value());\n    if (kern->csr_version < 5) {\n      // Register addresses are pushed back since previous versions\n      // doesn't have the start register\n      kern->cra_address_offset = 0;\n    } else {\n      // In case an old CSR version is queried before we program our new aocx,\n      // make sure the offset is set correctly when we query the correct CSR\n      // aocx from the newly compile aocx. Old CSR versions may be queried since\n      // users may trigger context creation whenever they try to query device\n      // info, which would lead to reading the CSR before programming the actual\n      // aocx and it would read from the default aocx provided by the BSP.\n      kern->cra_address_offset = 8;\n    }\n  } else {\n    ACL_KERNEL_IF_DEBUG_MSG(kern,\n                            \"No accelerator found or CRA ring root doesn't \"\n                            \"exist, not setting kern->csr_version\\n\");\n  }\n\n  // If environment variables set, configure the profile hardware\n  // start/stop cycle registers for *every* kernel.  The runtime can then\n  // call into the HAL to override values for the kernels individually.\n  if (profile_start_cycle) {\n#ifdef _WIN32\n    uint64_t start_cycle =\n        (uint64_t)_strtoui64(profile_start_cycle, NULL, 10); // Base 10\n    kern->io.printf(\n        \"Setting profiler start cycle from environment variable: %I64u\\n\",\n        start_cycle);\n#else // Linux\n    uint64_t start_cycle =\n        (uint64_t)strtoull(profile_start_cycle, NULL, 10); // Base 10\n    kern->io.printf(\n        \"Setting profiler start cycle from environment variable: %llu\\n\",\n        start_cycle);\n#endif\n    for (k = 0; k < kern->num_accel; ++k) {\n      if (acl_kernel_if_set_profile_start_cycle(kern, k, start_cycle)) {\n        kern->io.printf(\"Failed setting profiler start cycle for kernel %u\\n\",\n                        k);\n        return 1;\n      }\n    }\n  }\n  if (profile_stop_cycle) {\n#ifdef _WIN32\n    uint64_t stop_cycle =\n        (uint64_t)_strtoui64(profile_stop_cycle, NULL, 10); // Base 10\n    kern->io.printf(\n        \"Setting profiler stop cycle from environment variable: %I64u\\n\",\n        stop_cycle);\n#else // Linux\n    uint64_t stop_cycle =\n        (uint64_t)strtoull(profile_stop_cycle, NULL, 10); // Base 10\n    kern->io.printf(\n        \"Setting profiler stop cycle from environment variable: %llu\\n\",\n        stop_cycle);\n#endif\n    for (k = 0; k < kern->num_accel; ++k) {\n      if (acl_kernel_if_set_profile_stop_cycle(kern, k, stop_cycle)) {\n        kern->io.printf(\"Failed setting profiler stop cycle for kernel %u\\n\",\n                        k);\n        return 1;\n      }\n    }\n  }\n\n  // Set the profiler shared control and mark the start time if kern has autorun\n  // profiler kernel\n  if (kern->autorun_profiling_kernel_id != -1) {\n    kern->io.printf(\n        \"kernel space has profiler kernel, setting shared control\\n\");\n    // get the variable from ENV and set the profiler global var\n    set_env_shared_counter_val();\n    // set the profiler kernel's CRA shared control\n    acl_kernel_if_set_profile_shared_control(\n        kern, cl_uint(kern->autorun_profiling_kernel_id));\n    // Reset the counters now that the shared control is set\n    acl_kernel_if_reset_profile_counters(\n        kern, cl_uint(kern->autorun_profiling_kernel_id));\n    // Record the start time for these autorun kernels\n    acl_set_autorun_start_time();\n  }\n  return 0;\n}\n\nvoid acl_kernel_if_reset(acl_kernel_if *kern) {\n  unsigned int sw_resetn = 0; // Any write will cause reset - data is don't care\n  time_ns reset_time;\n  acl_assert_locked();\n\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \" KERNEL: Issuing kernel reset\\n\");\n\n  acl_kernel_if_write_32b(kern, OFFSET_SW_RESET, sw_resetn);\n\n  // This will stall while circuit is being reset.  Executing this read\n  // therefore ensures this circuit has come out of reset before proceeding.\n  acl_kernel_if_read_32b(kern, OFFSET_SW_RESET, &sw_resetn);\n\n  // Just in case, verify that value readback shows reset deasserted\n  reset_time = kern->io.get_time_ns();\n  while (sw_resetn == 0) {\n    if (kern->io.get_time_ns() - reset_time > (time_ns)(RESET_TIMEOUT)) {\n      kern->io.printf(\"Kernel failed to come out of reset. Read 0x%x\\n\",\n                      sw_resetn);\n      assert(sw_resetn != 0);\n    }\n\n    acl_kernel_if_read_32b(kern, OFFSET_SW_RESET, &sw_resetn);\n  }\n}\n\n// Launch a task on the specified kernel\n// It is assumed that the kernel is idle and ready to accept new tasks.\nvoid acl_kernel_if_launch_kernel_on_custom_sof(\n    acl_kernel_if *kern, cl_uint accel_id,\n    acl_dev_kernel_invocation_image_t *image, cl_int activation_id) {\n  acl_assert_locked();\n\n  // Enforce CSR register map version number between the HAL and hardware.\n  // Version is defined in upper 16-bits of the status register and cached\n  // in the kern->csr_version field.  The value is cached after PLL init to\n  // avoid reading back on every kernel launch, which would add overhead.\n  if (kern->csr_version.has_value() &&\n      !(kern->csr_version >= CSR_VERSION_ID_18_1 &&\n        kern->csr_version <= CSR_VERSION_ID)) {\n    kern->io.printf(\"Hardware CSR version ID differs from version expected by \"\n                    \"software.  Either:\\n\");\n    kern->io.printf(\"   a) Ensure your compiled design was generated by the \"\n                    \"same ACL build\\n\");\n    kern->io.printf(\"      currently in use, OR\\n\");\n    kern->io.printf(\"   b) The host can not communicate properly with the \"\n                    \"compiled kernel.\\n\");\n    kern->io.printf(\"Saw version=%u, expected=%u.\\n\", kern->csr_version.value(),\n                    CSR_VERSION_ID);\n    assert(0); // Assert here because no way to pass an error up to the user.\n               // clEnqueue has already returned.\n  }\n\n  ACL_KERNEL_IF_DEBUG_MSG(\n      kern, \":: Launching kernel %d on accelerator %d, device %d.\\n\",\n      activation_id, accel_id, kern->physical_device_id);\n\n  // Reset our expectations for timeout\n  kern->last_kern_update = acl_kernel_if_get_time_us(kern);\n\n  // Assert that the requested accellerator is currently empty\n  int next_launch_index = 0;\n  if (kern->accel_queue_front[accel_id] ==\n      (int)kern->accel_invoc_queue_depth[accel_id] - 1) {\n    next_launch_index = 0;\n  } else {\n    next_launch_index = kern->accel_queue_front[accel_id] + 1;\n  }\n  if (kern->accel_job_ids[accel_id][next_launch_index] >= 0) {\n    kern->io.printf(\n        \"Kernel launch requested when kernel not idle on accelerator %d\\n\",\n        accel_id);\n    kern->io.printf(\"   kernel physical id = %d\\n\", kern->physical_device_id);\n    assert(0);\n  }\n\n  // Option 3 CSRs start with the work_dim data member,\n  // and continue with the rest of the invocation image.\n  uintptr_t image_p;\n\n  // Set the shared control for the profiler\n  acl_kernel_if_set_profile_shared_control(kern, accel_id);\n\n  size_t image_size_static;\n  unsigned int offset;\n  acl_dev_kernel_invocation_image_181_t image_181;\n  if (kern->csr_version == CSR_VERSION_ID_18_1) {\n    offset = (unsigned int)KERNEL_OFFSET_INVOCATION_IMAGE_181;\n    image_p = (uintptr_t)&image_181;\n    image_size_static =\n        (size_t)((uintptr_t) & (image_181.arg_value) - (uintptr_t)&image_181) +\n        image->arg_value_size;\n\n    // Copy all relevant attributes from the default invocation image to\n    // the 18.1 compatible invocation image.\n    image_181.work_dim = image->work_dim;\n    image_181.work_group_size = image->work_group_size;\n    for (unsigned i = 0; i < 3; ++i) {\n      image_181.global_work_size[i] = image->global_work_size[i];\n      image_181.num_groups[i] = image->num_groups[i];\n      image_181.local_work_size[i] = image->local_work_size[i];\n      // Always 0 for 18.1 CRAs since global_work_offset was not supported in\n      // that version.\n      image_181.global_work_offset[i] = 0;\n    }\n    for (cl_uint i = 0; i < image->arg_value_size; ++i) {\n      image_181.arg_value[i] = image->arg_value[i];\n    }\n\n  } else {\n    offset = (unsigned int)(KERNEL_OFFSET_INVOCATION_IMAGE +\n                            kern->cra_address_offset);\n    image_p = (uintptr_t) & (image->work_dim);\n    image_size_static =\n        (size_t)((uintptr_t) & (image->arg_value) - (uintptr_t) &\n                 (image->work_dim));\n  }\n\n  // When csr version is 18.1, the kernel args is part of the image. otherwise,\n  // it is in dynamic memory.  Only write the static part of the invocation\n  // image if this kernel uses CRA control.\n  if (!kern->streaming_control_signal_names[accel_id]) {\n    if (kern->csr_version == CSR_VERSION_ID_18_1) {\n      // Just write everything for older CSR version\n      if ((kern->io.debug_verbosity) >= 2) {\n        print_invocation_image(kern, (char *)image_p, image_size_static,\n                               image_size_static, offset, true);\n      }\n      acl_kernel_cra_write_block(kern, accel_id, offset,\n                                 (unsigned int *)image_p, image_size_static);\n    } else {\n      char *img_cache_ptr = kern->static_img_cache[accel_id].get();\n      assert(img_cache_ptr && \"kernel image cache not initialized!\");\n      if (memcmp(img_cache_ptr, (char *)image_p, image_size_static) != 0) {\n        // Something changed in static part of the invocation image,\n        // write everything to csr\n        if ((kern->io.debug_verbosity) >= 2) {\n          print_invocation_image(kern, (char *)image_p, image_size_static,\n                                 image_size_static, offset, true);\n        }\n        acl_kernel_cra_write_block(kern, accel_id, offset,\n                                   (unsigned int *)image_p, image_size_static);\n        memcpy(img_cache_ptr, (char *)image_p, image_size_static);\n      } else if ((kern->io.debug_verbosity) >= 2) {\n        // Nothing's changed, just print the static part of the invocation image\n        print_invocation_image(kern, (char *)image_p, image_size_static,\n                               image_size_static, offset, true, false);\n      }\n    }\n  }\n\n  bool accel_has_agent_args = false;\n  if (kern->csr_version.has_value() &&\n      (kern->csr_version != CSR_VERSION_ID_18_1 && image->arg_value_size > 0)) {\n    accel_has_agent_args = true;\n    if (!kern->accel_arg_cache[accel_id]) {\n      // The first time invoking the kernel, just write all the arguments\n      if ((kern->io.debug_verbosity) >= 2) {\n        print_invocation_image(kern, image->arg_value, image->arg_value_size,\n                               image->arg_value_size,\n                               (unsigned int)(offset + image_size_static),\n                               false);\n      }\n      acl_kernel_cra_write_block(\n          kern, accel_id, offset + (unsigned int)image_size_static,\n          (unsigned int *)image->arg_value, image->arg_value_size);\n      // Initialize kernel argument cache and cache the values\n      kern->accel_arg_cache[accel_id] =\n          std::make_unique<char[]>(image->arg_value_size);\n      memcpy(kern->accel_arg_cache[accel_id].get(), (char *)image->arg_value,\n             image->arg_value_size);\n    } else {\n      char *arg_cache_ptr = kern->accel_arg_cache[accel_id].get();\n      assert(arg_cache_ptr && \"kernel argument cache not initialized!\");\n      for (size_t step = 0; step < image->arg_value_size;) {\n        size_t size_to_write = 0;\n        size_t cmp_size = (image->arg_value_size - step) > sizeof(int)\n                              ? sizeof(int)\n                              : (image->arg_value_size - step);\n        // Find range of changed arguments and record size of that block\n        while (cmp_size > 0 &&\n               memcmp(arg_cache_ptr + step + size_to_write,\n                      image->arg_value + step + size_to_write, cmp_size) != 0) {\n          size_to_write += cmp_size;\n          cmp_size =\n              (image->arg_value_size - step - size_to_write) > sizeof(int)\n                  ? sizeof(int)\n                  : (image->arg_value_size - step - size_to_write);\n        }\n        if (size_to_write == 0) {\n          // Current compared block is the same as before, skipping write\n          size_t size_to_skip = (image->arg_value_size - step > sizeof(int))\n                                    ? sizeof(int)\n                                    : (image->arg_value_size - step);\n          if ((kern->io.debug_verbosity) >= 2) {\n            print_invocation_image(\n                kern, image->arg_value, image->arg_value_size, size_to_skip,\n                (unsigned int)(offset + image_size_static), false, false, step);\n          }\n          step += size_to_skip;\n        } else {\n          // Write the changed argument block to csr\n          if ((kern->io.debug_verbosity) >= 2) {\n            print_invocation_image(\n                kern, image->arg_value, image->arg_value_size, size_to_write,\n                (unsigned int)(offset + image_size_static), false, true, step);\n          }\n          acl_kernel_cra_write_block(\n              kern, accel_id, offset + (unsigned int)(image_size_static + step),\n              (unsigned int *)(image->arg_value + step), size_to_write);\n          step += size_to_write;\n        }\n      }\n      // image->arg_value_size should not change\n      memcpy(arg_cache_ptr, (char *)image->arg_value, image->arg_value_size);\n    }\n  }\n\n  kern->accel_job_ids[accel_id][next_launch_index] = (int)activation_id;\n\n  // If kernel IRQ comes at this point for this kernel and this kernel was\n  // next to launch, its status will be set to CL_RUNNING and below call\n  // to update status will do nothing\n  if (kern->accel_queue_front[accel_id] == kern->accel_queue_back[accel_id]) {\n    acl_kernel_if_update_fn((int)(activation_id), CL_RUNNING);\n  }\n  kern->accel_queue_front[accel_id] = next_launch_index;\n\n  if (kern->streaming_control_signal_names[accel_id]) {\n    acl_get_hal()->simulation_streaming_kernel_start(\n        kern->physical_device_id,\n        kern->streaming_control_signal_names[accel_id]->start, accel_id,\n        accel_has_agent_args);\n    return;\n  }\n\n  // backwards compatibility for version prior to 2023.1\n  if (kern->csr_version < CSR_VERSION_ID) {\n    unsigned int new_csr = 0;\n    acl_kernel_cra_read(kern, accel_id, KERNEL_OFFSET_CSR, &new_csr);\n    ACL_KERNEL_SET_BIT(new_csr, KERNEL_CSR_START);\n    acl_kernel_cra_write(kern, accel_id, KERNEL_OFFSET_CSR, new_csr);\n  } else {\n    acl_kernel_cra_write(kern, accel_id, KERNEL_OFFSET_START_REG, 1);\n  }\n  // IRQ handler takes care of the completion event through\n  // acl_kernel_if_update_status()\n}\n\nvoid acl_kernel_if_launch_kernel(acl_kernel_if *kern,\n                                 acl_kernel_invocation_wrapper_t *wrapper) {\n  cl_int activation_id;\n  cl_uint accel_id;\n  acl_dev_kernel_invocation_image_t *image;\n  acl_assert_locked();\n\n  // Verify the callbacks are valid\n  assert(acl_kernel_if_update_fn != NULL);\n  assert(acl_process_printf_buffer_fn != NULL);\n\n  // Grab the parameters we are interested in from host memory\n  image = wrapper->image;\n  activation_id = image->activation_id;\n  accel_id = image->accel_id;\n\n  acl_kernel_if_launch_kernel_on_custom_sof(kern, accel_id, image,\n                                            activation_id);\n}\n\n// Queries status of pending kernel invocations. Returns the number of finished\n// kernel invocations in `finish_counter`, or 0 if no invocations have finished.\nstatic void acl_kernel_if_update_status_query(acl_kernel_if *kern,\n                                              const unsigned int accel_id,\n                                              const int activation_id,\n                                              unsigned int &finish_counter,\n                                              unsigned int &printf_size) {\n  // Default return value.\n  finish_counter = 0;\n\n  // Read the accelerator's status register\n  unsigned int csr = 0;\n  acl_kernel_cra_read(kern, accel_id, KERNEL_OFFSET_CSR, &csr);\n\n  // Ignore non-status bits.\n  // Required by Option 3 wrappers which now have a version info in\n  // top 16 bits.\n  csr = ACL_KERNEL_READ_BIT_RANGE(csr, KERNEL_CSR_LAST_STATUS_BIT, 0);\n\n  // Check for updated status bits\n  if (0 == (csr & KERNEL_CSR_STATUS_BITS_MASK)) {\n    return;\n  }\n\n  // Clear the status bits that we read\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \":: Accelerator %d reporting status %x.\\n\",\n                          accel_id, csr);\n\n  if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_DONE) == 1) {\n    ACL_KERNEL_IF_DEBUG_MSG(kern, \":: Accelerator %d is done.\\n\", accel_id);\n  }\n  if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_STALLED) == 1) {\n    ACL_KERNEL_IF_DEBUG_MSG(kern, \":: Accelerator %d is stalled.\\n\", accel_id);\n  }\n  if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_UNSTALL) == 1) {\n    ACL_KERNEL_IF_DEBUG_MSG(kern, \":: Accelerator %d is unstalled.\\n\",\n                            accel_id);\n  }\n  if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_PROFILE_TEMPORAL_STATUS) == 1) {\n    ACL_KERNEL_IF_DEBUG_MSG(\n        kern, \":: Accelerator %d ready for temporal profile readback.\\n\",\n        accel_id);\n  }\n\n  if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_DONE) == 0 &&\n      ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_STALLED) == 0 &&\n      ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_PROFILE_TEMPORAL_STATUS) == 0) {\n    return;\n  }\n\n  // read the printf buffer size from the kernel cra, just after the\n  // kernel arguments\n  printf_size = 0;\n  if (kern->accel_num_printfs[accel_id] > 0) {\n    acl_kernel_cra_read(kern, accel_id,\n                        KERNEL_OFFSET_PRINTF_BUFFER_SIZE +\n                            kern->cra_address_offset,\n                        &printf_size);\n    assert(printf_size <= ACL_PRINTF_BUFFER_TOTAL_SIZE);\n    ACL_KERNEL_IF_DEBUG_MSG(kern,\n                            \":: Accelerator %d printf buffer size is %d.\\n\",\n                            accel_id, printf_size);\n\n    // kernel is stalled because the printf buffer is full\n    if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_STALLED) == 1) {\n      // clear interrupt\n      unsigned int new_csr = 0;\n      acl_kernel_cra_read(kern, accel_id, KERNEL_OFFSET_CSR, &new_csr);\n      ACL_KERNEL_CLEAR_BIT(new_csr, KERNEL_CSR_STALLED);\n\n      ACL_KERNEL_IF_DEBUG_MSG(kern,\n                              \":: Calling acl_process_printf_buffer_fn with \"\n                              \"activation_id=%d and printf_size=%u.\\n\",\n                              activation_id, printf_size);\n      // update status, which will dump the printf buffer, set\n      // debug_dump_printf = 0\n      acl_process_printf_buffer_fn(activation_id, (int)printf_size, 0);\n\n      ACL_KERNEL_IF_DEBUG_MSG(\n          kern, \":: Accelerator %d new csr is %x.\\n\", accel_id,\n          ACL_KERNEL_READ_BIT_RANGE(new_csr, KERNEL_CSR_LAST_STATUS_BIT, 0));\n\n      acl_kernel_cra_write(kern, accel_id, KERNEL_OFFSET_CSR, new_csr);\n      return;\n    }\n  }\n\n  // Start profile counter readback if profile interrupt and not done\n  if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_PROFILE_TEMPORAL_STATUS) != 0 &&\n      ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_DONE) == 0) {\n    ACL_KERNEL_IF_DEBUG_MSG(\n        kern, \":: Issuing profile reset command:: Accelerator %d.\\n\", accel_id);\n\n    // Reset temporal profiling counter\n    unsigned int ctrl_val;\n    if (acl_kernel_cra_read(kern, accel_id, KERNEL_OFFSET_CSR, &ctrl_val)) {\n      ACL_KERNEL_IF_DEBUG_MSG(\n          kern, \":: Got bad status reading CSR ctrl reg:: Accelerator %d.\\n\",\n          accel_id);\n    }\n    ACL_KERNEL_SET_BIT(ctrl_val, KERNEL_CSR_PROFILE_TEMPORAL_RESET);\n    if (acl_kernel_cra_write(kern, accel_id, KERNEL_OFFSET_CSR, ctrl_val)) {\n      ACL_KERNEL_IF_DEBUG_MSG(\n          kern, \":: Got bad status writing CSR ctrl reg:: Accelerator %d.\\n\",\n          accel_id);\n    }\n\n    if (activation_id < 0) {\n      // This is an autorun kernel\n      acl_process_autorun_profiler_scan_chain(kern->physical_device_id,\n                                              accel_id);\n    } else {\n      acl_kernel_profile_fn(activation_id);\n    }\n    return;\n  }\n\n  if (kern->csr_version == CSR_VERSION_ID_18_1) {\n    // Only expect single completion for older csr version\n    finish_counter = 1;\n  } else {\n    acl_kernel_cra_read(kern, accel_id,\n                        KERNEL_OFFSET_FINISH_COUNTER + kern->cra_address_offset,\n                        &finish_counter);\n    ACL_KERNEL_IF_DEBUG_MSG(kern, \":: Accelerator %d has %d finishes.\\n\",\n                            accel_id, finish_counter);\n  }\n}\n\n// Processes finished kernel invocation.\nstatic void acl_kernel_if_update_status_finish(acl_kernel_if *kern,\n                                               const unsigned int accel_id,\n                                               const int activation_id,\n                                               const unsigned int printf_size) {\n#ifdef TEST_PROFILING_HARDWARE\n  // Test readback of fake profile data using the acl_hal_mmd function that\n  // would be called from the acl runtime.\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \":: testing profile hardware on accel_id=%u.\\n\",\n                          accel_id);\n\n  uint64_t data[10];\n  acl_hal_mmd_get_profile_data(kern->physical_device_id, accel_id, data, 6);\n  acl_hal_mmd_reset_profile_counters(kern->physical_device_id, accel_id);\n  acl_hal_mmd_get_profile_data(kern->physical_device_id, accel_id, data, 6);\n#endif\n\n  // Just clear the \"done\" bit.  The \"go\" bit should already have been\n  // cleared, but this is harmless anyway.\n  // Since csr version 4, done bit is cleared when finish counter is read.\n  if (kern->csr_version == CSR_VERSION_ID_18_1) {\n    unsigned int dum;\n    acl_kernel_cra_write(kern, accel_id, KERNEL_OFFSET_CSR, 0);\n    acl_kernel_cra_read(kern, accel_id, KERNEL_OFFSET_CSR, &dum);\n  }\n\n  if (kern->accel_num_printfs[accel_id] > 0) {\n    ACL_KERNEL_IF_DEBUG_MSG(kern,\n                            \":: Calling acl_process_printf_buffer_fn with \"\n                            \"activation_id=%d and printf_size=%u.\\n\",\n                            activation_id, printf_size);\n    acl_process_printf_buffer_fn(activation_id, (int)printf_size, 0);\n  }\n}\n\n// Called when we receive a kernel status interrupt.  Cycle through all of\n// the running accelerators and check for updated status.\nvoid acl_kernel_if_update_status(acl_kernel_if *kern) {\n  acl_assert_locked_or_sig();\n\n  ACL_KERNEL_IF_DEBUG_MSG_VERBOSE(kern, 5, \":: Updating kernel status.\\n\");\n\n  // Check which accelerators are done and update their status appropriately\n  for (unsigned int accel_id = 0; accel_id < kern->num_accel; ++accel_id) {\n    int next_queue_back;\n    if (kern->accel_queue_back[accel_id] ==\n        (int)kern->accel_invoc_queue_depth[accel_id] - 1) {\n      next_queue_back = 0;\n    } else {\n      next_queue_back = kern->accel_queue_back[accel_id] + 1;\n    }\n\n    // Skip idle kernel\n    if (kern->accel_job_ids[accel_id][next_queue_back] < 0) {\n      // If this is the autorun profiling kernel, we want to read back profiling\n      // data from it, so don't 'continue' (this kernel is always 'idle').\n      if (accel_id != (unsigned)kern->autorun_profiling_kernel_id) {\n        continue;\n      }\n    }\n\n    const int activation_id = kern->accel_job_ids[accel_id][next_queue_back];\n\n    unsigned int finish_counter = 0;\n    unsigned int printf_size = 0;\n\n    if (kern->streaming_control_signal_names[accel_id]) {\n      acl_get_hal()->simulation_streaming_kernel_done(\n          kern->physical_device_id,\n          kern->streaming_control_signal_names[accel_id]->done, finish_counter);\n    } else {\n      acl_kernel_if_update_status_query(kern, accel_id, activation_id,\n                                        finish_counter, printf_size);\n    }\n\n    if (!(finish_counter > 0)) {\n      continue;\n    }\n\n    kern->last_kern_update = acl_kernel_if_get_time_us(kern);\n\n    for (unsigned int i = 0; i < finish_counter; i++) {\n      const int activation_id = kern->accel_job_ids[accel_id][next_queue_back];\n\n      // Tell the host library this job is done\n      kern->accel_job_ids[accel_id][next_queue_back] = -1;\n\n      if (!kern->streaming_control_signal_names[accel_id]) {\n        acl_kernel_if_update_status_finish(kern, accel_id, activation_id,\n                                           printf_size);\n      }\n\n      // Executing the following update after reading from performance\n      // and efficiency monitors will clobber the throughput reported by\n      // the host.  This is because setting CL_COMPLETE triggers the\n      // completion timestamp - reading performance results through slave\n      // ports before setting CL_COMPLETE adds to the apparent kernel time.\n      //\n      acl_kernel_if_update_fn(activation_id, CL_COMPLETE);\n      kern->accel_queue_back[accel_id] = next_queue_back;\n\n      if (kern->accel_queue_back[accel_id] ==\n          (int)kern->accel_invoc_queue_depth[accel_id] - 1) {\n        next_queue_back = 0;\n      } else {\n        next_queue_back = kern->accel_queue_back[accel_id] + 1;\n      }\n\n      if (kern->accel_job_ids[accel_id][next_queue_back] > -1) {\n        acl_kernel_if_update_fn(kern->accel_job_ids[accel_id][next_queue_back],\n                                CL_RUNNING);\n      }\n    }\n  }\n}\n\nvoid acl_kernel_if_debug_dump_printf(acl_kernel_if *kern, unsigned k) {\n  acl_assert_locked();\n  unsigned int printf_size = 0;\n  int activation_id;\n  unsigned int next_queue_back;\n\n  if (kern->accel_queue_back[k] == (int)kern->accel_invoc_queue_depth[k] - 1)\n    next_queue_back = 0;\n  else\n    next_queue_back = kern->accel_queue_back[k] + 1;\n\n  if (kern->accel_num_printfs[k] > 0) {\n    acl_kernel_cra_read(\n        kern, k, KERNEL_OFFSET_PRINTF_BUFFER_SIZE + kern->cra_address_offset,\n        &printf_size);\n    assert(printf_size <= ACL_PRINTF_BUFFER_TOTAL_SIZE);\n    ACL_KERNEL_IF_DEBUG_MSG(\n        kern, \":: Accelerator %d printf buffer size is %d.\\n\", k, printf_size);\n    activation_id = kern->accel_job_ids[k][next_queue_back];\n    ACL_KERNEL_IF_DEBUG_MSG(kern,\n                            \":: Calling acl_process_printf_buffer_fn with \"\n                            \"activation_id=%d and printf_size=%u.\\n\",\n                            activation_id, printf_size);\n\n    // set debug_dump_printf to 1\n    acl_process_printf_buffer_fn(activation_id, (int)printf_size, 1);\n  }\n}\n\nvoid acl_kernel_if_dump_status(acl_kernel_if *kern) {\n  int expect_kernel = 0;\n  unsigned k, i;\n  acl_assert_locked();\n\n  for (k = 0; k < kern->num_accel; ++k) {\n    for (i = 0; i < kern->accel_invoc_queue_depth[k]; ++i) {\n      if (kern->accel_job_ids[k][i] >= 0) {\n        expect_kernel = 1;\n      }\n    }\n  }\n\n  if (!expect_kernel)\n    return;\n\n  for (k = 0; k < kern->num_accel; ++k) {\n    unsigned int csr;\n\n    // Read the accelerator's status register\n    acl_kernel_cra_read(kern, k, KERNEL_OFFSET_CSR, &csr);\n\n    kern->io.printf(\"  Kernel %2u Status: 0x%08x\", k, csr);\n    if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_RUNNING) &&\n        !ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_STALLED))\n      kern->io.printf(\" running\");\n    else if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_STALLED))\n      kern->io.printf(\" stalled\");\n    else\n      kern->io.printf(\" idle\");\n    if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_DONE))\n      kern->io.printf(\" finish-pending\");\n    if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_BUSY))\n      kern->io.printf(\" busy\");\n    if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_LSU_ACTIVE))\n      kern->io.printf(\" lsu_active\");\n    if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_WR_ACTIVE))\n      kern->io.printf(\" write_active\");\n    if (ACL_KERNEL_READ_BIT(csr, KERNEL_CSR_LMEM_INVALID_BANK))\n      kern->io.printf(\" lm_bank_exception\");\n\n    // Dump the printf buffer to stdout\n    acl_kernel_if_debug_dump_printf(kern, k);\n\n    unsigned buffered_kernel_invocation = 0;\n    for (i = 0; i < kern->accel_invoc_queue_depth[k]; ++i) {\n      unsigned int next_queue_back;\n      if (kern->accel_queue_back[k] + 1 + i >=\n          kern->accel_invoc_queue_depth[k]) {\n        // Loop back\n        next_queue_back = kern->accel_queue_back[k] + 1 + i -\n                          kern->accel_invoc_queue_depth[k];\n      } else {\n        next_queue_back = kern->accel_queue_back[k] + 1 + i;\n      }\n      if (kern->accel_job_ids[k][next_queue_back] >= 0) {\n        if (i > 0) {\n          buffered_kernel_invocation++;\n        }\n      }\n    }\n    // Don't need to print this if fast relaunch is disabled\n    if (kern->accel_invoc_queue_depth[k] > 1) {\n      printf(\"\\n    Hardware Invocation Buffer: %u queued\",\n             buffered_kernel_invocation);\n    }\n\n    kern->io.printf(\"\\n\");\n  }\n\n  kern->io.printf(\"\\n\");\n}\n\n// Called by the host program when there are spare cycles or\n// if the host has been waiting for a while in debug mode.\nvoid acl_kernel_if_check_kernel_status(acl_kernel_if *kern) {\n  acl_assert_locked();\n\n  if (kern->last_kern_update != 0 &&\n      (acl_kernel_if_get_time_us(kern) - kern->last_kern_update >\n       10 * 1000000) &&\n      kern->io.debug_verbosity > 0) {\n    kern->last_kern_update = acl_kernel_if_get_time_us(kern);\n    kern->io.printf(\n        \"No kernel updates in approximately 10 seconds for device %u\",\n        kern->physical_device_id);\n    kern->io.printf(\" ... a kernel may be hung?\\n\");\n    acl_kernel_if_dump_status(kern);\n  } else if (kern->io.debug_verbosity >= 3) {\n    // If ACL_HAL_DEBUG >= 3, the status will be printed even the server isn't\n    // hang every 10 seconds.\n    if (acl_kernel_if_get_time_us(kern) - kern->last_printf_dump >\n        10 * 1000000) {\n      kern->last_printf_dump = acl_kernel_if_get_time_us(kern);\n      acl_kernel_if_dump_status(kern);\n    }\n  }\n}\n\n// Called when a printf buffer processing is done for kernel, and\n// kernel can continue execution\nvoid acl_kernel_if_unstall_kernel(acl_kernel_if *kern, int activation_id) {\n  unsigned int k, i;\n  acl_assert_locked();\n\n  // Check which accelerator will be unstalled\n  for (k = 0; k < kern->num_accel; ++k) {\n    for (i = 0; i < kern->accel_invoc_queue_depth[k]; ++i) {\n\n      if (activation_id == kern->accel_job_ids[k][i]) {\n        // un-stall the kernel by writing to unstall bit\n        unsigned int new_csr = 0;\n        acl_kernel_cra_read(kern, k, KERNEL_OFFSET_CSR, &new_csr);\n\n        ACL_KERNEL_SET_BIT(new_csr, KERNEL_CSR_UNSTALL);\n        ACL_KERNEL_IF_DEBUG_MSG(\n            kern, \":: Accelerator %d new csr is %x.\\n\", k,\n            ACL_KERNEL_READ_BIT_RANGE(new_csr, KERNEL_CSR_LAST_STATUS_BIT, 0));\n\n        acl_kernel_cra_write(kern, k, KERNEL_OFFSET_CSR, new_csr);\n      }\n    }\n  }\n}\n\nvoid acl_kernel_if_close(acl_kernel_if *kern) {\n  acl_assert_locked();\n  kern->accel_csr.clear();\n  kern->accel_perf_mon.clear();\n  kern->accel_num_printfs.clear();\n  kern->accel_job_ids.clear();\n  kern->accel_invoc_queue_depth.clear();\n  kern->accel_queue_front.clear();\n  kern->accel_queue_back.clear();\n  kern->static_img_cache.clear();\n  kern->accel_arg_cache.clear();\n  kern->autorun_profiling_kernel_id = -1;\n}\n\n// Private utility function to get a single 64-bit word from the\n// profile_data register\nstatic uint64_t acl_kernel_if_get_profile_data_word(acl_kernel_if *kern,\n                                                    cl_uint accel_id) {\n  uint64_t read_result;\n  int status;\n  acl_assert_locked_or_sig();\n\n  assert(acl_kernel_if_is_valid(kern));\n\n#ifdef _WIN32\n  // Use 32-bit reads on Windows.\n  unsigned int low_word, high_word;\n  status = acl_kernel_cra_read(\n      kern, accel_id, KERNEL_OFFSET_CSR_PROFILE_DATA + kern->cra_address_offset,\n      &low_word);\n  if (status)\n    return (uint64_t)status;\n  ACL_KERNEL_IF_DEBUG_MSG(kern,\n                          \":: Read profile hardware:: Accelerator %d \"\n                          \"profile_data low_word is %x.\\n\",\n                          accel_id, low_word);\n  status = acl_kernel_cra_read(kern, accel_id,\n                               KERNEL_OFFSET_CSR_PROFILE_DATA + 4 +\n                                   kern->cra_address_offset,\n                               &high_word);\n  if (status)\n    return (uint64_t)status;\n  ACL_KERNEL_IF_DEBUG_MSG(kern,\n                          \":: Read profile hardware:: Accelerator %d \"\n                          \"profile_data high_word is %x.\\n\",\n                          accel_id, high_word);\n  read_result =\n      ((((uint64_t)high_word) & 0xFFFFFFFF) << 32) | (low_word & 0xFFFFFFFF);\n#else\n  status = acl_kernel_cra_read_64b(\n      kern, accel_id, KERNEL_OFFSET_CSR_PROFILE_DATA + kern->cra_address_offset,\n      &read_result);\n  if (status)\n    return (uint64_t)status;\n  ACL_KERNEL_IF_DEBUG_MSG(kern,\n                          \":: Read profile hardware:: Accelerator %d \"\n                          \"profile_data read_result is 0x%llx.\\n\",\n                          accel_id, read_result);\n#endif\n  return read_result;\n}\n\n// Internal HAL function that retrieves profile data from the hardware scan\n// chain through CSR accesses.\n// Returns 0 on success.\nint acl_kernel_if_get_profile_data(acl_kernel_if *kern, cl_uint accel_id,\n                                   uint64_t *data, unsigned int length) {\n  uint64_t i;\n  acl_assert_locked_or_sig();\n\n  if (length > 0) {\n    // Buffer the data\n    ACL_KERNEL_IF_DEBUG_MSG(\n        kern, \":: Issuing profile load_buffer command:: Accelerator %d.\\n\",\n        accel_id);\n    acl_kernel_if_issue_profile_hw_command(\n        kern, accel_id, KERNEL_CSR_PROFILE_LOAD_BUFFER_BIT, 1);\n\n    for (i = 0; i < length; i++) {\n      data[i] = acl_kernel_if_get_profile_data_word(kern, accel_id);\n      ACL_KERNEL_IF_DEBUG_MSG(kern,\n                              \":: Read profile hardware:: Accelerator %d \"\n                              \"profile_data word [%\" PRIu64 \"] is 0x%016llx.\\n\",\n                              accel_id, i, data[i]);\n    }\n\n    // The interrupt has been serviced - tell HW to deassert irq.\n    ACL_KERNEL_IF_DEBUG_MSG(\n        kern, \":: Issuing profile temporal reset command:: Accelerator %d.\\n\",\n        accel_id);\n    unsigned int ctrl_val;\n    int status;\n    status = acl_kernel_cra_read(kern, accel_id, KERNEL_OFFSET_CSR, &ctrl_val);\n    ACL_KERNEL_IF_DEBUG_MSG(\n        kern, \":: Finished reading profiler data from %d.\\n\", accel_id);\n    if (status)\n      return status;\n    ACL_KERNEL_SET_BIT(ctrl_val, KERNEL_CSR_PROFILE_TEMPORAL_RESET);\n    status = acl_kernel_cra_write(kern, accel_id, KERNEL_OFFSET_CSR, ctrl_val);\n    if (status)\n      return status;\n  }\n\n  return 0;\n}\n\n// Internal HAL function that resets the profile scan-chain hardware by\n// setting a bit in the profile_ctrl CSR.\n// Returns 0 on success.\nint acl_kernel_if_reset_profile_counters(acl_kernel_if *kern,\n                                         cl_uint accel_id) {\n  acl_assert_locked_or_sig();\n  ACL_KERNEL_IF_DEBUG_MSG(kern, \":: Reset profile hardware:: Accelerator %d.\\n\",\n                          accel_id);\n  return acl_kernel_if_issue_profile_hw_command(\n      kern, accel_id, KERNEL_CSR_PROFILE_RESET_BIT, 1);\n}\n\n// Internal HAL function that disables the profile hardware counters by\n// clearing a bit in the profile_ctrl CSR.\n// Returns 0 on success.\nint acl_kernel_if_disable_profile_counters(acl_kernel_if *kern,\n                                           cl_uint accel_id) {\n  acl_assert_locked();\n  ACL_KERNEL_IF_DEBUG_MSG(\n      kern, \":: Disable profile counters:: Accelerator %d.\\n\", accel_id);\n  return acl_kernel_if_issue_profile_hw_command(\n      kern, accel_id, KERNEL_CSR_PROFILE_ALLOW_PROFILING_BIT, 0);\n}\n\n// Internal HAL function that enables the profile hardware counters by\n// setting a bit in the profile_ctrl CSR.\n// Returns 0 on success.\nint acl_kernel_if_enable_profile_counters(acl_kernel_if *kern,\n                                          cl_uint accel_id) {\n  acl_assert_locked();\n  ACL_KERNEL_IF_DEBUG_MSG(\n      kern, \":: Enable profile counters:: Accelerator %d.\\n\", accel_id);\n  return acl_kernel_if_issue_profile_hw_command(\n      kern, accel_id, KERNEL_CSR_PROFILE_ALLOW_PROFILING_BIT, 1);\n}\n\n// Private utility function to set start cycle in the profile hardware\n// Returns 0 on success.\nint acl_kernel_if_set_profile_start_cycle(acl_kernel_if *kern, cl_uint accel_id,\n                                          uint64_t value) {\n  int status;\n  acl_assert_locked();\n  assert(acl_kernel_if_is_valid(kern));\n  assert(accel_id < kern->num_accel);\n  status = acl_kernel_cra_write_64b(\n      kern, accel_id,\n      KERNEL_OFFSET_CSR_PROFILE_START_CYCLE + kern->cra_address_offset, value);\n\n  return status;\n}\n\n// Private utility function to set stop cycle in the profile hardware\n// Returns 0 on success.\nint acl_kernel_if_set_profile_stop_cycle(acl_kernel_if *kern, cl_uint accel_id,\n                                         uint64_t value) {\n  int status;\n  acl_assert_locked();\n  assert(acl_kernel_if_is_valid(kern));\n  assert(accel_id < kern->num_accel);\n  status = acl_kernel_cra_write_64b(\n      kern, accel_id,\n      KERNEL_OFFSET_CSR_PROFILE_STOP_CYCLE + kern->cra_address_offset, value);\n  return status;\n}\n"
    },
    {
        "label": "acl_auto_configure.cpp",
        "data": "// Copyright (C) 2011-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n#ifdef _MSC_VER\n#pragma warning(push)\n#pragma warning(disable : 4255)\n#endif\n\n// System headers.\n#include <cstdint>\n#include <iostream>\n#include <sstream>\n#include <string>\n#include <utility>\n#include <vector>\n\n// Internal headers.\n#include <acl_auto.h>\n#include <acl_support.h>\n#include <acl_thread.h>\n#include <acl_util.h>\n#include <acl_version.h>\n#include <unref.h>\n\n#ifdef _MSC_VER\n#pragma warning(pop)\n#endif\n\n#include <acl_auto_configure.h>\n#include <acl_auto_configure_version.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\ninline void decrement_section_counters(std::vector<int> &counters) {\n  for (auto &x : counters) {\n    x--;\n  }\n}\n\ninline void check_section_counters(std::vector<int> &counters) {\n  for (auto &x : counters) {\n    assert(x >= 0 && \"The number of fields should be positive\");\n  }\n}\n// Reads the next word in str starting from start_pos. Stores the word in result\n// with leading and trailing whitespace removed. Returns the position\n// immediately following the word that was read or std::string::npos if the end\n// of string is reached.\nstatic std::string::size_type read_word(const std::string &str,\n                                        const std::string::size_type start_pos,\n                                        std::string &result) noexcept {\n  auto string_start = str.find_first_not_of(' ', start_pos);\n  auto string_end = str.find(' ', string_start);\n  if (string_start == std::string::npos) {\n    result == \"\";\n    return std::string::npos;\n  }\n  result = str.substr(string_start, string_end - string_start);\n\n  return string_end;\n}\n\n// Reads the next word in str and converts it into an unsigned.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\n// This is only used for getting the version_id as version_id is not counted\n// in any of the forward compatible sections\nstatic bool read_uint(const std::string &str, std::string::size_type &pos,\n                      unsigned &val) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  try {\n    val = static_cast<unsigned>(std::stoi(result));\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and converts it into an unsigned.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\nstatic bool read_uint_counters(const std::string &str,\n                               std::string::size_type &pos, unsigned &val,\n                               std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  try {\n    val = static_cast<unsigned>(std::stoi(result));\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and converts it into a boolean.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\nstatic bool read_bool_counters(const std::string &str,\n                               std::string::size_type &pos, bool &val,\n                               std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  try {\n    val = static_cast<bool>(std::stoi(result));\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and converts it into an unsigned.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\nstatic bool read_uint32_counters(const std::string &str,\n                                 std::string::size_type &pos, uint32_t &val,\n                                 std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  try {\n    val = static_cast<uint32_t>(std::stoul(result));\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and converts it into an unsigned 64-bit\n// fixed-length integer. Note this read utilizes stoull and fail if\n// unsigned long long is not 64-bit long on the platform.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\nstatic bool read_uint64_counters(const std::string &str, int base,\n                                 std::string::size_type &pos, uint64_t &val,\n                                 std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  try {\n    static_assert(sizeof(uint64_t) == sizeof(unsigned long long));\n    val = static_cast<uint64_t>(std::stoull(result, nullptr, base));\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and converts it into an unsigned.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\nstatic bool read_int_counters(const std::string &str,\n                              std::string::size_type &pos, int &val,\n                              std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  try {\n    val = std::stoi(result);\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and converts it into a unsigned long long.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\nstatic bool read_ulonglong_counters(const std::string &str,\n                                    std::string::size_type &pos,\n                                    unsigned long long &val,\n                                    std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  try {\n    val = std::stoull(result);\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and converts it into a uintptr_t.\n// Returns true if a valid integer was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word\n// even if an error occurs.\nstatic bool read_uintptr_counters(const std::string &str,\n                                  std::string::size_type &pos, uintptr_t &val,\n                                  std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n\n  size_t end = 0;\n  unsigned long long parsed;\n  try {\n    parsed = std::stoull(result, &end);\n  } catch (const std::exception &) {\n    return false;\n  }\n  if (end != result.size()) {\n    return false;\n  }\n\n  val = static_cast<uintptr_t>(parsed);\n  // To make sure the cast work\n  // As `unsigned long long` might have difference size comparing to `uintptr_t\n  if (val != parsed) {\n    return false;\n  }\n\n  return true;\n}\n\n// Reads the next word in str and converts it into an unsigned or using its\n// default value. Returns true if a valid integer was read or false if an error\n// occurred. pos is updated to the position immediately following the parsed\n// word even if an error occurs.\nstatic bool read_uint_def_counters(const std::string &str,\n                                   std::string::size_type &pos, unsigned &val,\n                                   unsigned def_val,\n                                   std::vector<int> &counters) noexcept {\n  std::string result;\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  try {\n    if (result == \"?\")\n      val = def_val;\n    else\n      val = static_cast<unsigned>(std::stoi(result));\n  } catch (const std::exception &e) {\n    UNREFERENCED_PARAMETER(e);\n    return false;\n  }\n  return true;\n}\n\n// Reads the next word in str and stores it in result.\n// Returns true if a non-empty substring was read or false if an error occurred.\n// pos is updated to the position immediately following the parsed word even if\n// an occurred.\nstatic int read_string_counters(const std::string &str,\n                                std::string::size_type &pos,\n                                std::string &result,\n                                std::vector<int> &counters) noexcept {\n  pos = read_word(str, pos, result);\n  decrement_section_counters(counters);\n  return result != \"\";\n}\n\nstatic bool\nread_global_mem_defs(const std::string &config_str,\n                     std::string::size_type &curr_pos,\n                     unsigned int &num_global_mem_systems,\n                     std::array<acl_system_global_mem_def_t, ACL_MAX_GLOBAL_MEM>\n                         &global_mem_defs,\n                     std::vector<int> &counters) noexcept {\n  bool result = read_uint_counters(config_str, curr_pos, num_global_mem_systems,\n                                   counters);\n\n  for (auto i = 0U; result && (i < num_global_mem_systems); i++) {\n    std::string gmem_name;\n    std::string gmem_id = \"-\"; // This should be the default value even if the\n                               // auto-discovery string doesn't have this field.\n    // read total number of fields in global_memories\n    int total_fields_global_memories = 0;\n    result = read_int_counters(config_str, curr_pos,\n                               total_fields_global_memories, counters);\n    counters.emplace_back(total_fields_global_memories);\n\n    // read global memory name\n    if (result) {\n      result = read_string_counters(config_str, curr_pos, gmem_name, counters);\n    }\n\n    // read global memory type\n    auto gmem_type =\n        static_cast<unsigned>(ACL_GLOBAL_MEM_DEVICE_PRIVATE); // Default\n    if (result) {\n      result = read_uint_counters(config_str, curr_pos, gmem_type, counters);\n      if (gmem_type >= static_cast<unsigned>(ACL_GLOBAL_MEM_TYPE_COUNT))\n        result = false;\n    }\n\n    auto num_dimms = 0U;\n    auto configuration_address = 0ULL;\n    auto burst_interleaved = 1U;\n    std::uintptr_t gmem_start = 0, gmem_end = 0;\n    acl_system_global_mem_allocation_type_t allocation_type =\n        ACL_GLOBAL_MEM_UNDEFINED_ALLOCATION;\n    std::string primary_interface;\n    std::vector<std::string> can_access;\n    if (result) {\n      gmem_start = ~gmem_start;\n\n      // read number of memory interfaces (DIMMS or banks) usable as device\n      // global memory\n      result = read_uint_counters(config_str, curr_pos, num_dimms, counters);\n\n      if (result && num_dimms > 1) {\n        // read memory configuration address\n        result = read_ulonglong_counters(config_str, curr_pos,\n                                         configuration_address, counters);\n        // read whether the memory access is burst-interleaved across memory\n        // interfaces\n        if (result) {\n          result = read_uint_counters(config_str, curr_pos, burst_interleaved,\n                                      counters);\n        }\n      }\n\n      int total_fields_memory_interface = 0;\n      if (result) {\n        result = read_int_counters(config_str, curr_pos,\n                                   total_fields_memory_interface, counters);\n      }\n\n      // Find start and end address of global memory.\n      // Assume memory range is contiguous, but start/end address pairs for\n      // each DIMM can be in any order.\n      for (auto j = 0U; result && (j < num_dimms); j++) {\n        counters.emplace_back(total_fields_memory_interface);\n        auto cur_gmem_start = 0ULL;\n        auto cur_gmem_end = 0ULL;\n        result = read_ulonglong_counters(config_str, curr_pos, cur_gmem_start,\n                                         counters) &&\n                 read_ulonglong_counters(config_str, curr_pos, cur_gmem_end,\n                                         counters);\n        if (gmem_start > cur_gmem_start)\n          gmem_start = static_cast<std::uintptr_t>(cur_gmem_start);\n        if (gmem_end < cur_gmem_end)\n          gmem_end = static_cast<std::uintptr_t>(cur_gmem_end);\n\n        /*****************************************************************\n          Since the introduction of autodiscovery forwards-compatibility,\n          new entries for the 'global memory interface' section start here\n         ****************************************************************/\n\n        // forward compatibility: bypassing remaining fields at the end of\n        // memory interface\n        while (result && counters.size() > 0 &&\n               counters.back() > 0) { // total_fields_memory_interface>0\n          std::string tmp;\n          result = result &&\n                   read_string_counters(config_str, curr_pos, tmp, counters);\n          check_section_counters(counters);\n        }\n        counters.pop_back(); // removing total_fields_memory_interface from\n                             // the list\n      }\n\n      /*****************************************************************\n        Since the introduction of autodiscovery forwards-compatibility,\n        new entries for the 'global memory' section start here.\n       ****************************************************************/\n\n      // read memory allocation_type\n      // These are new since the addition of forward compatibility; it is\n      // important that they come after all global memory fields included\n      // in version 23, when forward compatibility was added.\n      // Only try to read these if there are values left to read in the\n      // global memory subsection.\n      if (result && counters.back() > 0) {\n        auto alloc_type = 0U;\n        result = read_uint_counters(config_str, curr_pos, alloc_type, counters);\n        allocation_type =\n            static_cast<acl_system_global_mem_allocation_type_t>(alloc_type);\n      }\n\n      // read memory primary interface\n      if (result && counters.back() > 0) {\n        result = read_string_counters(config_str, curr_pos, primary_interface,\n                                      counters);\n        if (result && primary_interface == \"-\")\n          primary_interface = \"\";\n      }\n\n      // read size of memory can access list\n      if (result && counters.back() > 0) {\n        unsigned can_access_count = 0U;\n        result = read_uint_counters(config_str, curr_pos, can_access_count,\n                                    counters);\n        while (result && can_access_count-- && counters.size() > 0 &&\n               counters.back() > 0) {\n          std::string temp;\n          result = read_string_counters(config_str, curr_pos, temp, counters);\n          can_access.push_back(std::move(temp));\n        }\n      }\n\n      // read global memory id field. If it doesn't exist, then the value is \"-\"\n      // It's a new field introduced from 2024.2\n      if (result && counters.back() > 0) {\n        result = read_string_counters(config_str, curr_pos, gmem_id, counters);\n      }\n    }\n\n    if (result) {\n      // Global memory definition can't change across reprograms.\n      // If global memory definition changed, allocations will get messed up.\n      // The check won't be done here though. It will need to be done by the\n      // callers.\n      //\n      // IMPORTANT: If a new field is added here, make sure that field is\n      // also copied in acl_program.cpp:l_device_memory_definition_copy().\n      // For built-in kernels (and CL_CONTEXT_COMPILER_MODE=3), memory\n      // definition is copied from the one loaded from autodiscovery ROM\n      // to new program object's device definition.\n      global_mem_defs[i].num_global_banks = num_dimms;\n      global_mem_defs[i].config_addr =\n          static_cast<size_t>(configuration_address);\n      global_mem_defs[i].name = std::move(gmem_name);\n      global_mem_defs[i].range.begin = reinterpret_cast<void *>(gmem_start);\n      global_mem_defs[i].range.next = reinterpret_cast<void *>(gmem_end);\n      global_mem_defs[i].type =\n          static_cast<acl_system_global_mem_type_t>(gmem_type);\n      global_mem_defs[i].burst_interleaved = burst_interleaved;\n      global_mem_defs[i].allocation_type = allocation_type;\n      global_mem_defs[i].primary_interface = std::move(primary_interface);\n      global_mem_defs[i].can_access_list = std::move(can_access);\n      global_mem_defs[i].id = std::move(gmem_id);\n    }\n\n    // forward compatibility: bypassing remaining fields at the end of global\n    // memory\n    while (result && counters.size() > 0 &&\n           counters.back() > 0) { // total_fields_global_memories>0\n      std::string tmp;\n      result =\n          result && read_string_counters(config_str, curr_pos, tmp, counters);\n      check_section_counters(counters);\n    }\n    counters.pop_back(); // removing total_fields_global_memories\n  }\n\n  return result;\n}\n\nstatic bool\nread_hostpipe_infos(const std::string &config_str,\n                    std::string::size_type &curr_pos,\n                    std::vector<acl_hostpipe_info_t> &hostpipe_infos,\n                    std::vector<int> &counters) noexcept {\n  auto num_hostpipes = 0U;\n  bool result =\n      read_uint_counters(config_str, curr_pos, num_hostpipes, counters);\n\n  // read total number of fields in hostpipes\n  int total_fields_hostpipes = 0;\n  if (result) {\n    result = read_int_counters(config_str, curr_pos, total_fields_hostpipes,\n                               counters);\n  }\n\n  for (unsigned i = 0; result && (i < num_hostpipes); i++) {\n    counters.emplace_back(total_fields_hostpipes);\n    std::string name;\n\n    bool hostpipe_is_host_to_dev = false;\n    bool hostpipe_is_dev_to_host = false;\n    auto hostpipe_width = 0U;\n    auto hostpipe_max_buffer_depth = 0U;\n    result =\n        result && read_string_counters(config_str, curr_pos, name, counters) &&\n        read_bool_counters(config_str, curr_pos, hostpipe_is_host_to_dev,\n                           counters) &&\n        read_bool_counters(config_str, curr_pos, hostpipe_is_dev_to_host,\n                           counters) &&\n        read_uint_counters(config_str, curr_pos, hostpipe_width, counters) &&\n        read_uint_counters(config_str, curr_pos, hostpipe_max_buffer_depth,\n                           counters);\n    // is_host_to_dev and is_dev_to_host are exclusive because of the enum\n    // Type\n    hostpipe_infos.push_back(acl_hostpipe_info_t{\n        std::move(name), hostpipe_is_host_to_dev, hostpipe_is_dev_to_host,\n        hostpipe_width, hostpipe_max_buffer_depth});\n\n    /*****************************************************************\n      Since the introduction of autodiscovery forwards-compatibility,\n      new entries for the 'hostpipe' section start here.\n     ****************************************************************/\n\n    // forward compatibility: bypassing remaining fields at the end of\n    // hostpipes\n    while (result && counters.size() > 0 &&\n           counters.back() > 0) { // total_fields_hostpipes>0\n      std::string tmp;\n      result =\n          result && read_string_counters(config_str, curr_pos, tmp, counters);\n      check_section_counters(counters);\n    }\n    counters.pop_back(); // removing total_fields_hostpipes\n  }\n\n  return result;\n}\n\nstatic bool read_device_global_mem_defs(\n    const std::string &config_str, std::string::size_type &curr_pos,\n    std::unordered_map<std::string, acl_device_global_mem_def_t>\n        &device_global_mem_defs,\n    std::vector<int> &counters, std::string &err_str) noexcept {\n  unsigned int num_device_global = 0;\n  bool result =\n      read_uint_counters(config_str, curr_pos, num_device_global, counters);\n\n  // read total number of fields in device global\n  unsigned int total_fields_device_global = 0;\n  if (result) {\n    result = read_uint_counters(config_str, curr_pos,\n                                total_fields_device_global, counters);\n  }\n\n  // Clean up any residual information first\n  device_global_mem_defs.clear();\n\n  for (auto i = 0U; result && (i < num_device_global); i++) {\n    counters.emplace_back(total_fields_device_global);\n\n    // read device global name\n    std::string device_global_name;\n    if (result && counters.back() > 0) {\n      result = read_string_counters(config_str, curr_pos, device_global_name,\n                                    counters);\n    }\n\n    // read device global address\n    uint64_t dev_global_addr = 0; // Default\n    if (result && counters.back() > 0) {\n      // Parse in base 16\n      result = read_uint64_counters(config_str, 16, curr_pos, dev_global_addr,\n                                    counters);\n    }\n    // read device global address size\n    uint32_t dev_global_size = 0; // Default\n    if (result && counters.back() > 0) {\n      result =\n          read_uint32_counters(config_str, curr_pos, dev_global_size, counters);\n    }\n\n    // read device global properties\n    auto host_access =\n        static_cast<unsigned>(ACL_DEVICE_GLOBAL_HOST_ACCESS_READ_WRITE);\n    if (result && counters.back() > 0) {\n      result = read_uint_counters(config_str, curr_pos, host_access, counters);\n      if (host_access >=\n          static_cast<unsigned>(ACL_DEVICE_GLOBAL_HOST_ACCESS_TYPE_COUNT))\n        result = false;\n    }\n    bool can_skip_programming = false;\n    if (result && counters.back() > 0) {\n      result = read_bool_counters(config_str, curr_pos, can_skip_programming,\n                                  counters);\n    }\n    bool implement_in_csr = false;\n    if (result && counters.back() > 0) {\n      result =\n          read_bool_counters(config_str, curr_pos, implement_in_csr, counters);\n    }\n    bool reset_on_reuse = false;\n    if (result && counters.back() > 0) {\n      result =\n          read_bool_counters(config_str, curr_pos, reset_on_reuse, counters);\n    }\n\n    acl_device_global_mem_def_t dev_global_def = {\n        dev_global_addr,\n        dev_global_size,\n        static_cast<acl_device_global_host_access_t>(host_access),\n        can_skip_programming,\n        implement_in_csr,\n        reset_on_reuse};\n    bool ok =\n        device_global_mem_defs.insert({device_global_name, dev_global_def})\n            .second;\n    if (!ok) {\n      // Device global name already exist in map, but it should have been\n      // unique.\n      std::stringstream err_ss;\n      err_ss << \"Device global name should be unique. \" << device_global_name\n             << \" is repeated.\\n\";\n      err_str = err_ss.str();\n      result = false;\n    }\n\n    // forward compatibility: bypassing remaining fields at the end of device\n    // global memory\n    while (result && counters.size() > 0 &&\n           counters.back() > 0) { // total_fields_device_global>0\n      std::string tmp;\n      result =\n          result && read_string_counters(config_str, curr_pos, tmp, counters);\n      check_section_counters(counters);\n    }\n    counters.pop_back(); // removing total_fields_device_global\n  }\n\n  return result;\n}\n\nstatic bool read_streaming_kernel_arg_info(\n    const std::string &config_str, std::string::size_type &curr_pos,\n    bool &streaming_arg_info_available,\n    acl_streaming_kernel_arg_info &streaming_arg_info,\n    std::vector<int> &counters) noexcept {\n  unsigned int value = 0;\n  bool result = read_uint_counters(config_str, curr_pos, value, counters);\n  streaming_arg_info_available = value;\n\n  if (result && streaming_arg_info_available) {\n    streaming_arg_info = acl_streaming_kernel_arg_info{};\n    result = read_string_counters(config_str, curr_pos,\n                                  streaming_arg_info.interface_name, counters);\n  }\n  return result;\n}\n\nstatic bool read_hostpipe_mappings(\n    const std::string &config_str, std::string::size_type &curr_pos,\n    std::vector<acl_hostpipe_mapping> &hostpipe_mappings,\n    std::vector<int> &counters, std::string &err_str) noexcept {\n  unsigned int num_mappings = 0;\n  bool result =\n      read_uint_counters(config_str, curr_pos, num_mappings, counters);\n\n  unsigned int num_fields_per_mapping = 0;\n  if (result) {\n    result = read_uint_counters(config_str, curr_pos, num_fields_per_mapping,\n                                counters);\n  }\n\n  for (unsigned int i = 0; result && (i < num_mappings); i++) {\n    counters.emplace_back(num_fields_per_mapping);\n\n    acl_hostpipe_mapping mapping{};\n    result =\n        read_string_counters(config_str, curr_pos, mapping.logical_name,\n                             counters) &&\n        read_string_counters(config_str, curr_pos, mapping.physical_name,\n                             counters) &&\n        read_bool_counters(config_str, curr_pos, mapping.implement_in_csr,\n                           counters) &&\n        read_string_counters(config_str, curr_pos, mapping.csr_address,\n                             counters) &&\n        read_bool_counters(config_str, curr_pos, mapping.is_read, counters) &&\n        read_bool_counters(config_str, curr_pos, mapping.is_write, counters) &&\n        read_uint_counters(config_str, curr_pos, mapping.pipe_width,\n                           counters) &&\n        read_uint_counters(config_str, curr_pos, mapping.pipe_depth, counters);\n\n    // Start from 2024.0, there is a new field called protocol in the\n    // auto-discovery string\n    // This field isn't currently being used by the Runtime.\n    // It is reserved for the future when new protocols are\n    // supported and the Runtime needs to differentiate.\n    if (result && counters.back() > 0) {\n      result = result && read_int_counters(config_str, curr_pos,\n                                           mapping.protocol, counters);\n    }\n\n    // Start from 2024.2, there is a new field called is_stall_free in the\n    // auto-discovery string\n    if (result && counters.back() > 0) {\n      result = result && read_int_counters(config_str, curr_pos,\n                                           mapping.is_stall_free, counters);\n    }\n\n    hostpipe_mappings.emplace_back(mapping);\n\n    while (result && counters.back() > 0) {\n      std::string tmp;\n      result = read_string_counters(config_str, curr_pos, tmp, counters);\n    }\n    check_section_counters(counters);\n    counters.pop_back();\n  }\n\n  return result;\n}\n\n/*\nNew section added in 2024.1\n- Number of group of sideband signals -> each group map to each hostpipe\n  For each sideband signal group\n   - pipe logical name\n   - number of sideband signals (including data field)\n   - number of fields for each sideband signal\n   - port identifier, like 0 for data, 1,2,3 for different sideband signals\n   - port offset (in bits)\n   - sideband size (in bits)\n\nIf a hostpipe has no sideband signal (e.g, a pipe only has data field), it won't\nhave a sideband signal group. If none of the hostpipe has any sideband signals,\nthe whole section will just be 0, which represents the number of sideband signal\ngroup is 0.\n\n*/\n\nstatic bool read_sideband_mappings(\n    const std::string &config_str, std::string::size_type &curr_pos,\n    std::vector<acl_sideband_signal_mapping> &sideband_signal_mappings,\n    std::vector<int> &counters, std::string &err_str) noexcept {\n\n  // Get how many hostpipes have sideband signals\n  unsigned int num_of_sideband_groups = 0;\n  bool result = read_uint_counters(config_str, curr_pos, num_of_sideband_groups,\n                                   counters);\n\n  // If none of the hostpipes have sideband signals, this section ends here.\n  if (num_of_sideband_groups == 0) {\n    return result;\n  }\n\n  // Reaching here means we need to parse sideband signals.\n\n  for (unsigned int i = 0; result && (i < num_of_sideband_groups); i++) {\n\n    std::string logical_name;\n    unsigned num_sideband_signals = 0;\n    result =\n        read_string_counters(config_str, curr_pos, logical_name, counters) &&\n        read_uint_counters(config_str, curr_pos, num_sideband_signals,\n                           counters);\n    assert(num_sideband_signals >=\n           2); // If it has an entry, it must have 1 data signal + at least 1\n               // sideband signals\n\n    unsigned num_fields_per_sideband = 0;\n    if (result) {\n      result = read_uint_counters(config_str, curr_pos, num_fields_per_sideband,\n                                  counters);\n    }\n\n    for (unsigned int j = 0; result && (j < num_sideband_signals); j++) {\n      counters.emplace_back(num_fields_per_sideband);\n      acl_sideband_signal_mapping mapping{};\n      mapping.logical_name = logical_name;\n      result = read_uint_counters(config_str, curr_pos, mapping.port_identifier,\n                                  counters) &&\n               read_uint_counters(config_str, curr_pos, mapping.port_offset,\n                                  counters) &&\n               read_uint_counters(config_str, curr_pos, mapping.sideband_size,\n                                  counters);\n      sideband_signal_mappings.emplace_back(mapping);\n\n      while (result && counters.back() > 0) {\n        std::string tmp;\n        result = read_string_counters(config_str, curr_pos, tmp, counters);\n      }\n      check_section_counters(counters);\n      counters.pop_back();\n    }\n  }\n  return result;\n}\n\nstatic bool read_kernel_args(const std::string &config_str,\n                             const bool kernel_arg_info_available,\n                             std::string::size_type &curr_pos,\n                             std::vector<acl_kernel_arg_info_t> &args,\n                             std::vector<int> &counters) noexcept {\n  // Get the number of parameters\n  auto num_args = 0U;\n  bool result = read_uint_counters(config_str, curr_pos, num_args, counters);\n\n  if (result) {\n    args = std::vector<acl_kernel_arg_info_t>(num_args);\n  }\n\n  for (auto j = 0U; result && (j < num_args); j++) {\n    auto addr_space_type = 0U;\n    auto category = 0U;\n    auto size = 0U;\n    int total_fields_arguments = 0;\n    if (result) {\n      result = result && read_int_counters(config_str, curr_pos,\n                                           total_fields_arguments, counters);\n    }\n    counters.emplace_back(total_fields_arguments);\n    unsigned alignment = ACL_MEM_ALIGN; // Set default to 1024 bytes\n    result =\n        result &&\n        read_uint_counters(config_str, curr_pos, addr_space_type, counters) &&\n        read_uint_counters(config_str, curr_pos, category, counters) &&\n        read_uint_counters(config_str, curr_pos, size, counters);\n    if (result) {\n      result = result &&\n               read_uint_counters(config_str, curr_pos, alignment, counters);\n    }\n\n    std::string buffer_location = \"\";\n    if (result) {\n      unsigned int num_buffer_locations = 0;\n      result = result && read_uint_counters(config_str, curr_pos,\n                                            num_buffer_locations, counters);\n      for (unsigned int k = 0; result && (k < num_buffer_locations); k++) {\n        result = result && read_string_counters(config_str, curr_pos,\n                                                buffer_location, counters);\n      }\n      if (result && num_buffer_locations > 1) {\n        std::cerr << \"WARNING: kernel argument \" << j\n                  << \" has multiple buffer_location attributes which is not \"\n                     \"supported.\\nSelecting \"\n                  << buffer_location << \" as buffer location.\\n\";\n      }\n    }\n\n    // Only local mem contains the following params\n    auto aspace_id = 0U;\n    auto lmem_size_bytes = 0U;\n    if (result && (addr_space_type == ACL_ARG_ADDR_LOCAL)) {\n      result =\n          result &&\n          read_uint_counters(config_str, curr_pos, aspace_id, counters) &&\n          read_uint_counters(config_str, curr_pos, lmem_size_bytes, counters);\n    }\n\n    auto type_qualifier = 0U;\n    auto host_accessible = 0U;\n    std::string pipe_channel_id;\n    if (result) {\n      result = result && read_uint_counters(config_str, curr_pos,\n                                            type_qualifier, counters);\n      if (result && (type_qualifier == ACL_ARG_TYPE_PIPE)) {\n        result = result && read_uint_counters(config_str, curr_pos,\n                                              host_accessible, counters);\n        if (result && host_accessible) {\n          result = result && read_string_counters(config_str, curr_pos,\n                                                  pipe_channel_id, counters);\n        }\n      }\n    }\n\n    std::string name = \"\";\n    std::string type_name = \"\";\n    auto access_qualifier = 0U;\n    if (kernel_arg_info_available) {\n      if (result) {\n        result =\n            result &&\n            read_string_counters(config_str, curr_pos, name, counters) &&\n            read_string_counters(config_str, curr_pos, type_name, counters) &&\n            read_uint_counters(config_str, curr_pos, access_qualifier,\n                               counters);\n      }\n      if (type_name == \"0\")\n        type_name = \"\";\n    }\n\n    bool streaming_arg_info_available = false;\n    acl_streaming_kernel_arg_info streaming_arg_info;\n    if (result && counters.back() > 0) {\n      result = read_streaming_kernel_arg_info(config_str, curr_pos,\n                                              streaming_arg_info_available,\n                                              streaming_arg_info, counters);\n    }\n\n    /*****************************************************************\n      Since the introduction of autodiscovery forwards-compatibility,\n      new entries for each kernel argument section start here.\n     ****************************************************************/\n\n    if (result) {\n      args[j].name = std::move(name);\n      args[j].addr_space =\n          static_cast<acl_kernel_arg_addr_space_t>(addr_space_type);\n      args[j].access_qualifier =\n          static_cast<acl_kernel_arg_access_qualifier_t>(access_qualifier);\n      args[j].category = static_cast<acl_kernel_arg_category_t>(category);\n      args[j].size = size;\n      args[j].alignment = alignment;\n      args[j].aspace_number = aspace_id;\n      args[j].lmem_size_bytes = lmem_size_bytes;\n      args[j].type_name = std::move(type_name);\n      args[j].type_qualifier =\n          static_cast<acl_kernel_arg_type_qualifier_t>(type_qualifier);\n      args[j].host_accessible = host_accessible;\n      args[j].pipe_channel_id = std::move(pipe_channel_id);\n      args[j].buffer_location = std::move(buffer_location);\n      args[j].streaming_arg_info_available = streaming_arg_info_available;\n      args[j].streaming_arg_info = std::move(streaming_arg_info);\n    }\n    // forward compatibility: bypassing remaining fields at the end of\n    // arguments section\n    while (result && counters.size() > 0 &&\n           counters.back() > 0) { // total_fields_arguments>0\n      std::string tmp;\n      result =\n          result && read_string_counters(config_str, curr_pos, tmp, counters);\n      check_section_counters(counters);\n    }\n    counters.pop_back();\n  }\n\n  return result;\n}\n\nstatic bool read_streaming_kernel_control_info(\n    const std::string &config_str, std::string::size_type &curr_pos,\n    bool &streaming_control_info_available,\n    acl_streaming_kernel_control_info &streaming_control_info,\n    std::vector<int> &counters) noexcept {\n  unsigned int value = 0;\n  bool result = read_uint_counters(config_str, curr_pos, value, counters);\n  streaming_control_info_available = value;\n\n  if (result && streaming_control_info_available) {\n    streaming_control_info = acl_streaming_kernel_control_info{};\n    result = read_string_counters(config_str, curr_pos,\n                                  streaming_control_info.start, counters) &&\n             read_string_counters(config_str, curr_pos,\n                                  streaming_control_info.done, counters);\n  }\n\n  return result;\n}\n\nstatic bool read_accel_defs(const std::string &config_str,\n                            std::string::size_type &curr_pos,\n                            const bool kernel_arg_info_available,\n                            std::vector<acl_accel_def_t> &accel,\n                            std::vector<acl_hal_accel_def_t> &hal_info,\n                            std::vector<int> &counters,\n                            std::string &err_str) noexcept {\n  auto num_accel = 0U;\n  bool result = read_uint_counters(config_str, curr_pos, num_accel, counters);\n  if (result) {\n    accel = std::vector<acl_accel_def_t>(num_accel);\n    hal_info = std::vector<acl_hal_accel_def_t>(num_accel);\n  }\n  // Setup the accelerators\n  for (auto i = 0U; result && (i < num_accel); i++) {\n    accel[i].id = i;\n\n    accel[i].mem.begin = reinterpret_cast<void *>(0);\n    accel[i].mem.next = reinterpret_cast<void *>(0x00020000);\n\n    int total_fields_kernel = 0;\n    result = result && read_int_counters(config_str, curr_pos,\n                                         total_fields_kernel, counters);\n    counters.emplace_back(total_fields_kernel);\n\n    result = result && read_string_counters(config_str, curr_pos,\n                                            hal_info[i].name, counters);\n\n    if (!result)\n      break;\n    accel[i].iface.name = hal_info[i].name;\n\n    // Get kernel CRA address and range.\n    // The address is the offset from the CRA address of the first kernel CRA.\n    // That first kernel CRA comes after, for example, the PCIE CRA.\n    result = result &&\n             read_uint_counters(config_str, curr_pos, hal_info[i].csr.address,\n                                counters) &&\n             read_uint_counters(config_str, curr_pos, hal_info[i].csr.num_bytes,\n                                counters);\n\n    result = result && read_uint_counters(config_str, curr_pos,\n                                          accel[i].fast_launch_depth, counters);\n\n    // Get the kernel performance monitor address and range - used for\n    // profiling.  If the performance monitor is not instantiated, the\n    // range field here will be 0.\n    result = result &&\n             read_uint_counters(config_str, curr_pos,\n                                hal_info[i].perf_mon.address, counters) &&\n             read_uint_counters(config_str, curr_pos,\n                                hal_info[i].perf_mon.num_bytes, counters);\n\n    // Determine whether the kernel is workgroup-invariant.\n    result =\n        result && read_uint_counters(config_str, curr_pos,\n                                     accel[i].is_workgroup_invariant, counters);\n\n    // Determine whether the kernel is workitem-invariant.\n    result =\n        result && read_uint_counters(config_str, curr_pos,\n                                     accel[i].is_workitem_invariant, counters);\n    if (!accel[i].is_workgroup_invariant && accel[i].is_workitem_invariant) {\n      std::stringstream err_ss;\n      err_ss << \"FAILED to read auto-discovery string at byte \" << curr_pos\n             << \": kernel cannot be workitem-invariant while it is \"\n                \"workgroup-variant. \"\n                \"Full auto-discovery string value is \"\n             << config_str << \"\\n\";\n      err_str = err_ss.str();\n      result = false;\n    }\n\n    // Determine whether the kernel is vectorized.\n    result = result && read_uint_counters(config_str, curr_pos,\n                                          accel[i].num_vector_lanes, counters);\n\n    // Determine how much profiling data is available in the kernel\n    result = result &&\n             read_uint_counters(config_str, curr_pos,\n                                accel[i].profiling_words_to_readback, counters);\n\n    result =\n        result && read_kernel_args(config_str, kernel_arg_info_available,\n                                   curr_pos, accel[i].iface.args, counters);\n\n    // Get the number of printf format strings\n    auto num_printf_format_strings = 0U;\n    result = result && read_uint_counters(config_str, curr_pos,\n                                          num_printf_format_strings, counters);\n    accel[i].printf_format_info =\n        std::vector<acl_printf_info_t>(num_printf_format_strings);\n\n    // Disable fast relaunch when kernel has printf\n    if (accel[i].printf_format_info.size() > 0) {\n      accel[i].fast_launch_depth = 0;\n    }\n\n    // Get the arguments themselves\n    int total_fields_printf = 0;\n    if (result) {\n      result = read_int_counters(config_str, curr_pos, total_fields_printf,\n                                 counters);\n    }\n\n    for (auto j = 0U; result && (j < accel[i].printf_format_info.size()); j++) {\n      counters.emplace_back(total_fields_printf);\n      result =\n          read_uint_counters(config_str, curr_pos,\n                             accel[i].printf_format_info[j].index, counters) &&\n          read_string_counters(config_str, curr_pos,\n                               accel[i].printf_format_info[j].format_string,\n                               counters);\n\n      /*******************************************************************\n        Since the introduction of autodiscovery forwards-compatibility,\n        new entries for each kernel's 'printf' section start here.\n       ******************************************************************/\n\n      // forward compatibility: bypassing remaining fields at the end of\n      // printf calls section\n      while (result && counters.size() > 0 &&\n             counters.back() > 0) { // fields_printf>0\n        std::string tmp;\n        result =\n            result && read_string_counters(config_str, curr_pos, tmp, counters);\n        check_section_counters(counters);\n      }\n      counters.pop_back();\n    }\n\n    // Read the number of local mem systems, then aspaceID and static\n    // demand for each.\n    if (result) {\n      auto num_local_aspaces = 0U;\n      result =\n          read_uint_counters(config_str, curr_pos, num_local_aspaces, counters);\n\n      int total_fields_local_aspaces = 0;\n      // Read the number of fields in local mem systems\n      if (result) {\n        result = read_int_counters(config_str, curr_pos,\n                                   total_fields_local_aspaces, counters);\n      }\n      accel[i].local_aspaces =\n          std::vector<acl_local_aspace_info>(num_local_aspaces);\n\n      for (auto it = 0U; it < num_local_aspaces && result; ++it) {\n        counters.emplace_back(total_fields_local_aspaces);\n        result = read_uint_counters(config_str, curr_pos,\n                                    accel[i].local_aspaces[it].aspace_id,\n                                    counters) &&\n                 read_uint_counters(config_str, curr_pos,\n                                    accel[i].local_aspaces[it].static_demand,\n                                    counters);\n\n        /****************************************************************\n          Since the introduction of autodiscovery forwards-compatibility,\n          new entries for each kernel's 'local memory systems' section\n          start here.\n         ***************************************************************/\n\n        // forward compatibility: bypassing remaining fields at the end of\n        // local mem system section\n        while (result && counters.size() > 0 &&\n               counters.back() > 0) { // fields_local_aspaces>0\n          std::string tmp;\n          result = result &&\n                   read_string_counters(config_str, curr_pos, tmp, counters);\n          check_section_counters(counters);\n        }\n        counters.pop_back();\n      }\n    }\n\n    // Parse kernel attribute reqd_work_group_size.\n    if (result) {\n      std::vector<unsigned> wgs = {0U, 0U, 0U};\n      result = read_uint_counters(config_str, curr_pos, wgs[0], counters) &&\n               read_uint_counters(config_str, curr_pos, wgs[1], counters) &&\n               read_uint_counters(config_str, curr_pos, wgs[2], counters);\n\n      accel[i].compile_work_group_size[0] = wgs[0];\n      accel[i].compile_work_group_size[1] = wgs[1];\n      accel[i].compile_work_group_size[2] = wgs[2];\n    }\n\n    accel[i].max_work_group_size_arr[0] = 0;\n    accel[i].max_work_group_size_arr[1] = 0;\n    accel[i].max_work_group_size_arr[2] = 0;\n\n    // Parse kernel attribute max_work_group_size.\n    if (result) {\n      auto num_vals = 0U;\n      result = read_uint_counters(config_str, curr_pos, num_vals, counters);\n      if (result) {\n        // OpenCL supports only 3 dimensions in specifying work-group size\n        assert(\n            num_vals <= 3 &&\n            \"Unsupported number of Maximum work-group size values specified\");\n        accel[i].max_work_group_size = 1;\n        auto n = 0U;\n        while (result && n < num_vals) {\n          auto max_work_group_size_val = 0U;\n          result =\n              result && read_uint_counters(config_str, curr_pos,\n                                           max_work_group_size_val, counters);\n\n          accel[i].max_work_group_size_arr[n] = max_work_group_size_val;\n          accel[i].max_work_group_size *= max_work_group_size_val;\n          n++;\n        }\n      }\n    }\n\n    if (result) {\n      result = read_uint_counters(config_str, curr_pos,\n                                  accel[i].max_global_work_dim, counters);\n    }\n\n    if (result) {\n      result = read_uint_counters(config_str, curr_pos,\n                                  accel[i].uses_global_work_offset, counters);\n    }\n\n    /*******************************************************************\n      Since the introduction of autodiscovery forwards-compatibility,\n      new entries for each 'kernel description' section start here.\n     ******************************************************************/\n    accel[i].is_sycl_compile = 0; // Initializing for backward compatability\n    if (result && counters.back() > 0) {\n      result = read_uint_counters(config_str, curr_pos,\n                                  accel[i].is_sycl_compile, counters);\n    }\n\n    if (result && counters.back() > 0) {\n      result = read_streaming_kernel_control_info(\n          config_str, curr_pos, accel[i].streaming_control_info_available,\n          accel[i].streaming_control_info, counters);\n    }\n\n    // forward compatibility: bypassing remaining fields at the end of kernel\n    // description section\n    while (result && counters.size() > 0 &&\n           counters.back() > 0) { // total_fields_kernel>0\n      std::string tmp;\n      result =\n          result && read_string_counters(config_str, curr_pos, tmp, counters);\n      check_section_counters(counters);\n    }\n    counters.pop_back();\n  }\n\n  if (!result) {\n    accel.clear();\n    hal_info.clear();\n  }\n\n  return result;\n}\n\nbool acl_load_device_def_from_str(const std::string &config_str,\n                                  acl_device_def_autodiscovery_t &devdef,\n                                  std::string &err_str) noexcept {\n  acl_assert_locked();\n\n  bool result = !config_str.empty();\n  err_str.clear();\n\n  std::string::size_type curr_pos = 0;\n  int total_fields = 0;\n  std::vector<int> counters;\n\n  if (!result) {\n    std::stringstream err_ss;\n    err_ss << \"FAILED to read auto-discovery string at byte \" << curr_pos\n           << \": Expected non-zero value. Full string value is \" << config_str\n           << \"\\n\";\n    err_str = err_ss.str();\n  }\n\n  auto version_id = 0U;\n  if (result) {\n    result = read_uint(config_str, curr_pos, version_id);\n  }\n\n  if (result) {\n    if (version_id > static_cast<unsigned>(ACL_AUTO_CONFIGURE_VERSIONID) ||\n        version_id <\n            static_cast<unsigned>(\n                ACL_AUTO_CONFIGURE_BACKWARDS_COMPATIBLE_WITH_VERSIONID)) {\n      result = false;\n      std::stringstream err_ss;\n      err_ss << \"Error: The accelerator hardware currently programmed is \"\n                \"incompatible with this\\nversion of the runtime (\" ACL_VERSION\n                \" Commit \" ACL_GIT_COMMIT \").\"\n                \" Recompile the hardware with\\nthe same version of the \"\n                \"compiler and program that onto the board.\\n\";\n      err_str = err_ss.str();\n    }\n  }\n\n  /*********************************************************************************************************************************************\n   *                                                      STOP!!!! *\n   *                                                                                                                                           *\n   * If some add new fields is added, they will be at the end of the\n   *corresponding section or subsection                                       *\n   * Section: device and kernel * Subsection: e.g., kernel arguments, global\n   *mem, ... * If there is a field that no longer in use, it has ?\n   *(question-mark) as its data. * This rule is only applying to the fields that\n   *are not string *\n   *********************************************************************************************************************************************/\n  // read total number of fields in device description\n  if (result) {\n    result = read_int_counters(config_str, curr_pos, total_fields, counters);\n  }\n  counters.emplace_back(total_fields);\n\n  // Read rand_hash.\n  if (result) {\n    result = read_string_counters(config_str, curr_pos, devdef.binary_rand_hash,\n                                  counters);\n  }\n\n  // Read board name.\n  if (result) {\n    result = read_string_counters(config_str, curr_pos, devdef.name, counters);\n  }\n\n  // Check endianness field\n  if (result) {\n    result = read_uint_def_counters(config_str, curr_pos, devdef.is_big_endian,\n                                    0, counters);\n    if (devdef.is_big_endian > 1)\n      result = false;\n  }\n\n  // Set up device global memories\n  if (result) {\n    result = read_global_mem_defs(config_str, curr_pos,\n                                  devdef.num_global_mem_systems,\n                                  devdef.global_mem_defs, counters);\n  }\n\n  // Set up hostpipe information\n  if (result) {\n    result = read_hostpipe_infos(config_str, curr_pos, devdef.acl_hostpipe_info,\n                                 counters);\n  }\n\n  /*****************************************************************\n    Since the introduction of autodiscovery forwards-compatibility,\n    new entries for the 'device' section start here.\n   ****************************************************************/\n\n  auto kernel_arg_info_available = 0U;\n  if (result && counters.back() > 0) {\n    result = read_uint_counters(config_str, curr_pos, kernel_arg_info_available,\n                                counters);\n  }\n\n  // Read device global information.\n  if (result && counters.back() > 0) {\n    result = read_device_global_mem_defs(\n        config_str, curr_pos, devdef.device_global_mem_defs, counters, err_str);\n  }\n\n  // Check whether csr_ring_root exist in the IP.\n  if (result && counters.back() > 0) {\n    result = read_bool_counters(config_str, curr_pos,\n                                devdef.cra_ring_root_exist, counters);\n  }\n\n  // Read program scoped hostpipes mappings\n  if (result && counters.back() > 0) {\n    result = read_hostpipe_mappings(\n        config_str, curr_pos, devdef.hostpipe_mappings, counters, err_str);\n  }\n\n  // Starting from 2024.1, there is a new section that adds sideband signal\n  // information.\n\n  // Read program scoped hostpipes sideband signals mapping\n\n  if (result && counters.back() > 0) {\n    result = read_sideband_mappings(config_str, curr_pos,\n                                    devdef.sideband_signal_mappings, counters,\n                                    err_str);\n  }\n\n  // forward compatibility: bypassing remaining fields at the end of device\n  // description section\n  while (result && counters.size() > 0 &&\n         counters.back() > 0) { // total_fields>0\n    std::string tmp;\n    result =\n        result && read_string_counters(config_str, curr_pos, tmp, counters);\n    check_section_counters(counters);\n  }\n  counters.pop_back(); // removing total_fields\n\n  // Set up kernel description\n  if (result) {\n    result = read_accel_defs(config_str, curr_pos, kernel_arg_info_available,\n                             devdef.accel, devdef.hal_info, counters, err_str);\n  }\n\n  if (!result && err_str.empty()) {\n    std::stringstream err_ss;\n    err_ss << \"FAILED to read auto-discovery string at byte \" << curr_pos\n           << \". Full auto-discovery string value is \" << config_str << \"\\n\";\n    err_str = err_ss.str();\n  }\n\n  return result;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    },
    {
        "label": "acl_command.cpp",
        "data": "// Copyright (C) 2010-2021 Intel Corporation\n// SPDX-License-Identifier: BSD-3-Clause\n\n// System headers.\n#include <stdio.h>\n#include <vector>\n\n// External library headers.\n#include <CL/opencl.h>\n#include <acl_threadsupport/acl_threadsupport.h>\n\n// Internal headers.\n#include <acl.h>\n#include <acl_command.h>\n#include <acl_context.h>\n#include <acl_event.h>\n#include <acl_globals.h>\n#include <acl_hal.h>\n#include <acl_hostch.h>\n#include <acl_kernel.h>\n#include <acl_mem.h>\n#include <acl_program.h>\n#include <acl_svm.h>\n#include <acl_thread.h>\n#include <acl_types.h>\n#include <acl_usm.h>\n#include <acl_util.h>\n\n#ifdef __GNUC__\n#pragma GCC visibility push(protected)\n#endif\n\n// Commands\n// ========\n\n//////////////////////////////\n// OpenCL API\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclEnqueueBarrierIntelFPGA(cl_command_queue command_queue) {\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  // For in order queue, since every event is executed in sequence,\n  // there is an implicit barrier after each event.\n  // enqueue barrier does not need to do anything\n  if (!(command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE)) {\n    return CL_SUCCESS;\n  }\n  // OpenCL 1.2 spec: If event_wait_list is NULL, then this particular command\n  // waits until all previous enqueued commands to command_queue have completed.\n  cl_int status = clEnqueueBarrierWithWaitList(command_queue, 0, 0, NULL);\n  return status;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclEnqueueBarrier(cl_command_queue command_queue) {\n  return clEnqueueBarrierIntelFPGA(command_queue);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclEnqueueMarkerIntelFPGA(cl_command_queue command_queue, cl_event *event) {\n  cl_int result;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  if (!event)\n    return CL_INVALID_VALUE;\n\n  result = acl_create_event(command_queue, 0, 0, CL_COMMAND_MARKER, event);\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueMarker(cl_command_queue command_queue,\n                                                cl_event *event) {\n  return clEnqueueMarkerIntelFPGA(command_queue, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWaitForEventsIntelFPGA(\n    cl_command_queue command_queue, cl_uint num_event, const cl_event *events) {\n  cl_int result;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n  if (num_event == 0 || events == 0) {\n    return CL_INVALID_VALUE;\n  }\n  cl_event event = NULL;\n  result = acl_create_event(command_queue, num_event, events,\n                            CL_COMMAND_WAIT_FOR_EVENTS_INTELFPGA, &event);\n  // release the user event\n  clReleaseEvent(event);\n\n  // Adjust for weird irregularity in the APIs...\n  if (result == CL_INVALID_EVENT_WAIT_LIST) {\n    result = CL_INVALID_EVENT;\n  }\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueWaitForEvents(\n    cl_command_queue command_queue, cl_uint num_event, const cl_event *events) {\n  return clEnqueueWaitForEventsIntelFPGA(command_queue, num_event, events);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL\nclWaitForEventsIntelFPGA(cl_uint num_events, const cl_event *event_list) {\n  int num_live;\n  size_t num_iters = 0;\n  cl_int result = CL_SUCCESS;\n  const acl_hal_t *hal = acl_get_hal();\n  cl_context context;\n  bool first_yield_to_hal = true;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (num_events == 0 || event_list == 0) {\n    return CL_INVALID_VALUE;\n  }\n\n#ifndef REMOVE_VALID_CHECKS\n  result = acl_check_events(num_events, event_list);\n  if (result != CL_SUCCESS) {\n    return CL_INVALID_EVENT;\n  }\n#endif\n\n  // Cache the context, in case the first event ends up being thrown away\n  context = event_list[0]->context;\n\n  do {\n    cl_uint i;\n    num_live = 0;\n\n    // Update data structures in response to async msgs.\n    acl_idle_update(context);\n\n    for (i = 0; i < num_events && num_live == 0; i++) {\n      if (!acl_event_is_done(event_list[i])) {\n        acl_dump_event(event_list[i]);\n        num_live++;\n      }\n    }\n\n    if (num_live > 0) {\n      // Wait until signaled, without burning CPU.\n      if (!hal->yield) {\n        acl_wait_for_device_update(context);\n      } else {\n        // If all events we are waiting for didn't complete after yielding\n        // to hal once, let other threads run too\n        if (!first_yield_to_hal) {\n          acl_yield_lock_and_thread();\n        }\n        first_yield_to_hal = false;\n        hal->yield(context->num_devices, context->device);\n      }\n    }\n\n    if (debug_mode > 0) {\n      num_iters++;\n    }\n\n  } while (num_live > 0);\n\n  if (debug_mode > 0) {\n    printf(\"waitforevents %zu iters\\n\", num_iters);\n    fflush(stdout);\n  }\n\n#ifdef ACL_120\n  {\n    // In OpenCL 1.1 and later we return a special status if any event has\n    // negative execution status (signalling an error). But for 1.0 there is no\n    // special result.\n    cl_uint i = 0;\n    for (i = 0; i < num_events; ++i) {\n      if (event_list[i]->execution_status < 0)\n        return CL_EXEC_STATUS_ERROR_FOR_EVENTS_IN_WAIT_LIST;\n    }\n  }\n#endif\n\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clWaitForEvents(cl_uint num_events,\n                                                const cl_event *event_list) {\n  return clWaitForEventsIntelFPGA(num_events, event_list);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueMarkerWithWaitListIntelFPGA(\n    cl_command_queue command_queue, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  cl_int result;\n  cl_event ret_event = NULL;\n\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  if (!acl_command_queue_is_valid(command_queue)) {\n    return CL_INVALID_COMMAND_QUEUE;\n  }\n\n  // Spec says:\n  // \"enqueues a marker command which waits for either a list of events to\n  // complete, or if the list is empty it waits for all commands previously\n  // enqueued in command_queue to complete before it completes\"\n  if (!event) {\n    event = &ret_event;\n  }\n  if (command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE &&\n      num_events_in_wait_list == 0 && !command_queue->commands.empty()) {\n    size_t num_events = command_queue->commands.size();\n    std::vector<cl_event> all_events;\n    all_events.reserve((size_t)num_events);\n    for (cl_event e : command_queue->commands) {\n      all_events.push_back(e);\n    }\n    result = acl_create_event(command_queue, (cl_uint)num_events,\n                              all_events.data(), CL_COMMAND_MARKER, event);\n  } else {\n    // with empty event list in-order queues will just work, because Marker\n    // event will be added to end of queue and only complete when everytjing\n    // infront of it has completed\n    result = acl_create_event(command_queue, num_events_in_wait_list,\n                              event_wait_list, CL_COMMAND_MARKER, event);\n  }\n\n  if (ret_event)\n    clReleaseEvent(ret_event); // free the ret event if the caller doesn't want\n                               // to return it\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueMarkerWithWaitList(\n    cl_command_queue command_queue, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueMarkerWithWaitListIntelFPGA(\n      command_queue, num_events_in_wait_list, event_wait_list, event);\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueBarrierWithWaitListIntelFPGA(\n    cl_command_queue command_queue, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  cl_int result;\n  cl_event local_event;\n  std::scoped_lock lock{acl_mutex_wrapper};\n\n  result = clEnqueueMarkerWithWaitList(command_queue, num_events_in_wait_list,\n                                       event_wait_list, &local_event);\n  if (result != CL_SUCCESS) {\n    return result;\n  }\n\n  if (command_queue->properties & CL_QUEUE_OUT_OF_ORDER_EXEC_MODE_ENABLE) {\n    command_queue->last_barrier = local_event;\n  }\n\n  if (event) {\n    *event = local_event;\n  } else {\n    clReleaseEvent(local_event);\n  }\n  return result;\n}\n\nACL_EXPORT\nCL_API_ENTRY cl_int CL_API_CALL clEnqueueBarrierWithWaitList(\n    cl_command_queue command_queue, cl_uint num_events_in_wait_list,\n    const cl_event *event_wait_list, cl_event *event) {\n  return clEnqueueBarrierWithWaitListIntelFPGA(\n      command_queue, num_events_in_wait_list, event_wait_list, event);\n}\n\n//////////////////////////////\n// Internals\n\n// Submit a command.\n// Return a number bigger than zero if we made forward progress, and\n// zero otherwise.\nint acl_submit_command(cl_event event) {\n  int result = 0;\n  acl_assert_locked();\n  acl_print_debug_msg(\"   submit event %p %d \\n\", event, event->id);\n\n  if (event->execution_status == CL_QUEUED) {\n\n    // Each action must set CL_SUBMITTED if it actually does something.\n    // Otherwise it can fail and back off without updating any state at\n    // all, in which case the execution status should remain at\n    // CL_QUEUED.\n\n    switch (event->cmd.type) {\n    // Treat USM migrate and memadvise like sync primitives\n    // until they do something.\n    case CL_COMMAND_MIGRATEMEM_INTEL:\n    case CL_COMMAND_MEMADVISE_INTEL:\n    // These are synchronization primitives only: just set execution status.\n    case CL_COMMAND_WAIT_FOR_EVENTS_INTELFPGA:\n    case CL_COMMAND_MARKER:\n      acl_set_execution_status(event, CL_SUBMITTED);\n      acl_set_execution_status(event, CL_RUNNING);\n      acl_set_execution_status(event, CL_COMPLETE);\n      result = 1;\n      break;\n\n    // Map and unmap have trivial cases that we treat differently:\n    // they don't entail enqueueing a device operation.\n    // But the non-trivial cases do enqueue a device operation.\n    case CL_COMMAND_MAP_BUFFER:\n      result = acl_mem_map_buffer(event);\n      break;\n    case CL_COMMAND_UNMAP_MEM_OBJECT:\n      result = acl_mem_unmap_mem_object(event);\n      break;\n\n    // Read, Write, Copy buffer map directly to a single memory\n    // transfer device operation.\n    case CL_COMMAND_READ_BUFFER:\n    case CL_COMMAND_WRITE_BUFFER:\n    case CL_COMMAND_COPY_BUFFER:\n      result = acl_submit_mem_transfer_device_op(event);\n      break;\n\n    case CL_COMMAND_SVM_MEMCPY:\n    case CL_COMMAND_SVM_MEMFILL:\n    case CL_COMMAND_SVM_MAP:\n    case CL_COMMAND_SVM_UNMAP:\n    case CL_COMMAND_SVM_FREE:\n      result = acl_svm_op(event);\n      break;\n\n    case CL_COMMAND_MEMCPY_INTEL:\n    case CL_COMMAND_MEMFILL_INTEL:\n      result = acl_submit_usm_memcpy(event);\n      break;\n\n    // Enqueue a kernel launch device operation.\n    case CL_COMMAND_TASK:\n    case CL_COMMAND_NDRANGE_KERNEL:\n      result = acl_submit_kernel_device_op(event);\n      break;\n\n    // Enqueue an eager programming of the device.\n    case CL_COMMAND_PROGRAM_DEVICE_INTELFPGA:\n      result = acl_submit_program_device_op(event);\n      break;\n\n    case CL_COMMAND_MIGRATE_MEM_OBJECTS:\n      result = acl_submit_migrate_mem_device_op(event);\n      break;\n\n    case CL_COMMAND_READ_HOST_PIPE_INTEL:\n      result = acl_submit_read_program_hostpipe_device_op(event);\n      break;\n\n    case CL_COMMAND_WRITE_HOST_PIPE_INTEL:\n      result = acl_submit_write_program_hostpipe_device_op(event);\n      break;\n\n    case CL_COMMAND_READ_GLOBAL_VARIABLE_INTEL:\n      result = acl_submit_read_device_global_device_op(event);\n      break;\n\n    case CL_COMMAND_WRITE_GLOBAL_VARIABLE_INTEL:\n      result = acl_submit_write_device_global_device_op(event);\n      break;\n\n    default:\n      acl_print_debug_msg(\"    acl_submit_command: unknown cmd type %d\\n\",\n                          event->cmd.type);\n      break;\n    }\n\n    if (result) {\n      event->is_on_device_op_queue = 1;\n    }\n  } else {\n    acl_set_execution_status(event, ACL_INVALID_EXECUTION_TRANSITION);\n    result = 1;\n  }\n  return result;\n}\n\n#ifdef __GNUC__\n#pragma GCC visibility pop\n#endif\n"
    }
]