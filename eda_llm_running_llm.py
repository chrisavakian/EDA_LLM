# -*- coding: utf-8 -*-
"""EDA_LLM_Running_LLM.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1UBzpZWV89lcPflVoYyU0C6-0bc0GVOFD
"""

#NOTE: Just need to run Google Colab with CPU, no GPU necessary since we're not running the model on Colab

"""
TODO: Want to help the model understand how exactly to handle the problem. So basically when given a problem, tell the model to break the problem
down into smaller peices, then prompt the model to fill in each peice. Rather than what the model is doing right now which is run head first
into the problem.
"""

!pip install gradio
!pip install openai
!pip install subprocess
!pip install re

!unzip data.zip

import gradio as gr
import os
import json
import subprocess
import re
import time
import torch
from transformers import LlamaForCausalLM, LlamaTokenizer
import networkx as nx
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "meta-llama/Llama-3.2-1B" #Using a small model right now just to keep everything quick.
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForCausalLM.from_pretrained(model_name)

all_graphs = {}
all_src_codes = {}

def get_graph(kernel_name):
    if kernel_name in all_graphs.keys():
        return all_graphs[kernel_name]
    g_path = os.path.join("data", "graphs", f"{kernel_name}_processed_result.gexf")
    g = nx.read_gexf(g_path)
    all_graphs[kernel_name] = g
    return g

def get_src_code(kernel_name):
    if kernel_name in all_src_codes.keys():
        return all_src_codes[kernel_name]
    g_path = os.path.join("data", "sources", f"{kernel_name}_kernel.c")
    with open(g_path, "r") as f:
        src = f.read()
    all_src_codes[kernel_name] = src
    return src

def retrieve_relevant_information(message):
    """
    Retrieve relevant information from the dataset based on the message.
    """
    source_files = [fname for fname in os.listdir(os.path.join("data", "sources")) if fname.endswith('.c')]

    filenames_prompt = f"Here are all the dataset files available: {', '.join(source_files)}.\n Here is the user's input for which they want code written for: {message}\n Please respond with only the MOST relevant dataset file name, please don't respond with anything else, just the filename, I REPEAT, RESPOND WITH ONLY THE FILENAME AND NOTHING ELSE. ONLY ONE FILENAME. Here is an example response: 2mm_kernel.c"

    # Ask LLM to choose the relevant filename
    selected_file = api_request(filenames_prompt, "").strip()
    print("SELECTED FILE RESPONSE: " + selected_file)

    # Retrieve the content from the selected file
    relevant_data = []

    try:
        source_path = os.path.join("data", "sources", selected_file)

        with open(source_path, 'r') as f:
            src_code = f.read()
            relevant_data.append(f"Source Code for Kernel ({selected_file}):\n{src_code}")

    except Exception as e:
        print(f"Error in processing selected file: {e}")

    return "\n".join(relevant_data)

def api_request(prompt_engineer, message):
    """
    This function takes in the user's input, along with a prompt engineered message that will tailor the LLM's response, and give it to the API
    which will give us the LLM's response.

    prompt_engineer = the prompt engineered messaged that will tailor the LLM's response
    message = The user's input into Gradio (the problem the LLM needs to code up a solution)
    """

    input_text = prompt_engineer + message
    inputs = tokenizer(input_text, return_tensors="pt")
    with torch.no_grad():
        outputs = model.generate(**inputs, max_length=512, num_return_sequences=1)
    response = tokenizer.decode(outputs[0], skip_special_tokens=True)
    return response

def extract_code_from_text(generated_text):
    """
    When a LLM generates its response, it might generate it with extra text around the code.
    This function aims to isolate the code.

    generated_text = code generated by the LLM that contains the code, but may contain more than just the code.
    """
    # Find text within ``` ``` marks
    code_blocks = re.findall(r'```c\n(.*?)```', generated_text, re.DOTALL)

    # Return the first code block found, or an empty string if none are found
    return code_blocks[0] if code_blocks else ""

def parse_test_cases(test_cases_text):
    """
    Parse the LLM's test case output into structured test cases.

    Expected format from LLM:
    TEST CASE #n
    INPUT:
    <input for stdin>
    EXPECTED OUTPUT:
    <expected output>
    END TEST CASE
    """
    test_cases = []
    current_case = {}

    # Split by test case blocks
    blocks = re.split(r'TEST CASE #\d+', test_cases_text)

    for block in blocks:
        if not block.strip():
            continue

        # Extract input and expected output
        input_match = re.search(r'INPUT:\n(.*?)(?=EXPECTED OUTPUT:|$)', block, re.DOTALL)
        output_match = re.search(r'EXPECTED OUTPUT:\n(.*?)(?=END TEST CASE|$)', block, re.DOTALL)

        if input_match and output_match:
            test_cases.append({
                'input': input_match.group(1).strip(),
                'expected_output': output_match.group(1).strip()
            })

    return test_cases

def compile_and_run_c_code(c_code, test_cases):
    """
    Compiles and runs the generated C code against test cases.
    """
    c_code = extract_code_from_text(c_code)

    # Write the C code to a temporary file
    with open("generated_code.c", "w") as file:
        file.write(c_code)

    # Compile the C code
    compile_process = subprocess.run(
        ["gcc", "generated_code.c", "-o", "generated_code"],
        capture_output=True,
        text=True
    )

    # Check if compilation succeeded
    if compile_process.returncode != 0:
        return {"error": compile_process.stderr}

    # Execute the compiled code with each test case
    results = []
    for test_case in test_cases:
        try:
            # Pass test case input to the program
            run_process = subprocess.run(
                ["./generated_code"],
                input=test_case['input'],
                capture_output=True,
                text=True,
                timeout=30
            )
            actual_output = run_process.stdout.strip()
            expected_output = test_case['expected_output']

            results.append({
                "input": test_case['input'],
                "expected_output": expected_output,
                "actual_output": actual_output,
                "passed": actual_output == expected_output,
                "error": run_process.stderr.strip() if run_process.stderr else None
            })

        except subprocess.TimeoutExpired:
            results.append({
                "input": test_case['input'],
                "error": "Execution timed out - possible infinite loop detected",
                "passed": False
            })

    # Clean up compiled files
    subprocess.run(["rm", "generated_code", "generated_code.c"])

    return results

def handle_test_cases(message, response):
    """
    This function will generate test cases, and run them against the code that the LLM generated.
    """
    # Step 2: Generate initial test cases
    test_case_prompt = """Based on the following message and code, generate unit test cases in this exact format:

TEST CASE #1
INPUT:
<input that will be sent to stdin>
EXPECTED OUTPUT:
<exact expected output>
END TEST CASE

TEST CASE #2
...

Generate at least 3 test cases, including edge cases. Each test case must have clear input and expected output.

Original message: {message}

Generated code:
{code}
""".format(message=message, code=response)

    print("Prompt for generating test cases:")
    print(test_case_prompt)

    test_cases_text = api_request(test_case_prompt, "")
    print("Generated test cases:")
    print(test_cases_text)

    parsed_test_cases = parse_test_cases(test_cases_text)

    for retry in range(3):
        print(f"Attempting to compile and run the C code with generated test cases... Attempt {retry + 1}")
        results = compile_and_run_c_code(response, parsed_test_cases)
        print("Compilation and execution results:")
        print(results)

        # Check if any tests failed
        failed_tests = [r for r in results if not r.get('passed', False)]
        if failed_tests:
            failure_details = "\n".join(
                f"Test failed:\nInput: {test['input']}\n"
                f"Expected: {test.get('expected_output', 'N/A')}\n"
                f"Got: {test.get('actual_output', 'N/A')}\n"
                f"Error: {test.get('error', 'N/A')}"
                for test in failed_tests
            )

            feedback_prompt = f"""The code failed some test cases:
{failure_details}

Original code:
{response}

Please fix these issues and provide a corrected version that passes all test cases."""

            response = api_request(feedback_prompt, "")
            continue
        else:
            print("All tests passed successfully!")
            return response

    print("Code could not be fixed within the retry limit.")
    return "Error: Code could not be fixed within the retry limit. Latest version:\n" + response

def respond(message, chat_history):
    """
    This function will run the main loop of the code, where it will call all the other functions in the correct order.
    First collecting the user's input and prompting the LLM for its first draft. Then calling the function for test cases, which
    will create the test cases and run them against the LLM, further refining the LLM's answer. And finally adding it to the chat history when a
    final response has been created.

    message = User's input into Gradio
    chat_history = The current history of the chat.
    """

    # Retrieve relevant information from the dataset
    print("Getting relevant info from dataset")
    relevant_information = retrieve_relevant_information(message)
    print("RELVANT INFO: " + relevant_information)

    initial_prompt = "You will need to generate C code that will correctly, accurately, with good \
        syntax and proper coding etiquette. The C code that you will generate will be converted to RTL code. \
        You will give only C code, and you will give \
        all C code required for anyone to run the code on their own computer. \
        Ensure there is a main function in the code you will generate. There MUST be a main \
        function in the code you provide. Your code must not have any infinite loops of any kind in any \
        function, including the main function. \
        Your code must be comprehensive in completing the problem given to you, it must cover every edge case, and \
        follow best practices. \
        Please note that your code will be ran against test cases, so please format your code properly to handle \
        dealing with test cases properly. Your code will be only one file.\
        Here is the problem you will be coding for:\n" + "\n" + message + ". One more thing, here is the relevant information found in the dataset: " + relevant_information

    print("Initial prompt for generating C code:")
    print(initial_prompt)

    print("Generating initial C code based on the message...")
    results = api_request(initial_prompt, message)
    print("Generated C code:")
    print(results)

    #Don't want to deal with test cases and running it yet.
    results = handle_test_cases(message, results)

    chat_history.append((message, results))

    return chat_history, chat_history

# Create a Gradio interface for the chatbot
with gr.Blocks() as demo:
    chatbot = gr.Chatbot()
    message = gr.Textbox(placeholder="Type your message here...")
    clear = gr.Button("Clear")

    # Functionality to send message and display the chat history
    message.submit(respond, [message, chatbot], [chatbot, chatbot])
    clear.click(lambda: None, None, chatbot, queue=False)

# Launch the Gradio app
demo.launch(debug=True)